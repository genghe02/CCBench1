[{"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( train_dir , model_save_path = None , n_neighbors = None , knn_algo = 'ball_tree' , verbose = False ) : X = [ ] y = [ ] # Loop through each person in the training set for class_dir in os . listdir ( train_dir ) : if not os . path . isdir ( os . path . join ( train_dir , class_dir ) ) : continue # Loop through each training image for the current person for img_path in image_files_in_folder ( os . path . join ( train_dir , class_dir ) ) : image = face_recognition . load_image_file ( img_path ) face_bounding_boxes = face_recognition . face_locations ( image ) if len ( face_bounding_boxes ) != 1 : # If there are no people (or too many people) in a training image, skip the image. if verbose : print ( \"Image {} not suitable for training: {}\" . format ( img_path , \"Didn't find a face\" if len ( face_bounding_boxes ) < 1 else \"Found more than one face\" ) ) else : # Add face encoding for current image to the training set X . append ( face_recognition . face_encodings ( image , known_face_locations = face_bounding_boxes ) [ 0 ] ) y . append ( class_dir ) # Determine how many neighbors to use for weighting in the KNN classifier if n_neighbors is None : n_neighbors = int ( round ( math . sqrt ( len ( X ) ) ) ) if verbose : print ( \"Chose n_neighbors automatically:\" , n_neighbors ) # Create and train the KNN classifier knn_clf = neighbors . KNeighborsClassifier ( n_neighbors = n_neighbors , algorithm = knn_algo , weights = 'distance' ) knn_clf . fit ( X , y ) # Save the trained KNN classifier if model_save_path is not None : with open ( model_save_path , 'wb' ) as f : pickle . dump ( knn_clf , f ) return knn_clf\n\n\n        Summary:", "input": "", "output": "Trains a k - nearest neighbors classifier for face recognition ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predict ( X_img_path , knn_clf = None , model_path = None , distance_threshold = 0.6 ) : if not os . path . isfile ( X_img_path ) or os . path . splitext ( X_img_path ) [ 1 ] [ 1 : ] not in ALLOWED_EXTENSIONS : raise Exception ( \"Invalid image path: {}\" . format ( X_img_path ) ) if knn_clf is None and model_path is None : raise Exception ( \"Must supply knn classifier either thourgh knn_clf or model_path\" ) # Load a trained KNN model (if one was passed in) if knn_clf is None : with open ( model_path , 'rb' ) as f : knn_clf = pickle . load ( f ) # Load image file and find face locations X_img = face_recognition . load_image_file ( X_img_path ) X_face_locations = face_recognition . face_locations ( X_img ) # If no faces are found in the image, return an empty result. if len ( X_face_locations ) == 0 : return [ ] # Find encodings for faces in the test iamge faces_encodings = face_recognition . face_encodings ( X_img , known_face_locations = X_face_locations ) # Use the KNN model to find the best matches for the test face closest_distances = knn_clf . kneighbors ( faces_encodings , n_neighbors = 1 ) are_matches = [ closest_distances [ 0 ] [ i ] [ 0 ] <= distance_threshold for i in range ( len ( X_face_locations ) ) ] # Predict classes and remove classifications that aren't within the threshold return [ ( pred , loc ) if rec else ( \"unknown\" , loc ) for pred , loc , rec in zip ( knn_clf . predict ( faces_encodings ) , X_face_locations , are_matches ) ]\n\n\n        Summary:", "input": "", "output": "Recognizes faces in given image using a trained KNN classifier"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def show_prediction_labels_on_image ( img_path , predictions ) : pil_image = Image . open ( img_path ) . convert ( \"RGB\" ) draw = ImageDraw . Draw ( pil_image ) for name , ( top , right , bottom , left ) in predictions : # Draw a box around the face using the Pillow module draw . rectangle ( ( ( left , top ) , ( right , bottom ) ) , outline = ( 0 , 0 , 255 ) ) # There's a bug in Pillow where it blows up with non-UTF-8 text # when using the default bitmap font name = name . encode ( \"UTF-8\" ) # Draw a label with a name below the face text_width , text_height = draw . textsize ( name ) draw . rectangle ( ( ( left , bottom - text_height - 10 ) , ( right , bottom ) ) , fill = ( 0 , 0 , 255 ) , outline = ( 0 , 0 , 255 ) ) draw . text ( ( left + 6 , bottom - text_height - 5 ) , name , fill = ( 255 , 255 , 255 , 255 ) ) # Remove the drawing library from memory as per the Pillow docs del draw # Display the resulting image pil_image . show ( )\n\n\n        Summary:", "input": "", "output": "Shows the face recognition results visually ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _rect_to_css ( rect ) : return rect . top ( ) , rect . right ( ) , rect . bottom ( ) , rect . left ( )\n\n\n        Summary:", "input": "", "output": "Convert a dlib rect object to a plain tuple in ( top right bottom left ) order"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _trim_css_to_bounds ( css , image_shape ) : return max ( css [ 0 ] , 0 ) , min ( css [ 1 ] , image_shape [ 1 ] ) , min ( css [ 2 ] , image_shape [ 0 ] ) , max ( css [ 3 ] , 0 )\n\n\n        Summary:", "input": "", "output": "Make sure a tuple in ( top right bottom left ) order is within the bounds of the image ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def face_distance ( face_encodings , face_to_compare ) : if len ( face_encodings ) == 0 : return np . empty ( ( 0 ) ) return np . linalg . norm ( face_encodings - face_to_compare , axis = 1 )\n\n\n        Summary:", "input": "", "output": "Given a list of face encodings compare them to a known face encoding and get a euclidean distance for each comparison face . The distance tells you how similar the faces are ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_image_file ( file , mode = 'RGB' ) : im = PIL . Image . open ( file ) if mode : im = im . convert ( mode ) return np . array ( im )\n\n\n        Summary:", "input": "", "output": "Loads an image file ( . jpg . png etc ) into a numpy array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _raw_face_locations ( img , number_of_times_to_upsample = 1 , model = \"hog\" ) : if model == \"cnn\" : return cnn_face_detector ( img , number_of_times_to_upsample ) else : return face_detector ( img , number_of_times_to_upsample )\n\n\n        Summary:", "input": "", "output": "Returns an array of bounding boxes of human faces in a image"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def face_locations ( img , number_of_times_to_upsample = 1 , model = \"hog\" ) : if model == \"cnn\" : return [ _trim_css_to_bounds ( _rect_to_css ( face . rect ) , img . shape ) for face in _raw_face_locations ( img , number_of_times_to_upsample , \"cnn\" ) ] else : return [ _trim_css_to_bounds ( _rect_to_css ( face ) , img . shape ) for face in _raw_face_locations ( img , number_of_times_to_upsample , model ) ]\n\n\n        Summary:", "input": "", "output": "Returns an array of bounding boxes of human faces in a image"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def batch_face_locations ( images , number_of_times_to_upsample = 1 , batch_size = 128 ) : def convert_cnn_detections_to_css ( detections ) : return [ _trim_css_to_bounds ( _rect_to_css ( face . rect ) , images [ 0 ] . shape ) for face in detections ] raw_detections_batched = _raw_face_locations_batched ( images , number_of_times_to_upsample , batch_size ) return list ( map ( convert_cnn_detections_to_css , raw_detections_batched ) )\n\n\n        Summary:", "input": "", "output": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector If you are using a GPU this can give you much faster results since the GPU can process batches of images at once . If you aren t using a GPU you don t need this function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def face_landmarks ( face_image , face_locations = None , model = \"large\" ) : landmarks = _raw_face_landmarks ( face_image , face_locations , model ) landmarks_as_tuples = [ [ ( p . x , p . y ) for p in landmark . parts ( ) ] for landmark in landmarks ] # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png if model == 'large' : return [ { \"chin\" : points [ 0 : 17 ] , \"left_eyebrow\" : points [ 17 : 22 ] , \"right_eyebrow\" : points [ 22 : 27 ] , \"nose_bridge\" : points [ 27 : 31 ] , \"nose_tip\" : points [ 31 : 36 ] , \"left_eye\" : points [ 36 : 42 ] , \"right_eye\" : points [ 42 : 48 ] , \"top_lip\" : points [ 48 : 55 ] + [ points [ 64 ] ] + [ points [ 63 ] ] + [ points [ 62 ] ] + [ points [ 61 ] ] + [ points [ 60 ] ] , \"bottom_lip\" : points [ 54 : 60 ] + [ points [ 48 ] ] + [ points [ 60 ] ] + [ points [ 67 ] ] + [ points [ 66 ] ] + [ points [ 65 ] ] + [ points [ 64 ] ] } for points in landmarks_as_tuples ] elif model == 'small' : return [ { \"nose_tip\" : [ points [ 4 ] ] , \"left_eye\" : points [ 2 : 4 ] , \"right_eye\" : points [ 0 : 2 ] , } for points in landmarks_as_tuples ] else : raise ValueError ( \"Invalid landmarks model type. Supported models are ['small', 'large'].\" )\n\n\n        Summary:", "input": "", "output": "Given an image returns a dict of face feature locations ( eyes nose etc ) for each face in the image"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def face_encodings ( face_image , known_face_locations = None , num_jitters = 1 ) : raw_landmarks = _raw_face_landmarks ( face_image , known_face_locations , model = \"small\" ) return [ np . array ( face_encoder . compute_face_descriptor ( face_image , raw_landmark_set , num_jitters ) ) for raw_landmark_set in raw_landmarks ]\n\n\n        Summary:", "input": "", "output": "Given an image return the 128 - dimension face encoding for each face in the image ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_datatype_string ( s ) : sc = SparkContext . _active_spark_context def from_ddl_schema ( type_str ) : return _parse_datatype_json_string ( sc . _jvm . org . apache . spark . sql . types . StructType . fromDDL ( type_str ) . json ( ) ) def from_ddl_datatype ( type_str ) : return _parse_datatype_json_string ( sc . _jvm . org . apache . spark . sql . api . python . PythonSQLUtils . parseDataType ( type_str ) . json ( ) ) try : # DDL format, \"fieldname datatype, fieldname datatype\". return from_ddl_schema ( s ) except Exception as e : try : # For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc. return from_ddl_datatype ( s ) except : try : # For backwards compatibility, \"fieldname: datatype, fieldname: datatype\" case. return from_ddl_datatype ( \"struct<%s>\" % s . strip ( ) ) except : raise e\n\n\n        Summary:", "input": "", "output": "Parses the given data type string to a : class : DataType . The data type string format equals to : class : DataType . simpleString except that top level struct type can omit the struct< > and atomic types use typeName () as their format e . g . use byte instead of tinyint for : class : ByteType . We can also use int as a short name for : class : IntegerType . Since Spark 2 . 3 this also supports a schema in a DDL - formatted string and case - insensitive strings ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _int_size_to_type ( size ) : if size <= 8 : return ByteType if size <= 16 : return ShortType if size <= 32 : return IntegerType if size <= 64 : return LongType\n\n\n        Summary:", "input": "", "output": "Return the Catalyst datatype from the size of integers ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _infer_type ( obj ) : if obj is None : return NullType ( ) if hasattr ( obj , '__UDT__' ) : return obj . __UDT__ dataType = _type_mappings . get ( type ( obj ) ) if dataType is DecimalType : # the precision and scale of `obj` may be different from row to row. return DecimalType ( 38 , 18 ) elif dataType is not None : return dataType ( ) if isinstance ( obj , dict ) : for key , value in obj . items ( ) : if key is not None and value is not None : return MapType ( _infer_type ( key ) , _infer_type ( value ) , True ) return MapType ( NullType ( ) , NullType ( ) , True ) elif isinstance ( obj , list ) : for v in obj : if v is not None : return ArrayType ( _infer_type ( obj [ 0 ] ) , True ) return ArrayType ( NullType ( ) , True ) elif isinstance ( obj , array ) : if obj . typecode in _array_type_mappings : return ArrayType ( _array_type_mappings [ obj . typecode ] ( ) , False ) else : raise TypeError ( \"not supported type: array(%s)\" % obj . typecode ) else : try : return _infer_schema ( obj ) except TypeError : raise TypeError ( \"not supported type: %s\" % type ( obj ) )\n\n\n        Summary:", "input": "", "output": "Infer the DataType from obj"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _infer_schema ( row , names = None ) : if isinstance ( row , dict ) : items = sorted ( row . items ( ) ) elif isinstance ( row , ( tuple , list ) ) : if hasattr ( row , \"__fields__\" ) : # Row items = zip ( row . __fields__ , tuple ( row ) ) elif hasattr ( row , \"_fields\" ) : # namedtuple items = zip ( row . _fields , tuple ( row ) ) else : if names is None : names = [ '_%d' % i for i in range ( 1 , len ( row ) + 1 ) ] elif len ( names ) < len ( row ) : names . extend ( '_%d' % i for i in range ( len ( names ) + 1 , len ( row ) + 1 ) ) items = zip ( names , row ) elif hasattr ( row , \"__dict__\" ) : # object items = sorted ( row . __dict__ . items ( ) ) else : raise TypeError ( \"Can not infer schema for type: %s\" % type ( row ) ) fields = [ StructField ( k , _infer_type ( v ) , True ) for k , v in items ] return StructType ( fields )\n\n\n        Summary:", "input": "", "output": "Infer the schema from dict / namedtuple / object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _has_nulltype ( dt ) : if isinstance ( dt , StructType ) : return any ( _has_nulltype ( f . dataType ) for f in dt . fields ) elif isinstance ( dt , ArrayType ) : return _has_nulltype ( ( dt . elementType ) ) elif isinstance ( dt , MapType ) : return _has_nulltype ( dt . keyType ) or _has_nulltype ( dt . valueType ) else : return isinstance ( dt , NullType )\n\n\n        Summary:", "input": "", "output": "Return whether there is NullType in dt or not"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_converter ( dataType ) : if not _need_converter ( dataType ) : return lambda x : x if isinstance ( dataType , ArrayType ) : conv = _create_converter ( dataType . elementType ) return lambda row : [ conv ( v ) for v in row ] elif isinstance ( dataType , MapType ) : kconv = _create_converter ( dataType . keyType ) vconv = _create_converter ( dataType . valueType ) return lambda row : dict ( ( kconv ( k ) , vconv ( v ) ) for k , v in row . items ( ) ) elif isinstance ( dataType , NullType ) : return lambda x : None elif not isinstance ( dataType , StructType ) : return lambda x : x # dataType must be StructType names = [ f . name for f in dataType . fields ] converters = [ _create_converter ( f . dataType ) for f in dataType . fields ] convert_fields = any ( _need_converter ( f . dataType ) for f in dataType . fields ) def convert_struct ( obj ) : if obj is None : return if isinstance ( obj , ( tuple , list ) ) : if convert_fields : return tuple ( conv ( v ) for v , conv in zip ( obj , converters ) ) else : return tuple ( obj ) if isinstance ( obj , dict ) : d = obj elif hasattr ( obj , \"__dict__\" ) : # object d = obj . __dict__ else : raise TypeError ( \"Unexpected obj type: %s\" % type ( obj ) ) if convert_fields : return tuple ( [ conv ( d . get ( name ) ) for name , conv in zip ( names , converters ) ] ) else : return tuple ( [ d . get ( name ) for name in names ] ) return convert_struct\n\n\n        Summary:", "input": "", "output": "Create a converter to drop the names of fields in obj"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_type_verifier ( dataType , nullable = True , name = None ) : if name is None : new_msg = lambda msg : msg new_name = lambda n : \"field %s\" % n else : new_msg = lambda msg : \"%s: %s\" % ( name , msg ) new_name = lambda n : \"field %s in %s\" % ( n , name ) def verify_nullability ( obj ) : if obj is None : if nullable : return True else : raise ValueError ( new_msg ( \"This field is not nullable, but got None\" ) ) else : return False _type = type ( dataType ) def assert_acceptable_types ( obj ) : assert _type in _acceptable_types , new_msg ( \"unknown datatype: %s for object %r\" % ( dataType , obj ) ) def verify_acceptable_types ( obj ) : # subclass of them can not be fromInternal in JVM if type ( obj ) not in _acceptable_types [ _type ] : raise TypeError ( new_msg ( \"%s can not accept object %r in type %s\" % ( dataType , obj , type ( obj ) ) ) ) if isinstance ( dataType , StringType ) : # StringType can work with any types verify_value = lambda _ : _ elif isinstance ( dataType , UserDefinedType ) : verifier = _make_type_verifier ( dataType . sqlType ( ) , name = name ) def verify_udf ( obj ) : if not ( hasattr ( obj , '__UDT__' ) and obj . __UDT__ == dataType ) : raise ValueError ( new_msg ( \"%r is not an instance of type %r\" % ( obj , dataType ) ) ) verifier ( dataType . toInternal ( obj ) ) verify_value = verify_udf elif isinstance ( dataType , ByteType ) : def verify_byte ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) if obj < - 128 or obj > 127 : raise ValueError ( new_msg ( \"object of ByteType out of range, got: %s\" % obj ) ) verify_value = verify_byte elif isinstance ( dataType , ShortType ) : def verify_short ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) if obj < - 32768 or obj > 32767 : raise ValueError ( new_msg ( \"object of ShortType out of range, got: %s\" % obj ) ) verify_value = verify_short elif isinstance ( dataType , IntegerType ) : def verify_integer ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) if obj < - 2147483648 or obj > 2147483647 : raise ValueError ( new_msg ( \"object of IntegerType out of range, got: %s\" % obj ) ) verify_value = verify_integer elif isinstance ( dataType , ArrayType ) : element_verifier = _make_type_verifier ( dataType . elementType , dataType . containsNull , name = \"element in array %s\" % name ) def verify_array ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) for i in obj : element_verifier ( i ) verify_value = verify_array elif isinstance ( dataType , MapType ) : key_verifier = _make_type_verifier ( dataType . keyType , False , name = \"key of map %s\" % name ) value_verifier = _make_type_verifier ( dataType . valueType , dataType . valueContainsNull , name = \"value of map %s\" % name ) def verify_map ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) for k , v in obj . items ( ) : key_verifier ( k ) value_verifier ( v ) verify_value = verify_map elif isinstance ( dataType , StructType ) : verifiers = [ ] for f in dataType . fields : verifier = _make_type_verifier ( f . dataType , f . nullable , name = new_name ( f . name ) ) verifiers . append ( ( f . name , verifier ) ) def verify_struct ( obj ) : assert_acceptable_types ( obj ) if isinstance ( obj , dict ) : for f , verifier in verifiers : verifier ( obj . get ( f ) ) elif isinstance ( obj , Row ) and getattr ( obj , \"__from_dict__\" , False ) : # the order in obj could be different than dataType.fields for f , verifier in verifiers : verifier ( obj [ f ] ) elif isinstance ( obj , ( tuple , list ) ) : if len ( obj ) != len ( verifiers ) : raise ValueError ( new_msg ( \"Length of object (%d) does not match with \" \"length of fields (%d)\" % ( len ( obj ) , len ( verifiers ) ) ) ) for v , ( _ , verifier ) in zip ( obj , verifiers ) : verifier ( v ) elif hasattr ( obj , \"__dict__\" ) : d = obj . __dict__ for f , verifier in verifiers : verifier ( d . get ( f ) ) else : raise TypeError ( new_msg ( \"StructType can not accept object %r in type %s\" % ( obj , type ( obj ) ) ) ) verify_value = verify_struct else : def verify_default ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) verify_value = verify_default def verify ( obj ) : if not verify_nullability ( obj ) : verify_value ( obj ) return verify\n\n\n        Summary:", "input": "", "output": "Make a verifier that checks the type of obj against dataType and raises a TypeError if they do not match ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_arrow_type ( dt ) : import pyarrow as pa if type ( dt ) == BooleanType : arrow_type = pa . bool_ ( ) elif type ( dt ) == ByteType : arrow_type = pa . int8 ( ) elif type ( dt ) == ShortType : arrow_type = pa . int16 ( ) elif type ( dt ) == IntegerType : arrow_type = pa . int32 ( ) elif type ( dt ) == LongType : arrow_type = pa . int64 ( ) elif type ( dt ) == FloatType : arrow_type = pa . float32 ( ) elif type ( dt ) == DoubleType : arrow_type = pa . float64 ( ) elif type ( dt ) == DecimalType : arrow_type = pa . decimal128 ( dt . precision , dt . scale ) elif type ( dt ) == StringType : arrow_type = pa . string ( ) elif type ( dt ) == BinaryType : arrow_type = pa . binary ( ) elif type ( dt ) == DateType : arrow_type = pa . date32 ( ) elif type ( dt ) == TimestampType : # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read arrow_type = pa . timestamp ( 'us' , tz = 'UTC' ) elif type ( dt ) == ArrayType : if type ( dt . elementType ) in [ StructType , TimestampType ] : raise TypeError ( \"Unsupported type in conversion to Arrow: \" + str ( dt ) ) arrow_type = pa . list_ ( to_arrow_type ( dt . elementType ) ) elif type ( dt ) == StructType : if any ( type ( field . dataType ) == StructType for field in dt ) : raise TypeError ( \"Nested StructType not supported in conversion to Arrow\" ) fields = [ pa . field ( field . name , to_arrow_type ( field . dataType ) , nullable = field . nullable ) for field in dt ] arrow_type = pa . struct ( fields ) else : raise TypeError ( \"Unsupported type in conversion to Arrow: \" + str ( dt ) ) return arrow_type\n\n\n        Summary:", "input": "", "output": "Convert Spark data type to pyarrow type"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_arrow_schema ( schema ) : import pyarrow as pa fields = [ pa . field ( field . name , to_arrow_type ( field . dataType ) , nullable = field . nullable ) for field in schema ] return pa . schema ( fields )\n\n\n        Summary:", "input": "", "output": "Convert a schema from Spark to Arrow"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_arrow_type ( at ) : import pyarrow . types as types if types . is_boolean ( at ) : spark_type = BooleanType ( ) elif types . is_int8 ( at ) : spark_type = ByteType ( ) elif types . is_int16 ( at ) : spark_type = ShortType ( ) elif types . is_int32 ( at ) : spark_type = IntegerType ( ) elif types . is_int64 ( at ) : spark_type = LongType ( ) elif types . is_float32 ( at ) : spark_type = FloatType ( ) elif types . is_float64 ( at ) : spark_type = DoubleType ( ) elif types . is_decimal ( at ) : spark_type = DecimalType ( precision = at . precision , scale = at . scale ) elif types . is_string ( at ) : spark_type = StringType ( ) elif types . is_binary ( at ) : spark_type = BinaryType ( ) elif types . is_date32 ( at ) : spark_type = DateType ( ) elif types . is_timestamp ( at ) : spark_type = TimestampType ( ) elif types . is_list ( at ) : if types . is_timestamp ( at . value_type ) : raise TypeError ( \"Unsupported type in conversion from Arrow: \" + str ( at ) ) spark_type = ArrayType ( from_arrow_type ( at . value_type ) ) elif types . is_struct ( at ) : if any ( types . is_struct ( field . type ) for field in at ) : raise TypeError ( \"Nested StructType not supported in conversion from Arrow: \" + str ( at ) ) return StructType ( [ StructField ( field . name , from_arrow_type ( field . type ) , nullable = field . nullable ) for field in at ] ) else : raise TypeError ( \"Unsupported type in conversion from Arrow: \" + str ( at ) ) return spark_type\n\n\n        Summary:", "input": "", "output": "Convert pyarrow type to Spark data type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_arrow_schema ( arrow_schema ) : return StructType ( [ StructField ( field . name , from_arrow_type ( field . type ) , nullable = field . nullable ) for field in arrow_schema ] )\n\n\n        Summary:", "input": "", "output": "Convert schema from Arrow to Spark ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_series_localize_timestamps ( s , timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) from pandas . api . types import is_datetime64tz_dtype tz = timezone or _get_local_timezone ( ) # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if is_datetime64tz_dtype ( s . dtype ) : return s . dt . tz_convert ( tz ) . dt . tz_localize ( None ) else : return s\n\n\n        Summary:", "input": "", "output": "Convert timezone aware timestamps to timezone - naive in the specified timezone or local timezone ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_dataframe_localize_timestamps ( pdf , timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) for column , series in pdf . iteritems ( ) : pdf [ column ] = _check_series_localize_timestamps ( series , timezone ) return pdf\n\n\n        Summary:", "input": "", "output": "Convert timezone aware timestamps to timezone - naive in the specified timezone or local timezone"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_series_convert_timestamps_internal ( s , timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) from pandas . api . types import is_datetime64_dtype , is_datetime64tz_dtype # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if is_datetime64_dtype ( s . dtype ) : # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive # timestamp is during the hour when the clock is adjusted backward during due to # daylight saving time (dst). # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500). # # Here we explicit choose to use standard time. This matches the default behavior of # pytz. # # Here are some code to help understand this behavior: # >>> import datetime # >>> import pandas as pd # >>> import pytz # >>> # >>> t = datetime.datetime(2015, 11, 1, 1, 30) # >>> ts = pd.Series([t]) # >>> tz = pytz.timezone('America/New_York') # >>> # >>> ts.dt.tz_localize(tz, ambiguous=True) # 0 2015-11-01 01:30:00-04:00 # dtype: datetime64[ns, America/New_York] # >>> # >>> ts.dt.tz_localize(tz, ambiguous=False) # 0 2015-11-01 01:30:00-05:00 # dtype: datetime64[ns, America/New_York] # >>> # >>> str(tz.localize(t)) # '2015-11-01 01:30:00-05:00' tz = timezone or _get_local_timezone ( ) return s . dt . tz_localize ( tz , ambiguous = False ) . dt . tz_convert ( 'UTC' ) elif is_datetime64tz_dtype ( s . dtype ) : return s . dt . tz_convert ( 'UTC' ) else : return s\n\n\n        Summary:", "input": "", "output": "Convert a tz - naive timestamp in the specified timezone or local timezone to UTC normalized for Spark internal storage"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_series_convert_timestamps_localize ( s , from_timezone , to_timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) import pandas as pd from pandas . api . types import is_datetime64tz_dtype , is_datetime64_dtype from_tz = from_timezone or _get_local_timezone ( ) to_tz = to_timezone or _get_local_timezone ( ) # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if is_datetime64tz_dtype ( s . dtype ) : return s . dt . tz_convert ( to_tz ) . dt . tz_localize ( None ) elif is_datetime64_dtype ( s . dtype ) and from_tz != to_tz : # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT. return s . apply ( lambda ts : ts . tz_localize ( from_tz , ambiguous = False ) . tz_convert ( to_tz ) . tz_localize ( None ) if ts is not pd . NaT else pd . NaT ) else : return s\n\n\n        Summary:", "input": "", "output": "Convert timestamp to timezone - naive in the specified timezone or local timezone"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add ( self , field , data_type = None , nullable = True , metadata = None ) : if isinstance ( field , StructField ) : self . fields . append ( field ) self . names . append ( field . name ) else : if isinstance ( field , str ) and data_type is None : raise ValueError ( \"Must specify DataType if passing name of struct_field to create.\" ) if isinstance ( data_type , str ) : data_type_f = _parse_datatype_json_value ( data_type ) else : data_type_f = data_type self . fields . append ( StructField ( field , data_type_f , nullable , metadata ) ) self . names . append ( field ) # Precalculated list of fields that need conversion with fromInternal/toInternal functions self . _needConversion = [ f . needConversion ( ) for f in self ] self . _needSerializeAnyField = any ( self . _needConversion ) return self\n\n\n        Summary:", "input": "", "output": "Construct a StructType by adding new elements to it to define the schema . The method accepts either :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cachedSqlType ( cls ) : if not hasattr ( cls , \"_cached_sql_type\" ) : cls . _cached_sql_type = cls . sqlType ( ) return cls . _cached_sql_type\n\n\n        Summary:", "input": "", "output": "Cache the sqlType () into class because it s heavy used in toInternal ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asDict ( self , recursive = False ) : if not hasattr ( self , \"__fields__\" ) : raise TypeError ( \"Cannot convert a Row class into dict\" ) if recursive : def conv ( obj ) : if isinstance ( obj , Row ) : return obj . asDict ( True ) elif isinstance ( obj , list ) : return [ conv ( o ) for o in obj ] elif isinstance ( obj , dict ) : return dict ( ( k , conv ( v ) ) for k , v in obj . items ( ) ) else : return obj return dict ( zip ( self . __fields__ , ( conv ( o ) for o in self ) ) ) else : return dict ( zip ( self . __fields__ , self ) )\n\n\n        Summary:", "input": "", "output": "Return as an dict"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self ) : if self . hasSummary : return LinearRegressionTrainingSummary ( super ( LinearRegressionModel , self ) . summary ) else : raise RuntimeError ( \"No training summary available for this %s\" % self . __class__ . __name__ )\n\n\n        Summary:", "input": "", "output": "Gets summary ( e . g . residuals mse r - squared ) of model on training set . An exception is thrown if trainingSummary is None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def evaluate ( self , dataset ) : if not isinstance ( dataset , DataFrame ) : raise ValueError ( \"dataset must be a DataFrame but got %s.\" % type ( dataset ) ) java_lr_summary = self . _call_java ( \"evaluate\" , dataset ) return LinearRegressionSummary ( java_lr_summary )\n\n\n        Summary:", "input": "", "output": "Evaluates the model on a test dataset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self ) : if self . hasSummary : return GeneralizedLinearRegressionTrainingSummary ( super ( GeneralizedLinearRegressionModel , self ) . summary ) else : raise RuntimeError ( \"No training summary available for this %s\" % self . __class__ . __name__ )\n\n\n        Summary:", "input": "", "output": "Gets summary ( e . g . residuals deviance pValues ) of model on training set . An exception is thrown if trainingSummary is None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def evaluate ( self , dataset ) : if not isinstance ( dataset , DataFrame ) : raise ValueError ( \"dataset must be a DataFrame but got %s.\" % type ( dataset ) ) java_glr_summary = self . _call_java ( \"evaluate\" , dataset ) return GeneralizedLinearRegressionSummary ( java_glr_summary )\n\n\n        Summary:", "input": "", "output": "Evaluates the model on a test dataset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_local_dirs ( sub ) : path = os . environ . get ( \"SPARK_LOCAL_DIRS\" , \"/tmp\" ) dirs = path . split ( \",\" ) if len ( dirs ) > 1 : # different order in different processes and instances rnd = random . Random ( os . getpid ( ) + id ( dirs ) ) random . shuffle ( dirs , rnd . random ) return [ os . path . join ( d , \"python\" , str ( os . getpid ( ) ) , sub ) for d in dirs ]\n\n\n        Summary:", "input": "", "output": "Get all the directories"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_spill_dir ( self , n ) : return os . path . join ( self . localdirs [ n % len ( self . localdirs ) ] , str ( n ) )\n\n\n        Summary:", "input": "", "output": "Choose one directory for spill by number n"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mergeValues ( self , iterator ) : # speedup attribute lookup creator , comb = self . agg . createCombiner , self . agg . mergeValue c , data , pdata , hfun , batch = 0 , self . data , self . pdata , self . _partition , self . batch limit = self . memory_limit for k , v in iterator : d = pdata [ hfun ( k ) ] if pdata else data d [ k ] = comb ( d [ k ] , v ) if k in d else creator ( v ) c += 1 if c >= batch : if get_used_memory ( ) >= limit : self . _spill ( ) limit = self . _next_limit ( ) batch /= 2 c = 0 else : batch *= 1.5 if get_used_memory ( ) >= limit : self . _spill ( )\n\n\n        Summary:", "input": "", "output": "Combine the items by creator and combiner"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mergeCombiners ( self , iterator , limit = None ) : if limit is None : limit = self . memory_limit # speedup attribute lookup comb , hfun , objsize = self . agg . mergeCombiners , self . _partition , self . _object_size c , data , pdata , batch = 0 , self . data , self . pdata , self . batch for k , v in iterator : d = pdata [ hfun ( k ) ] if pdata else data d [ k ] = comb ( d [ k ] , v ) if k in d else v if not limit : continue c += objsize ( v ) if c > batch : if get_used_memory ( ) > limit : self . _spill ( ) limit = self . _next_limit ( ) batch /= 2 c = 0 else : batch *= 1.5 if limit and get_used_memory ( ) >= limit : self . _spill ( )\n\n\n        Summary:", "input": "", "output": "Merge ( K V ) pair by mergeCombiner"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _spill ( self ) : global MemoryBytesSpilled , DiskBytesSpilled path = self . _get_spill_dir ( self . spills ) if not os . path . exists ( path ) : os . makedirs ( path ) used_memory = get_used_memory ( ) if not self . pdata : # The data has not been partitioned, it will iterator the # dataset once, write them into different files, has no # additional memory. It only called when the memory goes # above limit at the first time. # open all the files for writing streams = [ open ( os . path . join ( path , str ( i ) ) , 'wb' ) for i in range ( self . partitions ) ] for k , v in self . data . items ( ) : h = self . _partition ( k ) # put one item in batch, make it compatible with load_stream # it will increase the memory if dump them in batch self . serializer . dump_stream ( [ ( k , v ) ] , streams [ h ] ) for s in streams : DiskBytesSpilled += s . tell ( ) s . close ( ) self . data . clear ( ) self . pdata . extend ( [ { } for i in range ( self . partitions ) ] ) else : for i in range ( self . partitions ) : p = os . path . join ( path , str ( i ) ) with open ( p , \"wb\" ) as f : # dump items in batch self . serializer . dump_stream ( iter ( self . pdata [ i ] . items ( ) ) , f ) self . pdata [ i ] . clear ( ) DiskBytesSpilled += os . path . getsize ( p ) self . spills += 1 gc . collect ( ) # release the memory as much as possible MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20\n\n\n        Summary:", "input": "", "output": "dump already partitioned data into disks ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def items ( self ) : if not self . pdata and not self . spills : return iter ( self . data . items ( ) ) return self . _external_items ( )\n\n\n        Summary:", "input": "", "output": "Return all merged items as iterator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _external_items ( self ) : assert not self . data if any ( self . pdata ) : self . _spill ( ) # disable partitioning and spilling when merge combiners from disk self . pdata = [ ] try : for i in range ( self . partitions ) : for v in self . _merged_items ( i ) : yield v self . data . clear ( ) # remove the merged partition for j in range ( self . spills ) : path = self . _get_spill_dir ( j ) os . remove ( os . path . join ( path , str ( i ) ) ) finally : self . _cleanup ( )\n\n\n        Summary:", "input": "", "output": "Return all partitioned items as iterator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _recursive_merged_items ( self , index ) : subdirs = [ os . path . join ( d , \"parts\" , str ( index ) ) for d in self . localdirs ] m = ExternalMerger ( self . agg , self . memory_limit , self . serializer , subdirs , self . scale * self . partitions , self . partitions , self . batch ) m . pdata = [ { } for _ in range ( self . partitions ) ] limit = self . _next_limit ( ) for j in range ( self . spills ) : path = self . _get_spill_dir ( j ) p = os . path . join ( path , str ( index ) ) with open ( p , 'rb' ) as f : m . mergeCombiners ( self . serializer . load_stream ( f ) , 0 ) if get_used_memory ( ) > limit : m . _spill ( ) limit = self . _next_limit ( ) return m . _external_items ( )\n\n\n        Summary:", "input": "", "output": "merge the partitioned items and return the as iterator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_path ( self , n ) : d = self . local_dirs [ n % len ( self . local_dirs ) ] if not os . path . exists ( d ) : os . makedirs ( d ) return os . path . join ( d , str ( n ) )\n\n\n        Summary:", "input": "", "output": "Choose one directory for spill by number n"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sorted ( self , iterator , key = None , reverse = False ) : global MemoryBytesSpilled , DiskBytesSpilled batch , limit = 100 , self . _next_limit ( ) chunks , current_chunk = [ ] , [ ] iterator = iter ( iterator ) while True : # pick elements in batch chunk = list ( itertools . islice ( iterator , batch ) ) current_chunk . extend ( chunk ) if len ( chunk ) < batch : break used_memory = get_used_memory ( ) if used_memory > limit : # sort them inplace will save memory current_chunk . sort ( key = key , reverse = reverse ) path = self . _get_path ( len ( chunks ) ) with open ( path , 'wb' ) as f : self . serializer . dump_stream ( current_chunk , f ) def load ( f ) : for v in self . serializer . load_stream ( f ) : yield v # close the file explicit once we consume all the items # to avoid ResourceWarning in Python3 f . close ( ) chunks . append ( load ( open ( path , 'rb' ) ) ) current_chunk = [ ] MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20 DiskBytesSpilled += os . path . getsize ( path ) os . unlink ( path ) # data will be deleted after close elif not chunks : batch = min ( int ( batch * 1.5 ) , 10000 ) current_chunk . sort ( key = key , reverse = reverse ) if not chunks : return current_chunk if current_chunk : chunks . append ( iter ( current_chunk ) ) return heapq . merge ( chunks , key = key , reverse = reverse )\n\n\n        Summary:", "input": "", "output": "Sort the elements in iterator do external sort when the memory goes above the limit ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _spill ( self ) : global MemoryBytesSpilled , DiskBytesSpilled if self . _file is None : self . _open_file ( ) used_memory = get_used_memory ( ) pos = self . _file . tell ( ) self . _ser . dump_stream ( self . values , self . _file ) self . values = [ ] gc . collect ( ) DiskBytesSpilled += self . _file . tell ( ) - pos MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20\n\n\n        Summary:", "input": "", "output": "dump the values into disk"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _spill ( self ) : global MemoryBytesSpilled , DiskBytesSpilled path = self . _get_spill_dir ( self . spills ) if not os . path . exists ( path ) : os . makedirs ( path ) used_memory = get_used_memory ( ) if not self . pdata : # The data has not been partitioned, it will iterator the # data once, write them into different files, has no # additional memory. It only called when the memory goes # above limit at the first time. # open all the files for writing streams = [ open ( os . path . join ( path , str ( i ) ) , 'wb' ) for i in range ( self . partitions ) ] # If the number of keys is small, then the overhead of sort is small # sort them before dumping into disks self . _sorted = len ( self . data ) < self . SORT_KEY_LIMIT if self . _sorted : self . serializer = self . flattened_serializer ( ) for k in sorted ( self . data . keys ( ) ) : h = self . _partition ( k ) self . serializer . dump_stream ( [ ( k , self . data [ k ] ) ] , streams [ h ] ) else : for k , v in self . data . items ( ) : h = self . _partition ( k ) self . serializer . dump_stream ( [ ( k , v ) ] , streams [ h ] ) for s in streams : DiskBytesSpilled += s . tell ( ) s . close ( ) self . data . clear ( ) # self.pdata is cached in `mergeValues` and `mergeCombiners` self . pdata . extend ( [ { } for i in range ( self . partitions ) ] ) else : for i in range ( self . partitions ) : p = os . path . join ( path , str ( i ) ) with open ( p , \"wb\" ) as f : # dump items in batch if self . _sorted : # sort by key only (stable) sorted_items = sorted ( self . pdata [ i ] . items ( ) , key = operator . itemgetter ( 0 ) ) self . serializer . dump_stream ( sorted_items , f ) else : self . serializer . dump_stream ( self . pdata [ i ] . items ( ) , f ) self . pdata [ i ] . clear ( ) DiskBytesSpilled += os . path . getsize ( p ) self . spills += 1 gc . collect ( ) # release the memory as much as possible MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20\n\n\n        Summary:", "input": "", "output": "dump already partitioned data into disks ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _merge_sorted_items ( self , index ) : def load_partition ( j ) : path = self . _get_spill_dir ( j ) p = os . path . join ( path , str ( index ) ) with open ( p , 'rb' , 65536 ) as f : for v in self . serializer . load_stream ( f ) : yield v disk_items = [ load_partition ( j ) for j in range ( self . spills ) ] if self . _sorted : # all the partitions are already sorted sorted_items = heapq . merge ( disk_items , key = operator . itemgetter ( 0 ) ) else : # Flatten the combined values, so it will not consume huge # memory during merging sort. ser = self . flattened_serializer ( ) sorter = ExternalSorter ( self . memory_limit , ser ) sorted_items = sorter . sorted ( itertools . chain ( * disk_items ) , key = operator . itemgetter ( 0 ) ) return ( ( k , vs ) for k , vs in GroupByKey ( sorted_items ) )\n\n\n        Summary:", "input": "", "output": "load a partition from disk then sort and group by key"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def worker ( sock , authenticated ) : signal . signal ( SIGHUP , SIG_DFL ) signal . signal ( SIGCHLD , SIG_DFL ) signal . signal ( SIGTERM , SIG_DFL ) # restore the handler for SIGINT, # it's useful for debugging (show the stacktrace before exit) signal . signal ( SIGINT , signal . default_int_handler ) # Read the socket using fdopen instead of socket.makefile() because the latter # seems to be very slow; note that we need to dup() the file descriptor because # otherwise writes also cause a seek that makes us miss data on the read side. infile = os . fdopen ( os . dup ( sock . fileno ( ) ) , \"rb\" , 65536 ) outfile = os . fdopen ( os . dup ( sock . fileno ( ) ) , \"wb\" , 65536 ) if not authenticated : client_secret = UTF8Deserializer ( ) . loads ( infile ) if os . environ [ \"PYTHON_WORKER_FACTORY_SECRET\" ] == client_secret : write_with_length ( \"ok\" . encode ( \"utf-8\" ) , outfile ) outfile . flush ( ) else : write_with_length ( \"err\" . encode ( \"utf-8\" ) , outfile ) outfile . flush ( ) sock . close ( ) return 1 exit_code = 0 try : worker_main ( infile , outfile ) except SystemExit as exc : exit_code = compute_real_exit_code ( exc . code ) finally : try : outfile . flush ( ) except Exception : pass return exit_code\n\n\n        Summary:", "input": "", "output": "Called by a worker process after the fork () ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def portable_hash ( x ) : if sys . version_info >= ( 3 , 2 , 3 ) and 'PYTHONHASHSEED' not in os . environ : raise Exception ( \"Randomness of hash of string should be disabled via PYTHONHASHSEED\" ) if x is None : return 0 if isinstance ( x , tuple ) : h = 0x345678 for i in x : h ^= portable_hash ( i ) h *= 1000003 h &= sys . maxsize h ^= len ( x ) if h == - 1 : h = - 2 return int ( h ) return hash ( x )\n\n\n        Summary:", "input": "", "output": "This function returns consistent hash code for builtin types especially for None and tuple with None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_memory ( s ) : units = { 'g' : 1024 , 'm' : 1 , 't' : 1 << 20 , 'k' : 1.0 / 1024 } if s [ - 1 ] . lower ( ) not in units : raise ValueError ( \"invalid format: \" + s ) return int ( float ( s [ : - 1 ] ) * units [ s [ - 1 ] . lower ( ) ] )\n\n\n        Summary:", "input": "", "output": "Parse a memory string in the format supported by Java ( e . g . 1g 200m ) and return the value in MiB"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ignore_unicode_prefix ( f ) : if sys . version >= '3' : # the representation of unicode string in Python 3 does not have prefix 'u', # so remove the prefix 'u' for doc tests literal_re = re . compile ( r\"(\\W|^)[uU](['])\" , re . UNICODE ) f . __doc__ = literal_re . sub ( r'\\1\\2' , f . __doc__ ) return f\n\n\n        Summary:", "input": "", "output": "Ignore the u prefix of string in doc tests to make it works in both python 2 and 3"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cache ( self ) : self . is_cached = True self . persist ( StorageLevel . MEMORY_ONLY ) return self\n\n\n        Summary:", "input": "", "output": "Persist this RDD with the default storage level ( C { MEMORY_ONLY } ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def persist ( self , storageLevel = StorageLevel . MEMORY_ONLY ) : self . is_cached = True javaStorageLevel = self . ctx . _getJavaStorageLevel ( storageLevel ) self . _jrdd . persist ( javaStorageLevel ) return self\n\n\n        Summary:", "input": "", "output": "Set this RDD s storage level to persist its values across operations after the first time it is computed . This can only be used to assign a new storage level if the RDD does not have a storage level set yet . If no storage level is specified defaults to ( C { MEMORY_ONLY } ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unpersist ( self , blocking = False ) : self . is_cached = False self . _jrdd . unpersist ( blocking ) return self\n\n\n        Summary:", "input": "", "output": "Mark the RDD as non - persistent and remove all blocks for it from memory and disk ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getCheckpointFile ( self ) : checkpointFile = self . _jrdd . rdd ( ) . getCheckpointFile ( ) if checkpointFile . isDefined ( ) : return checkpointFile . get ( )\n\n\n        Summary:", "input": "", "output": "Gets the name of the file to which this RDD was checkpointed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map ( self , f , preservesPartitioning = False ) : def func ( _ , iterator ) : return map ( fail_on_stopiteration ( f ) , iterator ) return self . mapPartitionsWithIndex ( func , preservesPartitioning )\n\n\n        Summary:", "input": "", "output": "Return a new RDD by applying a function to each element of this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def flatMap ( self , f , preservesPartitioning = False ) : def func ( s , iterator ) : return chain . from_iterable ( map ( fail_on_stopiteration ( f ) , iterator ) ) return self . mapPartitionsWithIndex ( func , preservesPartitioning )\n\n\n        Summary:", "input": "", "output": "Return a new RDD by first applying a function to all elements of this RDD and then flattening the results ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mapPartitions ( self , f , preservesPartitioning = False ) : def func ( s , iterator ) : return f ( iterator ) return self . mapPartitionsWithIndex ( func , preservesPartitioning )\n\n\n        Summary:", "input": "", "output": "Return a new RDD by applying a function to each partition of this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mapPartitionsWithSplit ( self , f , preservesPartitioning = False ) : warnings . warn ( \"mapPartitionsWithSplit is deprecated; \" \"use mapPartitionsWithIndex instead\" , DeprecationWarning , stacklevel = 2 ) return self . mapPartitionsWithIndex ( f , preservesPartitioning )\n\n\n        Summary:", "input": "", "output": "Deprecated : use mapPartitionsWithIndex instead ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def distinct ( self , numPartitions = None ) : return self . map ( lambda x : ( x , None ) ) . reduceByKey ( lambda x , _ : x , numPartitions ) . map ( lambda x : x [ 0 ] )\n\n\n        Summary:", "input": "", "output": "Return a new RDD containing the distinct elements in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sample ( self , withReplacement , fraction , seed = None ) : assert fraction >= 0.0 , \"Negative fraction value: %s\" % fraction return self . mapPartitionsWithIndex ( RDDSampler ( withReplacement , fraction , seed ) . func , True )\n\n\n        Summary:", "input": "", "output": "Return a sampled subset of this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def randomSplit ( self , weights , seed = None ) : s = float ( sum ( weights ) ) cweights = [ 0.0 ] for w in weights : cweights . append ( cweights [ - 1 ] + w / s ) if seed is None : seed = random . randint ( 0 , 2 ** 32 - 1 ) return [ self . mapPartitionsWithIndex ( RDDRangeSampler ( lb , ub , seed ) . func , True ) for lb , ub in zip ( cweights , cweights [ 1 : ] ) ]\n\n\n        Summary:", "input": "", "output": "Randomly splits this RDD with the provided weights ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def takeSample ( self , withReplacement , num , seed = None ) : numStDev = 10.0 if num < 0 : raise ValueError ( \"Sample size cannot be negative.\" ) elif num == 0 : return [ ] initialCount = self . count ( ) if initialCount == 0 : return [ ] rand = random . Random ( seed ) if ( not withReplacement ) and num >= initialCount : # shuffle current RDD and return samples = self . collect ( ) rand . shuffle ( samples ) return samples maxSampleSize = sys . maxsize - int ( numStDev * sqrt ( sys . maxsize ) ) if num > maxSampleSize : raise ValueError ( \"Sample size cannot be greater than %d.\" % maxSampleSize ) fraction = RDD . _computeFractionForSampleSize ( num , initialCount , withReplacement ) samples = self . sample ( withReplacement , fraction , seed ) . collect ( ) # If the first sample didn't turn out large enough, keep trying to take samples; # this shouldn't happen often because we use a big multiplier for their initial size. # See: scala/spark/RDD.scala while len ( samples ) < num : # TODO: add log warning for when more than one iteration was run seed = rand . randint ( 0 , sys . maxsize ) samples = self . sample ( withReplacement , fraction , seed ) . collect ( ) rand . shuffle ( samples ) return samples [ 0 : num ]\n\n\n        Summary:", "input": "", "output": "Return a fixed - size sampled subset of this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _computeFractionForSampleSize ( sampleSizeLowerBound , total , withReplacement ) : fraction = float ( sampleSizeLowerBound ) / total if withReplacement : numStDev = 5 if ( sampleSizeLowerBound < 12 ) : numStDev = 9 return fraction + numStDev * sqrt ( fraction / total ) else : delta = 0.00005 gamma = - log ( delta ) / total return min ( 1 , fraction + gamma + sqrt ( gamma * gamma + 2 * gamma * fraction ) )\n\n\n        Summary:", "input": "", "output": "Returns a sampling rate that guarantees a sample of size > = sampleSizeLowerBound 99 . 99% of the time ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , other ) : if self . _jrdd_deserializer == other . _jrdd_deserializer : rdd = RDD ( self . _jrdd . union ( other . _jrdd ) , self . ctx , self . _jrdd_deserializer ) else : # These RDDs contain data in different serialized formats, so we # must normalize them to the default serializer. self_copy = self . _reserialize ( ) other_copy = other . _reserialize ( ) rdd = RDD ( self_copy . _jrdd . union ( other_copy . _jrdd ) , self . ctx , self . ctx . serializer ) if ( self . partitioner == other . partitioner and self . getNumPartitions ( ) == rdd . getNumPartitions ( ) ) : rdd . partitioner = self . partitioner return rdd\n\n\n        Summary:", "input": "", "output": "Return the union of this RDD and another one ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def intersection ( self , other ) : return self . map ( lambda v : ( v , None ) ) . cogroup ( other . map ( lambda v : ( v , None ) ) ) . filter ( lambda k_vs : all ( k_vs [ 1 ] ) ) . keys ( )\n\n\n        Summary:", "input": "", "output": "Return the intersection of this RDD and another one . The output will not contain any duplicate elements even if the input RDDs did ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def repartitionAndSortWithinPartitions ( self , numPartitions = None , partitionFunc = portable_hash , ascending = True , keyfunc = lambda x : x ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) memory = _parse_memory ( self . ctx . _conf . get ( \"spark.python.worker.memory\" , \"512m\" ) ) serializer = self . _jrdd_deserializer def sortPartition ( iterator ) : sort = ExternalSorter ( memory * 0.9 , serializer ) . sorted return iter ( sort ( iterator , key = lambda k_v : keyfunc ( k_v [ 0 ] ) , reverse = ( not ascending ) ) ) return self . partitionBy ( numPartitions , partitionFunc ) . mapPartitions ( sortPartition , True )\n\n\n        Summary:", "input": "", "output": "Repartition the RDD according to the given partitioner and within each resulting partition sort records by their keys ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sortByKey ( self , ascending = True , numPartitions = None , keyfunc = lambda x : x ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) memory = self . _memory_limit ( ) serializer = self . _jrdd_deserializer def sortPartition ( iterator ) : sort = ExternalSorter ( memory * 0.9 , serializer ) . sorted return iter ( sort ( iterator , key = lambda kv : keyfunc ( kv [ 0 ] ) , reverse = ( not ascending ) ) ) if numPartitions == 1 : if self . getNumPartitions ( ) > 1 : self = self . coalesce ( 1 ) return self . mapPartitions ( sortPartition , True ) # first compute the boundary of each part via sampling: we want to partition # the key-space into bins such that the bins have roughly the same # number of (key, value) pairs falling into them rddSize = self . count ( ) if not rddSize : return self # empty RDD maxSampleSize = numPartitions * 20.0 # constant from Spark's RangePartitioner fraction = min ( maxSampleSize / max ( rddSize , 1 ) , 1.0 ) samples = self . sample ( False , fraction , 1 ) . map ( lambda kv : kv [ 0 ] ) . collect ( ) samples = sorted ( samples , key = keyfunc ) # we have numPartitions many parts but one of the them has # an implicit boundary bounds = [ samples [ int ( len ( samples ) * ( i + 1 ) / numPartitions ) ] for i in range ( 0 , numPartitions - 1 ) ] def rangePartitioner ( k ) : p = bisect . bisect_left ( bounds , keyfunc ( k ) ) if ascending : return p else : return numPartitions - 1 - p return self . partitionBy ( numPartitions , rangePartitioner ) . mapPartitions ( sortPartition , True )\n\n\n        Summary:", "input": "", "output": "Sorts this RDD which is assumed to consist of ( key value ) pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sortBy ( self , keyfunc , ascending = True , numPartitions = None ) : return self . keyBy ( keyfunc ) . sortByKey ( ascending , numPartitions ) . values ( )\n\n\n        Summary:", "input": "", "output": "Sorts this RDD by the given keyfunc"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cartesian ( self , other ) : # Due to batching, we can't use the Java cartesian method. deserializer = CartesianDeserializer ( self . _jrdd_deserializer , other . _jrdd_deserializer ) return RDD ( self . _jrdd . cartesian ( other . _jrdd ) , self . ctx , deserializer )\n\n\n        Summary:", "input": "", "output": "Return the Cartesian product of this RDD and another one that is the RDD of all pairs of elements C { ( a b ) } where C { a } is in C { self } and C { b } is in C { other } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupBy ( self , f , numPartitions = None , partitionFunc = portable_hash ) : return self . map ( lambda x : ( f ( x ) , x ) ) . groupByKey ( numPartitions , partitionFunc )\n\n\n        Summary:", "input": "", "output": "Return an RDD of grouped items ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pipe ( self , command , env = None , checkCode = False ) : if env is None : env = dict ( ) def func ( iterator ) : pipe = Popen ( shlex . split ( command ) , env = env , stdin = PIPE , stdout = PIPE ) def pipe_objs ( out ) : for obj in iterator : s = unicode ( obj ) . rstrip ( '\\n' ) + '\\n' out . write ( s . encode ( 'utf-8' ) ) out . close ( ) Thread ( target = pipe_objs , args = [ pipe . stdin ] ) . start ( ) def check_return_code ( ) : pipe . wait ( ) if checkCode and pipe . returncode : raise Exception ( \"Pipe function `%s' exited \" \"with error code %d\" % ( command , pipe . returncode ) ) else : for i in range ( 0 ) : yield i return ( x . rstrip ( b'\\n' ) . decode ( 'utf-8' ) for x in chain ( iter ( pipe . stdout . readline , b'' ) , check_return_code ( ) ) ) return self . mapPartitions ( func )\n\n\n        Summary:", "input": "", "output": "Return an RDD created by piping elements to a forked external process ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def foreach ( self , f ) : f = fail_on_stopiteration ( f ) def processPartition ( iterator ) : for x in iterator : f ( x ) return iter ( [ ] ) self . mapPartitions ( processPartition ) . count ( )\n\n\n        Summary:", "input": "", "output": "Applies a function to all elements of this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def foreachPartition ( self , f ) : def func ( it ) : r = f ( it ) try : return iter ( r ) except TypeError : return iter ( [ ] ) self . mapPartitions ( func ) . count ( )\n\n\n        Summary:", "input": "", "output": "Applies a function to each partition of this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def collect ( self ) : with SCCallSiteSync ( self . context ) as css : sock_info = self . ctx . _jvm . PythonRDD . collectAndServe ( self . _jrdd . rdd ( ) ) return list ( _load_from_socket ( sock_info , self . _jrdd_deserializer ) )\n\n\n        Summary:", "input": "", "output": "Return a list that contains all of the elements in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduce ( self , f ) : f = fail_on_stopiteration ( f ) def func ( iterator ) : iterator = iter ( iterator ) try : initial = next ( iterator ) except StopIteration : return yield reduce ( f , iterator , initial ) vals = self . mapPartitions ( func ) . collect ( ) if vals : return reduce ( f , vals ) raise ValueError ( \"Can not reduce() empty RDD\" )\n\n\n        Summary:", "input": "", "output": "Reduces the elements of this RDD using the specified commutative and associative binary operator . Currently reduces partitions locally ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def treeReduce ( self , f , depth = 2 ) : if depth < 1 : raise ValueError ( \"Depth cannot be smaller than 1 but got %d.\" % depth ) zeroValue = None , True # Use the second entry to indicate whether this is a dummy value. def op ( x , y ) : if x [ 1 ] : return y elif y [ 1 ] : return x else : return f ( x [ 0 ] , y [ 0 ] ) , False reduced = self . map ( lambda x : ( x , False ) ) . treeAggregate ( zeroValue , op , op , depth ) if reduced [ 1 ] : raise ValueError ( \"Cannot reduce empty RDD.\" ) return reduced [ 0 ]\n\n\n        Summary:", "input": "", "output": "Reduces the elements of this RDD in a multi - level tree pattern ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fold ( self , zeroValue , op ) : op = fail_on_stopiteration ( op ) def func ( iterator ) : acc = zeroValue for obj in iterator : acc = op ( acc , obj ) yield acc # collecting result of mapPartitions here ensures that the copy of # zeroValue provided to each partition is unique from the one provided # to the final reduce call vals = self . mapPartitions ( func ) . collect ( ) return reduce ( op , vals , zeroValue )\n\n\n        Summary:", "input": "", "output": "Aggregate the elements of each partition and then the results for all the partitions using a given associative function and a neutral zero value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def aggregate ( self , zeroValue , seqOp , combOp ) : seqOp = fail_on_stopiteration ( seqOp ) combOp = fail_on_stopiteration ( combOp ) def func ( iterator ) : acc = zeroValue for obj in iterator : acc = seqOp ( acc , obj ) yield acc # collecting result of mapPartitions here ensures that the copy of # zeroValue provided to each partition is unique from the one provided # to the final reduce call vals = self . mapPartitions ( func ) . collect ( ) return reduce ( combOp , vals , zeroValue )\n\n\n        Summary:", "input": "", "output": "Aggregate the elements of each partition and then the results for all the partitions using a given combine functions and a neutral zero value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def treeAggregate ( self , zeroValue , seqOp , combOp , depth = 2 ) : if depth < 1 : raise ValueError ( \"Depth cannot be smaller than 1 but got %d.\" % depth ) if self . getNumPartitions ( ) == 0 : return zeroValue def aggregatePartition ( iterator ) : acc = zeroValue for obj in iterator : acc = seqOp ( acc , obj ) yield acc partiallyAggregated = self . mapPartitions ( aggregatePartition ) numPartitions = partiallyAggregated . getNumPartitions ( ) scale = max ( int ( ceil ( pow ( numPartitions , 1.0 / depth ) ) ) , 2 ) # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree # aggregation. while numPartitions > scale + numPartitions / scale : numPartitions /= scale curNumPartitions = int ( numPartitions ) def mapPartition ( i , iterator ) : for obj in iterator : yield ( i % curNumPartitions , obj ) partiallyAggregated = partiallyAggregated . mapPartitionsWithIndex ( mapPartition ) . reduceByKey ( combOp , curNumPartitions ) . values ( ) return partiallyAggregated . reduce ( combOp )\n\n\n        Summary:", "input": "", "output": "Aggregates the elements of this RDD in a multi - level tree pattern ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def max ( self , key = None ) : if key is None : return self . reduce ( max ) return self . reduce ( lambda a , b : max ( a , b , key = key ) )\n\n\n        Summary:", "input": "", "output": "Find the maximum item in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def min ( self , key = None ) : if key is None : return self . reduce ( min ) return self . reduce ( lambda a , b : min ( a , b , key = key ) )\n\n\n        Summary:", "input": "", "output": "Find the minimum item in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sum ( self ) : return self . mapPartitions ( lambda x : [ sum ( x ) ] ) . fold ( 0 , operator . add )\n\n\n        Summary:", "input": "", "output": "Add up the elements in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stats ( self ) : def redFunc ( left_counter , right_counter ) : return left_counter . mergeStats ( right_counter ) return self . mapPartitions ( lambda i : [ StatCounter ( i ) ] ) . reduce ( redFunc )\n\n\n        Summary:", "input": "", "output": "Return a L { StatCounter } object that captures the mean variance and count of the RDD s elements in one operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def histogram ( self , buckets ) : if isinstance ( buckets , int ) : if buckets < 1 : raise ValueError ( \"number of buckets must be >= 1\" ) # filter out non-comparable elements def comparable ( x ) : if x is None : return False if type ( x ) is float and isnan ( x ) : return False return True filtered = self . filter ( comparable ) # faster than stats() def minmax ( a , b ) : return min ( a [ 0 ] , b [ 0 ] ) , max ( a [ 1 ] , b [ 1 ] ) try : minv , maxv = filtered . map ( lambda x : ( x , x ) ) . reduce ( minmax ) except TypeError as e : if \" empty \" in str ( e ) : raise ValueError ( \"can not generate buckets from empty RDD\" ) raise if minv == maxv or buckets == 1 : return [ minv , maxv ] , [ filtered . count ( ) ] try : inc = ( maxv - minv ) / buckets except TypeError : raise TypeError ( \"Can not generate buckets with non-number in RDD\" ) if isinf ( inc ) : raise ValueError ( \"Can not generate buckets with infinite value\" ) # keep them as integer if possible inc = int ( inc ) if inc * buckets != maxv - minv : inc = ( maxv - minv ) * 1.0 / buckets buckets = [ i * inc + minv for i in range ( buckets ) ] buckets . append ( maxv ) # fix accumulated error even = True elif isinstance ( buckets , ( list , tuple ) ) : if len ( buckets ) < 2 : raise ValueError ( \"buckets should have more than one value\" ) if any ( i is None or isinstance ( i , float ) and isnan ( i ) for i in buckets ) : raise ValueError ( \"can not have None or NaN in buckets\" ) if sorted ( buckets ) != list ( buckets ) : raise ValueError ( \"buckets should be sorted\" ) if len ( set ( buckets ) ) != len ( buckets ) : raise ValueError ( \"buckets should not contain duplicated values\" ) minv = buckets [ 0 ] maxv = buckets [ - 1 ] even = False inc = None try : steps = [ buckets [ i + 1 ] - buckets [ i ] for i in range ( len ( buckets ) - 1 ) ] except TypeError : pass # objects in buckets do not support '-' else : if max ( steps ) - min ( steps ) < 1e-10 : # handle precision errors even = True inc = ( maxv - minv ) / ( len ( buckets ) - 1 ) else : raise TypeError ( \"buckets should be a list or tuple or number(int or long)\" ) def histogram ( iterator ) : counters = [ 0 ] * len ( buckets ) for i in iterator : if i is None or ( type ( i ) is float and isnan ( i ) ) or i > maxv or i < minv : continue t = ( int ( ( i - minv ) / inc ) if even else bisect . bisect_right ( buckets , i ) - 1 ) counters [ t ] += 1 # add last two together last = counters . pop ( ) counters [ - 1 ] += last return [ counters ] def mergeCounters ( a , b ) : return [ i + j for i , j in zip ( a , b ) ] return buckets , self . mapPartitions ( histogram ) . reduce ( mergeCounters )\n\n\n        Summary:", "input": "", "output": "Compute a histogram using the provided buckets . The buckets are all open to the right except for the last which is closed . e . g . [ 1 10 20 50 ] means the buckets are [ 1 10 ) [ 10 20 ) [ 20 50 ] which means 1< = x<10 10< = x<20 20< = x< = 50 . And on the input of 1 and 50 we would have a histogram of 1 0 1 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countByValue ( self ) : def countPartition ( iterator ) : counts = defaultdict ( int ) for obj in iterator : counts [ obj ] += 1 yield counts def mergeMaps ( m1 , m2 ) : for k , v in m2 . items ( ) : m1 [ k ] += v return m1 return self . mapPartitions ( countPartition ) . reduce ( mergeMaps )\n\n\n        Summary:", "input": "", "output": "Return the count of each unique value in this RDD as a dictionary of ( value count ) pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def top ( self , num , key = None ) : def topIterator ( iterator ) : yield heapq . nlargest ( num , iterator , key = key ) def merge ( a , b ) : return heapq . nlargest ( num , a + b , key = key ) return self . mapPartitions ( topIterator ) . reduce ( merge )\n\n\n        Summary:", "input": "", "output": "Get the top N elements from an RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def takeOrdered ( self , num , key = None ) : def merge ( a , b ) : return heapq . nsmallest ( num , a + b , key ) return self . mapPartitions ( lambda it : [ heapq . nsmallest ( num , it , key ) ] ) . reduce ( merge )\n\n\n        Summary:", "input": "", "output": "Get the N elements from an RDD ordered in ascending order or as specified by the optional key function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take ( self , num ) : items = [ ] totalParts = self . getNumPartitions ( ) partsScanned = 0 while len ( items ) < num and partsScanned < totalParts : # The number of partitions to try in this iteration. # It is ok for this number to be greater than totalParts because # we actually cap it at totalParts in runJob. numPartsToTry = 1 if partsScanned > 0 : # If we didn't find any rows after the previous iteration, # quadruple and retry. Otherwise, interpolate the number of # partitions we need to try, but overestimate it by 50%. # We also cap the estimation in the end. if len ( items ) == 0 : numPartsToTry = partsScanned * 4 else : # the first parameter of max is >=1 whenever partsScanned >= 2 numPartsToTry = int ( 1.5 * num * partsScanned / len ( items ) ) - partsScanned numPartsToTry = min ( max ( numPartsToTry , 1 ) , partsScanned * 4 ) left = num - len ( items ) def takeUpToNumLeft ( iterator ) : iterator = iter ( iterator ) taken = 0 while taken < left : try : yield next ( iterator ) except StopIteration : return taken += 1 p = range ( partsScanned , min ( partsScanned + numPartsToTry , totalParts ) ) res = self . context . runJob ( self , takeUpToNumLeft , p ) items += res partsScanned += numPartsToTry return items [ : num ]\n\n\n        Summary:", "input": "", "output": "Take the first num elements of the RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsNewAPIHadoopDataset ( self , conf , keyConverter = None , valueConverter = None ) : jconf = self . ctx . _dictToJavaMap ( conf ) pickledRDD = self . _pickled ( ) self . ctx . _jvm . PythonRDD . saveAsHadoopDataset ( pickledRDD . _jrdd , True , jconf , keyConverter , valueConverter , True )\n\n\n        Summary:", "input": "", "output": "Output a Python RDD of key - value pairs ( of form C { RDD [ ( K V ) ] } ) to any Hadoop file system using the new Hadoop OutputFormat API ( mapreduce package ) . Keys / values are converted for output using either user specified converters or by default L { org . apache . spark . api . python . JavaToWritableConverter } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsNewAPIHadoopFile ( self , path , outputFormatClass , keyClass = None , valueClass = None , keyConverter = None , valueConverter = None , conf = None ) : jconf = self . ctx . _dictToJavaMap ( conf ) pickledRDD = self . _pickled ( ) self . ctx . _jvm . PythonRDD . saveAsNewAPIHadoopFile ( pickledRDD . _jrdd , True , path , outputFormatClass , keyClass , valueClass , keyConverter , valueConverter , jconf )\n\n\n        Summary:", "input": "", "output": "Output a Python RDD of key - value pairs ( of form C { RDD [ ( K V ) ] } ) to any Hadoop file system using the new Hadoop OutputFormat API ( mapreduce package ) . Key and value types will be inferred if not specified . Keys and values are converted for output using either user specified converters or L { org . apache . spark . api . python . JavaToWritableConverter } . The C { conf } is applied on top of the base Hadoop conf associated with the SparkContext of this RDD to create a merged Hadoop MapReduce job configuration for saving the data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsSequenceFile ( self , path , compressionCodecClass = None ) : pickledRDD = self . _pickled ( ) self . ctx . _jvm . PythonRDD . saveAsSequenceFile ( pickledRDD . _jrdd , True , path , compressionCodecClass )\n\n\n        Summary:", "input": "", "output": "Output a Python RDD of key - value pairs ( of form C { RDD [ ( K V ) ] } ) to any Hadoop file system using the L { org . apache . hadoop . io . Writable } types that we convert from the RDD s key and value types . The mechanism is as follows :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsPickleFile ( self , path , batchSize = 10 ) : if batchSize == 0 : ser = AutoBatchedSerializer ( PickleSerializer ( ) ) else : ser = BatchedSerializer ( PickleSerializer ( ) , batchSize ) self . _reserialize ( ser ) . _jrdd . saveAsObjectFile ( path )\n\n\n        Summary:", "input": "", "output": "Save this RDD as a SequenceFile of serialized objects . The serializer used is L { pyspark . serializers . PickleSerializer } default batch size is 10 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsTextFile ( self , path , compressionCodecClass = None ) : def func ( split , iterator ) : for x in iterator : if not isinstance ( x , ( unicode , bytes ) ) : x = unicode ( x ) if isinstance ( x , unicode ) : x = x . encode ( \"utf-8\" ) yield x keyed = self . mapPartitionsWithIndex ( func ) keyed . _bypass_serializer = True if compressionCodecClass : compressionCodec = self . ctx . _jvm . java . lang . Class . forName ( compressionCodecClass ) keyed . _jrdd . map ( self . ctx . _jvm . BytesToString ( ) ) . saveAsTextFile ( path , compressionCodec ) else : keyed . _jrdd . map ( self . ctx . _jvm . BytesToString ( ) ) . saveAsTextFile ( path )\n\n\n        Summary:", "input": "", "output": "Save this RDD as a text file using string representations of elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduceByKey ( self , func , numPartitions = None , partitionFunc = portable_hash ) : return self . combineByKey ( lambda x : x , func , func , numPartitions , partitionFunc )\n\n\n        Summary:", "input": "", "output": "Merge the values for each key using an associative and commutative reduce function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduceByKeyLocally ( self , func ) : func = fail_on_stopiteration ( func ) def reducePartition ( iterator ) : m = { } for k , v in iterator : m [ k ] = func ( m [ k ] , v ) if k in m else v yield m def mergeMaps ( m1 , m2 ) : for k , v in m2 . items ( ) : m1 [ k ] = func ( m1 [ k ] , v ) if k in m1 else v return m1 return self . mapPartitions ( reducePartition ) . reduce ( mergeMaps )\n\n\n        Summary:", "input": "", "output": "Merge the values for each key using an associative and commutative reduce function but return the results immediately to the master as a dictionary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def partitionBy ( self , numPartitions , partitionFunc = portable_hash ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) partitioner = Partitioner ( numPartitions , partitionFunc ) if self . partitioner == partitioner : return self # Transferring O(n) objects to Java is too expensive. # Instead, we'll form the hash buckets in Python, # transferring O(numPartitions) objects to Java. # Each object is a (splitNumber, [objects]) pair. # In order to avoid too huge objects, the objects are # grouped into chunks. outputSerializer = self . ctx . _unbatched_serializer limit = ( _parse_memory ( self . ctx . _conf . get ( \"spark.python.worker.memory\" , \"512m\" ) ) / 2 ) def add_shuffle_key ( split , iterator ) : buckets = defaultdict ( list ) c , batch = 0 , min ( 10 * numPartitions , 1000 ) for k , v in iterator : buckets [ partitionFunc ( k ) % numPartitions ] . append ( ( k , v ) ) c += 1 # check used memory and avg size of chunk of objects if ( c % 1000 == 0 and get_used_memory ( ) > limit or c > batch ) : n , size = len ( buckets ) , 0 for split in list ( buckets . keys ( ) ) : yield pack_long ( split ) d = outputSerializer . dumps ( buckets [ split ] ) del buckets [ split ] yield d size += len ( d ) avg = int ( size / n ) >> 20 # let 1M < avg < 10M if avg < 1 : batch *= 1.5 elif avg > 10 : batch = max ( int ( batch / 1.5 ) , 1 ) c = 0 for split , items in buckets . items ( ) : yield pack_long ( split ) yield outputSerializer . dumps ( items ) keyed = self . mapPartitionsWithIndex ( add_shuffle_key , preservesPartitioning = True ) keyed . _bypass_serializer = True with SCCallSiteSync ( self . context ) as css : pairRDD = self . ctx . _jvm . PairwiseRDD ( keyed . _jrdd . rdd ( ) ) . asJavaPairRDD ( ) jpartitioner = self . ctx . _jvm . PythonPartitioner ( numPartitions , id ( partitionFunc ) ) jrdd = self . ctx . _jvm . PythonRDD . valueOfPair ( pairRDD . partitionBy ( jpartitioner ) ) rdd = RDD ( jrdd , self . ctx , BatchedSerializer ( outputSerializer ) ) rdd . partitioner = partitioner return rdd\n\n\n        Summary:", "input": "", "output": "Return a copy of the RDD partitioned using the specified partitioner ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combineByKey ( self , createCombiner , mergeValue , mergeCombiners , numPartitions = None , partitionFunc = portable_hash ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) serializer = self . ctx . serializer memory = self . _memory_limit ( ) agg = Aggregator ( createCombiner , mergeValue , mergeCombiners ) def combineLocally ( iterator ) : merger = ExternalMerger ( agg , memory * 0.9 , serializer ) merger . mergeValues ( iterator ) return merger . items ( ) locally_combined = self . mapPartitions ( combineLocally , preservesPartitioning = True ) shuffled = locally_combined . partitionBy ( numPartitions , partitionFunc ) def _mergeCombiners ( iterator ) : merger = ExternalMerger ( agg , memory , serializer ) merger . mergeCombiners ( iterator ) return merger . items ( ) return shuffled . mapPartitions ( _mergeCombiners , preservesPartitioning = True )\n\n\n        Summary:", "input": "", "output": "Generic function to combine the elements for each key using a custom set of aggregation functions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def aggregateByKey ( self , zeroValue , seqFunc , combFunc , numPartitions = None , partitionFunc = portable_hash ) : def createZero ( ) : return copy . deepcopy ( zeroValue ) return self . combineByKey ( lambda v : seqFunc ( createZero ( ) , v ) , seqFunc , combFunc , numPartitions , partitionFunc )\n\n\n        Summary:", "input": "", "output": "Aggregate the values of each key using given combine functions and a neutral zero value . This function can return a different result type U than the type of the values in this RDD V . Thus we need one operation for merging a V into a U and one operation for merging two U s The former operation is used for merging values within a partition and the latter is used for merging values between partitions . To avoid memory allocation both of these functions are allowed to modify and return their first argument instead of creating a new U ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def foldByKey ( self , zeroValue , func , numPartitions = None , partitionFunc = portable_hash ) : def createZero ( ) : return copy . deepcopy ( zeroValue ) return self . combineByKey ( lambda v : func ( createZero ( ) , v ) , func , func , numPartitions , partitionFunc )\n\n\n        Summary:", "input": "", "output": "Merge the values for each key using an associative function func and a neutral zeroValue which may be added to the result an arbitrary number of times and must not change the result ( e . g . 0 for addition or 1 for multiplication . ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupByKey ( self , numPartitions = None , partitionFunc = portable_hash ) : def createCombiner ( x ) : return [ x ] def mergeValue ( xs , x ) : xs . append ( x ) return xs def mergeCombiners ( a , b ) : a . extend ( b ) return a memory = self . _memory_limit ( ) serializer = self . _jrdd_deserializer agg = Aggregator ( createCombiner , mergeValue , mergeCombiners ) def combine ( iterator ) : merger = ExternalMerger ( agg , memory * 0.9 , serializer ) merger . mergeValues ( iterator ) return merger . items ( ) locally_combined = self . mapPartitions ( combine , preservesPartitioning = True ) shuffled = locally_combined . partitionBy ( numPartitions , partitionFunc ) def groupByKey ( it ) : merger = ExternalGroupBy ( agg , memory , serializer ) merger . mergeCombiners ( it ) return merger . items ( ) return shuffled . mapPartitions ( groupByKey , True ) . mapValues ( ResultIterable )\n\n\n        Summary:", "input": "", "output": "Group the values for each key in the RDD into a single sequence . Hash - partitions the resulting RDD with numPartitions partitions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def flatMapValues ( self , f ) : flat_map_fn = lambda kv : ( ( kv [ 0 ] , x ) for x in f ( kv [ 1 ] ) ) return self . flatMap ( flat_map_fn , preservesPartitioning = True )\n\n\n        Summary:", "input": "", "output": "Pass each value in the key - value pair RDD through a flatMap function without changing the keys ; this also retains the original RDD s partitioning ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mapValues ( self , f ) : map_values_fn = lambda kv : ( kv [ 0 ] , f ( kv [ 1 ] ) ) return self . map ( map_values_fn , preservesPartitioning = True )\n\n\n        Summary:", "input": "", "output": "Pass each value in the key - value pair RDD through a map function without changing the keys ; this also retains the original RDD s partitioning ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sampleByKey ( self , withReplacement , fractions , seed = None ) : for fraction in fractions . values ( ) : assert fraction >= 0.0 , \"Negative fraction value: %s\" % fraction return self . mapPartitionsWithIndex ( RDDStratifiedSampler ( withReplacement , fractions , seed ) . func , True )\n\n\n        Summary:", "input": "", "output": "Return a subset of this RDD sampled by key ( via stratified sampling ) . Create a sample of this RDD using variable sampling rates for different keys as specified by fractions a key to sampling rate map ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def subtractByKey ( self , other , numPartitions = None ) : def filter_func ( pair ) : key , ( val1 , val2 ) = pair return val1 and not val2 return self . cogroup ( other , numPartitions ) . filter ( filter_func ) . flatMapValues ( lambda x : x [ 0 ] )\n\n\n        Summary:", "input": "", "output": "Return each ( key value ) pair in C { self } that has no pair with matching key in C { other } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def subtract ( self , other , numPartitions = None ) : # note: here 'True' is just a placeholder rdd = other . map ( lambda x : ( x , True ) ) return self . map ( lambda x : ( x , True ) ) . subtractByKey ( rdd , numPartitions ) . keys ( )\n\n\n        Summary:", "input": "", "output": "Return each value in C { self } that is not contained in C { other } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def coalesce ( self , numPartitions , shuffle = False ) : if shuffle : # Decrease the batch size in order to distribute evenly the elements across output # partitions. Otherwise, repartition will possibly produce highly skewed partitions. batchSize = min ( 10 , self . ctx . _batchSize or 1024 ) ser = BatchedSerializer ( PickleSerializer ( ) , batchSize ) selfCopy = self . _reserialize ( ser ) jrdd_deserializer = selfCopy . _jrdd_deserializer jrdd = selfCopy . _jrdd . coalesce ( numPartitions , shuffle ) else : jrdd_deserializer = self . _jrdd_deserializer jrdd = self . _jrdd . coalesce ( numPartitions , shuffle ) return RDD ( jrdd , self . ctx , jrdd_deserializer )\n\n\n        Summary:", "input": "", "output": "Return a new RDD that is reduced into numPartitions partitions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def zip ( self , other ) : def get_batch_size ( ser ) : if isinstance ( ser , BatchedSerializer ) : return ser . batchSize return 1 # not batched def batch_as ( rdd , batchSize ) : return rdd . _reserialize ( BatchedSerializer ( PickleSerializer ( ) , batchSize ) ) my_batch = get_batch_size ( self . _jrdd_deserializer ) other_batch = get_batch_size ( other . _jrdd_deserializer ) if my_batch != other_batch or not my_batch : # use the smallest batchSize for both of them batchSize = min ( my_batch , other_batch ) if batchSize <= 0 : # auto batched or unlimited batchSize = 100 other = batch_as ( other , batchSize ) self = batch_as ( self , batchSize ) if self . getNumPartitions ( ) != other . getNumPartitions ( ) : raise ValueError ( \"Can only zip with RDD which has the same number of partitions\" ) # There will be an Exception in JVM if there are different number # of items in each partitions. pairRDD = self . _jrdd . zip ( other . _jrdd ) deserializer = PairDeserializer ( self . _jrdd_deserializer , other . _jrdd_deserializer ) return RDD ( pairRDD , self . ctx , deserializer )\n\n\n        Summary:", "input": "", "output": "Zips this RDD with another one returning key - value pairs with the first element in each RDD second element in each RDD etc . Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition ( e . g . one was made through a map on the other ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def zipWithIndex ( self ) : starts = [ 0 ] if self . getNumPartitions ( ) > 1 : nums = self . mapPartitions ( lambda it : [ sum ( 1 for i in it ) ] ) . collect ( ) for i in range ( len ( nums ) - 1 ) : starts . append ( starts [ - 1 ] + nums [ i ] ) def func ( k , it ) : for i , v in enumerate ( it , starts [ k ] ) : yield v , i return self . mapPartitionsWithIndex ( func )\n\n\n        Summary:", "input": "", "output": "Zips this RDD with its element indices ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def zipWithUniqueId ( self ) : n = self . getNumPartitions ( ) def func ( k , it ) : for i , v in enumerate ( it ) : yield v , i * n + k return self . mapPartitionsWithIndex ( func )\n\n\n        Summary:", "input": "", "output": "Zips this RDD with generated unique Long ids ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getStorageLevel ( self ) : java_storage_level = self . _jrdd . getStorageLevel ( ) storage_level = StorageLevel ( java_storage_level . useDisk ( ) , java_storage_level . useMemory ( ) , java_storage_level . useOffHeap ( ) , java_storage_level . deserialized ( ) , java_storage_level . replication ( ) ) return storage_level\n\n\n        Summary:", "input": "", "output": "Get the RDD s current storage level ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _defaultReducePartitions ( self ) : if self . ctx . _conf . contains ( \"spark.default.parallelism\" ) : return self . ctx . defaultParallelism else : return self . getNumPartitions ( )\n\n\n        Summary:", "input": "", "output": "Returns the default number of partitions to use during reduce tasks ( e . g . groupBy ) . If spark . default . parallelism is set then we ll use the value from SparkContext defaultParallelism otherwise we ll use the number of partitions in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lookup ( self , key ) : values = self . filter ( lambda kv : kv [ 0 ] == key ) . values ( ) if self . partitioner is not None : return self . ctx . runJob ( values , lambda x : x , [ self . partitioner ( key ) ] ) return values . collect ( )\n\n\n        Summary:", "input": "", "output": "Return the list of values in the RDD for key key . This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java_object_rdd ( self ) : rdd = self . _pickled ( ) return self . ctx . _jvm . SerDeUtil . pythonToJava ( rdd . _jrdd , True )\n\n\n        Summary:", "input": "", "output": "Return a JavaRDD of Object by unpickling"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countApprox ( self , timeout , confidence = 0.95 ) : drdd = self . mapPartitions ( lambda it : [ float ( sum ( 1 for i in it ) ) ] ) return int ( drdd . sumApprox ( timeout , confidence ) )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sumApprox ( self , timeout , confidence = 0.95 ) : jrdd = self . mapPartitions ( lambda it : [ float ( sum ( it ) ) ] ) . _to_java_object_rdd ( ) jdrdd = self . ctx . _jvm . JavaDoubleRDD . fromRDD ( jrdd . rdd ( ) ) r = jdrdd . sumApprox ( timeout , confidence ) . getFinalValue ( ) return BoundedFloat ( r . mean ( ) , r . confidence ( ) , r . low ( ) , r . high ( ) )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def meanApprox ( self , timeout , confidence = 0.95 ) : jrdd = self . map ( float ) . _to_java_object_rdd ( ) jdrdd = self . ctx . _jvm . JavaDoubleRDD . fromRDD ( jrdd . rdd ( ) ) r = jdrdd . meanApprox ( timeout , confidence ) . getFinalValue ( ) return BoundedFloat ( r . mean ( ) , r . confidence ( ) , r . low ( ) , r . high ( ) )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countApproxDistinct ( self , relativeSD = 0.05 ) : if relativeSD < 0.000017 : raise ValueError ( \"relativeSD should be greater than 0.000017\" ) # the hash space in Java is 2^32 hashRDD = self . map ( lambda x : portable_hash ( x ) & 0xFFFFFFFF ) return hashRDD . _to_java_object_rdd ( ) . countApproxDistinct ( relativeSD )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toLocalIterator ( self ) : with SCCallSiteSync ( self . context ) as css : sock_info = self . ctx . _jvm . PythonRDD . toLocalIteratorAndServe ( self . _jrdd . rdd ( ) ) return _load_from_socket ( sock_info , self . _jrdd_deserializer )\n\n\n        Summary:", "input": "", "output": "Return an iterator that contains all of the elements in this RDD . The iterator will consume as much memory as the largest partition in this RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mapPartitions ( self , f , preservesPartitioning = False ) : def func ( s , iterator ) : return f ( iterator ) return PipelinedRDD ( self . rdd , func , preservesPartitioning , isFromBarrier = True )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_seq ( sc , cols , converter = None ) : if converter : cols = [ converter ( c ) for c in cols ] return sc . _jvm . PythonUtils . toSeq ( cols )\n\n\n        Summary:", "input": "", "output": "Convert a list of Column ( or names ) into a JVM Seq of Column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_list ( sc , cols , converter = None ) : if converter : cols = [ converter ( c ) for c in cols ] return sc . _jvm . PythonUtils . toList ( cols )\n\n\n        Summary:", "input": "", "output": "Convert a list of Column ( or names ) into a JVM ( Scala ) List of Column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unary_op ( name , doc = \"unary operator\" ) : def _ ( self ) : jc = getattr ( self . _jc , name ) ( ) return Column ( jc ) _ . __doc__ = doc return _\n\n\n        Summary:", "input": "", "output": "Create a method for given unary operator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _bin_op ( name , doc = \"binary operator\" ) : def _ ( self , other ) : jc = other . _jc if isinstance ( other , Column ) else other njc = getattr ( self . _jc , name ) ( jc ) return Column ( njc ) _ . __doc__ = doc return _\n\n\n        Summary:", "input": "", "output": "Create a method for given binary operator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reverse_op ( name , doc = \"binary operator\" ) : def _ ( self , other ) : jother = _create_column_from_literal ( other ) jc = getattr ( jother , name ) ( self . _jc ) return Column ( jc ) _ . __doc__ = doc return _\n\n\n        Summary:", "input": "", "output": "Create a method for binary operator ( this object is on right side )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def substr ( self , startPos , length ) : if type ( startPos ) != type ( length ) : raise TypeError ( \"startPos and length must be the same type. \" \"Got {startPos_t} and {length_t}, respectively.\" . format ( startPos_t = type ( startPos ) , length_t = type ( length ) , ) ) if isinstance ( startPos , int ) : jc = self . _jc . substr ( startPos , length ) elif isinstance ( startPos , Column ) : jc = self . _jc . substr ( startPos . _jc , length . _jc ) else : raise TypeError ( \"Unexpected type: %s\" % type ( startPos ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Return a : class : Column which is a substring of the column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isin ( self , * cols ) : if len ( cols ) == 1 and isinstance ( cols [ 0 ] , ( list , set ) ) : cols = cols [ 0 ] cols = [ c . _jc if isinstance ( c , Column ) else _create_column_from_literal ( c ) for c in cols ] sc = SparkContext . _active_spark_context jc = getattr ( self . _jc , \"isin\" ) ( _to_seq ( sc , cols ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def alias ( self , * alias , * * kwargs ) : metadata = kwargs . pop ( 'metadata' , None ) assert not kwargs , 'Unexpected kwargs where passed: %s' % kwargs sc = SparkContext . _active_spark_context if len ( alias ) == 1 : if metadata : jmeta = sc . _jvm . org . apache . spark . sql . types . Metadata . fromJson ( json . dumps ( metadata ) ) return Column ( getattr ( self . _jc , \"as\" ) ( alias [ 0 ] , jmeta ) ) else : return Column ( getattr ( self . _jc , \"as\" ) ( alias [ 0 ] ) ) else : if metadata : raise ValueError ( 'metadata can only be provided for a single column' ) return Column ( getattr ( self . _jc , \"as\" ) ( _to_seq ( sc , list ( alias ) ) ) )\n\n\n        Summary:", "input": "", "output": "Returns this column aliased with a new name or names ( in the case of expressions that return more than one column such as explode ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cast ( self , dataType ) : if isinstance ( dataType , basestring ) : jc = self . _jc . cast ( dataType ) elif isinstance ( dataType , DataType ) : from pyspark . sql import SparkSession spark = SparkSession . builder . getOrCreate ( ) jdt = spark . _jsparkSession . parseDataType ( dataType . json ( ) ) jc = self . _jc . cast ( jdt ) else : raise TypeError ( \"unexpected type: %s\" % type ( dataType ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Convert the column into type dataType ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def when ( self , condition , value ) : if not isinstance ( condition , Column ) : raise TypeError ( \"condition should be a Column\" ) v = value . _jc if isinstance ( value , Column ) else value jc = self . _jc . when ( condition . _jc , v ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Evaluates a list of conditions and returns one of multiple possible result expressions . If : func : Column . otherwise is not invoked None is returned for unmatched conditions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def otherwise ( self , value ) : v = value . _jc if isinstance ( value , Column ) else value jc = self . _jc . otherwise ( v ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Evaluates a list of conditions and returns one of multiple possible result expressions . If : func : Column . otherwise is not invoked None is returned for unmatched conditions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def over ( self , window ) : from pyspark . sql . window import WindowSpec if not isinstance ( window , WindowSpec ) : raise TypeError ( \"window should be WindowSpec\" ) jc = self . _jc . over ( window . _jspec ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Define a windowing column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , vector ) : if isinstance ( vector , RDD ) : vector = vector . map ( _convert_to_vector ) else : vector = _convert_to_vector ( vector ) return self . call ( \"transform\" , vector )\n\n\n        Summary:", "input": "", "output": "Applies transformation on a vector or an RDD [ Vector ] ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fit ( self , dataset ) : dataset = dataset . map ( _convert_to_vector ) jmodel = callMLlibFunc ( \"fitStandardScaler\" , self . withMean , self . withStd , dataset ) return StandardScalerModel ( jmodel )\n\n\n        Summary:", "input": "", "output": "Computes the mean and variance and stores as a model to be used for later scaling ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fit ( self , data ) : jmodel = callMLlibFunc ( \"fitChiSqSelector\" , self . selectorType , self . numTopFeatures , self . percentile , self . fpr , self . fdr , self . fwe , data ) return ChiSqSelectorModel ( jmodel )\n\n\n        Summary:", "input": "", "output": "Returns a ChiSquared feature selector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fit ( self , data ) : jmodel = callMLlibFunc ( \"fitPCA\" , self . k , data ) return PCAModel ( jmodel )\n\n\n        Summary:", "input": "", "output": "Computes a [[ PCAModel ]] that contains the principal components of the input vectors . : param data : source vectors"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , document ) : if isinstance ( document , RDD ) : return document . map ( self . transform ) freq = { } for term in document : i = self . indexOf ( term ) freq [ i ] = 1.0 if self . binary else freq . get ( i , 0 ) + 1.0 return Vectors . sparse ( self . numFeatures , freq . items ( ) )\n\n\n        Summary:", "input": "", "output": "Transforms the input document ( list of terms ) to term frequency vectors or transform the RDD of document to RDD of term frequency vectors ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fit ( self , dataset ) : if not isinstance ( dataset , RDD ) : raise TypeError ( \"dataset should be an RDD of term frequency vectors\" ) jmodel = callMLlibFunc ( \"fitIDF\" , self . minDocFreq , dataset . map ( _convert_to_vector ) ) return IDFModel ( jmodel )\n\n\n        Summary:", "input": "", "output": "Computes the inverse document frequency ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def findSynonyms ( self , word , num ) : if not isinstance ( word , basestring ) : word = _convert_to_vector ( word ) words , similarity = self . call ( \"findSynonyms\" , word , num ) return zip ( words , similarity )\n\n\n        Summary:", "input": "", "output": "Find synonyms of a word"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( cls , sc , path ) : jmodel = sc . _jvm . org . apache . spark . mllib . feature . Word2VecModel . load ( sc . _jsc . sc ( ) , path ) model = sc . _jvm . org . apache . spark . mllib . api . python . Word2VecModelWrapper ( jmodel ) return Word2VecModel ( model )\n\n\n        Summary:", "input": "", "output": "Load a model from the given path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , vector ) : if isinstance ( vector , RDD ) : vector = vector . map ( _convert_to_vector ) else : vector = _convert_to_vector ( vector ) return callMLlibFunc ( \"elementwiseProductVector\" , self . scalingVector , vector )\n\n\n        Summary:", "input": "", "output": "Computes the Hadamard product of the vector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predict ( self , x ) : if isinstance ( x , RDD ) : return self . call ( \"predict\" , x . map ( _convert_to_vector ) ) else : return self . call ( \"predict\" , _convert_to_vector ( x ) )\n\n\n        Summary:", "input": "", "output": "Predict values for a single data point or an RDD of points using the model trained ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trainClassifier ( cls , data , numClasses , categoricalFeaturesInfo , impurity = \"gini\" , maxDepth = 5 , maxBins = 32 , minInstancesPerNode = 1 , minInfoGain = 0.0 ) : return cls . _train ( data , \"classification\" , numClasses , categoricalFeaturesInfo , impurity , maxDepth , maxBins , minInstancesPerNode , minInfoGain )\n\n\n        Summary:", "input": "", "output": "Train a decision tree model for classification ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trainRegressor ( cls , data , categoricalFeaturesInfo , impurity = \"variance\" , maxDepth = 5 , maxBins = 32 , minInstancesPerNode = 1 , minInfoGain = 0.0 ) : return cls . _train ( data , \"regression\" , 0 , categoricalFeaturesInfo , impurity , maxDepth , maxBins , minInstancesPerNode , minInfoGain )\n\n\n        Summary:", "input": "", "output": "Train a decision tree model for regression ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trainClassifier ( cls , data , numClasses , categoricalFeaturesInfo , numTrees , featureSubsetStrategy = \"auto\" , impurity = \"gini\" , maxDepth = 4 , maxBins = 32 , seed = None ) : return cls . _train ( data , \"classification\" , numClasses , categoricalFeaturesInfo , numTrees , featureSubsetStrategy , impurity , maxDepth , maxBins , seed )\n\n\n        Summary:", "input": "", "output": "Train a random forest model for binary or multiclass classification ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trainRegressor ( cls , data , categoricalFeaturesInfo , numTrees , featureSubsetStrategy = \"auto\" , impurity = \"variance\" , maxDepth = 4 , maxBins = 32 , seed = None ) : return cls . _train ( data , \"regression\" , 0 , categoricalFeaturesInfo , numTrees , featureSubsetStrategy , impurity , maxDepth , maxBins , seed )\n\n\n        Summary:", "input": "", "output": "Train a random forest model for regression ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trainClassifier ( cls , data , categoricalFeaturesInfo , loss = \"logLoss\" , numIterations = 100 , learningRate = 0.1 , maxDepth = 3 , maxBins = 32 ) : return cls . _train ( data , \"classification\" , categoricalFeaturesInfo , loss , numIterations , learningRate , maxDepth , maxBins )\n\n\n        Summary:", "input": "", "output": "Train a gradient - boosted trees model for classification ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set ( self , key , value ) : # Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet. if self . _jconf is not None : self . _jconf . set ( key , unicode ( value ) ) else : self . _conf [ key ] = unicode ( value ) return self\n\n\n        Summary:", "input": "", "output": "Set a configuration property ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setIfMissing ( self , key , value ) : if self . get ( key ) is None : self . set ( key , value ) return self\n\n\n        Summary:", "input": "", "output": "Set a configuration property if not already set ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setExecutorEnv ( self , key = None , value = None , pairs = None ) : if ( key is not None and pairs is not None ) or ( key is None and pairs is None ) : raise Exception ( \"Either pass one key-value pair or a list of pairs\" ) elif key is not None : self . set ( \"spark.executorEnv.\" + key , value ) elif pairs is not None : for ( k , v ) in pairs : self . set ( \"spark.executorEnv.\" + k , v ) return self\n\n\n        Summary:", "input": "", "output": "Set an environment variable to be passed to executors ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setAll ( self , pairs ) : for ( k , v ) in pairs : self . set ( k , v ) return self\n\n\n        Summary:", "input": "", "output": "Set multiple parameters passed as a list of key - value pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( self , key , defaultValue = None ) : if defaultValue is None : # Py4J doesn't call the right get() if we pass None if self . _jconf is not None : if not self . _jconf . contains ( key ) : return None return self . _jconf . get ( key ) else : if key not in self . _conf : return None return self . _conf [ key ] else : if self . _jconf is not None : return self . _jconf . get ( key , defaultValue ) else : return self . _conf . get ( key , defaultValue )\n\n\n        Summary:", "input": "", "output": "Get the configured value for some key or return a default otherwise ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getAll ( self ) : if self . _jconf is not None : return [ ( elem . _1 ( ) , elem . _2 ( ) ) for elem in self . _jconf . getAll ( ) ] else : return self . _conf . items ( )\n\n\n        Summary:", "input": "", "output": "Get all values as a list of key - value pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def contains ( self , key ) : if self . _jconf is not None : return self . _jconf . contains ( key ) else : return key in self . _conf\n\n\n        Summary:", "input": "", "output": "Does this configuration contain a given key?"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toDebugString ( self ) : if self . _jconf is not None : return self . _jconf . toDebugString ( ) else : return '\\n' . join ( '%s=%s' % ( k , v ) for k , v in self . _conf . items ( ) )\n\n\n        Summary:", "input": "", "output": "Returns a printable version of the configuration as a list of key = value pairs one per line ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def listDatabases ( self ) : iter = self . _jcatalog . listDatabases ( ) . toLocalIterator ( ) databases = [ ] while iter . hasNext ( ) : jdb = iter . next ( ) databases . append ( Database ( name = jdb . name ( ) , description = jdb . description ( ) , locationUri = jdb . locationUri ( ) ) ) return databases\n\n\n        Summary:", "input": "", "output": "Returns a list of databases available across all sessions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def listTables ( self , dbName = None ) : if dbName is None : dbName = self . currentDatabase ( ) iter = self . _jcatalog . listTables ( dbName ) . toLocalIterator ( ) tables = [ ] while iter . hasNext ( ) : jtable = iter . next ( ) tables . append ( Table ( name = jtable . name ( ) , database = jtable . database ( ) , description = jtable . description ( ) , tableType = jtable . tableType ( ) , isTemporary = jtable . isTemporary ( ) ) ) return tables\n\n\n        Summary:", "input": "", "output": "Returns a list of tables / views in the specified database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def listFunctions ( self , dbName = None ) : if dbName is None : dbName = self . currentDatabase ( ) iter = self . _jcatalog . listFunctions ( dbName ) . toLocalIterator ( ) functions = [ ] while iter . hasNext ( ) : jfunction = iter . next ( ) functions . append ( Function ( name = jfunction . name ( ) , description = jfunction . description ( ) , className = jfunction . className ( ) , isTemporary = jfunction . isTemporary ( ) ) ) return functions\n\n\n        Summary:", "input": "", "output": "Returns a list of functions registered in the specified database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def listColumns ( self , tableName , dbName = None ) : if dbName is None : dbName = self . currentDatabase ( ) iter = self . _jcatalog . listColumns ( dbName , tableName ) . toLocalIterator ( ) columns = [ ] while iter . hasNext ( ) : jcolumn = iter . next ( ) columns . append ( Column ( name = jcolumn . name ( ) , description = jcolumn . description ( ) , dataType = jcolumn . dataType ( ) , nullable = jcolumn . nullable ( ) , isPartition = jcolumn . isPartition ( ) , isBucket = jcolumn . isBucket ( ) ) ) return columns\n\n\n        Summary:", "input": "", "output": "Returns a list of columns for the given table / view in the specified database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def createExternalTable ( self , tableName , path = None , source = None , schema = None , * * options ) : warnings . warn ( \"createExternalTable is deprecated since Spark 2.2, please use createTable instead.\" , DeprecationWarning ) return self . createTable ( tableName , path , source , schema , * * options )\n\n\n        Summary:", "input": "", "output": "Creates a table based on the dataset in a data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def createTable ( self , tableName , path = None , source = None , schema = None , * * options ) : if path is not None : options [ \"path\" ] = path if source is None : source = self . _sparkSession . _wrapped . _conf . defaultDataSourceName ( ) if schema is None : df = self . _jcatalog . createTable ( tableName , source , options ) else : if not isinstance ( schema , StructType ) : raise TypeError ( \"schema should be StructType\" ) scala_datatype = self . _jsparkSession . parseDataType ( schema . json ( ) ) df = self . _jcatalog . createTable ( tableName , source , scala_datatype , options ) return DataFrame ( df , self . _sparkSession . _wrapped )\n\n\n        Summary:", "input": "", "output": "Creates a table based on the dataset in a data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _load_from_socket ( port , auth_secret ) : ( sockfile , sock ) = local_connect_and_auth ( port , auth_secret ) # The barrier() call may block forever, so no timeout sock . settimeout ( None ) # Make a barrier() function call. write_int ( BARRIER_FUNCTION , sockfile ) sockfile . flush ( ) # Collect result. res = UTF8Deserializer ( ) . loads ( sockfile ) # Release resources. sockfile . close ( ) sock . close ( ) return res\n\n\n        Summary:", "input": "", "output": "Load data from a given socket this is a blocking method thus only return when the socket connection has been closed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _getOrCreate ( cls ) : if not isinstance ( cls . _taskContext , BarrierTaskContext ) : cls . _taskContext = object . __new__ ( cls ) return cls . _taskContext\n\n\n        Summary:", "input": "", "output": "Internal function to get or create global BarrierTaskContext . We need to make sure BarrierTaskContext is returned from here because it is needed in python worker reuse scenario see SPARK - 25921 for more details ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _initialize ( cls , port , secret ) : cls . _port = port cls . _secret = secret\n\n\n        Summary:", "input": "", "output": "Initialize BarrierTaskContext other methods within BarrierTaskContext can only be called after BarrierTaskContext is initialized ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def barrier ( self ) : if self . _port is None or self . _secret is None : raise Exception ( \"Not supported to call barrier() before initialize \" + \"BarrierTaskContext.\" ) else : _load_from_socket ( self . _port , self . _secret )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getTaskInfos ( self ) : if self . _port is None or self . _secret is None : raise Exception ( \"Not supported to call getTaskInfos() before initialize \" + \"BarrierTaskContext.\" ) else : addresses = self . _localProperties . get ( \"addresses\" , \"\" ) return [ BarrierTaskInfo ( h . strip ( ) ) for h in addresses . split ( \",\" ) ]\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def since ( version ) : import re indent_p = re . compile ( r'\\n( +)' ) def deco ( f ) : indents = indent_p . findall ( f . __doc__ ) indent = ' ' * ( min ( len ( m ) for m in indents ) if indents else 0 ) f . __doc__ = f . __doc__ . rstrip ( ) + \"\\n\\n%s.. versionadded:: %s\" % ( indent , version ) return f return deco\n\n\n        Summary:", "input": "", "output": "A decorator that annotates a function to append the version of Spark the function was added ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy_func ( f , name = None , sinceversion = None , doc = None ) : # See # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python fn = types . FunctionType ( f . __code__ , f . __globals__ , name or f . __name__ , f . __defaults__ , f . __closure__ ) # in case f was given attrs (note this dict is a shallow copy): fn . __dict__ . update ( f . __dict__ ) if doc is not None : fn . __doc__ = doc if sinceversion is not None : fn = since ( sinceversion ) ( fn ) return fn\n\n\n        Summary:", "input": "", "output": "Returns a function with same code globals defaults closure and name ( or provide a new name ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def keyword_only ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if len ( args ) > 0 : raise TypeError ( \"Method %s forces keyword arguments.\" % func . __name__ ) self . _input_kwargs = kwargs return func ( self , * * kwargs ) return wrapper\n\n\n        Summary:", "input": "", "output": "A decorator that forces keyword arguments in the wrapped method and saves actual input keyword arguments in _input_kwargs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gen_param_header ( name , doc , defaultValueStr , typeConverter ) : template = '''class Has$Name(Params): \"\"\" Mixin for param $name: $doc \"\"\" $name = Param(Params._dummy(), \"$name\", \"$doc\", typeConverter=$typeConverter) def __init__(self): super(Has$Name, self).__init__()''' if defaultValueStr is not None : template += ''' self._setDefault($name=$defaultValueStr)''' Name = name [ 0 ] . upper ( ) + name [ 1 : ] if typeConverter is None : typeConverter = str ( None ) return template . replace ( \"$name\" , name ) . replace ( \"$Name\" , Name ) . replace ( \"$doc\" , doc ) . replace ( \"$defaultValueStr\" , str ( defaultValueStr ) ) . replace ( \"$typeConverter\" , typeConverter )\n\n\n        Summary:", "input": "", "output": "Generates the header part for shared variables"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gen_param_code ( name , doc , defaultValueStr ) : # TODO: How to correctly inherit instance attributes? template = ''' def set$Name(self, value): \"\"\" Sets the value of :py:attr:`$name`. \"\"\" return self._set($name=value) def get$Name(self): \"\"\" Gets the value of $name or its default value. \"\"\" return self.getOrDefault(self.$name)''' Name = name [ 0 ] . upper ( ) + name [ 1 : ] return template . replace ( \"$name\" , name ) . replace ( \"$Name\" , Name ) . replace ( \"$doc\" , doc ) . replace ( \"$defaultValueStr\" , str ( defaultValueStr ) )\n\n\n        Summary:", "input": "", "output": "Generates Python code for a shared param class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( self , rdd , k = 4 , maxIterations = 20 , minDivisibleClusterSize = 1.0 , seed = - 1888008604 ) : java_model = callMLlibFunc ( \"trainBisectingKMeans\" , rdd . map ( _convert_to_vector ) , k , maxIterations , minDivisibleClusterSize , seed ) return BisectingKMeansModel ( java_model )\n\n\n        Summary:", "input": "", "output": "Runs the bisecting k - means algorithm return the model ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , rdd , k , maxIterations = 100 , runs = 1 , initializationMode = \"k-means||\" , seed = None , initializationSteps = 2 , epsilon = 1e-4 , initialModel = None ) : if runs != 1 : warnings . warn ( \"The param `runs` has no effect since Spark 2.0.0.\" ) clusterInitialModel = [ ] if initialModel is not None : if not isinstance ( initialModel , KMeansModel ) : raise Exception ( \"initialModel is of \" + str ( type ( initialModel ) ) + \". It needs \" \"to be of <type 'KMeansModel'>\" ) clusterInitialModel = [ _convert_to_vector ( c ) for c in initialModel . clusterCenters ] model = callMLlibFunc ( \"trainKMeansModel\" , rdd . map ( _convert_to_vector ) , k , maxIterations , runs , initializationMode , seed , initializationSteps , epsilon , clusterInitialModel ) centers = callJavaFunc ( rdd . context , model . clusterCenters ) return KMeansModel ( [ c . toArray ( ) for c in centers ] )\n\n\n        Summary:", "input": "", "output": "Train a k - means clustering model ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , rdd , k , convergenceTol = 1e-3 , maxIterations = 100 , seed = None , initialModel = None ) : initialModelWeights = None initialModelMu = None initialModelSigma = None if initialModel is not None : if initialModel . k != k : raise Exception ( \"Mismatched cluster count, initialModel.k = %s, however k = %s\" % ( initialModel . k , k ) ) initialModelWeights = list ( initialModel . weights ) initialModelMu = [ initialModel . gaussians [ i ] . mu for i in range ( initialModel . k ) ] initialModelSigma = [ initialModel . gaussians [ i ] . sigma for i in range ( initialModel . k ) ] java_model = callMLlibFunc ( \"trainGaussianMixtureModel\" , rdd . map ( _convert_to_vector ) , k , convergenceTol , maxIterations , seed , initialModelWeights , initialModelMu , initialModelSigma ) return GaussianMixtureModel ( java_model )\n\n\n        Summary:", "input": "", "output": "Train a Gaussian Mixture clustering model ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( cls , sc , path ) : model = cls . _load_java ( sc , path ) wrapper = sc . _jvm . org . apache . spark . mllib . api . python . PowerIterationClusteringModelWrapper ( model ) return PowerIterationClusteringModel ( wrapper )\n\n\n        Summary:", "input": "", "output": "Load a model from the given path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , rdd , k , maxIterations = 100 , initMode = \"random\" ) : model = callMLlibFunc ( \"trainPowerIterationClusteringModel\" , rdd . map ( _convert_to_vector ) , int ( k ) , int ( maxIterations ) , initMode ) return PowerIterationClusteringModel ( model )\n\n\n        Summary:", "input": "", "output": "r : param rdd : An RDD of ( i j s \\ : sub : ij \\ ) tuples representing the affinity matrix which is the matrix A in the PIC paper . The similarity s \\ : sub : ij \\ must be nonnegative . This is a symmetric matrix and hence s \\ : sub : ij \\ = s \\ : sub : ji \\ For any ( i j ) with nonzero similarity there should be either ( i j s \\ : sub : ij \\ ) or ( j i s \\ : sub : ji \\ ) in the input . Tuples with i = j are ignored because it is assumed s \\ : sub : ij \\ = 0 . 0 . : param k : Number of clusters . : param maxIterations : Maximum number of iterations of the PIC algorithm . ( default : 100 ) : param initMode : Initialization mode . This can be either random to use a random vector as vertex properties or degree to use normalized sum similarities . ( default : random )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update ( self , data , decayFactor , timeUnit ) : if not isinstance ( data , RDD ) : raise TypeError ( \"Data should be of an RDD, got %s.\" % type ( data ) ) data = data . map ( _convert_to_vector ) decayFactor = float ( decayFactor ) if timeUnit not in [ \"batches\" , \"points\" ] : raise ValueError ( \"timeUnit should be 'batches' or 'points', got %s.\" % timeUnit ) vectorCenters = [ _convert_to_vector ( center ) for center in self . centers ] updatedModel = callMLlibFunc ( \"updateStreamingKMeansModel\" , vectorCenters , self . _clusterWeights , data , decayFactor , timeUnit ) self . centers = array ( updatedModel [ 0 ] ) self . _clusterWeights = list ( updatedModel [ 1 ] ) return self\n\n\n        Summary:", "input": "", "output": "Update the centroids according to data"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setHalfLife ( self , halfLife , timeUnit ) : self . _timeUnit = timeUnit self . _decayFactor = exp ( log ( 0.5 ) / halfLife ) return self\n\n\n        Summary:", "input": "", "output": "Set number of batches after which the centroids of that particular batch has half the weightage ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setInitialCenters ( self , centers , weights ) : self . _model = StreamingKMeansModel ( centers , weights ) return self\n\n\n        Summary:", "input": "", "output": "Set initial centers . Should be set before calling trainOn ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setRandomCenters ( self , dim , weight , seed ) : rng = random . RandomState ( seed ) clusterCenters = rng . randn ( self . _k , dim ) clusterWeights = tile ( weight , self . _k ) self . _model = StreamingKMeansModel ( clusterCenters , clusterWeights ) return self\n\n\n        Summary:", "input": "", "output": "Set the initial centres to be random samples from a gaussian population with constant weights ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trainOn ( self , dstream ) : self . _validate ( dstream ) def update ( rdd ) : self . _model . update ( rdd , self . _decayFactor , self . _timeUnit ) dstream . foreachRDD ( update )\n\n\n        Summary:", "input": "", "output": "Train the model on the incoming dstream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predictOn ( self , dstream ) : self . _validate ( dstream ) return dstream . map ( lambda x : self . _model . predict ( x ) )\n\n\n        Summary:", "input": "", "output": "Make predictions on a dstream . Returns a transformed dstream object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predictOnValues ( self , dstream ) : self . _validate ( dstream ) return dstream . mapValues ( lambda x : self . _model . predict ( x ) )\n\n\n        Summary:", "input": "", "output": "Make predictions on a keyed dstream . Returns a transformed dstream object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def describeTopics ( self , maxTermsPerTopic = None ) : if maxTermsPerTopic is None : topics = self . call ( \"describeTopics\" ) else : topics = self . call ( \"describeTopics\" , maxTermsPerTopic ) return topics\n\n\n        Summary:", "input": "", "output": "Return the topics described by weighted terms ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( cls , sc , path ) : if not isinstance ( sc , SparkContext ) : raise TypeError ( \"sc should be a SparkContext, got type %s\" % type ( sc ) ) if not isinstance ( path , basestring ) : raise TypeError ( \"path should be a basestring, got type %s\" % type ( path ) ) model = callMLlibFunc ( \"loadLDAModel\" , sc , path ) return LDAModel ( model )\n\n\n        Summary:", "input": "", "output": "Load the LDAModel from disk ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , rdd , k = 10 , maxIterations = 20 , docConcentration = - 1.0 , topicConcentration = - 1.0 , seed = None , checkpointInterval = 10 , optimizer = \"em\" ) : model = callMLlibFunc ( \"trainLDAModel\" , rdd , k , maxIterations , docConcentration , topicConcentration , seed , checkpointInterval , optimizer ) return LDAModel ( model )\n\n\n        Summary:", "input": "", "output": "Train a LDA model ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java_object_rdd ( rdd ) : rdd = rdd . _reserialize ( AutoBatchedSerializer ( PickleSerializer ( ) ) ) return rdd . ctx . _jvm . org . apache . spark . mllib . api . python . SerDe . pythonToJava ( rdd . _jrdd , True )\n\n\n        Summary:", "input": "", "output": "Return a JavaRDD of Object by unpickling"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _py2java ( sc , obj ) : if isinstance ( obj , RDD ) : obj = _to_java_object_rdd ( obj ) elif isinstance ( obj , DataFrame ) : obj = obj . _jdf elif isinstance ( obj , SparkContext ) : obj = obj . _jsc elif isinstance ( obj , list ) : obj = [ _py2java ( sc , x ) for x in obj ] elif isinstance ( obj , JavaObject ) : pass elif isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : pass else : data = bytearray ( PickleSerializer ( ) . dumps ( obj ) ) obj = sc . _jvm . org . apache . spark . mllib . api . python . SerDe . loads ( data ) return obj\n\n\n        Summary:", "input": "", "output": "Convert Python object into Java"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def callJavaFunc ( sc , func , * args ) : args = [ _py2java ( sc , a ) for a in args ] return _java2py ( sc , func ( * args ) )\n\n\n        Summary:", "input": "", "output": "Call Java Function"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def callMLlibFunc ( name , * args ) : sc = SparkContext . getOrCreate ( ) api = getattr ( sc . _jvm . PythonMLLibAPI ( ) , name ) return callJavaFunc ( sc , api , * args )\n\n\n        Summary:", "input": "", "output": "Call API in PythonMLLibAPI"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def inherit_doc ( cls ) : for name , func in vars ( cls ) . items ( ) : # only inherit docstring for public functions if name . startswith ( \"_\" ) : continue if not func . __doc__ : for parent in cls . __bases__ : parent_func = getattr ( parent , name , None ) if parent_func and getattr ( parent_func , \"__doc__\" , None ) : func . __doc__ = parent_func . __doc__ break return cls\n\n\n        Summary:", "input": "", "output": "A decorator that makes a class inherit documentation from its parents ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def call ( self , name , * a ) : return callJavaFunc ( self . _sc , getattr ( self . _java_model , name ) , * a )\n\n\n        Summary:", "input": "", "output": "Call method of java_model"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def count ( self ) : return self . mapPartitions ( lambda i : [ sum ( 1 for _ in i ) ] ) . reduce ( operator . add )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def filter ( self , f ) : def func ( iterator ) : return filter ( f , iterator ) return self . mapPartitions ( func , True )\n\n\n        Summary:", "input": "", "output": "Return a new DStream containing only the elements that satisfy predicate ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map ( self , f , preservesPartitioning = False ) : def func ( iterator ) : return map ( f , iterator ) return self . mapPartitions ( func , preservesPartitioning )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying a function to each element of DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mapPartitionsWithIndex ( self , f , preservesPartitioning = False ) : return self . transform ( lambda rdd : rdd . mapPartitionsWithIndex ( f , preservesPartitioning ) )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD is generated by applying mapPartitionsWithIndex () to each RDDs of this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduce ( self , func ) : return self . map ( lambda x : ( None , x ) ) . reduceByKey ( func , 1 ) . map ( lambda x : x [ 1 ] )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD has a single element generated by reducing each RDD of this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduceByKey ( self , func , numPartitions = None ) : if numPartitions is None : numPartitions = self . _sc . defaultParallelism return self . combineByKey ( lambda x : x , func , func , numPartitions )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying reduceByKey to each RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combineByKey ( self , createCombiner , mergeValue , mergeCombiners , numPartitions = None ) : if numPartitions is None : numPartitions = self . _sc . defaultParallelism def func ( rdd ) : return rdd . combineByKey ( createCombiner , mergeValue , mergeCombiners , numPartitions ) return self . transform ( func )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying combineByKey to each RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def partitionBy ( self , numPartitions , partitionFunc = portable_hash ) : return self . transform ( lambda rdd : rdd . partitionBy ( numPartitions , partitionFunc ) )\n\n\n        Summary:", "input": "", "output": "Return a copy of the DStream in which each RDD are partitioned using the specified partitioner ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def foreachRDD ( self , func ) : if func . __code__ . co_argcount == 1 : old_func = func func = lambda t , rdd : old_func ( rdd ) jfunc = TransformFunction ( self . _sc , func , self . _jrdd_deserializer ) api = self . _ssc . _jvm . PythonDStream api . callForeachRDD ( self . _jdstream , jfunc )\n\n\n        Summary:", "input": "", "output": "Apply a function to each RDD in this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pprint ( self , num = 10 ) : def takeAndPrint ( time , rdd ) : taken = rdd . take ( num + 1 ) print ( \"-------------------------------------------\" ) print ( \"Time: %s\" % time ) print ( \"-------------------------------------------\" ) for record in taken [ : num ] : print ( record ) if len ( taken ) > num : print ( \"...\" ) print ( \"\" ) self . foreachRDD ( takeAndPrint )\n\n\n        Summary:", "input": "", "output": "Print the first num elements of each RDD generated in this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def persist ( self , storageLevel ) : self . is_cached = True javaStorageLevel = self . _sc . _getJavaStorageLevel ( storageLevel ) self . _jdstream . persist ( javaStorageLevel ) return self\n\n\n        Summary:", "input": "", "output": "Persist the RDDs of this DStream with the given storage level"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def checkpoint ( self , interval ) : self . is_checkpointed = True self . _jdstream . checkpoint ( self . _ssc . _jduration ( interval ) ) return self\n\n\n        Summary:", "input": "", "output": "Enable periodic checkpointing of RDDs of this DStream"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupByKey ( self , numPartitions = None ) : if numPartitions is None : numPartitions = self . _sc . defaultParallelism return self . transform ( lambda rdd : rdd . groupByKey ( numPartitions ) )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying groupByKey on each RDD ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countByValue ( self ) : return self . map ( lambda x : ( x , 1 ) ) . reduceByKey ( lambda x , y : x + y )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsTextFiles ( self , prefix , suffix = None ) : def saveAsTextFile ( t , rdd ) : path = rddToFileName ( prefix , suffix , t ) try : rdd . saveAsTextFile ( path ) except Py4JJavaError as e : # after recovered from checkpointing, the foreachRDD may # be called twice if 'FileAlreadyExistsException' not in str ( e ) : raise return self . foreachRDD ( saveAsTextFile )\n\n\n        Summary:", "input": "", "output": "Save each RDD in this DStream as at text file using string representation of elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , func ) : if func . __code__ . co_argcount == 1 : oldfunc = func func = lambda t , rdd : oldfunc ( rdd ) assert func . __code__ . co_argcount == 2 , \"func should take one or two arguments\" return TransformedDStream ( self , func )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD is generated by applying a function on each RDD of this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transformWith ( self , func , other , keepSerializer = False ) : if func . __code__ . co_argcount == 2 : oldfunc = func func = lambda t , a , b : oldfunc ( a , b ) assert func . __code__ . co_argcount == 3 , \"func should take two or three arguments\" jfunc = TransformFunction ( self . _sc , func , self . _jrdd_deserializer , other . _jrdd_deserializer ) dstream = self . _sc . _jvm . PythonTransformed2DStream ( self . _jdstream . dstream ( ) , other . _jdstream . dstream ( ) , jfunc ) jrdd_serializer = self . _jrdd_deserializer if keepSerializer else self . _sc . serializer return DStream ( dstream . asJavaDStream ( ) , self . _ssc , jrdd_serializer )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD is generated by applying a function on each RDD of this DStream and other DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , other ) : if self . _slideDuration != other . _slideDuration : raise ValueError ( \"the two DStream should have same slide duration\" ) return self . transformWith ( lambda a , b : a . union ( b ) , other , True )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by unifying data of another DStream with this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cogroup ( self , other , numPartitions = None ) : if numPartitions is None : numPartitions = self . _sc . defaultParallelism return self . transformWith ( lambda a , b : a . cogroup ( b , numPartitions ) , other )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying cogroup between RDDs of this DStream and other DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _jtime ( self , timestamp ) : if isinstance ( timestamp , datetime ) : timestamp = time . mktime ( timestamp . timetuple ( ) ) return self . _sc . _jvm . Time ( long ( timestamp * 1000 ) )\n\n\n        Summary:", "input": "", "output": "Convert datetime or unix_timestamp into Time"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def slice ( self , begin , end ) : jrdds = self . _jdstream . slice ( self . _jtime ( begin ) , self . _jtime ( end ) ) return [ RDD ( jrdd , self . _sc , self . _jrdd_deserializer ) for jrdd in jrdds ]\n\n\n        Summary:", "input": "", "output": "Return all the RDDs between begin to end ( both included )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def window ( self , windowDuration , slideDuration = None ) : self . _validate_window_param ( windowDuration , slideDuration ) d = self . _ssc . _jduration ( windowDuration ) if slideDuration is None : return DStream ( self . _jdstream . window ( d ) , self . _ssc , self . _jrdd_deserializer ) s = self . _ssc . _jduration ( slideDuration ) return DStream ( self . _jdstream . window ( d , s ) , self . _ssc , self . _jrdd_deserializer )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduceByWindow ( self , reduceFunc , invReduceFunc , windowDuration , slideDuration ) : keyed = self . map ( lambda x : ( 1 , x ) ) reduced = keyed . reduceByKeyAndWindow ( reduceFunc , invReduceFunc , windowDuration , slideDuration , 1 ) return reduced . map ( lambda kv : kv [ 1 ] )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countByWindow ( self , windowDuration , slideDuration ) : return self . map ( lambda x : 1 ) . reduceByWindow ( operator . add , operator . sub , windowDuration , slideDuration )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD has a single element generated by counting the number of elements in a window over this DStream . windowDuration and slideDuration are as defined in the window () operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countByValueAndWindow ( self , windowDuration , slideDuration , numPartitions = None ) : keyed = self . map ( lambda x : ( x , 1 ) ) counted = keyed . reduceByKeyAndWindow ( operator . add , operator . sub , windowDuration , slideDuration , numPartitions ) return counted . filter ( lambda kv : kv [ 1 ] > 0 )\n\n\n        Summary:", "input": "", "output": "Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupByKeyAndWindow ( self , windowDuration , slideDuration , numPartitions = None ) : ls = self . mapValues ( lambda x : [ x ] ) grouped = ls . reduceByKeyAndWindow ( lambda a , b : a . extend ( b ) or a , lambda a , b : a [ len ( b ) : ] , windowDuration , slideDuration , numPartitions ) return grouped . mapValues ( ResultIterable )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying groupByKey over a sliding window . Similar to DStream . groupByKey () but applies it over a sliding window ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reduceByKeyAndWindow ( self , func , invFunc , windowDuration , slideDuration = None , numPartitions = None , filterFunc = None ) : self . _validate_window_param ( windowDuration , slideDuration ) if numPartitions is None : numPartitions = self . _sc . defaultParallelism reduced = self . reduceByKey ( func , numPartitions ) if invFunc : def reduceFunc ( t , a , b ) : b = b . reduceByKey ( func , numPartitions ) r = a . union ( b ) . reduceByKey ( func , numPartitions ) if a else b if filterFunc : r = r . filter ( filterFunc ) return r def invReduceFunc ( t , a , b ) : b = b . reduceByKey ( func , numPartitions ) joined = a . leftOuterJoin ( b , numPartitions ) return joined . mapValues ( lambda kv : invFunc ( kv [ 0 ] , kv [ 1 ] ) if kv [ 1 ] is not None else kv [ 0 ] ) jreduceFunc = TransformFunction ( self . _sc , reduceFunc , reduced . _jrdd_deserializer ) jinvReduceFunc = TransformFunction ( self . _sc , invReduceFunc , reduced . _jrdd_deserializer ) if slideDuration is None : slideDuration = self . _slideDuration dstream = self . _sc . _jvm . PythonReducedWindowedDStream ( reduced . _jdstream . dstream ( ) , jreduceFunc , jinvReduceFunc , self . _ssc . _jduration ( windowDuration ) , self . _ssc . _jduration ( slideDuration ) ) return DStream ( dstream . asJavaDStream ( ) , self . _ssc , self . _sc . serializer ) else : return reduced . window ( windowDuration , slideDuration ) . reduceByKey ( func , numPartitions )\n\n\n        Summary:", "input": "", "output": "Return a new DStream by applying incremental reduceByKey over a sliding window ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def updateStateByKey ( self , updateFunc , numPartitions = None , initialRDD = None ) : if numPartitions is None : numPartitions = self . _sc . defaultParallelism if initialRDD and not isinstance ( initialRDD , RDD ) : initialRDD = self . _sc . parallelize ( initialRDD ) def reduceFunc ( t , a , b ) : if a is None : g = b . groupByKey ( numPartitions ) . mapValues ( lambda vs : ( list ( vs ) , None ) ) else : g = a . cogroup ( b . partitionBy ( numPartitions ) , numPartitions ) g = g . mapValues ( lambda ab : ( list ( ab [ 1 ] ) , list ( ab [ 0 ] ) [ 0 ] if len ( ab [ 0 ] ) else None ) ) state = g . mapValues ( lambda vs_s : updateFunc ( vs_s [ 0 ] , vs_s [ 1 ] ) ) return state . filter ( lambda k_v : k_v [ 1 ] is not None ) jreduceFunc = TransformFunction ( self . _sc , reduceFunc , self . _sc . serializer , self . _jrdd_deserializer ) if initialRDD : initialRDD = initialRDD . _reserialize ( self . _jrdd_deserializer ) dstream = self . _sc . _jvm . PythonStateDStream ( self . _jdstream . dstream ( ) , jreduceFunc , initialRDD . _jrdd ) else : dstream = self . _sc . _jvm . PythonStateDStream ( self . _jdstream . dstream ( ) , jreduceFunc ) return DStream ( dstream . asJavaDStream ( ) , self . _ssc , self . _sc . serializer )\n\n\n        Summary:", "input": "", "output": "Return a new state DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setParams ( self , minSupport = 0.3 , minConfidence = 0.8 , itemsCol = \"items\" , predictionCol = \"prediction\" , numPartitions = None ) : kwargs = self . _input_kwargs return self . _set ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "setParams ( self minSupport = 0 . 3 minConfidence = 0 . 8 itemsCol = items \\ predictionCol = prediction numPartitions = None )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setParams ( self , minSupport = 0.1 , maxPatternLength = 10 , maxLocalProjDBSize = 32000000 , sequenceCol = \"sequence\" ) : kwargs = self . _input_kwargs return self . _set ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "setParams ( self minSupport = 0 . 1 maxPatternLength = 10 maxLocalProjDBSize = 32000000 \\ sequenceCol = sequence )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def findFrequentSequentialPatterns ( self , dataset ) : self . _transfer_params_to_java ( ) jdf = self . _java_obj . findFrequentSequentialPatterns ( dataset . _jdf ) return DataFrame ( jdf , dataset . sql_ctx )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def first_spark_call ( ) : tb = traceback . extract_stack ( ) if len ( tb ) == 0 : return None file , line , module , what = tb [ len ( tb ) - 1 ] sparkpath = os . path . dirname ( file ) first_spark_frame = len ( tb ) - 1 for i in range ( 0 , len ( tb ) ) : file , line , fun , what = tb [ i ] if file . startswith ( sparkpath ) : first_spark_frame = i break if first_spark_frame == 0 : file , line , fun , what = tb [ 0 ] return CallSite ( function = fun , file = file , linenum = line ) sfile , sline , sfun , swhat = tb [ first_spark_frame ] ufile , uline , ufun , uwhat = tb [ first_spark_frame - 1 ] return CallSite ( function = sfun , file = ufile , linenum = uline )\n\n\n        Summary:", "input": "", "output": "Return a CallSite representing the first Spark call in the current call stack ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parsePoint ( line ) : values = [ float ( s ) for s in line . split ( ' ' ) ] if values [ 0 ] == - 1 : # Convert -1 labels to 0 for MLlib values [ 0 ] = 0 return LabeledPoint ( values [ 0 ] , values [ 1 : ] )\n\n\n        Summary:", "input": "", "output": "Parse a line of text into an MLlib LabeledPoint object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fMeasure ( self , label , beta = None ) : if beta is None : return self . call ( \"fMeasure\" , label ) else : return self . call ( \"fMeasure\" , label , beta )\n\n\n        Summary:", "input": "", "output": "Returns f - measure ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def precision ( self , label = None ) : if label is None : return self . call ( \"precision\" ) else : return self . call ( \"precision\" , float ( label ) )\n\n\n        Summary:", "input": "", "output": "Returns precision or precision for a given label ( category ) if specified ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def recall ( self , label = None ) : if label is None : return self . call ( \"recall\" ) else : return self . call ( \"recall\" , float ( label ) )\n\n\n        Summary:", "input": "", "output": "Returns recall or recall for a given label ( category ) if specified ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def f1Measure ( self , label = None ) : if label is None : return self . call ( \"f1Measure\" ) else : return self . call ( \"f1Measure\" , float ( label ) )\n\n\n        Summary:", "input": "", "output": "Returns f1Measure or f1Measure for a given label ( category ) if specified ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_corrected_pandas_type ( dt ) : import numpy as np if type ( dt ) == ByteType : return np . int8 elif type ( dt ) == ShortType : return np . int16 elif type ( dt ) == IntegerType : return np . int32 elif type ( dt ) == FloatType : return np . float32 else : return None\n\n\n        Summary:", "input": "", "output": "When converting Spark SQL records to Pandas DataFrame the inferred data type may be wrong . This method gets the corrected data type for Pandas if that type may be inferred uncorrectly ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rdd ( self ) : if self . _lazy_rdd is None : jrdd = self . _jdf . javaToPython ( ) self . _lazy_rdd = RDD ( jrdd , self . sql_ctx . _sc , BatchedSerializer ( PickleSerializer ( ) ) ) return self . _lazy_rdd\n\n\n        Summary:", "input": "", "output": "Returns the content as an : class : pyspark . RDD of : class : Row ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toJSON ( self , use_unicode = True ) : rdd = self . _jdf . toJSON ( ) return RDD ( rdd . toJavaRDD ( ) , self . _sc , UTF8Deserializer ( use_unicode ) )\n\n\n        Summary:", "input": "", "output": "Converts a : class : DataFrame into a : class : RDD of string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def schema ( self ) : if self . _schema is None : try : self . _schema = _parse_datatype_json_string ( self . _jdf . schema ( ) . json ( ) ) except AttributeError as e : raise Exception ( \"Unable to parse datatype from schema. %s\" % e ) return self . _schema\n\n\n        Summary:", "input": "", "output": "Returns the schema of this : class : DataFrame as a : class : pyspark . sql . types . StructType ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def explain ( self , extended = False ) : if extended : print ( self . _jdf . queryExecution ( ) . toString ( ) ) else : print ( self . _jdf . queryExecution ( ) . simpleString ( ) )\n\n\n        Summary:", "input": "", "output": "Prints the ( logical and physical ) plans to the console for debugging purpose ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def exceptAll ( self , other ) : return DataFrame ( self . _jdf . exceptAll ( other . _jdf ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Return a new : class : DataFrame containing rows in this : class : DataFrame but not in another : class : DataFrame while preserving duplicates ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def show ( self , n = 20 , truncate = True , vertical = False ) : if isinstance ( truncate , bool ) and truncate : print ( self . _jdf . showString ( n , 20 , vertical ) ) else : print ( self . _jdf . showString ( n , int ( truncate ) , vertical ) )\n\n\n        Summary:", "input": "", "output": "Prints the first n rows to the console ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _repr_html_ ( self ) : import cgi if not self . _support_repr_html : self . _support_repr_html = True if self . sql_ctx . _conf . isReplEagerEvalEnabled ( ) : max_num_rows = max ( self . sql_ctx . _conf . replEagerEvalMaxNumRows ( ) , 0 ) sock_info = self . _jdf . getRowsToPython ( max_num_rows , self . sql_ctx . _conf . replEagerEvalTruncate ( ) ) rows = list ( _load_from_socket ( sock_info , BatchedSerializer ( PickleSerializer ( ) ) ) ) head = rows [ 0 ] row_data = rows [ 1 : ] has_more_data = len ( row_data ) > max_num_rows row_data = row_data [ : max_num_rows ] html = \"<table border='1'>\\n\" # generate table head html += \"<tr><th>%s</th></tr>\\n\" % \"</th><th>\" . join ( map ( lambda x : cgi . escape ( x ) , head ) ) # generate table rows for row in row_data : html += \"<tr><td>%s</td></tr>\\n\" % \"</td><td>\" . join ( map ( lambda x : cgi . escape ( x ) , row ) ) html += \"</table>\\n\" if has_more_data : html += \"only showing top %d %s\\n\" % ( max_num_rows , \"row\" if max_num_rows == 1 else \"rows\" ) return html else : return None\n\n\n        Summary:", "input": "", "output": "Returns a dataframe with html code when you enabled eager evaluation by spark . sql . repl . eagerEval . enabled this only called by REPL you are using support eager evaluation with HTML ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def checkpoint ( self , eager = True ) : jdf = self . _jdf . checkpoint ( eager ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a checkpointed version of this Dataset . Checkpointing can be used to truncate the logical plan of this DataFrame which is especially useful in iterative algorithms where the plan may grow exponentially . It will be saved to files inside the checkpoint directory set with L { SparkContext . setCheckpointDir () } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def localCheckpoint ( self , eager = True ) : jdf = self . _jdf . localCheckpoint ( eager ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a locally checkpointed version of this Dataset . Checkpointing can be used to truncate the logical plan of this DataFrame which is especially useful in iterative algorithms where the plan may grow exponentially . Local checkpoints are stored in the executors using the caching subsystem and therefore they are not reliable ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def withWatermark ( self , eventTime , delayThreshold ) : if not eventTime or type ( eventTime ) is not str : raise TypeError ( \"eventTime should be provided as a string\" ) if not delayThreshold or type ( delayThreshold ) is not str : raise TypeError ( \"delayThreshold should be provided as a string interval\" ) jdf = self . _jdf . withWatermark ( eventTime , delayThreshold ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Defines an event time watermark for this : class : DataFrame . A watermark tracks a point in time before which we assume no more late data is going to arrive ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hint ( self , name , * parameters ) : if len ( parameters ) == 1 and isinstance ( parameters [ 0 ] , list ) : parameters = parameters [ 0 ] if not isinstance ( name , str ) : raise TypeError ( \"name should be provided as str, got {0}\" . format ( type ( name ) ) ) allowed_types = ( basestring , list , float , int ) for p in parameters : if not isinstance ( p , allowed_types ) : raise TypeError ( \"all parameters should be in {0}, got {1} of type {2}\" . format ( allowed_types , p , type ( p ) ) ) jdf = self . _jdf . hint ( name , self . _jseq ( parameters ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Specifies some hint on the current DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def collect ( self ) : with SCCallSiteSync ( self . _sc ) as css : sock_info = self . _jdf . collectToPython ( ) return list ( _load_from_socket ( sock_info , BatchedSerializer ( PickleSerializer ( ) ) ) )\n\n\n        Summary:", "input": "", "output": "Returns all the records as a list of : class : Row ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toLocalIterator ( self ) : with SCCallSiteSync ( self . _sc ) as css : sock_info = self . _jdf . toPythonIterator ( ) return _load_from_socket ( sock_info , BatchedSerializer ( PickleSerializer ( ) ) )\n\n\n        Summary:", "input": "", "output": "Returns an iterator that contains all of the rows in this : class : DataFrame . The iterator will consume as much memory as the largest partition in this DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def limit ( self , num ) : jdf = self . _jdf . limit ( num ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Limits the result count to the number specified ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def persist ( self , storageLevel = StorageLevel . MEMORY_AND_DISK ) : self . is_cached = True javaStorageLevel = self . _sc . _getJavaStorageLevel ( storageLevel ) self . _jdf . persist ( javaStorageLevel ) return self\n\n\n        Summary:", "input": "", "output": "Sets the storage level to persist the contents of the : class : DataFrame across operations after the first time it is computed . This can only be used to assign a new storage level if the : class : DataFrame does not have a storage level set yet . If no storage level is specified defaults to ( C { MEMORY_AND_DISK } ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def storageLevel ( self ) : java_storage_level = self . _jdf . storageLevel ( ) storage_level = StorageLevel ( java_storage_level . useDisk ( ) , java_storage_level . useMemory ( ) , java_storage_level . useOffHeap ( ) , java_storage_level . deserialized ( ) , java_storage_level . replication ( ) ) return storage_level\n\n\n        Summary:", "input": "", "output": "Get the : class : DataFrame s current storage level ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unpersist ( self , blocking = False ) : self . is_cached = False self . _jdf . unpersist ( blocking ) return self\n\n\n        Summary:", "input": "", "output": "Marks the : class : DataFrame as non - persistent and remove all blocks for it from memory and disk ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def coalesce ( self , numPartitions ) : return DataFrame ( self . _jdf . coalesce ( numPartitions ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame that has exactly numPartitions partitions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def repartition ( self , numPartitions , * cols ) : if isinstance ( numPartitions , int ) : if len ( cols ) == 0 : return DataFrame ( self . _jdf . repartition ( numPartitions ) , self . sql_ctx ) else : return DataFrame ( self . _jdf . repartition ( numPartitions , self . _jcols ( * cols ) ) , self . sql_ctx ) elif isinstance ( numPartitions , ( basestring , Column ) ) : cols = ( numPartitions , ) + cols return DataFrame ( self . _jdf . repartition ( self . _jcols ( * cols ) ) , self . sql_ctx ) else : raise TypeError ( \"numPartitions should be an int or Column\" )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame partitioned by the given partitioning expressions . The resulting DataFrame is hash partitioned ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sample ( self , withReplacement = None , fraction = None , seed = None ) : # For the cases below: # sample(True, 0.5 [, seed]) # sample(True, fraction=0.5 [, seed]) # sample(withReplacement=False, fraction=0.5 [, seed]) is_withReplacement_set = type ( withReplacement ) == bool and isinstance ( fraction , float ) # For the case below: # sample(faction=0.5 [, seed]) is_withReplacement_omitted_kwargs = withReplacement is None and isinstance ( fraction , float ) # For the case below: # sample(0.5 [, seed]) is_withReplacement_omitted_args = isinstance ( withReplacement , float ) if not ( is_withReplacement_set or is_withReplacement_omitted_kwargs or is_withReplacement_omitted_args ) : argtypes = [ str ( type ( arg ) ) for arg in [ withReplacement , fraction , seed ] if arg is not None ] raise TypeError ( \"withReplacement (optional), fraction (required) and seed (optional)\" \" should be a bool, float and number; however, \" \"got [%s].\" % \", \" . join ( argtypes ) ) if is_withReplacement_omitted_args : if fraction is not None : seed = fraction fraction = withReplacement withReplacement = None seed = long ( seed ) if seed is not None else None args = [ arg for arg in [ withReplacement , fraction , seed ] if arg is not None ] jdf = self . _jdf . sample ( * args ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a sampled subset of this : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sampleBy ( self , col , fractions , seed = None ) : if isinstance ( col , basestring ) : col = Column ( col ) elif not isinstance ( col , Column ) : raise ValueError ( \"col must be a string or a column, but got %r\" % type ( col ) ) if not isinstance ( fractions , dict ) : raise ValueError ( \"fractions must be a dict but got %r\" % type ( fractions ) ) for k , v in fractions . items ( ) : if not isinstance ( k , ( float , int , long , basestring ) ) : raise ValueError ( \"key must be float, int, long, or string, but got %r\" % type ( k ) ) fractions [ k ] = float ( v ) col = col . _jc seed = seed if seed is not None else random . randint ( 0 , sys . maxsize ) return DataFrame ( self . _jdf . stat ( ) . sampleBy ( col , self . _jmap ( fractions ) , seed ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a stratified sample without replacement based on the fraction given on each stratum ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def randomSplit ( self , weights , seed = None ) : for w in weights : if w < 0.0 : raise ValueError ( \"Weights must be positive. Found weight value: %s\" % w ) seed = seed if seed is not None else random . randint ( 0 , sys . maxsize ) rdd_array = self . _jdf . randomSplit ( _to_list ( self . sql_ctx . _sc , weights ) , long ( seed ) ) return [ DataFrame ( rdd , self . sql_ctx ) for rdd in rdd_array ]\n\n\n        Summary:", "input": "", "output": "Randomly splits this : class : DataFrame with the provided weights ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dtypes ( self ) : return [ ( str ( f . name ) , f . dataType . simpleString ( ) ) for f in self . schema . fields ]\n\n\n        Summary:", "input": "", "output": "Returns all column names and their data types as a list ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def colRegex ( self , colName ) : if not isinstance ( colName , basestring ) : raise ValueError ( \"colName should be provided as string\" ) jc = self . _jdf . colRegex ( colName ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Selects column based on the column name specified as a regex and returns it as : class : Column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def alias ( self , alias ) : assert isinstance ( alias , basestring ) , \"alias should be a string\" return DataFrame ( getattr ( self . _jdf , \"as\" ) ( alias ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame with an alias set ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def crossJoin ( self , other ) : jdf = self . _jdf . crossJoin ( other . _jdf ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns the cartesian product with another : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def join ( self , other , on = None , how = None ) : if on is not None and not isinstance ( on , list ) : on = [ on ] if on is not None : if isinstance ( on [ 0 ] , basestring ) : on = self . _jseq ( on ) else : assert isinstance ( on [ 0 ] , Column ) , \"on should be Column or list of Column\" on = reduce ( lambda x , y : x . __and__ ( y ) , on ) on = on . _jc if on is None and how is None : jdf = self . _jdf . join ( other . _jdf ) else : if how is None : how = \"inner\" if on is None : on = self . _jseq ( [ ] ) assert isinstance ( how , basestring ) , \"how should be basestring\" jdf = self . _jdf . join ( other . _jdf , on , how ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Joins with another : class : DataFrame using the given join expression ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sortWithinPartitions ( self , * cols , * * kwargs ) : jdf = self . _jdf . sortWithinPartitions ( self . _sort_cols ( cols , kwargs ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame with each partition sorted by the specified column ( s ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _jseq ( self , cols , converter = None ) : return _to_seq ( self . sql_ctx . _sc , cols , converter )\n\n\n        Summary:", "input": "", "output": "Return a JVM Seq of Columns from a list of Column or names"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _jcols ( self , * cols ) : if len ( cols ) == 1 and isinstance ( cols [ 0 ] , list ) : cols = cols [ 0 ] return self . _jseq ( cols , _to_java_column )\n\n\n        Summary:", "input": "", "output": "Return a JVM Seq of Columns from a list of Column or column names"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sort_cols ( self , cols , kwargs ) : if not cols : raise ValueError ( \"should sort by at least one column\" ) if len ( cols ) == 1 and isinstance ( cols [ 0 ] , list ) : cols = cols [ 0 ] jcols = [ _to_java_column ( c ) for c in cols ] ascending = kwargs . get ( 'ascending' , True ) if isinstance ( ascending , ( bool , int ) ) : if not ascending : jcols = [ jc . desc ( ) for jc in jcols ] elif isinstance ( ascending , list ) : jcols = [ jc if asc else jc . desc ( ) for asc , jc in zip ( ascending , jcols ) ] else : raise TypeError ( \"ascending can only be boolean or list, but got %s\" % type ( ascending ) ) return self . _jseq ( jcols )\n\n\n        Summary:", "input": "", "output": "Return a JVM Seq of Columns that describes the sort order"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def describe ( self , * cols ) : if len ( cols ) == 1 and isinstance ( cols [ 0 ] , list ) : cols = cols [ 0 ] jdf = self . _jdf . describe ( self . _jseq ( cols ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Computes basic statistics for numeric and string columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self , * statistics ) : if len ( statistics ) == 1 and isinstance ( statistics [ 0 ] , list ) : statistics = statistics [ 0 ] jdf = self . _jdf . summary ( self . _jseq ( statistics ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Computes specified statistics for numeric and string columns . Available statistics are : - count - mean - stddev - min - max - arbitrary approximate percentiles specified as a percentage ( eg 75% )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def head ( self , n = None ) : if n is None : rs = self . head ( 1 ) return rs [ 0 ] if rs else None return self . take ( n )\n\n\n        Summary:", "input": "", "output": "Returns the first n rows ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select ( self , * cols ) : jdf = self . _jdf . select ( self . _jcols ( * cols ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Projects a set of expressions and returns a new : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def selectExpr ( self , * expr ) : if len ( expr ) == 1 and isinstance ( expr [ 0 ] , list ) : expr = expr [ 0 ] jdf = self . _jdf . selectExpr ( self . _jseq ( expr ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Projects a set of SQL expressions and returns a new : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def filter ( self , condition ) : if isinstance ( condition , basestring ) : jdf = self . _jdf . filter ( condition ) elif isinstance ( condition , Column ) : jdf = self . _jdf . filter ( condition . _jc ) else : raise TypeError ( \"condition should be string or Column\" ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Filters rows using the given condition ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupBy ( self , * cols ) : jgd = self . _jdf . groupBy ( self . _jcols ( * cols ) ) from pyspark . sql . group import GroupedData return GroupedData ( jgd , self )\n\n\n        Summary:", "input": "", "output": "Groups the : class : DataFrame using the specified columns so we can run aggregation on them . See : class : GroupedData for all the available aggregate functions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , other ) : return DataFrame ( self . _jdf . union ( other . _jdf ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Return a new : class : DataFrame containing union of rows in this and another frame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unionByName ( self , other ) : return DataFrame ( self . _jdf . unionByName ( other . _jdf ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame containing union of rows in this and another frame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def intersect ( self , other ) : return DataFrame ( self . _jdf . intersect ( other . _jdf ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Return a new : class : DataFrame containing rows only in both this frame and another frame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def intersectAll ( self , other ) : return DataFrame ( self . _jdf . intersectAll ( other . _jdf ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Return a new : class : DataFrame containing rows in both this dataframe and other dataframe while preserving duplicates ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def subtract ( self , other ) : return DataFrame ( getattr ( self . _jdf , \"except\" ) ( other . _jdf ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Return a new : class : DataFrame containing rows in this frame but not in another frame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dropDuplicates ( self , subset = None ) : if subset is None : jdf = self . _jdf . dropDuplicates ( ) else : jdf = self . _jdf . dropDuplicates ( self . _jseq ( subset ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Return a new : class : DataFrame with duplicate rows removed optionally only considering certain columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dropna ( self , how = 'any' , thresh = None , subset = None ) : if how is not None and how not in [ 'any' , 'all' ] : raise ValueError ( \"how ('\" + how + \"') should be 'any' or 'all'\" ) if subset is None : subset = self . columns elif isinstance ( subset , basestring ) : subset = [ subset ] elif not isinstance ( subset , ( list , tuple ) ) : raise ValueError ( \"subset should be a list or tuple of column names\" ) if thresh is None : thresh = len ( subset ) if how == 'any' else 1 return DataFrame ( self . _jdf . na ( ) . drop ( thresh , self . _jseq ( subset ) ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame omitting rows with null values . : func : DataFrame . dropna and : func : DataFrameNaFunctions . drop are aliases of each other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fillna ( self , value , subset = None ) : if not isinstance ( value , ( float , int , long , basestring , bool , dict ) ) : raise ValueError ( \"value should be a float, int, long, string, bool or dict\" ) # Note that bool validates isinstance(int), but we don't want to # convert bools to floats if not isinstance ( value , bool ) and isinstance ( value , ( int , long ) ) : value = float ( value ) if isinstance ( value , dict ) : return DataFrame ( self . _jdf . na ( ) . fill ( value ) , self . sql_ctx ) elif subset is None : return DataFrame ( self . _jdf . na ( ) . fill ( value ) , self . sql_ctx ) else : if isinstance ( subset , basestring ) : subset = [ subset ] elif not isinstance ( subset , ( list , tuple ) ) : raise ValueError ( \"subset should be a list or tuple of column names\" ) return DataFrame ( self . _jdf . na ( ) . fill ( value , self . _jseq ( subset ) ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Replace null values alias for na . fill () . : func : DataFrame . fillna and : func : DataFrameNaFunctions . fill are aliases of each other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def replace ( self , to_replace , value = _NoValue , subset = None ) : if value is _NoValue : if isinstance ( to_replace , dict ) : value = None else : raise TypeError ( \"value argument is required when to_replace is not a dictionary.\" ) # Helper functions def all_of ( types ) : \"\"\"Given a type or tuple of types and a sequence of xs check if each x is instance of type(s) >>> all_of(bool)([True, False]) True >>> all_of(basestring)([\"a\", 1]) False \"\"\" def all_of_ ( xs ) : return all ( isinstance ( x , types ) for x in xs ) return all_of_ all_of_bool = all_of ( bool ) all_of_str = all_of ( basestring ) all_of_numeric = all_of ( ( float , int , long ) ) # Validate input types valid_types = ( bool , float , int , long , basestring , list , tuple ) if not isinstance ( to_replace , valid_types + ( dict , ) ) : raise ValueError ( \"to_replace should be a bool, float, int, long, string, list, tuple, or dict. \" \"Got {0}\" . format ( type ( to_replace ) ) ) if not isinstance ( value , valid_types ) and value is not None and not isinstance ( to_replace , dict ) : raise ValueError ( \"If to_replace is not a dict, value should be \" \"a bool, float, int, long, string, list, tuple or None. \" \"Got {0}\" . format ( type ( value ) ) ) if isinstance ( to_replace , ( list , tuple ) ) and isinstance ( value , ( list , tuple ) ) : if len ( to_replace ) != len ( value ) : raise ValueError ( \"to_replace and value lists should be of the same length. \" \"Got {0} and {1}\" . format ( len ( to_replace ) , len ( value ) ) ) if not ( subset is None or isinstance ( subset , ( list , tuple , basestring ) ) ) : raise ValueError ( \"subset should be a list or tuple of column names, \" \"column name or None. Got {0}\" . format ( type ( subset ) ) ) # Reshape input arguments if necessary if isinstance ( to_replace , ( float , int , long , basestring ) ) : to_replace = [ to_replace ] if isinstance ( to_replace , dict ) : rep_dict = to_replace if value is not None : warnings . warn ( \"to_replace is a dict and value is not None. value will be ignored.\" ) else : if isinstance ( value , ( float , int , long , basestring ) ) or value is None : value = [ value for _ in range ( len ( to_replace ) ) ] rep_dict = dict ( zip ( to_replace , value ) ) if isinstance ( subset , basestring ) : subset = [ subset ] # Verify we were not passed in mixed type generics. if not any ( all_of_type ( rep_dict . keys ( ) ) and all_of_type ( x for x in rep_dict . values ( ) if x is not None ) for all_of_type in [ all_of_bool , all_of_str , all_of_numeric ] ) : raise ValueError ( \"Mixed type replacements are not supported\" ) if subset is None : return DataFrame ( self . _jdf . na ( ) . replace ( '*' , rep_dict ) , self . sql_ctx ) else : return DataFrame ( self . _jdf . na ( ) . replace ( self . _jseq ( subset ) , self . _jmap ( rep_dict ) ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame replacing a value with another value . : func : DataFrame . replace and : func : DataFrameNaFunctions . replace are aliases of each other . Values to_replace and value must have the same type and can only be numerics booleans or strings . Value can have None . When replacing the new value will be cast to the type of the existing column . For numeric replacements all values to be replaced should have unique floating point representation . In case of conflicts ( for example with { 42 : - 1 42 . 0 : 1 } ) and arbitrary replacement will be used ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def approxQuantile ( self , col , probabilities , relativeError ) : if not isinstance ( col , ( basestring , list , tuple ) ) : raise ValueError ( \"col should be a string, list or tuple, but got %r\" % type ( col ) ) isStr = isinstance ( col , basestring ) if isinstance ( col , tuple ) : col = list ( col ) elif isStr : col = [ col ] for c in col : if not isinstance ( c , basestring ) : raise ValueError ( \"columns should be strings, but got %r\" % type ( c ) ) col = _to_list ( self . _sc , col ) if not isinstance ( probabilities , ( list , tuple ) ) : raise ValueError ( \"probabilities should be a list or tuple\" ) if isinstance ( probabilities , tuple ) : probabilities = list ( probabilities ) for p in probabilities : if not isinstance ( p , ( float , int , long ) ) or p < 0 or p > 1 : raise ValueError ( \"probabilities should be numerical (float, int, long) in [0,1].\" ) probabilities = _to_list ( self . _sc , probabilities ) if not isinstance ( relativeError , ( float , int , long ) ) or relativeError < 0 : raise ValueError ( \"relativeError should be numerical (float, int, long) >= 0.\" ) relativeError = float ( relativeError ) jaq = self . _jdf . stat ( ) . approxQuantile ( col , probabilities , relativeError ) jaq_list = [ list ( j ) for j in jaq ] return jaq_list [ 0 ] if isStr else jaq_list\n\n\n        Summary:", "input": "", "output": "Calculates the approximate quantiles of numerical columns of a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corr ( self , col1 , col2 , method = None ) : if not isinstance ( col1 , basestring ) : raise ValueError ( \"col1 should be a string.\" ) if not isinstance ( col2 , basestring ) : raise ValueError ( \"col2 should be a string.\" ) if not method : method = \"pearson\" if not method == \"pearson\" : raise ValueError ( \"Currently only the calculation of the Pearson Correlation \" + \"coefficient is supported.\" ) return self . _jdf . stat ( ) . corr ( col1 , col2 , method )\n\n\n        Summary:", "input": "", "output": "Calculates the correlation of two columns of a DataFrame as a double value . Currently only supports the Pearson Correlation Coefficient . : func : DataFrame . corr and : func : DataFrameStatFunctions . corr are aliases of each other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cov ( self , col1 , col2 ) : if not isinstance ( col1 , basestring ) : raise ValueError ( \"col1 should be a string.\" ) if not isinstance ( col2 , basestring ) : raise ValueError ( \"col2 should be a string.\" ) return self . _jdf . stat ( ) . cov ( col1 , col2 )\n\n\n        Summary:", "input": "", "output": "Calculate the sample covariance for the given columns specified by their names as a double value . : func : DataFrame . cov and : func : DataFrameStatFunctions . cov are aliases ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def crosstab ( self , col1 , col2 ) : if not isinstance ( col1 , basestring ) : raise ValueError ( \"col1 should be a string.\" ) if not isinstance ( col2 , basestring ) : raise ValueError ( \"col2 should be a string.\" ) return DataFrame ( self . _jdf . stat ( ) . crosstab ( col1 , col2 ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Computes a pair - wise frequency table of the given columns . Also known as a contingency table . The number of distinct values for each column should be less than 1e4 . At most 1e6 non - zero pair frequencies will be returned . The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2 . The name of the first column will be $col1_$col2 . Pairs that have no occurrences will have zero as their counts . : func : DataFrame . crosstab and : func : DataFrameStatFunctions . crosstab are aliases ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def freqItems ( self , cols , support = None ) : if isinstance ( cols , tuple ) : cols = list ( cols ) if not isinstance ( cols , list ) : raise ValueError ( \"cols must be a list or tuple of column names as strings.\" ) if not support : support = 0.01 return DataFrame ( self . _jdf . stat ( ) . freqItems ( _to_seq ( self . _sc , cols ) , support ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Finding frequent items for columns possibly with false positives . Using the frequent element count algorithm described in https : // doi . org / 10 . 1145 / 762471 . 762473 proposed by Karp Schenker and Papadimitriou . : func : DataFrame . freqItems and : func : DataFrameStatFunctions . freqItems are aliases ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def withColumn ( self , colName , col ) : assert isinstance ( col , Column ) , \"col should be Column\" return DataFrame ( self . _jdf . withColumn ( colName , col . _jc ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame by adding a column or replacing the existing column that has the same name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def withColumnRenamed ( self , existing , new ) : return DataFrame ( self . _jdf . withColumnRenamed ( existing , new ) , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame by renaming an existing column . This is a no - op if schema doesn t contain the given column name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def drop ( self , * cols ) : if len ( cols ) == 1 : col = cols [ 0 ] if isinstance ( col , basestring ) : jdf = self . _jdf . drop ( col ) elif isinstance ( col , Column ) : jdf = self . _jdf . drop ( col . _jc ) else : raise TypeError ( \"col should be a string or a Column\" ) else : for col in cols : if not isinstance ( col , basestring ) : raise TypeError ( \"each col in the param list should be a string\" ) jdf = self . _jdf . drop ( self . _jseq ( cols ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : DataFrame that drops the specified column . This is a no - op if schema doesn t contain the given column name ( s ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toDF ( self , * cols ) : jdf = self . _jdf . toDF ( self . _jseq ( cols ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Returns a new class : DataFrame that with new specified column names"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , func ) : result = func ( self ) assert isinstance ( result , DataFrame ) , \"Func returned an instance of type [%s], \" \"should have been DataFrame.\" % type ( result ) return result\n\n\n        Summary:", "input": "", "output": "Returns a new class : DataFrame . Concise syntax for chaining custom transformations ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toPandas ( self ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) import pandas as pd if self . sql_ctx . _conf . pandasRespectSessionTimeZone ( ) : timezone = self . sql_ctx . _conf . sessionLocalTimeZone ( ) else : timezone = None if self . sql_ctx . _conf . arrowEnabled ( ) : use_arrow = True try : from pyspark . sql . types import to_arrow_schema from pyspark . sql . utils import require_minimum_pyarrow_version require_minimum_pyarrow_version ( ) to_arrow_schema ( self . schema ) except Exception as e : if self . sql_ctx . _conf . arrowFallbackEnabled ( ) : msg = ( \"toPandas attempted Arrow optimization because \" \"'spark.sql.execution.arrow.enabled' is set to true; however, \" \"failed by the reason below:\\n %s\\n\" \"Attempting non-optimization as \" \"'spark.sql.execution.arrow.fallback.enabled' is set to \" \"true.\" % _exception_message ( e ) ) warnings . warn ( msg ) use_arrow = False else : msg = ( \"toPandas attempted Arrow optimization because \" \"'spark.sql.execution.arrow.enabled' is set to true, but has reached \" \"the error below and will not continue because automatic fallback \" \"with 'spark.sql.execution.arrow.fallback.enabled' has been set to \" \"false.\\n %s\" % _exception_message ( e ) ) warnings . warn ( msg ) raise # Try to use Arrow optimization when the schema is supported and the required version # of PyArrow is found, if 'spark.sql.execution.arrow.enabled' is enabled. if use_arrow : try : from pyspark . sql . types import _check_dataframe_localize_timestamps import pyarrow batches = self . _collectAsArrow ( ) if len ( batches ) > 0 : table = pyarrow . Table . from_batches ( batches ) # Pandas DataFrame created from PyArrow uses datetime64[ns] for date type # values, but we should use datetime.date to match the behavior with when # Arrow optimization is disabled. pdf = table . to_pandas ( date_as_object = True ) return _check_dataframe_localize_timestamps ( pdf , timezone ) else : return pd . DataFrame . from_records ( [ ] , columns = self . columns ) except Exception as e : # We might have to allow fallback here as well but multiple Spark jobs can # be executed. So, simply fail in this case for now. msg = ( \"toPandas attempted Arrow optimization because \" \"'spark.sql.execution.arrow.enabled' is set to true, but has reached \" \"the error below and can not continue. Note that \" \"'spark.sql.execution.arrow.fallback.enabled' does not have an effect \" \"on failures in the middle of computation.\\n %s\" % _exception_message ( e ) ) warnings . warn ( msg ) raise # Below is toPandas without Arrow optimization. pdf = pd . DataFrame . from_records ( self . collect ( ) , columns = self . columns ) dtype = { } for field in self . schema : pandas_type = _to_corrected_pandas_type ( field . dataType ) # SPARK-21766: if an integer field is nullable and has null values, it can be # inferred by pandas as float column. Once we convert the column with NaN back # to integer type e.g., np.int16, we will hit exception. So we use the inferred # float type, not the corrected type from the schema in this case. if pandas_type is not None and not ( isinstance ( field . dataType , IntegralType ) and field . nullable and pdf [ field . name ] . isnull ( ) . any ( ) ) : dtype [ field . name ] = pandas_type for f , t in dtype . items ( ) : pdf [ f ] = pdf [ f ] . astype ( t , copy = False ) if timezone is None : return pdf else : from pyspark . sql . types import _check_series_convert_timestamps_local_tz for field in self . schema : # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if isinstance ( field . dataType , TimestampType ) : pdf [ field . name ] = _check_series_convert_timestamps_local_tz ( pdf [ field . name ] , timezone ) return pdf\n\n\n        Summary:", "input": "", "output": "Returns the contents of this : class : DataFrame as Pandas pandas . DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _collectAsArrow ( self ) : with SCCallSiteSync ( self . _sc ) as css : sock_info = self . _jdf . collectAsArrowToPython ( ) # Collect list of un-ordered batches where last element is a list of correct order indices results = list ( _load_from_socket ( sock_info , ArrowCollectSerializer ( ) ) ) batches = results [ : - 1 ] batch_order = results [ - 1 ] # Re-order the batch list using the correct order return [ batches [ i ] for i in batch_order ]\n\n\n        Summary:", "input": "", "output": "Returns all records as a list of ArrowRecordBatches pyarrow must be installed and available on driver and worker Python environments ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asDict ( self , sample = False ) : return { 'count' : self . count ( ) , 'mean' : self . mean ( ) , 'sum' : self . sum ( ) , 'min' : self . min ( ) , 'max' : self . max ( ) , 'stdev' : self . stdev ( ) if sample else self . sampleStdev ( ) , 'variance' : self . variance ( ) if sample else self . sampleVariance ( ) }\n\n\n        Summary:", "input": "", "output": "Returns the : class : StatCounter members as a dict ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _list_function_infos ( jvm ) : jinfos = jvm . org . apache . spark . sql . api . python . PythonSQLUtils . listBuiltinFunctionInfos ( ) infos = [ ] for jinfo in jinfos : name = jinfo . getName ( ) usage = jinfo . getUsage ( ) usage = usage . replace ( \"_FUNC_\" , name ) if usage is not None else usage infos . append ( ExpressionInfo ( className = jinfo . getClassName ( ) , name = name , usage = usage , arguments = jinfo . getArguments ( ) . replace ( \"_FUNC_\" , name ) , examples = jinfo . getExamples ( ) . replace ( \"_FUNC_\" , name ) , note = jinfo . getNote ( ) , since = jinfo . getSince ( ) , deprecated = jinfo . getDeprecated ( ) ) ) return sorted ( infos , key = lambda i : i . name )\n\n\n        Summary:", "input": "", "output": "Returns a list of function information via JVM . Sorts wrapped expression infos by name and returns them ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_pretty_usage ( usage ) : if usage is not None and usage . strip ( ) != \"\" : usage = \"\\n\" . join ( map ( lambda u : u . strip ( ) , usage . split ( \"\\n\" ) ) ) return \"%s\\n\\n\" % usage\n\n\n        Summary:", "input": "", "output": "Makes the usage description pretty and returns a formatted string if usage is not an empty string . Otherwise returns None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_pretty_arguments ( arguments ) : if arguments . startswith ( \"\\n Arguments:\" ) : arguments = \"\\n\" . join ( map ( lambda u : u [ 6 : ] , arguments . strip ( ) . split ( \"\\n\" ) [ 1 : ] ) ) return \"**Arguments:**\\n\\n%s\\n\\n\" % arguments\n\n\n        Summary:", "input": "", "output": "Makes the arguments description pretty and returns a formatted string if arguments starts with the argument prefix . Otherwise returns None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_pretty_examples ( examples ) : if examples . startswith ( \"\\n Examples:\" ) : examples = \"\\n\" . join ( map ( lambda u : u [ 6 : ] , examples . strip ( ) . split ( \"\\n\" ) [ 1 : ] ) ) return \"**Examples:**\\n\\n```\\n%s\\n```\\n\\n\" % examples\n\n\n        Summary:", "input": "", "output": "Makes the examples description pretty and returns a formatted string if examples starts with the example prefix . Otherwise returns None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_pretty_note ( note ) : if note != \"\" : note = \"\\n\" . join ( map ( lambda n : n [ 4 : ] , note . split ( \"\\n\" ) ) ) return \"**Note:**\\n%s\\n\" % note\n\n\n        Summary:", "input": "", "output": "Makes the note description pretty and returns a formatted string if note is not an empty string . Otherwise returns None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_pretty_deprecated ( deprecated ) : if deprecated != \"\" : deprecated = \"\\n\" . join ( map ( lambda n : n [ 4 : ] , deprecated . split ( \"\\n\" ) ) ) return \"**Deprecated:**\\n%s\\n\" % deprecated\n\n\n        Summary:", "input": "", "output": "Makes the deprecated description pretty and returns a formatted string if deprecated is not an empty string . Otherwise returns None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate_sql_markdown ( jvm , path ) : with open ( path , 'w' ) as mdfile : for info in _list_function_infos ( jvm ) : name = info . name usage = _make_pretty_usage ( info . usage ) arguments = _make_pretty_arguments ( info . arguments ) examples = _make_pretty_examples ( info . examples ) note = _make_pretty_note ( info . note ) since = info . since deprecated = _make_pretty_deprecated ( info . deprecated ) mdfile . write ( \"### %s\\n\\n\" % name ) if usage is not None : mdfile . write ( \"%s\\n\\n\" % usage . strip ( ) ) if arguments is not None : mdfile . write ( arguments ) if examples is not None : mdfile . write ( examples ) if note is not None : mdfile . write ( note ) if since is not None and since != \"\" : mdfile . write ( \"**Since:** %s\\n\\n\" % since . strip ( ) ) if deprecated is not None : mdfile . write ( deprecated ) mdfile . write ( \"<br/>\\n\\n\" )\n\n\n        Summary:", "input": "", "output": "Generates a markdown file after listing the function information . The output file is created in path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predict ( self , x ) : if isinstance ( x , RDD ) : return x . map ( lambda v : self . predict ( v ) ) x = _convert_to_vector ( x ) if self . numClasses == 2 : margin = self . weights . dot ( x ) + self . _intercept if margin > 0 : prob = 1 / ( 1 + exp ( - margin ) ) else : exp_margin = exp ( margin ) prob = exp_margin / ( 1 + exp_margin ) if self . _threshold is None : return prob else : return 1 if prob > self . _threshold else 0 else : best_class = 0 max_margin = 0.0 if x . size + 1 == self . _dataWithBiasSize : for i in range ( 0 , self . _numClasses - 1 ) : margin = x . dot ( self . _weightsMatrix [ i ] [ 0 : x . size ] ) + self . _weightsMatrix [ i ] [ x . size ] if margin > max_margin : max_margin = margin best_class = i + 1 else : for i in range ( 0 , self . _numClasses - 1 ) : margin = x . dot ( self . _weightsMatrix [ i ] ) if margin > max_margin : max_margin = margin best_class = i + 1 return best_class\n\n\n        Summary:", "input": "", "output": "Predict values for a single data point or an RDD of points using the model trained ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save ( self , sc , path ) : java_model = sc . _jvm . org . apache . spark . mllib . classification . LogisticRegressionModel ( _py2java ( sc , self . _coeff ) , self . intercept , self . numFeatures , self . numClasses ) java_model . save ( sc . _jsc . sc ( ) , path )\n\n\n        Summary:", "input": "", "output": "Save this model to the given path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , data , iterations = 100 , initialWeights = None , regParam = 0.0 , regType = \"l2\" , intercept = False , corrections = 10 , tolerance = 1e-6 , validateData = True , numClasses = 2 ) : def train ( rdd , i ) : return callMLlibFunc ( \"trainLogisticRegressionModelWithLBFGS\" , rdd , int ( iterations ) , i , float ( regParam ) , regType , bool ( intercept ) , int ( corrections ) , float ( tolerance ) , bool ( validateData ) , int ( numClasses ) ) if initialWeights is None : if numClasses == 2 : initialWeights = [ 0.0 ] * len ( data . first ( ) . features ) else : if intercept : initialWeights = [ 0.0 ] * ( len ( data . first ( ) . features ) + 1 ) * ( numClasses - 1 ) else : initialWeights = [ 0.0 ] * len ( data . first ( ) . features ) * ( numClasses - 1 ) return _regression_train_wrapper ( train , LogisticRegressionModel , data , initialWeights )\n\n\n        Summary:", "input": "", "output": "Train a logistic regression model on the given data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predict ( self , x ) : if isinstance ( x , RDD ) : return x . map ( lambda v : self . predict ( v ) ) x = _convert_to_vector ( x ) margin = self . weights . dot ( x ) + self . intercept if self . _threshold is None : return margin else : return 1 if margin > self . _threshold else 0\n\n\n        Summary:", "input": "", "output": "Predict values for a single data point or an RDD of points using the model trained ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save ( self , sc , path ) : java_model = sc . _jvm . org . apache . spark . mllib . classification . SVMModel ( _py2java ( sc , self . _coeff ) , self . intercept ) java_model . save ( sc . _jsc . sc ( ) , path )\n\n\n        Summary:", "input": "", "output": "Save this model to the given path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( cls , sc , path ) : java_model = sc . _jvm . org . apache . spark . mllib . classification . SVMModel . load ( sc . _jsc . sc ( ) , path ) weights = _java2py ( sc , java_model . weights ( ) ) intercept = java_model . intercept ( ) threshold = java_model . getThreshold ( ) . get ( ) model = SVMModel ( weights , intercept ) model . setThreshold ( threshold ) return model\n\n\n        Summary:", "input": "", "output": "Load a model from the given path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , data , lambda_ = 1.0 ) : first = data . first ( ) if not isinstance ( first , LabeledPoint ) : raise ValueError ( \"`data` should be an RDD of LabeledPoint\" ) labels , pi , theta = callMLlibFunc ( \"trainNaiveBayesModel\" , data , lambda_ ) return NaiveBayesModel ( labels . toArray ( ) , pi . toArray ( ) , numpy . array ( theta ) )\n\n\n        Summary:", "input": "", "output": "Train a Naive Bayes model given an RDD of ( label features ) vectors ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def heappush ( heap , item ) : heap . append ( item ) _siftdown ( heap , 0 , len ( heap ) - 1 )\n\n\n        Summary:", "input": "", "output": "Push item onto heap maintaining the heap invariant ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def heappop ( heap ) : lastelt = heap . pop ( ) # raises appropriate IndexError if heap is empty if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt _siftup ( heap , 0 ) return returnitem return lastelt\n\n\n        Summary:", "input": "", "output": "Pop the smallest item off the heap maintaining the heap invariant ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def heapreplace ( heap , item ) : returnitem = heap [ 0 ] # raises appropriate IndexError if heap is empty heap [ 0 ] = item _siftup ( heap , 0 ) return returnitem\n\n\n        Summary:", "input": "", "output": "Pop and return the current smallest value and add the new item ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def heappushpop ( heap , item ) : if heap and heap [ 0 ] < item : item , heap [ 0 ] = heap [ 0 ] , item _siftup ( heap , 0 ) return item\n\n\n        Summary:", "input": "", "output": "Fast version of a heappush followed by a heappop ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def heapify ( x ) : n = len ( x ) # Transform bottom-up. The largest index there's any point to looking at # is the largest with a child index in-range, so must have 2*i + 1 < n, # or i < (n-1)/2. If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so # j-1 is the largest, which is n//2 - 1. If n is odd = 2*j+1, this is # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1. for i in reversed ( range ( n // 2 ) ) : _siftup ( x , i )\n\n\n        Summary:", "input": "", "output": "Transform list into a heap in - place in O ( len ( x )) time ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _heappop_max ( heap ) : lastelt = heap . pop ( ) # raises appropriate IndexError if heap is empty if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt _siftup_max ( heap , 0 ) return returnitem return lastelt\n\n\n        Summary:", "input": "", "output": "Maxheap version of a heappop ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _heapreplace_max ( heap , item ) : returnitem = heap [ 0 ] # raises appropriate IndexError if heap is empty heap [ 0 ] = item _siftup_max ( heap , 0 ) return returnitem\n\n\n        Summary:", "input": "", "output": "Maxheap version of a heappop followed by a heappush ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _heapify_max ( x ) : n = len ( x ) for i in reversed ( range ( n // 2 ) ) : _siftup_max ( x , i )\n\n\n        Summary:", "input": "", "output": "Transform list into a maxheap in - place in O ( len ( x )) time ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _siftdown_max ( heap , startpos , pos ) : newitem = heap [ pos ] # Follow the path to the root, moving parents down until finding a place # newitem fits. while pos > startpos : parentpos = ( pos - 1 ) >> 1 parent = heap [ parentpos ] if parent < newitem : heap [ pos ] = parent pos = parentpos continue break heap [ pos ] = newitem\n\n\n        Summary:", "input": "", "output": "Maxheap variant of _siftdown"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _siftup_max ( heap , pos ) : endpos = len ( heap ) startpos = pos newitem = heap [ pos ] # Bubble up the larger child until hitting a leaf. childpos = 2 * pos + 1 # leftmost child position while childpos < endpos : # Set childpos to index of larger child. rightpos = childpos + 1 if rightpos < endpos and not heap [ rightpos ] < heap [ childpos ] : childpos = rightpos # Move the larger child up. heap [ pos ] = heap [ childpos ] pos = childpos childpos = 2 * pos + 1 # The leaf at pos is empty now. Put newitem there, and bubble it up # to its final resting place (by sifting its parents down). heap [ pos ] = newitem _siftdown_max ( heap , startpos , pos )\n\n\n        Summary:", "input": "", "output": "Maxheap variant of _siftup"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def merge ( iterables , key = None , reverse = False ) : h = [ ] h_append = h . append if reverse : _heapify = _heapify_max _heappop = _heappop_max _heapreplace = _heapreplace_max direction = - 1 else : _heapify = heapify _heappop = heappop _heapreplace = heapreplace direction = 1 if key is None : for order , it in enumerate ( map ( iter , iterables ) ) : try : h_append ( [ next ( it ) , order * direction , it ] ) except StopIteration : pass _heapify ( h ) while len ( h ) > 1 : try : while True : value , order , it = s = h [ 0 ] yield value s [ 0 ] = next ( it ) # raises StopIteration when exhausted _heapreplace ( h , s ) # restore heap condition except StopIteration : _heappop ( h ) # remove empty iterator if h : # fast case when only a single iterator remains value , order , it = h [ 0 ] yield value for value in it : yield value return for order , it in enumerate ( map ( iter , iterables ) ) : try : value = next ( it ) h_append ( [ key ( value ) , order * direction , value , it ] ) except StopIteration : pass _heapify ( h ) while len ( h ) > 1 : try : while True : key_value , order , value , it = s = h [ 0 ] yield value value = next ( it ) s [ 0 ] = key ( value ) s [ 2 ] = value _heapreplace ( h , s ) except StopIteration : _heappop ( h ) if h : key_value , order , value , it = h [ 0 ] yield value for value in it : yield value\n\n\n        Summary:", "input": "", "output": "Merge multiple sorted inputs into a single sorted output ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nsmallest ( n , iterable , key = None ) : # Short-cut for n==1 is to use min() if n == 1 : it = iter ( iterable ) sentinel = object ( ) if key is None : result = min ( it , default = sentinel ) else : result = min ( it , default = sentinel , key = key ) return [ ] if result is sentinel else [ result ] # When n>=size, it's faster to use sorted() try : size = len ( iterable ) except ( TypeError , AttributeError ) : pass else : if n >= size : return sorted ( iterable , key = key ) [ : n ] # When key is none, use simpler decoration if key is None : it = iter ( iterable ) # put the range(n) first so that zip() doesn't # consume one too many elements from the iterator result = [ ( elem , i ) for i , elem in zip ( range ( n ) , it ) ] if not result : return result _heapify_max ( result ) top = result [ 0 ] [ 0 ] order = n _heapreplace = _heapreplace_max for elem in it : if elem < top : _heapreplace ( result , ( elem , order ) ) top = result [ 0 ] [ 0 ] order += 1 result . sort ( ) return [ r [ 0 ] for r in result ] # General case, slowest method it = iter ( iterable ) result = [ ( key ( elem ) , i , elem ) for i , elem in zip ( range ( n ) , it ) ] if not result : return result _heapify_max ( result ) top = result [ 0 ] [ 0 ] order = n _heapreplace = _heapreplace_max for elem in it : k = key ( elem ) if k < top : _heapreplace ( result , ( k , order , elem ) ) top = result [ 0 ] [ 0 ] order += 1 result . sort ( ) return [ r [ 2 ] for r in result ]\n\n\n        Summary:", "input": "", "output": "Find the n smallest elements in a dataset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nlargest ( n , iterable , key = None ) : # Short-cut for n==1 is to use max() if n == 1 : it = iter ( iterable ) sentinel = object ( ) if key is None : result = max ( it , default = sentinel ) else : result = max ( it , default = sentinel , key = key ) return [ ] if result is sentinel else [ result ] # When n>=size, it's faster to use sorted() try : size = len ( iterable ) except ( TypeError , AttributeError ) : pass else : if n >= size : return sorted ( iterable , key = key , reverse = True ) [ : n ] # When key is none, use simpler decoration if key is None : it = iter ( iterable ) result = [ ( elem , i ) for i , elem in zip ( range ( 0 , - n , - 1 ) , it ) ] if not result : return result heapify ( result ) top = result [ 0 ] [ 0 ] order = - n _heapreplace = heapreplace for elem in it : if top < elem : _heapreplace ( result , ( elem , order ) ) top = result [ 0 ] [ 0 ] order -= 1 result . sort ( reverse = True ) return [ r [ 0 ] for r in result ] # General case, slowest method it = iter ( iterable ) result = [ ( key ( elem ) , i , elem ) for i , elem in zip ( range ( 0 , - n , - 1 ) , it ) ] if not result : return result heapify ( result ) top = result [ 0 ] [ 0 ] order = - n _heapreplace = heapreplace for elem in it : k = key ( elem ) if top < k : _heapreplace ( result , ( k , order , elem ) ) top = result [ 0 ] [ 0 ] order -= 1 result . sort ( reverse = True ) return [ r [ 2 ] for r in result ]\n\n\n        Summary:", "input": "", "output": "Find the n largest elements in a dataset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corr ( dataset , column , method = \"pearson\" ) : sc = SparkContext . _active_spark_context javaCorrObj = _jvm ( ) . org . apache . spark . ml . stat . Correlation args = [ _py2java ( sc , arg ) for arg in ( dataset , column , method ) ] return _java2py ( sc , javaCorrObj . corr ( * args ) )\n\n\n        Summary:", "input": "", "output": "Compute the correlation matrix with specified method using dataset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def metrics ( * metrics ) : sc = SparkContext . _active_spark_context js = JavaWrapper . _new_java_obj ( \"org.apache.spark.ml.stat.Summarizer.metrics\" , _to_seq ( sc , metrics ) ) return SummaryBuilder ( js )\n\n\n        Summary:", "input": "", "output": "Given a list of metrics provides a builder that it turns computes metrics from a column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self , featuresCol , weightCol = None ) : featuresCol , weightCol = Summarizer . _check_param ( featuresCol , weightCol ) return Column ( self . _java_obj . summary ( featuresCol . _jc , weightCol . _jc ) )\n\n\n        Summary:", "input": "", "output": "Returns an aggregate object that contains the summary of the column with the requested metrics ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corr ( x , y = None , method = None ) : # Check inputs to determine whether a single value or a matrix is needed for output. # Since it's legal for users to use the method name as the second argument, we need to # check if y is used to specify the method name instead. if type ( y ) == str : raise TypeError ( \"Use 'method=' to specify method name.\" ) if not y : return callMLlibFunc ( \"corr\" , x . map ( _convert_to_vector ) , method ) . toArray ( ) else : return callMLlibFunc ( \"corr\" , x . map ( float ) , y . map ( float ) , method )\n\n\n        Summary:", "input": "", "output": "Compute the correlation ( matrix ) for the input RDD ( s ) using the specified method . Methods currently supported : I { pearson ( default ) spearman } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parallelFitTasks ( est , train , eva , validation , epm , collectSubModel ) : modelIter = est . fitMultiple ( train , epm ) def singleTask ( ) : index , model = next ( modelIter ) metric = eva . evaluate ( model . transform ( validation , epm [ index ] ) ) return index , metric , model if collectSubModel else None return [ singleTask ] * len ( epm )\n\n\n        Summary:", "input": "", "output": "Creates a list of callables which can be called from different threads to fit and evaluate an estimator in parallel . Each callable returns an ( index metric ) pair ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def baseOn ( self , * args ) : if isinstance ( args [ 0 ] , dict ) : self . baseOn ( * args [ 0 ] . items ( ) ) else : for ( param , value ) in args : self . addGrid ( param , [ value ] ) return self\n\n\n        Summary:", "input": "", "output": "Sets the given parameters in this grid to fixed values . Accepts either a parameter dictionary or a list of ( parameter value ) pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def build ( self ) : keys = self . _param_grid . keys ( ) grid_values = self . _param_grid . values ( ) def to_key_value_pairs ( keys , values ) : return [ ( key , key . typeConverter ( value ) ) for key , value in zip ( keys , values ) ] return [ dict ( to_key_value_pairs ( keys , prod ) ) for prod in itertools . product ( * grid_values ) ]\n\n\n        Summary:", "input": "", "output": "Builds and returns all combinations of parameters specified by the param grid ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_java_impl ( cls , java_stage ) : # Load information from java_stage to the instance. estimator = JavaParams . _from_java ( java_stage . getEstimator ( ) ) evaluator = JavaParams . _from_java ( java_stage . getEvaluator ( ) ) epms = [ estimator . _transfer_param_map_from_java ( epm ) for epm in java_stage . getEstimatorParamMaps ( ) ] return estimator , epms , evaluator\n\n\n        Summary:", "input": "", "output": "Return Python estimator estimatorParamMaps and evaluator from a Java ValidatorParams ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java_impl ( self ) : gateway = SparkContext . _gateway cls = SparkContext . _jvm . org . apache . spark . ml . param . ParamMap java_epms = gateway . new_array ( cls , len ( self . getEstimatorParamMaps ( ) ) ) for idx , epm in enumerate ( self . getEstimatorParamMaps ( ) ) : java_epms [ idx ] = self . getEstimator ( ) . _transfer_param_map_to_java ( epm ) java_estimator = self . getEstimator ( ) . _to_java ( ) java_evaluator = self . getEvaluator ( ) . _to_java ( ) return java_estimator , java_epms , java_evaluator\n\n\n        Summary:", "input": "", "output": "Return Java estimator estimatorParamMaps and evaluator from this Python instance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_java ( cls , java_stage ) : estimator , epms , evaluator = super ( CrossValidator , cls ) . _from_java_impl ( java_stage ) numFolds = java_stage . getNumFolds ( ) seed = java_stage . getSeed ( ) parallelism = java_stage . getParallelism ( ) collectSubModels = java_stage . getCollectSubModels ( ) # Create a new instance of this stage. py_stage = cls ( estimator = estimator , estimatorParamMaps = epms , evaluator = evaluator , numFolds = numFolds , seed = seed , parallelism = parallelism , collectSubModels = collectSubModels ) py_stage . _resetUid ( java_stage . uid ( ) ) return py_stage\n\n\n        Summary:", "input": "", "output": "Given a Java CrossValidator create and return a Python wrapper of it . Used for ML persistence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java ( self ) : estimator , epms , evaluator = super ( CrossValidator , self ) . _to_java_impl ( ) _java_obj = JavaParams . _new_java_obj ( \"org.apache.spark.ml.tuning.CrossValidator\" , self . uid ) _java_obj . setEstimatorParamMaps ( epms ) _java_obj . setEvaluator ( evaluator ) _java_obj . setEstimator ( estimator ) _java_obj . setSeed ( self . getSeed ( ) ) _java_obj . setNumFolds ( self . getNumFolds ( ) ) _java_obj . setParallelism ( self . getParallelism ( ) ) _java_obj . setCollectSubModels ( self . getCollectSubModels ( ) ) return _java_obj\n\n\n        Summary:", "input": "", "output": "Transfer this instance to a Java CrossValidator . Used for ML persistence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , extra = None ) : if extra is None : extra = dict ( ) bestModel = self . bestModel . copy ( extra ) avgMetrics = self . avgMetrics subModels = self . subModels return CrossValidatorModel ( bestModel , avgMetrics , subModels )\n\n\n        Summary:", "input": "", "output": "Creates a copy of this instance with a randomly generated uid and some extra params . This copies the underlying bestModel creates a deep copy of the embedded paramMap and copies the embedded and extra parameters over . It does not copy the extra Params into the subModels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setParams ( self , estimator = None , estimatorParamMaps = None , evaluator = None , trainRatio = 0.75 , parallelism = 1 , collectSubModels = False , seed = None ) : kwargs = self . _input_kwargs return self . _set ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "setParams ( self estimator = None estimatorParamMaps = None evaluator = None trainRatio = 0 . 75 \\ parallelism = 1 collectSubModels = False seed = None ) : Sets params for the train validation split ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , extra = None ) : if extra is None : extra = dict ( ) newTVS = Params . copy ( self , extra ) if self . isSet ( self . estimator ) : newTVS . setEstimator ( self . getEstimator ( ) . copy ( extra ) ) # estimatorParamMaps remain the same if self . isSet ( self . evaluator ) : newTVS . setEvaluator ( self . getEvaluator ( ) . copy ( extra ) ) return newTVS\n\n\n        Summary:", "input": "", "output": "Creates a copy of this instance with a randomly generated uid and some extra params . This copies creates a deep copy of the embedded paramMap and copies the embedded and extra parameters over ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_java ( cls , java_stage ) : estimator , epms , evaluator = super ( TrainValidationSplit , cls ) . _from_java_impl ( java_stage ) trainRatio = java_stage . getTrainRatio ( ) seed = java_stage . getSeed ( ) parallelism = java_stage . getParallelism ( ) collectSubModels = java_stage . getCollectSubModels ( ) # Create a new instance of this stage. py_stage = cls ( estimator = estimator , estimatorParamMaps = epms , evaluator = evaluator , trainRatio = trainRatio , seed = seed , parallelism = parallelism , collectSubModels = collectSubModels ) py_stage . _resetUid ( java_stage . uid ( ) ) return py_stage\n\n\n        Summary:", "input": "", "output": "Given a Java TrainValidationSplit create and return a Python wrapper of it . Used for ML persistence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java ( self ) : estimator , epms , evaluator = super ( TrainValidationSplit , self ) . _to_java_impl ( ) _java_obj = JavaParams . _new_java_obj ( \"org.apache.spark.ml.tuning.TrainValidationSplit\" , self . uid ) _java_obj . setEstimatorParamMaps ( epms ) _java_obj . setEvaluator ( evaluator ) _java_obj . setEstimator ( estimator ) _java_obj . setTrainRatio ( self . getTrainRatio ( ) ) _java_obj . setSeed ( self . getSeed ( ) ) _java_obj . setParallelism ( self . getParallelism ( ) ) _java_obj . setCollectSubModels ( self . getCollectSubModels ( ) ) return _java_obj\n\n\n        Summary:", "input": "", "output": "Transfer this instance to a Java TrainValidationSplit . Used for ML persistence . : return : Java object equivalent to this instance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , extra = None ) : if extra is None : extra = dict ( ) bestModel = self . bestModel . copy ( extra ) validationMetrics = list ( self . validationMetrics ) subModels = self . subModels return TrainValidationSplitModel ( bestModel , validationMetrics , subModels )\n\n\n        Summary:", "input": "", "output": "Creates a copy of this instance with a randomly generated uid and some extra params . This copies the underlying bestModel creates a deep copy of the embedded paramMap and copies the embedded and extra parameters over . And this creates a shallow copy of the validationMetrics . It does not copy the extra Params into the subModels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_java ( cls , java_stage ) : # Load information from java_stage to the instance. bestModel = JavaParams . _from_java ( java_stage . bestModel ( ) ) estimator , epms , evaluator = super ( TrainValidationSplitModel , cls ) . _from_java_impl ( java_stage ) # Create a new instance of this stage. py_stage = cls ( bestModel = bestModel ) . setEstimator ( estimator ) py_stage = py_stage . setEstimatorParamMaps ( epms ) . setEvaluator ( evaluator ) if java_stage . hasSubModels ( ) : py_stage . subModels = [ JavaParams . _from_java ( sub_model ) for sub_model in java_stage . subModels ( ) ] py_stage . _resetUid ( java_stage . uid ( ) ) return py_stage\n\n\n        Summary:", "input": "", "output": "Given a Java TrainValidationSplitModel create and return a Python wrapper of it . Used for ML persistence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java ( self ) : sc = SparkContext . _active_spark_context # TODO: persst validation metrics as well _java_obj = JavaParams . _new_java_obj ( \"org.apache.spark.ml.tuning.TrainValidationSplitModel\" , self . uid , self . bestModel . _to_java ( ) , _py2java ( sc , [ ] ) ) estimator , epms , evaluator = super ( TrainValidationSplitModel , self ) . _to_java_impl ( ) _java_obj . set ( \"evaluator\" , evaluator ) _java_obj . set ( \"estimator\" , estimator ) _java_obj . set ( \"estimatorParamMaps\" , epms ) if self . subModels is not None : java_sub_models = [ sub_model . _to_java ( ) for sub_model in self . subModels ] _java_obj . setSubModels ( java_sub_models ) return _java_obj\n\n\n        Summary:", "input": "", "output": "Transfer this instance to a Java TrainValidationSplitModel . Used for ML persistence . : return : Java object equivalent to this instance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( self , key , default = _NoValue ) : self . _checkType ( key , \"key\" ) if default is _NoValue : return self . _jconf . get ( key ) else : if default is not None : self . _checkType ( default , \"default\" ) return self . _jconf . get ( key , default )\n\n\n        Summary:", "input": "", "output": "Returns the value of Spark runtime configuration property for the given key assuming it is set ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _checkType ( self , obj , identifier ) : if not isinstance ( obj , basestring ) : raise TypeError ( \"expected %s '%s' to be a string (was '%s')\" % ( identifier , obj , type ( obj ) . __name__ ) )\n\n\n        Summary:", "input": "", "output": "Assert that an object is of type str ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_function ( name , doc = \"\" ) : def _ ( col ) : sc = SparkContext . _active_spark_context jc = getattr ( sc . _jvm . functions , name ) ( col . _jc if isinstance ( col , Column ) else col ) return Column ( jc ) _ . __name__ = name _ . __doc__ = doc return _\n\n\n        Summary:", "input": "", "output": "Create a PySpark function by its name"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_function_over_column ( name , doc = \"\" ) : def _ ( col ) : sc = SparkContext . _active_spark_context jc = getattr ( sc . _jvm . functions , name ) ( _to_java_column ( col ) ) return Column ( jc ) _ . __name__ = name _ . __doc__ = doc return _\n\n\n        Summary:", "input": "", "output": "Similar with _create_function but creates a PySpark function that takes a column ( as string as well ) . This is mainly for PySpark functions to take strings as column names ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_deprecated_function ( func , message ) : def _ ( col ) : warnings . warn ( message , DeprecationWarning ) return func ( col ) return functools . wraps ( func ) ( _ )\n\n\n        Summary:", "input": "", "output": "Wrap the deprecated function to print out deprecation warnings"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_binary_mathfunction ( name , doc = \"\" ) : def _ ( col1 , col2 ) : sc = SparkContext . _active_spark_context # For legacy reasons, the arguments here can be implicitly converted into floats, # if they are not columns or strings. if isinstance ( col1 , Column ) : arg1 = col1 . _jc elif isinstance ( col1 , basestring ) : arg1 = _create_column_from_name ( col1 ) else : arg1 = float ( col1 ) if isinstance ( col2 , Column ) : arg2 = col2 . _jc elif isinstance ( col2 , basestring ) : arg2 = _create_column_from_name ( col2 ) else : arg2 = float ( col2 ) jc = getattr ( sc . _jvm . functions , name ) ( arg1 , arg2 ) return Column ( jc ) _ . __name__ = name _ . __doc__ = doc return _\n\n\n        Summary:", "input": "", "output": "Create a binary mathfunction by name"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_window_function ( name , doc = '' ) : def _ ( ) : sc = SparkContext . _active_spark_context jc = getattr ( sc . _jvm . functions , name ) ( ) return Column ( jc ) _ . __name__ = name _ . __doc__ = 'Window function: ' + doc return _\n\n\n        Summary:", "input": "", "output": "Create a window function by name"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def approx_count_distinct ( col , rsd = None ) : sc = SparkContext . _active_spark_context if rsd is None : jc = sc . _jvm . functions . approx_count_distinct ( _to_java_column ( col ) ) else : jc = sc . _jvm . functions . approx_count_distinct ( _to_java_column ( col ) , rsd ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Aggregate function : returns a new : class : Column for approximate distinct count of column col ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def broadcast ( df ) : sc = SparkContext . _active_spark_context return DataFrame ( sc . _jvm . functions . broadcast ( df . _jdf ) , df . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Marks a DataFrame as small enough for use in broadcast joins ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def countDistinct ( col , * cols ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . countDistinct ( _to_java_column ( col ) , _to_seq ( sc , cols , _to_java_column ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Returns a new : class : Column for distinct count of col or cols ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def last ( col , ignorenulls = False ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . last ( _to_java_column ( col ) , ignorenulls ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Aggregate function : returns the last value in a group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanvl ( col1 , col2 ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . nanvl ( _to_java_column ( col1 ) , _to_java_column ( col2 ) ) )\n\n\n        Summary:", "input": "", "output": "Returns col1 if it is not NaN or col2 if col1 is NaN ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rand ( seed = None ) : sc = SparkContext . _active_spark_context if seed is not None : jc = sc . _jvm . functions . rand ( seed ) else : jc = sc . _jvm . functions . rand ( ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Generates a random column with independent and identically distributed ( i . i . d . ) samples from U [ 0 . 0 1 . 0 ] ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def round ( col , scale = 0 ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . round ( _to_java_column ( col ) , scale ) )\n\n\n        Summary:", "input": "", "output": "Round the given value to scale decimal places using HALF_UP rounding mode if scale > = 0 or at integral part when scale < 0 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shiftLeft ( col , numBits ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . shiftLeft ( _to_java_column ( col ) , numBits ) )\n\n\n        Summary:", "input": "", "output": "Shift the given value numBits left ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shiftRight ( col , numBits ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . shiftRight ( _to_java_column ( col ) , numBits ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "( Signed ) shift the given value numBits right ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def expr ( str ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . expr ( str ) )\n\n\n        Summary:", "input": "", "output": "Parses the expression string into the column that it represents"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def when ( condition , value ) : sc = SparkContext . _active_spark_context if not isinstance ( condition , Column ) : raise TypeError ( \"condition should be a Column\" ) v = value . _jc if isinstance ( value , Column ) else value jc = sc . _jvm . functions . when ( condition . _jc , v ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Evaluates a list of conditions and returns one of multiple possible result expressions . If : func : Column . otherwise is not invoked None is returned for unmatched conditions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def log ( arg1 , arg2 = None ) : sc = SparkContext . _active_spark_context if arg2 is None : jc = sc . _jvm . functions . log ( _to_java_column ( arg1 ) ) else : jc = sc . _jvm . functions . log ( arg1 , _to_java_column ( arg2 ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Returns the first argument - based logarithm of the second argument ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def conv ( col , fromBase , toBase ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . conv ( _to_java_column ( col ) , fromBase , toBase ) )\n\n\n        Summary:", "input": "", "output": "Convert a number in a string column from one base to another ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lag ( col , offset = 1 , default = None ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . lag ( _to_java_column ( col ) , offset , default ) )\n\n\n        Summary:", "input": "", "output": "Window function : returns the value that is offset rows before the current row and defaultValue if there is less than offset rows before the current row . For example an offset of one will return the previous row at any given point in the window partition ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ntile ( n ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . ntile ( int ( n ) ) )\n\n\n        Summary:", "input": "", "output": "Window function : returns the ntile group id ( from 1 to n inclusive ) in an ordered window partition . For example if n is 4 the first quarter of the rows will get value 1 the second quarter will get 2 the third quarter will get 3 and the last quarter will get 4 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def date_format ( date , format ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . date_format ( _to_java_column ( date ) , format ) )\n\n\n        Summary:", "input": "", "output": "Converts a date / timestamp / string to a value of string in the format specified by the date format given by the second argument ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def date_add ( start , days ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . date_add ( _to_java_column ( start ) , days ) )\n\n\n        Summary:", "input": "", "output": "Returns the date that is days days after start"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def datediff ( end , start ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . datediff ( _to_java_column ( end ) , _to_java_column ( start ) ) )\n\n\n        Summary:", "input": "", "output": "Returns the number of days from start to end ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_months ( start , months ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . add_months ( _to_java_column ( start ) , months ) )\n\n\n        Summary:", "input": "", "output": "Returns the date that is months months after start"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def months_between ( date1 , date2 , roundOff = True ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . months_between ( _to_java_column ( date1 ) , _to_java_column ( date2 ) , roundOff ) )\n\n\n        Summary:", "input": "", "output": "Returns number of months between dates date1 and date2 . If date1 is later than date2 then the result is positive . If date1 and date2 are on the same day of month or both are the last day of month returns an integer ( time of day will be ignored ) . The result is rounded off to 8 digits unless roundOff is set to False ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_date ( col , format = None ) : sc = SparkContext . _active_spark_context if format is None : jc = sc . _jvm . functions . to_date ( _to_java_column ( col ) ) else : jc = sc . _jvm . functions . to_date ( _to_java_column ( col ) , format ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Converts a : class : Column of : class : pyspark . sql . types . StringType or : class : pyspark . sql . types . TimestampType into : class : pyspark . sql . types . DateType using the optionally specified format . Specify formats according to DateTimeFormatter <https : // docs . oracle . com / javase / 8 / docs / api / java / time / format / DateTimeFormatter . html > _ . # noqa By default it follows casting rules to : class : pyspark . sql . types . DateType if the format is omitted ( equivalent to col . cast ( date ) ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def date_trunc ( format , timestamp ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . date_trunc ( format , _to_java_column ( timestamp ) ) )\n\n\n        Summary:", "input": "", "output": "Returns timestamp truncated to the unit specified by the format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def next_day ( date , dayOfWeek ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . next_day ( _to_java_column ( date ) , dayOfWeek ) )\n\n\n        Summary:", "input": "", "output": "Returns the first date which is later than the value of the date column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def last_day ( date ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . last_day ( _to_java_column ( date ) ) )\n\n\n        Summary:", "input": "", "output": "Returns the last day of the month which the given date belongs to ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unix_timestamp ( timestamp = None , format = 'yyyy-MM-dd HH:mm:ss' ) : sc = SparkContext . _active_spark_context if timestamp is None : return Column ( sc . _jvm . functions . unix_timestamp ( ) ) return Column ( sc . _jvm . functions . unix_timestamp ( _to_java_column ( timestamp ) , format ) )\n\n\n        Summary:", "input": "", "output": "Convert time string with given pattern ( yyyy - MM - dd HH : mm : ss by default ) to Unix time stamp ( in seconds ) using the default timezone and the default locale return null if fail ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_utc_timestamp ( timestamp , tz ) : warnings . warn ( \"Deprecated in 3.0. See SPARK-25496\" , DeprecationWarning ) sc = SparkContext . _active_spark_context if isinstance ( tz , Column ) : tz = _to_java_column ( tz ) return Column ( sc . _jvm . functions . from_utc_timestamp ( _to_java_column ( timestamp ) , tz ) )\n\n\n        Summary:", "input": "", "output": "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE . This function takes a timestamp which is timezone - agnostic and interprets it as a timestamp in UTC and renders that timestamp as a timestamp in the given time zone ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def window ( timeColumn , windowDuration , slideDuration = None , startTime = None ) : def check_string_field ( field , fieldName ) : if not field or type ( field ) is not str : raise TypeError ( \"%s should be provided as a string\" % fieldName ) sc = SparkContext . _active_spark_context time_col = _to_java_column ( timeColumn ) check_string_field ( windowDuration , \"windowDuration\" ) if slideDuration and startTime : check_string_field ( slideDuration , \"slideDuration\" ) check_string_field ( startTime , \"startTime\" ) res = sc . _jvm . functions . window ( time_col , windowDuration , slideDuration , startTime ) elif slideDuration : check_string_field ( slideDuration , \"slideDuration\" ) res = sc . _jvm . functions . window ( time_col , windowDuration , slideDuration ) elif startTime : check_string_field ( startTime , \"startTime\" ) res = sc . _jvm . functions . window ( time_col , windowDuration , windowDuration , startTime ) else : res = sc . _jvm . functions . window ( time_col , windowDuration ) return Column ( res )\n\n\n        Summary:", "input": "", "output": "Bucketize rows into one or more time windows given a timestamp specifying column . Window starts are inclusive but the window ends are exclusive e . g . 12 : 05 will be in the window [ 12 : 05 12 : 10 ) but not in [ 12 : 00 12 : 05 ) . Windows can support microsecond precision . Windows in the order of months are not supported ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hash ( * cols ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . hash ( _to_seq ( sc , cols , _to_java_column ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Calculates the hash code of given columns and returns the result as an int column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concat_ws ( sep , * cols ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . concat_ws ( sep , _to_seq ( sc , cols , _to_java_column ) ) )\n\n\n        Summary:", "input": "", "output": "Concatenates multiple input string columns together into a single string column using the given separator ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def decode ( col , charset ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . decode ( _to_java_column ( col ) , charset ) )\n\n\n        Summary:", "input": "", "output": "Computes the first argument into a string from a binary using the provided character set ( one of US - ASCII ISO - 8859 - 1 UTF - 8 UTF - 16BE UTF - 16LE UTF - 16 ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format_number ( col , d ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . format_number ( _to_java_column ( col ) , d ) )\n\n\n        Summary:", "input": "", "output": "Formats the number X to a format like # -- # -- # . -- rounded to d decimal places with HALF_EVEN round mode and returns the result as a string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format_string ( format , * cols ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . format_string ( format , _to_seq ( sc , cols , _to_java_column ) ) )\n\n\n        Summary:", "input": "", "output": "Formats the arguments in printf - style and returns the result as a string column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def instr ( str , substr ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . instr ( _to_java_column ( str ) , substr ) )\n\n\n        Summary:", "input": "", "output": "Locate the position of the first occurrence of substr column in the given string . Returns null if either of the arguments are null ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def substring ( str , pos , len ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . substring ( _to_java_column ( str ) , pos , len ) )\n\n\n        Summary:", "input": "", "output": "Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def substring_index ( str , delim , count ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . substring_index ( _to_java_column ( str ) , delim , count ) )\n\n\n        Summary:", "input": "", "output": "Returns the substring from string str before count occurrences of the delimiter delim . If count is positive everything the left of the final delimiter ( counting from left ) is returned . If count is negative every to the right of the final delimiter ( counting from the right ) is returned . substring_index performs a case - sensitive match when searching for delim ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def levenshtein ( left , right ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . levenshtein ( _to_java_column ( left ) , _to_java_column ( right ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Computes the Levenshtein distance of the two given strings ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def locate ( substr , str , pos = 1 ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . locate ( substr , _to_java_column ( str ) , pos ) )\n\n\n        Summary:", "input": "", "output": "Locate the position of the first occurrence of substr in a string column after position pos ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lpad ( col , len , pad ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . lpad ( _to_java_column ( col ) , len , pad ) )\n\n\n        Summary:", "input": "", "output": "Left - pad the string column to width len with pad ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def repeat ( col , n ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . repeat ( _to_java_column ( col ) , n ) )\n\n\n        Summary:", "input": "", "output": "Repeats a string column n times and returns it as a new string column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def split ( str , pattern , limit = - 1 ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . split ( _to_java_column ( str ) , pattern , limit ) )\n\n\n        Summary:", "input": "", "output": "Splits str around matches of the given pattern ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def regexp_extract ( str , pattern , idx ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . regexp_extract ( _to_java_column ( str ) , pattern , idx ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "r Extract a specific group matched by a Java regex from the specified string column . If the regex did not match or the specified group did not match an empty string is returned ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def regexp_replace ( str , pattern , replacement ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . regexp_replace ( _to_java_column ( str ) , pattern , replacement ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "r Replace all substrings of the specified string value that match regexp with rep ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def translate ( srcCol , matching , replace ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . translate ( _to_java_column ( srcCol ) , matching , replace ) )\n\n\n        Summary:", "input": "", "output": "A function translate any character in the srcCol by a character in matching . The characters in replace is corresponding to the characters in matching . The translate will happen when any character in the string matching with the character in the matching ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def arrays_overlap ( a1 , a2 ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . arrays_overlap ( _to_java_column ( a1 ) , _to_java_column ( a2 ) ) )\n\n\n        Summary:", "input": "", "output": "Collection function : returns true if the arrays contain any common non - null element ; if not returns null if both the arrays are non - empty and any of them contains a null element ; returns false otherwise ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def slice ( x , start , length ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . slice ( _to_java_column ( x ) , start , length ) )\n\n\n        Summary:", "input": "", "output": "Collection function : returns an array containing all the elements in x from index start ( or starting from the end if start is negative ) with the specified length . >>> df = spark . createDataFrame ( [ ( [ 1 2 3 ] ) ( [ 4 5 ] ) ] [ x ] ) >>> df . select ( slice ( df . x 2 2 ) . alias ( sliced )) . collect () [ Row ( sliced = [ 2 3 ] ) Row ( sliced = [ 5 ] ) ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array_join ( col , delimiter , null_replacement = None ) : sc = SparkContext . _active_spark_context if null_replacement is None : return Column ( sc . _jvm . functions . array_join ( _to_java_column ( col ) , delimiter ) ) else : return Column ( sc . _jvm . functions . array_join ( _to_java_column ( col ) , delimiter , null_replacement ) )\n\n\n        Summary:", "input": "", "output": "Concatenates the elements of column using the delimiter . Null values are replaced with null_replacement if set otherwise they are ignored ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concat ( * cols ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . concat ( _to_seq ( sc , cols , _to_java_column ) ) )\n\n\n        Summary:", "input": "", "output": "Concatenates multiple input columns together into a single column . The function works with strings binary and compatible array columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array_position ( col , value ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . array_position ( _to_java_column ( col ) , value ) )\n\n\n        Summary:", "input": "", "output": "Collection function : Locates the position of the first occurrence of the given value in the given array . Returns null if either of the arguments are null ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def element_at ( col , extraction ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . element_at ( _to_java_column ( col ) , extraction ) )\n\n\n        Summary:", "input": "", "output": "Collection function : Returns element of array at given index in extraction if col is array . Returns value for the given key in extraction if col is map ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array_remove ( col , element ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . array_remove ( _to_java_column ( col ) , element ) )\n\n\n        Summary:", "input": "", "output": "Collection function : Remove all elements that equal to element from the given array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def explode ( col ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . explode ( _to_java_column ( col ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Returns a new row for each element in the given array or map . Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_json_object ( col , path ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . get_json_object ( _to_java_column ( col ) , path ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Extracts json object from a json string based on json path specified and returns json string of the extracted json object . It will return null if the input json string is invalid ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def json_tuple ( col , * fields ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . json_tuple ( _to_java_column ( col ) , _to_seq ( sc , fields ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Creates a new row for a json column according to the given field names ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_json ( col , schema , options = { } ) : sc = SparkContext . _active_spark_context if isinstance ( schema , DataType ) : schema = schema . json ( ) elif isinstance ( schema , Column ) : schema = _to_java_column ( schema ) jc = sc . _jvm . functions . from_json ( _to_java_column ( col ) , schema , options ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Parses a column containing a JSON string into a : class : MapType with : class : StringType as keys type : class : StructType or : class : ArrayType with the specified schema . Returns null in the case of an unparseable string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def schema_of_json ( json , options = { } ) : if isinstance ( json , basestring ) : col = _create_column_from_literal ( json ) elif isinstance ( json , Column ) : col = _to_java_column ( json ) else : raise TypeError ( \"schema argument should be a column or string\" ) sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . schema_of_json ( col , options ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Parses a JSON string and infers its schema in DDL format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def schema_of_csv ( csv , options = { } ) : if isinstance ( csv , basestring ) : col = _create_column_from_literal ( csv ) elif isinstance ( csv , Column ) : col = _to_java_column ( csv ) else : raise TypeError ( \"schema argument should be a column or string\" ) sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . schema_of_csv ( col , options ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Parses a CSV string and infers its schema in DDL format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_csv ( col , options = { } ) : sc = SparkContext . _active_spark_context jc = sc . _jvm . functions . to_csv ( _to_java_column ( col ) , options ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Converts a column containing a : class : StructType into a CSV string . Throws an exception in the case of an unsupported type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def size ( col ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . size ( _to_java_column ( col ) ) )\n\n\n        Summary:", "input": "", "output": "Collection function : returns the length of the array or map stored in the column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_array ( col , asc = True ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . sort_array ( _to_java_column ( col ) , asc ) )\n\n\n        Summary:", "input": "", "output": "Collection function : sorts the input array in ascending or descending order according to the natural ordering of the array elements . Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array_repeat ( col , count ) : sc = SparkContext . _active_spark_context return Column ( sc . _jvm . functions . array_repeat ( _to_java_column ( col ) , count ) )\n\n\n        Summary:", "input": "", "output": "Collection function : creates an array containing a column repeated count times ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map_concat ( * cols ) : sc = SparkContext . _active_spark_context if len ( cols ) == 1 and isinstance ( cols [ 0 ] , ( list , set ) ) : cols = cols [ 0 ] jc = sc . _jvm . functions . map_concat ( _to_seq ( sc , cols , _to_java_column ) ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Returns the union of all the given maps ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sequence ( start , stop , step = None ) : sc = SparkContext . _active_spark_context if step is None : return Column ( sc . _jvm . functions . sequence ( _to_java_column ( start ) , _to_java_column ( stop ) ) ) else : return Column ( sc . _jvm . functions . sequence ( _to_java_column ( start ) , _to_java_column ( stop ) , _to_java_column ( step ) ) )\n\n\n        Summary:", "input": "", "output": "Generate a sequence of integers from start to stop incrementing by step . If step is not set incrementing by 1 if start is less than or equal to stop otherwise - 1 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_csv ( col , schema , options = { } ) : sc = SparkContext . _active_spark_context if isinstance ( schema , basestring ) : schema = _create_column_from_literal ( schema ) elif isinstance ( schema , Column ) : schema = _to_java_column ( schema ) else : raise TypeError ( \"schema argument should be a column or string\" ) jc = sc . _jvm . functions . from_csv ( _to_java_column ( col ) , schema , options ) return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Parses a column containing a CSV string to a row with the specified schema . Returns null in the case of an unparseable string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def udf ( f = None , returnType = StringType ( ) ) : # The following table shows most of Python data and SQL type conversions in normal UDFs that # are not yet visible to the user. Some of behaviors are buggy and might be changed in the near # future. The table might have to be eventually documented externally. # Please see SPARK-25666's PR to see the codes in order to generate the table below. # # +-----------------------------+--------------+----------+------+-------+---------------+---------------+--------------------+-----------------------------+----------+----------------------+---------+--------------------+-----------------+------------+--------------+------------------+----------------------+ # noqa # |SQL Type \\ Python Value(Type)|None(NoneType)|True(bool)|1(int)|1(long)| a(str)| a(unicode)| 1970-01-01(date)|1970-01-01 00:00:00(datetime)|1.0(float)|array('i', [1])(array)|[1](list)| (1,)(tuple)| ABC(bytearray)| 1(Decimal)|{'a': 1}(dict)|Row(kwargs=1)(Row)|Row(namedtuple=1)(Row)| # noqa # +-----------------------------+--------------+----------+------+-------+---------------+---------------+--------------------+-----------------------------+----------+----------------------+---------+--------------------+-----------------+------------+--------------+------------------+----------------------+ # noqa # | boolean| None| True| None| None| None| None| None| None| None| None| None| None| None| None| None| X| X| # noqa # | tinyint| None| None| 1| 1| None| None| None| None| None| None| None| None| None| None| None| X| X| # noqa # | smallint| None| None| 1| 1| None| None| None| None| None| None| None| None| None| None| None| X| X| # noqa # | int| None| None| 1| 1| None| None| None| None| None| None| None| None| None| None| None| X| X| # noqa # | bigint| None| None| 1| 1| None| None| None| None| None| None| None| None| None| None| None| X| X| # noqa # | string| None| u'true'| u'1'| u'1'| u'a'| u'a'|u'java.util.Grego...| u'java.util.Grego...| u'1.0'| u'[I@24a83055'| u'[1]'|u'[Ljava.lang.Obj...| u'[B@49093632'| u'1'| u'{a=1}'| X| X| # noqa # | date| None| X| X| X| X| X|datetime.date(197...| datetime.date(197...| X| X| X| X| X| X| X| X| X| # noqa # | timestamp| None| X| X| X| X| X| X| datetime.datetime...| X| X| X| X| X| X| X| X| X| # noqa # | float| None| None| None| None| None| None| None| None| 1.0| None| None| None| None| None| None| X| X| # noqa # | double| None| None| None| None| None| None| None| None| 1.0| None| None| None| None| None| None| X| X| # noqa # | array<int>| None| None| None| None| None| None| None| None| None| [1]| [1]| [1]| [65, 66, 67]| None| None| X| X| # noqa # | binary| None| None| None| None|bytearray(b'a')|bytearray(b'a')| None| None| None| None| None| None|bytearray(b'ABC')| None| None| X| X| # noqa # | decimal(10,0)| None| None| None| None| None| None| None| None| None| None| None| None| None|Decimal('1')| None| X| X| # noqa # | map<string,int>| None| None| None| None| None| None| None| None| None| None| None| None| None| None| {u'a': 1}| X| X| # noqa # | struct<_1:int>| None| X| X| X| X| X| X| X| X| X|Row(_1=1)| Row(_1=1)| X| X| Row(_1=None)| Row(_1=1)| Row(_1=1)| # noqa # +-----------------------------+--------------+----------+------+-------+---------------+---------------+--------------------+-----------------------------+----------+----------------------+---------+--------------------+-----------------+------------+--------------+------------------+----------------------+ # noqa # # Note: DDL formatted string is used for 'SQL Type' for simplicity. This string can be # used in `returnType`. # Note: The values inside of the table are generated by `repr`. # Note: Python 2 is used to generate this table since it is used to check the backward # compatibility often in practice. # Note: 'X' means it throws an exception during the conversion. # decorator @udf, @udf(), @udf(dataType()) if f is None or isinstance ( f , ( str , DataType ) ) : # If DataType has been passed as a positional argument # for decorator use it as a returnType return_type = f or returnType return functools . partial ( _create_udf , returnType = return_type , evalType = PythonEvalType . SQL_BATCHED_UDF ) else : return _create_udf ( f = f , returnType = returnType , evalType = PythonEvalType . SQL_BATCHED_UDF )\n\n\n        Summary:", "input": "", "output": "Creates a user defined function ( UDF ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pandas_udf ( f = None , returnType = None , functionType = None ) : # The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that # are not yet visible to the user. Some of behaviors are buggy and might be changed in the near # future. The table might have to be eventually documented externally. # Please see SPARK-25798's PR to see the codes in order to generate the table below. # # +-----------------------------+----------------------+----------+-------+--------+--------------------+--------------------+--------+---------+---------+---------+------------+------------+------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+-------------+-----------------+------------------+-----------+--------------------------------+ # noqa # |SQL Type \\ Pandas Value(Type)|None(object(NoneType))|True(bool)|1(int8)|1(int16)| 1(int32)| 1(int64)|1(uint8)|1(uint16)|1(uint32)|1(uint64)|1.0(float16)|1.0(float32)|1.0(float64)|1970-01-01 00:00:00(datetime64[ns])|1970-01-01 00:00:00-05:00(datetime64[ns, US/Eastern])|a(object(string))| 1(object(Decimal))|[1 2 3](object(array[int32]))|1.0(float128)|(1+0j)(complex64)|(1+0j)(complex128)|A(category)|1 days 00:00:00(timedelta64[ns])| # noqa # +-----------------------------+----------------------+----------+-------+--------+--------------------+--------------------+--------+---------+---------+---------+------------+------------+------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+-------------+-----------------+------------------+-----------+--------------------------------+ # noqa # | boolean| None| True| True| True| True| True| True| True| True| True| False| False| False| False| False| X| X| X| False| False| False| X| False| # noqa # | tinyint| None| 1| 1| 1| 1| 1| X| X| X| X| 1| 1| 1| X| X| X| X| X| X| X| X| 0| X| # noqa # | smallint| None| 1| 1| 1| 1| 1| 1| X| X| X| 1| 1| 1| X| X| X| X| X| X| X| X| X| X| # noqa # | int| None| 1| 1| 1| 1| 1| 1| 1| X| X| 1| 1| 1| X| X| X| X| X| X| X| X| X| X| # noqa # | bigint| None| 1| 1| 1| 1| 1| 1| 1| 1| X| 1| 1| 1| 0| 18000000000000| X| X| X| X| X| X| X| X| # noqa # | float| None| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| X| X| X|1.401298464324817...| X| X| X| X| X| X| # noqa # | double| None| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| 1.0| X| X| X| X| X| X| X| X| X| X| # noqa # | date| None| X| X| X|datetime.date(197...| X| X| X| X| X| X| X| X| datetime.date(197...| X| X| X| X| X| X| X| X| X| # noqa # | timestamp| None| X| X| X| X|datetime.datetime...| X| X| X| X| X| X| X| datetime.datetime...| datetime.datetime...| X| X| X| X| X| X| X| X| # noqa # | string| None| u''|u'\\x01'| u'\\x01'| u'\\x01'| u'\\x01'| u'\\x01'| u'\\x01'| u'\\x01'| u'\\x01'| u''| u''| u''| X| X| u'a'| X| X| u''| u''| u''| X| X| # noqa # | decimal(10,0)| None| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| Decimal('1')| X| X| X| X| X| X| # noqa # | array<int>| None| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| [1, 2, 3]| X| X| X| X| X| # noqa # | map<string,int>| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| # noqa # | struct<_1:int>| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| # noqa # | binary| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| X| # noqa # +-----------------------------+----------------------+----------+-------+--------+--------------------+--------------------+--------+---------+---------+---------+------------+------------+------------+-----------------------------------+-----------------------------------------------------+-----------------+--------------------+-----------------------------+-------------+-----------------+------------------+-----------+--------------------------------+ # noqa # # Note: DDL formatted string is used for 'SQL Type' for simplicity. This string can be # used in `returnType`. # Note: The values inside of the table are generated by `repr`. # Note: Python 2 is used to generate this table since it is used to check the backward # compatibility often in practice. # Note: Pandas 0.19.2 and PyArrow 0.9.0 are used. # Note: Timezone is Singapore timezone. # Note: 'X' means it throws an exception during the conversion. # Note: 'binary' type is only supported with PyArrow 0.10.0+ (SPARK-23555). # decorator @pandas_udf(returnType, functionType) is_decorator = f is None or isinstance ( f , ( str , DataType ) ) if is_decorator : # If DataType has been passed as a positional argument # for decorator use it as a returnType return_type = f or returnType if functionType is not None : # @pandas_udf(dataType, functionType=functionType) # @pandas_udf(returnType=dataType, functionType=functionType) eval_type = functionType elif returnType is not None and isinstance ( returnType , int ) : # @pandas_udf(dataType, functionType) eval_type = returnType else : # @pandas_udf(dataType) or @pandas_udf(returnType=dataType) eval_type = PythonEvalType . SQL_SCALAR_PANDAS_UDF else : return_type = returnType if functionType is not None : eval_type = functionType else : eval_type = PythonEvalType . SQL_SCALAR_PANDAS_UDF if return_type is None : raise ValueError ( \"Invalid returnType: returnType can not be None\" ) if eval_type not in [ PythonEvalType . SQL_SCALAR_PANDAS_UDF , PythonEvalType . SQL_GROUPED_MAP_PANDAS_UDF , PythonEvalType . SQL_GROUPED_AGG_PANDAS_UDF ] : raise ValueError ( \"Invalid functionType: \" \"functionType must be one the values from PandasUDFType\" ) if is_decorator : return functools . partial ( _create_udf , returnType = return_type , evalType = eval_type ) else : return _create_udf ( f = f , returnType = return_type , evalType = eval_type )\n\n\n        Summary:", "input": "", "output": "Creates a vectorized user defined function ( UDF ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_str ( value ) : if isinstance ( value , bool ) : return str ( value ) . lower ( ) elif value is None : return value else : return str ( value )\n\n\n        Summary:", "input": "", "output": "A wrapper over str () but converts bool values to lower case strings . If None is given just returns None instead of converting it to string None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_opts ( self , schema = None , * * options ) : if schema is not None : self . schema ( schema ) for k , v in options . items ( ) : if v is not None : self . option ( k , v )\n\n\n        Summary:", "input": "", "output": "Set named options ( filter out those the value is None )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format ( self , source ) : self . _jreader = self . _jreader . format ( source ) return self\n\n\n        Summary:", "input": "", "output": "Specifies the input data source format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def schema ( self , schema ) : from pyspark . sql import SparkSession spark = SparkSession . builder . getOrCreate ( ) if isinstance ( schema , StructType ) : jschema = spark . _jsparkSession . parseDataType ( schema . json ( ) ) self . _jreader = self . _jreader . schema ( jschema ) elif isinstance ( schema , basestring ) : self . _jreader = self . _jreader . schema ( schema ) else : raise TypeError ( \"schema should be StructType or string\" ) return self\n\n\n        Summary:", "input": "", "output": "Specifies the input schema ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def option ( self , key , value ) : self . _jreader = self . _jreader . option ( key , to_str ( value ) ) return self\n\n\n        Summary:", "input": "", "output": "Adds an input option for the underlying data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def options ( self , * * options ) : for k in options : self . _jreader = self . _jreader . option ( k , to_str ( options [ k ] ) ) return self\n\n\n        Summary:", "input": "", "output": "Adds input options for the underlying data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( self , path = None , format = None , schema = None , * * options ) : if format is not None : self . format ( format ) if schema is not None : self . schema ( schema ) self . options ( * * options ) if isinstance ( path , basestring ) : return self . _df ( self . _jreader . load ( path ) ) elif path is not None : if type ( path ) != list : path = [ path ] return self . _df ( self . _jreader . load ( self . _spark . _sc . _jvm . PythonUtils . toSeq ( path ) ) ) else : return self . _df ( self . _jreader . load ( ) )\n\n\n        Summary:", "input": "", "output": "Loads data from a data source and returns it as a : class DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def json ( self , path , schema = None , primitivesAsString = None , prefersDecimal = None , allowComments = None , allowUnquotedFieldNames = None , allowSingleQuotes = None , allowNumericLeadingZero = None , allowBackslashEscapingAnyCharacter = None , mode = None , columnNameOfCorruptRecord = None , dateFormat = None , timestampFormat = None , multiLine = None , allowUnquotedControlChars = None , lineSep = None , samplingRatio = None , dropFieldIfAllNull = None , encoding = None , locale = None ) : self . _set_opts ( schema = schema , primitivesAsString = primitivesAsString , prefersDecimal = prefersDecimal , allowComments = allowComments , allowUnquotedFieldNames = allowUnquotedFieldNames , allowSingleQuotes = allowSingleQuotes , allowNumericLeadingZero = allowNumericLeadingZero , allowBackslashEscapingAnyCharacter = allowBackslashEscapingAnyCharacter , mode = mode , columnNameOfCorruptRecord = columnNameOfCorruptRecord , dateFormat = dateFormat , timestampFormat = timestampFormat , multiLine = multiLine , allowUnquotedControlChars = allowUnquotedControlChars , lineSep = lineSep , samplingRatio = samplingRatio , dropFieldIfAllNull = dropFieldIfAllNull , encoding = encoding , locale = locale ) if isinstance ( path , basestring ) : path = [ path ] if type ( path ) == list : return self . _df ( self . _jreader . json ( self . _spark . _sc . _jvm . PythonUtils . toSeq ( path ) ) ) elif isinstance ( path , RDD ) : def func ( iterator ) : for x in iterator : if not isinstance ( x , basestring ) : x = unicode ( x ) if isinstance ( x , unicode ) : x = x . encode ( \"utf-8\" ) yield x keyed = path . mapPartitions ( func ) keyed . _bypass_serializer = True jrdd = keyed . _jrdd . map ( self . _spark . _jvm . BytesToString ( ) ) return self . _df ( self . _jreader . json ( jrdd ) ) else : raise TypeError ( \"path can be only string, list or RDD\" )\n\n\n        Summary:", "input": "", "output": "Loads JSON files and returns the results as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parquet ( self , * paths ) : return self . _df ( self . _jreader . parquet ( _to_seq ( self . _spark . _sc , paths ) ) )\n\n\n        Summary:", "input": "", "output": "Loads Parquet files returning the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def text ( self , paths , wholetext = False , lineSep = None ) : self . _set_opts ( wholetext = wholetext , lineSep = lineSep ) if isinstance ( paths , basestring ) : paths = [ paths ] return self . _df ( self . _jreader . text ( self . _spark . _sc . _jvm . PythonUtils . toSeq ( paths ) ) )\n\n\n        Summary:", "input": "", "output": "Loads text files and returns a : class : DataFrame whose schema starts with a string column named value and followed by partitioned columns if there are any . The text files must be encoded as UTF - 8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def csv ( self , path , schema = None , sep = None , encoding = None , quote = None , escape = None , comment = None , header = None , inferSchema = None , ignoreLeadingWhiteSpace = None , ignoreTrailingWhiteSpace = None , nullValue = None , nanValue = None , positiveInf = None , negativeInf = None , dateFormat = None , timestampFormat = None , maxColumns = None , maxCharsPerColumn = None , maxMalformedLogPerPartition = None , mode = None , columnNameOfCorruptRecord = None , multiLine = None , charToEscapeQuoteEscaping = None , samplingRatio = None , enforceSchema = None , emptyValue = None , locale = None , lineSep = None ) : self . _set_opts ( schema = schema , sep = sep , encoding = encoding , quote = quote , escape = escape , comment = comment , header = header , inferSchema = inferSchema , ignoreLeadingWhiteSpace = ignoreLeadingWhiteSpace , ignoreTrailingWhiteSpace = ignoreTrailingWhiteSpace , nullValue = nullValue , nanValue = nanValue , positiveInf = positiveInf , negativeInf = negativeInf , dateFormat = dateFormat , timestampFormat = timestampFormat , maxColumns = maxColumns , maxCharsPerColumn = maxCharsPerColumn , maxMalformedLogPerPartition = maxMalformedLogPerPartition , mode = mode , columnNameOfCorruptRecord = columnNameOfCorruptRecord , multiLine = multiLine , charToEscapeQuoteEscaping = charToEscapeQuoteEscaping , samplingRatio = samplingRatio , enforceSchema = enforceSchema , emptyValue = emptyValue , locale = locale , lineSep = lineSep ) if isinstance ( path , basestring ) : path = [ path ] if type ( path ) == list : return self . _df ( self . _jreader . csv ( self . _spark . _sc . _jvm . PythonUtils . toSeq ( path ) ) ) elif isinstance ( path , RDD ) : def func ( iterator ) : for x in iterator : if not isinstance ( x , basestring ) : x = unicode ( x ) if isinstance ( x , unicode ) : x = x . encode ( \"utf-8\" ) yield x keyed = path . mapPartitions ( func ) keyed . _bypass_serializer = True jrdd = keyed . _jrdd . map ( self . _spark . _jvm . BytesToString ( ) ) # see SPARK-22112 # There aren't any jvm api for creating a dataframe from rdd storing csv. # We can do it through creating a jvm dataset firstly and using the jvm api # for creating a dataframe from dataset storing csv. jdataset = self . _spark . _ssql_ctx . createDataset ( jrdd . rdd ( ) , self . _spark . _jvm . Encoders . STRING ( ) ) return self . _df ( self . _jreader . csv ( jdataset ) ) else : raise TypeError ( \"path can be only string, list or RDD\" )\n\n\n        Summary:", "input": "", "output": "r Loads a CSV file and returns the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def orc ( self , path ) : if isinstance ( path , basestring ) : path = [ path ] return self . _df ( self . _jreader . orc ( _to_seq ( self . _spark . _sc , path ) ) )\n\n\n        Summary:", "input": "", "output": "Loads ORC files returning the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def jdbc ( self , url , table , column = None , lowerBound = None , upperBound = None , numPartitions = None , predicates = None , properties = None ) : if properties is None : properties = dict ( ) jprop = JavaClass ( \"java.util.Properties\" , self . _spark . _sc . _gateway . _gateway_client ) ( ) for k in properties : jprop . setProperty ( k , properties [ k ] ) if column is not None : assert lowerBound is not None , \"lowerBound can not be None when ``column`` is specified\" assert upperBound is not None , \"upperBound can not be None when ``column`` is specified\" assert numPartitions is not None , \"numPartitions can not be None when ``column`` is specified\" return self . _df ( self . _jreader . jdbc ( url , table , column , int ( lowerBound ) , int ( upperBound ) , int ( numPartitions ) , jprop ) ) if predicates is not None : gateway = self . _spark . _sc . _gateway jpredicates = utils . toJArray ( gateway , gateway . jvm . java . lang . String , predicates ) return self . _df ( self . _jreader . jdbc ( url , table , jpredicates , jprop ) ) return self . _df ( self . _jreader . jdbc ( url , table , jprop ) )\n\n\n        Summary:", "input": "", "output": "Construct a : class : DataFrame representing the database table named table accessible via JDBC URL url and connection properties ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mode ( self , saveMode ) : # At the JVM side, the default value of mode is already set to \"error\". # So, if the given saveMode is None, we will not call JVM-side's mode method. if saveMode is not None : self . _jwrite = self . _jwrite . mode ( saveMode ) return self\n\n\n        Summary:", "input": "", "output": "Specifies the behavior when data or table already exists ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format ( self , source ) : self . _jwrite = self . _jwrite . format ( source ) return self\n\n\n        Summary:", "input": "", "output": "Specifies the underlying output data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def option ( self , key , value ) : self . _jwrite = self . _jwrite . option ( key , to_str ( value ) ) return self\n\n\n        Summary:", "input": "", "output": "Adds an output option for the underlying data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def options ( self , * * options ) : for k in options : self . _jwrite = self . _jwrite . option ( k , to_str ( options [ k ] ) ) return self\n\n\n        Summary:", "input": "", "output": "Adds output options for the underlying data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def partitionBy ( self , * cols ) : if len ( cols ) == 1 and isinstance ( cols [ 0 ] , ( list , tuple ) ) : cols = cols [ 0 ] self . _jwrite = self . _jwrite . partitionBy ( _to_seq ( self . _spark . _sc , cols ) ) return self\n\n\n        Summary:", "input": "", "output": "Partitions the output by the given columns on the file system ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sortBy ( self , col , * cols ) : if isinstance ( col , ( list , tuple ) ) : if cols : raise ValueError ( \"col is a {0} but cols are not empty\" . format ( type ( col ) ) ) col , cols = col [ 0 ] , col [ 1 : ] if not all ( isinstance ( c , basestring ) for c in cols ) or not ( isinstance ( col , basestring ) ) : raise TypeError ( \"all names should be `str`\" ) self . _jwrite = self . _jwrite . sortBy ( col , _to_seq ( self . _spark . _sc , cols ) ) return self\n\n\n        Summary:", "input": "", "output": "Sorts the output in each bucket by the given columns on the file system ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save ( self , path = None , format = None , mode = None , partitionBy = None , * * options ) : self . mode ( mode ) . options ( * * options ) if partitionBy is not None : self . partitionBy ( partitionBy ) if format is not None : self . format ( format ) if path is None : self . _jwrite . save ( ) else : self . _jwrite . save ( path )\n\n\n        Summary:", "input": "", "output": "Saves the contents of the : class : DataFrame to a data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def insertInto ( self , tableName , overwrite = False ) : self . _jwrite . mode ( \"overwrite\" if overwrite else \"append\" ) . insertInto ( tableName )\n\n\n        Summary:", "input": "", "output": "Inserts the content of the : class : DataFrame to the specified table ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsTable ( self , name , format = None , mode = None , partitionBy = None , * * options ) : self . mode ( mode ) . options ( * * options ) if partitionBy is not None : self . partitionBy ( partitionBy ) if format is not None : self . format ( format ) self . _jwrite . saveAsTable ( name )\n\n\n        Summary:", "input": "", "output": "Saves the content of the : class : DataFrame as the specified table ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def json ( self , path , mode = None , compression = None , dateFormat = None , timestampFormat = None , lineSep = None , encoding = None ) : self . mode ( mode ) self . _set_opts ( compression = compression , dateFormat = dateFormat , timestampFormat = timestampFormat , lineSep = lineSep , encoding = encoding ) self . _jwrite . json ( path )\n\n\n        Summary:", "input": "", "output": "Saves the content of the : class : DataFrame in JSON format ( JSON Lines text format or newline - delimited JSON <http : // jsonlines . org / > _ ) at the specified path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parquet ( self , path , mode = None , partitionBy = None , compression = None ) : self . mode ( mode ) if partitionBy is not None : self . partitionBy ( partitionBy ) self . _set_opts ( compression = compression ) self . _jwrite . parquet ( path )\n\n\n        Summary:", "input": "", "output": "Saves the content of the : class : DataFrame in Parquet format at the specified path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def text ( self , path , compression = None , lineSep = None ) : self . _set_opts ( compression = compression , lineSep = lineSep ) self . _jwrite . text ( path )\n\n\n        Summary:", "input": "", "output": "Saves the content of the DataFrame in a text file at the specified path . The text files will be encoded as UTF - 8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def csv ( self , path , mode = None , compression = None , sep = None , quote = None , escape = None , header = None , nullValue = None , escapeQuotes = None , quoteAll = None , dateFormat = None , timestampFormat = None , ignoreLeadingWhiteSpace = None , ignoreTrailingWhiteSpace = None , charToEscapeQuoteEscaping = None , encoding = None , emptyValue = None , lineSep = None ) : self . mode ( mode ) self . _set_opts ( compression = compression , sep = sep , quote = quote , escape = escape , header = header , nullValue = nullValue , escapeQuotes = escapeQuotes , quoteAll = quoteAll , dateFormat = dateFormat , timestampFormat = timestampFormat , ignoreLeadingWhiteSpace = ignoreLeadingWhiteSpace , ignoreTrailingWhiteSpace = ignoreTrailingWhiteSpace , charToEscapeQuoteEscaping = charToEscapeQuoteEscaping , encoding = encoding , emptyValue = emptyValue , lineSep = lineSep ) self . _jwrite . csv ( path )\n\n\n        Summary:", "input": "", "output": "r Saves the content of the : class : DataFrame in CSV format at the specified path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def orc ( self , path , mode = None , partitionBy = None , compression = None ) : self . mode ( mode ) if partitionBy is not None : self . partitionBy ( partitionBy ) self . _set_opts ( compression = compression ) self . _jwrite . orc ( path )\n\n\n        Summary:", "input": "", "output": "Saves the content of the : class : DataFrame in ORC format at the specified path ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def jdbc ( self , url , table , mode = None , properties = None ) : if properties is None : properties = dict ( ) jprop = JavaClass ( \"java.util.Properties\" , self . _spark . _sc . _gateway . _gateway_client ) ( ) for k in properties : jprop . setProperty ( k , properties [ k ] ) self . mode ( mode ) . _jwrite . jdbc ( url , table , jprop )\n\n\n        Summary:", "input": "", "output": "Saves the content of the : class : DataFrame to an external database table via JDBC ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def createStream ( ssc , kinesisAppName , streamName , endpointUrl , regionName , initialPositionInStream , checkpointInterval , storageLevel = StorageLevel . MEMORY_AND_DISK_2 , awsAccessKeyId = None , awsSecretKey = None , decoder = utf8_decoder , stsAssumeRoleArn = None , stsSessionName = None , stsExternalId = None ) : jlevel = ssc . _sc . _getJavaStorageLevel ( storageLevel ) jduration = ssc . _jduration ( checkpointInterval ) try : # Use KinesisUtilsPythonHelper to access Scala's KinesisUtils helper = ssc . _jvm . org . apache . spark . streaming . kinesis . KinesisUtilsPythonHelper ( ) except TypeError as e : if str ( e ) == \"'JavaPackage' object is not callable\" : _print_missing_jar ( \"Streaming's Kinesis\" , \"streaming-kinesis-asl\" , \"streaming-kinesis-asl-assembly\" , ssc . sparkContext . version ) raise jstream = helper . createStream ( ssc . _jssc , kinesisAppName , streamName , endpointUrl , regionName , initialPositionInStream , jduration , jlevel , awsAccessKeyId , awsSecretKey , stsAssumeRoleArn , stsSessionName , stsExternalId ) stream = DStream ( jstream , ssc , NoOpSerializer ( ) ) return stream . map ( lambda v : decoder ( v ) )\n\n\n        Summary:", "input": "", "output": "Create an input stream that pulls messages from a Kinesis stream . This uses the Kinesis Client Library ( KCL ) to pull messages from Kinesis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def choose_jira_assignee ( issue , asf_jira ) : while True : try : reporter = issue . fields . reporter commentors = map ( lambda x : x . author , issue . fields . comment . comments ) candidates = set ( commentors ) candidates . add ( reporter ) candidates = list ( candidates ) print ( \"JIRA is unassigned, choose assignee\" ) for idx , author in enumerate ( candidates ) : if author . key == \"apachespark\" : continue annotations = [ \"Reporter\" ] if author == reporter else [ ] if author in commentors : annotations . append ( \"Commentor\" ) print ( \"[%d] %s (%s)\" % ( idx , author . displayName , \",\" . join ( annotations ) ) ) raw_assignee = input ( \"Enter number of user, or userid, to assign to (blank to leave unassigned):\" ) if raw_assignee == \"\" : return None else : try : id = int ( raw_assignee ) assignee = candidates [ id ] except : # assume it's a user id, and try to assign (might fail, we just prompt again) assignee = asf_jira . user ( raw_assignee ) asf_jira . assign_issue ( issue . key , assignee . key ) return assignee except KeyboardInterrupt : raise except : traceback . print_exc ( ) print ( \"Error assigning JIRA, try again (or leave blank and fix manually)\" )\n\n\n        Summary:", "input": "", "output": "Prompt the user to choose who to assign the issue to in jira given a list of candidates including the original reporter and all commentors"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def standardize_jira_ref ( text ) : jira_refs = [ ] components = [ ] # If the string is compliant, no need to process any further if ( re . search ( r'^\\[SPARK-[0-9]{3,6}\\](\\[[A-Z0-9_\\s,]+\\] )+\\S+' , text ) ) : return text # Extract JIRA ref(s): pattern = re . compile ( r'(SPARK[-\\s]*[0-9]{3,6})+' , re . IGNORECASE ) for ref in pattern . findall ( text ) : # Add brackets, replace spaces with a dash, & convert to uppercase jira_refs . append ( '[' + re . sub ( r'\\s+' , '-' , ref . upper ( ) ) + ']' ) text = text . replace ( ref , '' ) # Extract spark component(s): # Look for alphanumeric chars, spaces, dashes, periods, and/or commas pattern = re . compile ( r'(\\[[\\w\\s,.-]+\\])' , re . IGNORECASE ) for component in pattern . findall ( text ) : components . append ( component . upper ( ) ) text = text . replace ( component , '' ) # Cleanup any remaining symbols: pattern = re . compile ( r'^\\W+(.*)' , re . IGNORECASE ) if ( pattern . search ( text ) is not None ) : text = pattern . search ( text ) . groups ( ) [ 0 ] # Assemble full text (JIRA ref(s), module(s), remaining text) clean_text = '' . join ( jira_refs ) . strip ( ) + '' . join ( components ) . strip ( ) + \" \" + text . strip ( ) # Replace multiple spaces with a single space, e.g. if no jira refs and/or components were # included clean_text = re . sub ( r'\\s+' , ' ' , clean_text . strip ( ) ) return clean_text\n\n\n        Summary:", "input": "", "output": "Standardize the [ SPARK - XXXXX ] [ MODULE ] prefix Converts [ SPARK - XXX ] [ mllib ] Issue [ MLLib ] SPARK - XXX . Issue or SPARK XXX [ MLLIB ] : Issue to [ SPARK - XXX ] [ MLLIB ] Issue"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_libsvm_line ( line ) : items = line . split ( None ) label = float ( items [ 0 ] ) nnz = len ( items ) - 1 indices = np . zeros ( nnz , dtype = np . int32 ) values = np . zeros ( nnz ) for i in xrange ( nnz ) : index , value = items [ 1 + i ] . split ( \":\" ) indices [ i ] = int ( index ) - 1 values [ i ] = float ( value ) return label , indices , values\n\n\n        Summary:", "input": "", "output": "Parses a line in LIBSVM format into ( label indices values ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_labeled_point_to_libsvm ( p ) : from pyspark . mllib . regression import LabeledPoint assert isinstance ( p , LabeledPoint ) items = [ str ( p . label ) ] v = _convert_to_vector ( p . features ) if isinstance ( v , SparseVector ) : nnz = len ( v . indices ) for i in xrange ( nnz ) : items . append ( str ( v . indices [ i ] + 1 ) + \":\" + str ( v . values [ i ] ) ) else : for i in xrange ( len ( v ) ) : items . append ( str ( i + 1 ) + \":\" + str ( v [ i ] ) ) return \" \" . join ( items )\n\n\n        Summary:", "input": "", "output": "Converts a LabeledPoint to a string in LIBSVM format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def loadLibSVMFile ( sc , path , numFeatures = - 1 , minPartitions = None ) : from pyspark . mllib . regression import LabeledPoint lines = sc . textFile ( path , minPartitions ) parsed = lines . map ( lambda l : MLUtils . _parse_libsvm_line ( l ) ) if numFeatures <= 0 : parsed . cache ( ) numFeatures = parsed . map ( lambda x : - 1 if x [ 1 ] . size == 0 else x [ 1 ] [ - 1 ] ) . reduce ( max ) + 1 return parsed . map ( lambda x : LabeledPoint ( x [ 0 ] , Vectors . sparse ( numFeatures , x [ 1 ] , x [ 2 ] ) ) )\n\n\n        Summary:", "input": "", "output": "Loads labeled data in the LIBSVM format into an RDD of LabeledPoint . The LIBSVM format is a text - based format used by LIBSVM and LIBLINEAR . Each line represents a labeled sparse feature vector using the following format :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def saveAsLibSVMFile ( data , dir ) : lines = data . map ( lambda p : MLUtils . _convert_labeled_point_to_libsvm ( p ) ) lines . saveAsTextFile ( dir )\n\n\n        Summary:", "input": "", "output": "Save labeled data in LIBSVM format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def loadLabeledPoints ( sc , path , minPartitions = None ) : minPartitions = minPartitions or min ( sc . defaultParallelism , 2 ) return callMLlibFunc ( \"loadLabeledPoints\" , sc , path , minPartitions )\n\n\n        Summary:", "input": "", "output": "Load labeled points saved using RDD . saveAsTextFile ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def appendBias ( data ) : vec = _convert_to_vector ( data ) if isinstance ( vec , SparseVector ) : newIndices = np . append ( vec . indices , len ( vec ) ) newValues = np . append ( vec . values , 1.0 ) return SparseVector ( len ( vec ) + 1 , newIndices , newValues ) else : return _convert_to_vector ( np . append ( vec . toArray ( ) , 1.0 ) )\n\n\n        Summary:", "input": "", "output": "Returns a new vector with 1 . 0 ( bias ) appended to the end of the input vector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convertVectorColumnsToML ( dataset , * cols ) : if not isinstance ( dataset , DataFrame ) : raise TypeError ( \"Input dataset must be a DataFrame but got {}.\" . format ( type ( dataset ) ) ) return callMLlibFunc ( \"convertVectorColumnsToML\" , dataset , list ( cols ) )\n\n\n        Summary:", "input": "", "output": "Converts vector columns in an input DataFrame from the : py : class : pyspark . mllib . linalg . Vector type to the new : py : class : pyspark . ml . linalg . Vector type under the spark . ml package ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generateLinearInput ( intercept , weights , xMean , xVariance , nPoints , seed , eps ) : weights = [ float ( weight ) for weight in weights ] xMean = [ float ( mean ) for mean in xMean ] xVariance = [ float ( var ) for var in xVariance ] return list ( callMLlibFunc ( \"generateLinearInputWrapper\" , float ( intercept ) , weights , xMean , xVariance , int ( nPoints ) , int ( seed ) , float ( eps ) ) )\n\n\n        Summary:", "input": "", "output": ": param : intercept bias factor the term c in X w + c : param : weights feature vector the term w in X w + c : param : xMean Point around which the data X is centered . : param : xVariance Variance of the given data : param : nPoints Number of points to be generated : param : seed Random Seed : param : eps Used to scale the noise . If eps is set high the amount of gaussian noise added is more ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generateLinearRDD ( sc , nexamples , nfeatures , eps , nParts = 2 , intercept = 0.0 ) : return callMLlibFunc ( \"generateLinearRDDWrapper\" , sc , int ( nexamples ) , int ( nfeatures ) , float ( eps ) , int ( nParts ) , float ( intercept ) )\n\n\n        Summary:", "input": "", "output": "Generate an RDD of LabeledPoints ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , data , iterations = 100 , step = 1.0 , miniBatchFraction = 1.0 , initialWeights = None , regParam = 0.0 , regType = None , intercept = False , validateData = True , convergenceTol = 0.001 ) : warnings . warn ( \"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\" , DeprecationWarning ) def train ( rdd , i ) : return callMLlibFunc ( \"trainLinearRegressionModelWithSGD\" , rdd , int ( iterations ) , float ( step ) , float ( miniBatchFraction ) , i , float ( regParam ) , regType , bool ( intercept ) , bool ( validateData ) , float ( convergenceTol ) ) return _regression_train_wrapper ( train , LinearRegressionModel , data , initialWeights )\n\n\n        Summary:", "input": "", "output": "Train a linear regression model using Stochastic Gradient Descent ( SGD ) . This solves the least squares regression formulation"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def predict ( self , x ) : if isinstance ( x , RDD ) : return x . map ( lambda v : self . predict ( v ) ) return np . interp ( x , self . boundaries , self . predictions )\n\n\n        Summary:", "input": "", "output": "Predict labels for provided features . Using a piecewise linear function . 1 ) If x exactly matches a boundary then associated prediction is returned . In case there are multiple predictions with the same boundary then one of them is returned . Which one is undefined ( same as java . util . Arrays . binarySearch ) . 2 ) If x is lower or higher than all boundaries then first or last prediction is returned respectively . In case there are multiple predictions with the same boundary then the lowest or highest is returned respectively . 3 ) If x falls between two values in boundary array then prediction is treated as piecewise linear function and interpolated value is returned . In case there are multiple values with the same boundary then the same rules as in 2 ) are used ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save ( self , sc , path ) : java_boundaries = _py2java ( sc , self . boundaries . tolist ( ) ) java_predictions = _py2java ( sc , self . predictions . tolist ( ) ) java_model = sc . _jvm . org . apache . spark . mllib . regression . IsotonicRegressionModel ( java_boundaries , java_predictions , self . isotonic ) java_model . save ( sc . _jsc . sc ( ) , path )\n\n\n        Summary:", "input": "", "output": "Save an IsotonicRegressionModel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( cls , sc , path ) : java_model = sc . _jvm . org . apache . spark . mllib . regression . IsotonicRegressionModel . load ( sc . _jsc . sc ( ) , path ) py_boundaries = _java2py ( sc , java_model . boundaryVector ( ) ) . toArray ( ) py_predictions = _java2py ( sc , java_model . predictionVector ( ) ) . toArray ( ) return IsotonicRegressionModel ( py_boundaries , py_predictions , java_model . isotonic )\n\n\n        Summary:", "input": "", "output": "Load an IsotonicRegressionModel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , data , isotonic = True ) : boundaries , predictions = callMLlibFunc ( \"trainIsotonicRegressionModel\" , data . map ( _convert_to_vector ) , bool ( isotonic ) ) return IsotonicRegressionModel ( boundaries . toArray ( ) , predictions . toArray ( ) , isotonic )\n\n\n        Summary:", "input": "", "output": "Train an isotonic regression model on the given data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def columnSimilarities ( self , threshold = 0.0 ) : java_sims_mat = self . _java_matrix_wrapper . call ( \"columnSimilarities\" , float ( threshold ) ) return CoordinateMatrix ( java_sims_mat )\n\n\n        Summary:", "input": "", "output": "Compute similarities between columns of this matrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tallSkinnyQR ( self , computeQ = False ) : decomp = JavaModelWrapper ( self . _java_matrix_wrapper . call ( \"tallSkinnyQR\" , computeQ ) ) if computeQ : java_Q = decomp . call ( \"Q\" ) Q = RowMatrix ( java_Q ) else : Q = None R = decomp . call ( \"R\" ) return QRDecomposition ( Q , R )\n\n\n        Summary:", "input": "", "output": "Compute the QR decomposition of this RowMatrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def computeSVD ( self , k , computeU = False , rCond = 1e-9 ) : j_model = self . _java_matrix_wrapper . call ( \"computeSVD\" , int ( k ) , bool ( computeU ) , float ( rCond ) ) return SingularValueDecomposition ( j_model )\n\n\n        Summary:", "input": "", "output": "Computes the singular value decomposition of the RowMatrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def multiply ( self , matrix ) : if not isinstance ( matrix , DenseMatrix ) : raise ValueError ( \"Only multiplication with DenseMatrix \" \"is supported.\" ) j_model = self . _java_matrix_wrapper . call ( \"multiply\" , matrix ) return RowMatrix ( j_model )\n\n\n        Summary:", "input": "", "output": "Multiply this matrix by a local dense matrix on the right ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def U ( self ) : u = self . call ( \"U\" ) if u is not None : mat_name = u . getClass ( ) . getSimpleName ( ) if mat_name == \"RowMatrix\" : return RowMatrix ( u ) elif mat_name == \"IndexedRowMatrix\" : return IndexedRowMatrix ( u ) else : raise TypeError ( \"Expected RowMatrix/IndexedRowMatrix got %s\" % mat_name )\n\n\n        Summary:", "input": "", "output": "Returns a distributed matrix whose columns are the left singular vectors of the SingularValueDecomposition if computeU was set to be True ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rows ( self ) : # We use DataFrames for serialization of IndexedRows from # Java, so we first convert the RDD of rows to a DataFrame # on the Scala/Java side. Then we map each Row in the # DataFrame back to an IndexedRow on this side. rows_df = callMLlibFunc ( \"getIndexedRows\" , self . _java_matrix_wrapper . _java_model ) rows = rows_df . rdd . map ( lambda row : IndexedRow ( row [ 0 ] , row [ 1 ] ) ) return rows\n\n\n        Summary:", "input": "", "output": "Rows of the IndexedRowMatrix stored as an RDD of IndexedRows ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toBlockMatrix ( self , rowsPerBlock = 1024 , colsPerBlock = 1024 ) : java_block_matrix = self . _java_matrix_wrapper . call ( \"toBlockMatrix\" , rowsPerBlock , colsPerBlock ) return BlockMatrix ( java_block_matrix , rowsPerBlock , colsPerBlock )\n\n\n        Summary:", "input": "", "output": "Convert this matrix to a BlockMatrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def multiply ( self , matrix ) : if not isinstance ( matrix , DenseMatrix ) : raise ValueError ( \"Only multiplication with DenseMatrix \" \"is supported.\" ) return IndexedRowMatrix ( self . _java_matrix_wrapper . call ( \"multiply\" , matrix ) )\n\n\n        Summary:", "input": "", "output": "Multiply this matrix by a local dense matrix on the right ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def entries ( self ) : # We use DataFrames for serialization of MatrixEntry entries # from Java, so we first convert the RDD of entries to a # DataFrame on the Scala/Java side. Then we map each Row in # the DataFrame back to a MatrixEntry on this side. entries_df = callMLlibFunc ( \"getMatrixEntries\" , self . _java_matrix_wrapper . _java_model ) entries = entries_df . rdd . map ( lambda row : MatrixEntry ( row [ 0 ] , row [ 1 ] , row [ 2 ] ) ) return entries\n\n\n        Summary:", "input": "", "output": "Entries of the CoordinateMatrix stored as an RDD of MatrixEntries ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def blocks ( self ) : # We use DataFrames for serialization of sub-matrix blocks # from Java, so we first convert the RDD of blocks to a # DataFrame on the Scala/Java side. Then we map each Row in # the DataFrame back to a sub-matrix block on this side. blocks_df = callMLlibFunc ( \"getMatrixBlocks\" , self . _java_matrix_wrapper . _java_model ) blocks = blocks_df . rdd . map ( lambda row : ( ( row [ 0 ] [ 0 ] , row [ 0 ] [ 1 ] ) , row [ 1 ] ) ) return blocks\n\n\n        Summary:", "input": "", "output": "The RDD of sub - matrix blocks (( blockRowIndex blockColIndex ) sub - matrix ) that form this distributed matrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def persist ( self , storageLevel ) : if not isinstance ( storageLevel , StorageLevel ) : raise TypeError ( \"`storageLevel` should be a StorageLevel, got %s\" % type ( storageLevel ) ) javaStorageLevel = self . _java_matrix_wrapper . _sc . _getJavaStorageLevel ( storageLevel ) self . _java_matrix_wrapper . call ( \"persist\" , javaStorageLevel ) return self\n\n\n        Summary:", "input": "", "output": "Persists the underlying RDD with the specified storage level ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add ( self , other ) : if not isinstance ( other , BlockMatrix ) : raise TypeError ( \"Other should be a BlockMatrix, got %s\" % type ( other ) ) other_java_block_matrix = other . _java_matrix_wrapper . _java_model java_block_matrix = self . _java_matrix_wrapper . call ( \"add\" , other_java_block_matrix ) return BlockMatrix ( java_block_matrix , self . rowsPerBlock , self . colsPerBlock )\n\n\n        Summary:", "input": "", "output": "Adds two block matrices together . The matrices must have the same size and matching rowsPerBlock and colsPerBlock values . If one of the sub matrix blocks that are being added is a SparseMatrix the resulting sub matrix block will also be a SparseMatrix even if it is being added to a DenseMatrix . If two dense sub matrix blocks are added the output block will also be a DenseMatrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transpose ( self ) : java_transposed_matrix = self . _java_matrix_wrapper . call ( \"transpose\" ) return BlockMatrix ( java_transposed_matrix , self . colsPerBlock , self . rowsPerBlock )\n\n\n        Summary:", "input": "", "output": "Transpose this BlockMatrix . Returns a new BlockMatrix instance sharing the same underlying data . Is a lazy operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _vector_size ( v ) : if isinstance ( v , Vector ) : return len ( v ) elif type ( v ) in ( array . array , list , tuple , xrange ) : return len ( v ) elif type ( v ) == np . ndarray : if v . ndim == 1 or ( v . ndim == 2 and v . shape [ 1 ] == 1 ) : return len ( v ) else : raise ValueError ( \"Cannot treat an ndarray of shape %s as a vector\" % str ( v . shape ) ) elif _have_scipy and scipy . sparse . issparse ( v ) : assert v . shape [ 1 ] == 1 , \"Expected column vector\" return v . shape [ 0 ] else : raise TypeError ( \"Cannot treat type %s as a vector\" % type ( v ) )\n\n\n        Summary:", "input": "", "output": "Returns the size of the vector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parse ( s ) : start = s . find ( '[' ) if start == - 1 : raise ValueError ( \"Array should start with '['.\" ) end = s . find ( ']' ) if end == - 1 : raise ValueError ( \"Array should end with ']'.\" ) s = s [ start + 1 : end ] try : values = [ float ( val ) for val in s . split ( ',' ) if val ] except ValueError : raise ValueError ( \"Unable to parse values from %s\" % s ) return DenseVector ( values )\n\n\n        Summary:", "input": "", "output": "Parse string representation back into the DenseVector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dot ( self , other ) : if type ( other ) == np . ndarray : if other . ndim > 1 : assert len ( self ) == other . shape [ 0 ] , \"dimension mismatch\" return np . dot ( self . array , other ) elif _have_scipy and scipy . sparse . issparse ( other ) : assert len ( self ) == other . shape [ 0 ] , \"dimension mismatch\" return other . transpose ( ) . dot ( self . toArray ( ) ) else : assert len ( self ) == _vector_size ( other ) , \"dimension mismatch\" if isinstance ( other , SparseVector ) : return other . dot ( self ) elif isinstance ( other , Vector ) : return np . dot ( self . toArray ( ) , other . toArray ( ) ) else : return np . dot ( self . toArray ( ) , other )\n\n\n        Summary:", "input": "", "output": "Compute the dot product of two Vectors . We support ( Numpy array list SparseVector or SciPy sparse ) and a target NumPy array that is either 1 - or 2 - dimensional . Equivalent to calling numpy . dot of the two vectors ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def squared_distance ( self , other ) : assert len ( self ) == _vector_size ( other ) , \"dimension mismatch\" if isinstance ( other , SparseVector ) : return other . squared_distance ( self ) elif _have_scipy and scipy . sparse . issparse ( other ) : return _convert_to_vector ( other ) . squared_distance ( self ) if isinstance ( other , Vector ) : other = other . toArray ( ) elif not isinstance ( other , np . ndarray ) : other = np . array ( other ) diff = self . toArray ( ) - other return np . dot ( diff , diff )\n\n\n        Summary:", "input": "", "output": "Squared distance of two Vectors ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parse ( s ) : start = s . find ( '(' ) if start == - 1 : raise ValueError ( \"Tuple should start with '('\" ) end = s . find ( ')' ) if end == - 1 : raise ValueError ( \"Tuple should end with ')'\" ) s = s [ start + 1 : end ] . strip ( ) size = s [ : s . find ( ',' ) ] try : size = int ( size ) except ValueError : raise ValueError ( \"Cannot parse size %s.\" % size ) ind_start = s . find ( '[' ) if ind_start == - 1 : raise ValueError ( \"Indices array should start with '['.\" ) ind_end = s . find ( ']' ) if ind_end == - 1 : raise ValueError ( \"Indices array should end with ']'\" ) new_s = s [ ind_start + 1 : ind_end ] ind_list = new_s . split ( ',' ) try : indices = [ int ( ind ) for ind in ind_list if ind ] except ValueError : raise ValueError ( \"Unable to parse indices from %s.\" % new_s ) s = s [ ind_end + 1 : ] . strip ( ) val_start = s . find ( '[' ) if val_start == - 1 : raise ValueError ( \"Values array should start with '['.\" ) val_end = s . find ( ']' ) if val_end == - 1 : raise ValueError ( \"Values array should end with ']'.\" ) val_list = s [ val_start + 1 : val_end ] . split ( ',' ) try : values = [ float ( val ) for val in val_list if val ] except ValueError : raise ValueError ( \"Unable to parse values from %s.\" % s ) return SparseVector ( size , indices , values )\n\n\n        Summary:", "input": "", "output": "Parse string representation back into the SparseVector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dot ( self , other ) : if isinstance ( other , np . ndarray ) : if other . ndim not in [ 2 , 1 ] : raise ValueError ( \"Cannot call dot with %d-dimensional array\" % other . ndim ) assert len ( self ) == other . shape [ 0 ] , \"dimension mismatch\" return np . dot ( self . values , other [ self . indices ] ) assert len ( self ) == _vector_size ( other ) , \"dimension mismatch\" if isinstance ( other , DenseVector ) : return np . dot ( other . array [ self . indices ] , self . values ) elif isinstance ( other , SparseVector ) : # Find out common indices. self_cmind = np . in1d ( self . indices , other . indices , assume_unique = True ) self_values = self . values [ self_cmind ] if self_values . size == 0 : return 0.0 else : other_cmind = np . in1d ( other . indices , self . indices , assume_unique = True ) return np . dot ( self_values , other . values [ other_cmind ] ) else : return self . dot ( _convert_to_vector ( other ) )\n\n\n        Summary:", "input": "", "output": "Dot product with a SparseVector or 1 - or 2 - dimensional Numpy array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def squared_distance ( self , other ) : assert len ( self ) == _vector_size ( other ) , \"dimension mismatch\" if isinstance ( other , np . ndarray ) or isinstance ( other , DenseVector ) : if isinstance ( other , np . ndarray ) and other . ndim != 1 : raise Exception ( \"Cannot call squared_distance with %d-dimensional array\" % other . ndim ) if isinstance ( other , DenseVector ) : other = other . array sparse_ind = np . zeros ( other . size , dtype = bool ) sparse_ind [ self . indices ] = True dist = other [ sparse_ind ] - self . values result = np . dot ( dist , dist ) other_ind = other [ ~ sparse_ind ] result += np . dot ( other_ind , other_ind ) return result elif isinstance ( other , SparseVector ) : result = 0.0 i , j = 0 , 0 while i < len ( self . indices ) and j < len ( other . indices ) : if self . indices [ i ] == other . indices [ j ] : diff = self . values [ i ] - other . values [ j ] result += diff * diff i += 1 j += 1 elif self . indices [ i ] < other . indices [ j ] : result += self . values [ i ] * self . values [ i ] i += 1 else : result += other . values [ j ] * other . values [ j ] j += 1 while i < len ( self . indices ) : result += self . values [ i ] * self . values [ i ] i += 1 while j < len ( other . indices ) : result += other . values [ j ] * other . values [ j ] j += 1 return result else : return self . squared_distance ( _convert_to_vector ( other ) )\n\n\n        Summary:", "input": "", "output": "Squared distance from a SparseVector or 1 - dimensional NumPy array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toArray ( self ) : arr = np . zeros ( ( self . size , ) , dtype = np . float64 ) arr [ self . indices ] = self . values return arr\n\n\n        Summary:", "input": "", "output": "Returns a copy of this SparseVector as a 1 - dimensional NumPy array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asML ( self ) : return newlinalg . SparseVector ( self . size , self . indices , self . values )\n\n\n        Summary:", "input": "", "output": "Convert this vector to the new mllib - local representation . This does NOT copy the data ; it copies references ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dense ( * elements ) : if len ( elements ) == 1 and not isinstance ( elements [ 0 ] , ( float , int , long ) ) : # it's list, numpy.array or other iterable object. elements = elements [ 0 ] return DenseVector ( elements )\n\n\n        Summary:", "input": "", "output": "Create a dense vector of 64 - bit floats from a Python list or numbers ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fromML ( vec ) : if isinstance ( vec , newlinalg . DenseVector ) : return DenseVector ( vec . array ) elif isinstance ( vec , newlinalg . SparseVector ) : return SparseVector ( vec . size , vec . indices , vec . values ) else : raise TypeError ( \"Unsupported vector type %s\" % type ( vec ) )\n\n\n        Summary:", "input": "", "output": "Convert a vector from the new mllib - local representation . This does NOT copy the data ; it copies references ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def squared_distance ( v1 , v2 ) : v1 , v2 = _convert_to_vector ( v1 ) , _convert_to_vector ( v2 ) return v1 . squared_distance ( v2 )\n\n\n        Summary:", "input": "", "output": "Squared distance between two vectors . a and b can be of type SparseVector DenseVector np . ndarray or array . array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parse ( s ) : if s . find ( '(' ) == - 1 and s . find ( '[' ) != - 1 : return DenseVector . parse ( s ) elif s . find ( '(' ) != - 1 : return SparseVector . parse ( s ) else : raise ValueError ( \"Cannot find tokens '[' or '(' from the input string.\" )\n\n\n        Summary:", "input": "", "output": "Parse a string representation back into the Vector ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _equals ( v1_indices , v1_values , v2_indices , v2_values ) : v1_size = len ( v1_values ) v2_size = len ( v2_values ) k1 = 0 k2 = 0 all_equal = True while all_equal : while k1 < v1_size and v1_values [ k1 ] == 0 : k1 += 1 while k2 < v2_size and v2_values [ k2 ] == 0 : k2 += 1 if k1 >= v1_size or k2 >= v2_size : return k1 >= v1_size and k2 >= v2_size all_equal = v1_indices [ k1 ] == v2_indices [ k2 ] and v1_values [ k1 ] == v2_values [ k2 ] k1 += 1 k2 += 1 return all_equal\n\n\n        Summary:", "input": "", "output": "Check equality between sparse / dense vectors v1_indices and v2_indices assume to be strictly increasing ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_to_array ( array_like , dtype ) : if isinstance ( array_like , bytes ) : return np . frombuffer ( array_like , dtype = dtype ) return np . asarray ( array_like , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Convert Matrix attributes which are array - like or buffer to array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toArray ( self ) : if self . isTransposed : return np . asfortranarray ( self . values . reshape ( ( self . numRows , self . numCols ) ) ) else : return self . values . reshape ( ( self . numRows , self . numCols ) , order = 'F' )\n\n\n        Summary:", "input": "", "output": "Return an numpy . ndarray"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toSparse ( self ) : if self . isTransposed : values = np . ravel ( self . toArray ( ) , order = 'F' ) else : values = self . values indices = np . nonzero ( values ) [ 0 ] colCounts = np . bincount ( indices // self . numRows ) colPtrs = np . cumsum ( np . hstack ( ( 0 , colCounts , np . zeros ( self . numCols - colCounts . size ) ) ) ) values = values [ indices ] rowIndices = indices % self . numRows return SparseMatrix ( self . numRows , self . numCols , colPtrs , rowIndices , values )\n\n\n        Summary:", "input": "", "output": "Convert to SparseMatrix"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asML ( self ) : return newlinalg . DenseMatrix ( self . numRows , self . numCols , self . values , self . isTransposed )\n\n\n        Summary:", "input": "", "output": "Convert this matrix to the new mllib - local representation . This does NOT copy the data ; it copies references ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toArray ( self ) : A = np . zeros ( ( self . numRows , self . numCols ) , dtype = np . float64 , order = 'F' ) for k in xrange ( self . colPtrs . size - 1 ) : startptr = self . colPtrs [ k ] endptr = self . colPtrs [ k + 1 ] if self . isTransposed : A [ k , self . rowIndices [ startptr : endptr ] ] = self . values [ startptr : endptr ] else : A [ self . rowIndices [ startptr : endptr ] , k ] = self . values [ startptr : endptr ] return A\n\n\n        Summary:", "input": "", "output": "Return an numpy . ndarray"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asML ( self ) : return newlinalg . SparseMatrix ( self . numRows , self . numCols , self . colPtrs , self . rowIndices , self . values , self . isTransposed )\n\n\n        Summary:", "input": "", "output": "Convert this matrix to the new mllib - local representation . This does NOT copy the data ; it copies references ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sparse ( numRows , numCols , colPtrs , rowIndices , values ) : return SparseMatrix ( numRows , numCols , colPtrs , rowIndices , values )\n\n\n        Summary:", "input": "", "output": "Create a SparseMatrix"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fromML ( mat ) : if isinstance ( mat , newlinalg . DenseMatrix ) : return DenseMatrix ( mat . numRows , mat . numCols , mat . values , mat . isTransposed ) elif isinstance ( mat , newlinalg . SparseMatrix ) : return SparseMatrix ( mat . numRows , mat . numCols , mat . colPtrs , mat . rowIndices , mat . values , mat . isTransposed ) else : raise TypeError ( \"Unsupported matrix type %s\" % type ( mat ) )\n\n\n        Summary:", "input": "", "output": "Convert a matrix from the new mllib - local representation . This does NOT copy the data ; it copies references ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def approxNearestNeighbors ( self , dataset , key , numNearestNeighbors , distCol = \"distCol\" ) : return self . _call_java ( \"approxNearestNeighbors\" , dataset , key , numNearestNeighbors , distCol )\n\n\n        Summary:", "input": "", "output": "Given a large dataset and an item approximately find at most k items which have the closest distance to the item . If the : py : attr : outputCol is missing the method will transform the data ; if the : py : attr : outputCol exists it will use that . This allows caching of the transformed data when necessary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def approxSimilarityJoin ( self , datasetA , datasetB , threshold , distCol = \"distCol\" ) : threshold = TypeConverters . toFloat ( threshold ) return self . _call_java ( \"approxSimilarityJoin\" , datasetA , datasetB , threshold , distCol )\n\n\n        Summary:", "input": "", "output": "Join two datasets to approximately find all pairs of rows whose distance are smaller than the threshold . If the : py : attr : outputCol is missing the method will transform the data ; if the : py : attr : outputCol exists it will use that . This allows caching of the transformed data when necessary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_labels ( cls , labels , inputCol , outputCol = None , handleInvalid = None ) : sc = SparkContext . _active_spark_context java_class = sc . _gateway . jvm . java . lang . String jlabels = StringIndexerModel . _new_java_array ( labels , java_class ) model = StringIndexerModel . _create_from_java_class ( \"org.apache.spark.ml.feature.StringIndexerModel\" , jlabels ) model . setInputCol ( inputCol ) if outputCol is not None : model . setOutputCol ( outputCol ) if handleInvalid is not None : model . setHandleInvalid ( handleInvalid ) return model\n\n\n        Summary:", "input": "", "output": "Construct the model directly from an array of label strings requires an active SparkContext ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_arrays_of_labels ( cls , arrayOfLabels , inputCols , outputCols = None , handleInvalid = None ) : sc = SparkContext . _active_spark_context java_class = sc . _gateway . jvm . java . lang . String jlabels = StringIndexerModel . _new_java_array ( arrayOfLabels , java_class ) model = StringIndexerModel . _create_from_java_class ( \"org.apache.spark.ml.feature.StringIndexerModel\" , jlabels ) model . setInputCols ( inputCols ) if outputCols is not None : model . setOutputCols ( outputCols ) if handleInvalid is not None : model . setHandleInvalid ( handleInvalid ) return model\n\n\n        Summary:", "input": "", "output": "Construct the model directly from an array of array of label strings requires an active SparkContext ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setParams ( self , inputCol = None , outputCol = None , stopWords = None , caseSensitive = False , locale = None ) : kwargs = self . _input_kwargs return self . _set ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "setParams ( self inputCol = None outputCol = None stopWords = None caseSensitive = false \\ locale = None ) Sets params for this StopWordRemover ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def loadDefaultStopWords ( language ) : stopWordsObj = _jvm ( ) . org . apache . spark . ml . feature . StopWordsRemover return list ( stopWordsObj . loadDefaultStopWords ( language ) )\n\n\n        Summary:", "input": "", "output": "Loads the default stop words for the given language . Supported languages : danish dutch english finnish french german hungarian italian norwegian portuguese russian spanish swedish turkish"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def findSynonyms ( self , word , num ) : if not isinstance ( word , basestring ) : word = _convert_to_vector ( word ) return self . _call_java ( \"findSynonyms\" , word , num )\n\n\n        Summary:", "input": "", "output": "Find num number of words closest in similarity to word . word can be a string or vector representation . Returns a dataframe with two fields word and similarity ( which gives the cosine similarity ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def findSynonymsArray ( self , word , num ) : if not isinstance ( word , basestring ) : word = _convert_to_vector ( word ) tuples = self . _java_obj . findSynonymsArray ( word , num ) return list ( map ( lambda st : ( st . _1 ( ) , st . _2 ( ) ) , list ( tuples ) ) )\n\n\n        Summary:", "input": "", "output": "Find num number of words closest in similarity to word . word can be a string or vector representation . Returns an array with two fields word and similarity ( which gives the cosine similarity ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def install_exception_handler ( ) : original = py4j . protocol . get_return_value # The original `get_return_value` is not patched, it's idempotent. patched = capture_sql_exception ( original ) # only patch the one used in py4j.java_gateway (call Java API) py4j . java_gateway . get_return_value = patched\n\n\n        Summary:", "input": "", "output": "Hook an exception handler into Py4j which could capture some SQL exceptions in Java ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toJArray ( gateway , jtype , arr ) : jarr = gateway . new_array ( jtype , len ( arr ) ) for i in range ( 0 , len ( arr ) ) : jarr [ i ] = arr [ i ] return jarr\n\n\n        Summary:", "input": "", "output": "Convert python list to java type array : param gateway : Py4j Gateway : param jtype : java type of element in array : param arr : python type list"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def require_minimum_pandas_version ( ) : # TODO(HyukjinKwon): Relocate and deduplicate the version specification. minimum_pandas_version = \"0.19.2\" from distutils . version import LooseVersion try : import pandas have_pandas = True except ImportError : have_pandas = False if not have_pandas : raise ImportError ( \"Pandas >= %s must be installed; however, \" \"it was not found.\" % minimum_pandas_version ) if LooseVersion ( pandas . __version__ ) < LooseVersion ( minimum_pandas_version ) : raise ImportError ( \"Pandas >= %s must be installed; however, \" \"your version was %s.\" % ( minimum_pandas_version , pandas . __version__ ) )\n\n\n        Summary:", "input": "", "output": "Raise ImportError if minimum version of Pandas is not installed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def require_minimum_pyarrow_version ( ) : # TODO(HyukjinKwon): Relocate and deduplicate the version specification. minimum_pyarrow_version = \"0.12.1\" from distutils . version import LooseVersion try : import pyarrow have_arrow = True except ImportError : have_arrow = False if not have_arrow : raise ImportError ( \"PyArrow >= %s must be installed; however, \" \"it was not found.\" % minimum_pyarrow_version ) if LooseVersion ( pyarrow . __version__ ) < LooseVersion ( minimum_pyarrow_version ) : raise ImportError ( \"PyArrow >= %s must be installed; however, \" \"your version was %s.\" % ( minimum_pyarrow_version , pyarrow . __version__ ) )\n\n\n        Summary:", "input": "", "output": "Raise ImportError if minimum version of pyarrow is not installed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def launch_gateway ( conf = None , popen_kwargs = None ) : if \"PYSPARK_GATEWAY_PORT\" in os . environ : gateway_port = int ( os . environ [ \"PYSPARK_GATEWAY_PORT\" ] ) gateway_secret = os . environ [ \"PYSPARK_GATEWAY_SECRET\" ] # Process already exists proc = None else : SPARK_HOME = _find_spark_home ( ) # Launch the Py4j gateway using Spark's run command so that we pick up the # proper classpath and settings from spark-env.sh on_windows = platform . system ( ) == \"Windows\" script = \"./bin/spark-submit.cmd\" if on_windows else \"./bin/spark-submit\" command = [ os . path . join ( SPARK_HOME , script ) ] if conf : for k , v in conf . getAll ( ) : command += [ '--conf' , '%s=%s' % ( k , v ) ] submit_args = os . environ . get ( \"PYSPARK_SUBMIT_ARGS\" , \"pyspark-shell\" ) if os . environ . get ( \"SPARK_TESTING\" ) : submit_args = ' ' . join ( [ \"--conf spark.ui.enabled=false\" , submit_args ] ) command = command + shlex . split ( submit_args ) # Create a temporary directory where the gateway server should write the connection # information. conn_info_dir = tempfile . mkdtemp ( ) try : fd , conn_info_file = tempfile . mkstemp ( dir = conn_info_dir ) os . close ( fd ) os . unlink ( conn_info_file ) env = dict ( os . environ ) env [ \"_PYSPARK_DRIVER_CONN_INFO_PATH\" ] = conn_info_file # Launch the Java gateway. popen_kwargs = { } if popen_kwargs is None else popen_kwargs # We open a pipe to stdin so that the Java gateway can die when the pipe is broken popen_kwargs [ 'stdin' ] = PIPE # We always set the necessary environment variables. popen_kwargs [ 'env' ] = env if not on_windows : # Don't send ctrl-c / SIGINT to the Java gateway: def preexec_func ( ) : signal . signal ( signal . SIGINT , signal . SIG_IGN ) popen_kwargs [ 'preexec_fn' ] = preexec_func proc = Popen ( command , * * popen_kwargs ) else : # preexec_fn not supported on Windows proc = Popen ( command , * * popen_kwargs ) # Wait for the file to appear, or for the process to exit, whichever happens first. while not proc . poll ( ) and not os . path . isfile ( conn_info_file ) : time . sleep ( 0.1 ) if not os . path . isfile ( conn_info_file ) : raise Exception ( \"Java gateway process exited before sending its port number\" ) with open ( conn_info_file , \"rb\" ) as info : gateway_port = read_int ( info ) gateway_secret = UTF8Deserializer ( ) . loads ( info ) finally : shutil . rmtree ( conn_info_dir ) # In Windows, ensure the Java child processes do not linger after Python has exited. # In UNIX-based systems, the child process can kill itself on broken pipe (i.e. when # the parent process' stdin sends an EOF). In Windows, however, this is not possible # because java.lang.Process reads directly from the parent process' stdin, contending # with any opportunity to read an EOF from the parent. Note that this is only best # effort and will not take effect if the python process is violently terminated. if on_windows : # In Windows, the child process here is \"spark-submit.cmd\", not the JVM itself # (because the UNIX \"exec\" command is not available). This means we cannot simply # call proc.kill(), which kills only the \"spark-submit.cmd\" process but not the # JVMs. Instead, we use \"taskkill\" with the tree-kill option \"/t\" to terminate all # child processes in the tree (http://technet.microsoft.com/en-us/library/bb491009.aspx) def killChild ( ) : Popen ( [ \"cmd\" , \"/c\" , \"taskkill\" , \"/f\" , \"/t\" , \"/pid\" , str ( proc . pid ) ] ) atexit . register ( killChild ) # Connect to the gateway gateway = JavaGateway ( gateway_parameters = GatewayParameters ( port = gateway_port , auth_token = gateway_secret , auto_convert = True ) ) # Store a reference to the Popen object for use by the caller (e.g., in reading stdout/stderr) gateway . proc = proc # Import the classes used by PySpark java_import ( gateway . jvm , \"org.apache.spark.SparkConf\" ) java_import ( gateway . jvm , \"org.apache.spark.api.java.*\" ) java_import ( gateway . jvm , \"org.apache.spark.api.python.*\" ) java_import ( gateway . jvm , \"org.apache.spark.ml.python.*\" ) java_import ( gateway . jvm , \"org.apache.spark.mllib.api.python.*\" ) # TODO(davies): move into sql java_import ( gateway . jvm , \"org.apache.spark.sql.*\" ) java_import ( gateway . jvm , \"org.apache.spark.sql.api.python.*\" ) java_import ( gateway . jvm , \"org.apache.spark.sql.hive.*\" ) java_import ( gateway . jvm , \"scala.Tuple2\" ) return gateway\n\n\n        Summary:", "input": "", "output": "launch jvm gateway : param conf : spark configuration passed to spark - submit : param popen_kwargs : Dictionary of kwargs to pass to Popen when spawning the py4j JVM . This is a developer feature intended for use in customizing how pyspark interacts with the py4j JVM ( e . g . capturing stdout / stderr ) . : return :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _do_server_auth ( conn , auth_secret ) : write_with_length ( auth_secret . encode ( \"utf-8\" ) , conn ) conn . flush ( ) reply = UTF8Deserializer ( ) . loads ( conn ) if reply != \"ok\" : conn . close ( ) raise Exception ( \"Unexpected reply from iterator server.\" )\n\n\n        Summary:", "input": "", "output": "Performs the authentication protocol defined by the SocketAuthHelper class on the given file - like object conn ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def local_connect_and_auth ( port , auth_secret ) : sock = None errors = [ ] # Support for both IPv4 and IPv6. # On most of IPv6-ready systems, IPv6 will take precedence. for res in socket . getaddrinfo ( \"127.0.0.1\" , port , socket . AF_UNSPEC , socket . SOCK_STREAM ) : af , socktype , proto , _ , sa = res try : sock = socket . socket ( af , socktype , proto ) sock . settimeout ( 15 ) sock . connect ( sa ) sockfile = sock . makefile ( \"rwb\" , 65536 ) _do_server_auth ( sockfile , auth_secret ) return ( sockfile , sock ) except socket . error as e : emsg = _exception_message ( e ) errors . append ( \"tried to connect to %s, but an error occured: %s\" % ( sa , emsg ) ) sock . close ( ) sock = None raise Exception ( \"could not open socket: %s\" % errors )\n\n\n        Summary:", "input": "", "output": "Connect to local host authenticate with it and return a ( sockfile sock ) for that connection . Handles IPV4 & IPV6 does some error handling . : param port : param auth_secret : return : a tuple with ( sockfile sock )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ensure_callback_server_started ( gw ) : # getattr will fallback to JVM, so we cannot test by hasattr() if \"_callback_server\" not in gw . __dict__ or gw . _callback_server is None : gw . callback_server_parameters . eager_load = True gw . callback_server_parameters . daemonize = True gw . callback_server_parameters . daemonize_connections = True gw . callback_server_parameters . port = 0 gw . start_callback_server ( gw . callback_server_parameters ) cbport = gw . _callback_server . server_socket . getsockname ( ) [ 1 ] gw . _callback_server . port = cbport # gateway with real port gw . _python_proxy_port = gw . _callback_server . port # get the GatewayServer object in JVM by ID jgws = JavaObject ( \"GATEWAY_SERVER\" , gw . _gateway_client ) # update the port of CallbackClient with real port jgws . resetCallbackClient ( jgws . getCallbackClient ( ) . getAddress ( ) , gw . _python_proxy_port )\n\n\n        Summary:", "input": "", "output": "Start callback server if not already started . The callback server is needed if the Java driver process needs to callback into the Python driver process to execute Python code ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _find_spark_home ( ) : # If the environment has SPARK_HOME set trust it. if \"SPARK_HOME\" in os . environ : return os . environ [ \"SPARK_HOME\" ] def is_spark_home ( path ) : \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\" return ( os . path . isfile ( os . path . join ( path , \"bin/spark-submit\" ) ) and ( os . path . isdir ( os . path . join ( path , \"jars\" ) ) or os . path . isdir ( os . path . join ( path , \"assembly\" ) ) ) ) paths = [ \"../\" , os . path . dirname ( os . path . realpath ( __file__ ) ) ] # Add the path of the PySpark module if it exists if sys . version < \"3\" : import imp try : module_home = imp . find_module ( \"pyspark\" ) [ 1 ] paths . append ( module_home ) # If we are installed in edit mode also look two dirs up paths . append ( os . path . join ( module_home , \"../../\" ) ) except ImportError : # Not pip installed no worries pass else : from importlib . util import find_spec try : module_home = os . path . dirname ( find_spec ( \"pyspark\" ) . origin ) paths . append ( module_home ) # If we are installed in edit mode also look two dirs up paths . append ( os . path . join ( module_home , \"../../\" ) ) except ImportError : # Not pip installed no worries pass # Normalize the paths paths = [ os . path . abspath ( p ) for p in paths ] try : return next ( path for path in paths if is_spark_home ( path ) ) except StopIteration : print ( \"Could not find valid SPARK_HOME while searching {0}\" . format ( paths ) , file = sys . stderr ) sys . exit ( - 1 )\n\n\n        Summary:", "input": "", "output": "Find the SPARK_HOME ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def computeContribs ( urls , rank ) : num_urls = len ( urls ) for url in urls : yield ( url , rank / num_urls )\n\n\n        Summary:", "input": "", "output": "Calculates URL contributions to the rank of other URLs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self ) : if self . hasSummary : return GaussianMixtureSummary ( super ( GaussianMixtureModel , self ) . summary ) else : raise RuntimeError ( \"No training summary available for this %s\" % self . __class__ . __name__ )\n\n\n        Summary:", "input": "", "output": "Gets summary ( e . g . cluster assignments cluster sizes ) of the model trained on the training set . An exception is thrown if no summary exists ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self ) : if self . hasSummary : return KMeansSummary ( super ( KMeansModel , self ) . summary ) else : raise RuntimeError ( \"No training summary available for this %s\" % self . __class__ . __name__ )\n\n\n        Summary:", "input": "", "output": "Gets summary ( e . g . cluster assignments cluster sizes ) of the model trained on the training set . An exception is thrown if no summary exists ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self ) : if self . hasSummary : return BisectingKMeansSummary ( super ( BisectingKMeansModel , self ) . summary ) else : raise RuntimeError ( \"No training summary available for this %s\" % self . __class__ . __name__ )\n\n\n        Summary:", "input": "", "output": "Gets summary ( e . g . cluster assignments cluster sizes ) of the model trained on the training set . An exception is thrown if no summary exists ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def imageSchema ( self ) : if self . _imageSchema is None : ctx = SparkContext . _active_spark_context jschema = ctx . _jvm . org . apache . spark . ml . image . ImageSchema . imageSchema ( ) self . _imageSchema = _parse_datatype_json_string ( jschema . json ( ) ) return self . _imageSchema\n\n\n        Summary:", "input": "", "output": "Returns the image schema ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ocvTypes ( self ) : if self . _ocvTypes is None : ctx = SparkContext . _active_spark_context self . _ocvTypes = dict ( ctx . _jvm . org . apache . spark . ml . image . ImageSchema . javaOcvTypes ( ) ) return self . _ocvTypes\n\n\n        Summary:", "input": "", "output": "Returns the OpenCV type mapping supported ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def columnSchema ( self ) : if self . _columnSchema is None : ctx = SparkContext . _active_spark_context jschema = ctx . _jvm . org . apache . spark . ml . image . ImageSchema . columnSchema ( ) self . _columnSchema = _parse_datatype_json_string ( jschema . json ( ) ) return self . _columnSchema\n\n\n        Summary:", "input": "", "output": "Returns the schema for the image column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def imageFields ( self ) : if self . _imageFields is None : ctx = SparkContext . _active_spark_context self . _imageFields = list ( ctx . _jvm . org . apache . spark . ml . image . ImageSchema . imageFields ( ) ) return self . _imageFields\n\n\n        Summary:", "input": "", "output": "Returns field names of image columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def undefinedImageType ( self ) : if self . _undefinedImageType is None : ctx = SparkContext . _active_spark_context self . _undefinedImageType = ctx . _jvm . org . apache . spark . ml . image . ImageSchema . undefinedImageType ( ) return self . _undefinedImageType\n\n\n        Summary:", "input": "", "output": "Returns the name of undefined image type for the invalid image ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toNDArray ( self , image ) : if not isinstance ( image , Row ) : raise TypeError ( \"image argument should be pyspark.sql.types.Row; however, \" \"it got [%s].\" % type ( image ) ) if any ( not hasattr ( image , f ) for f in self . imageFields ) : raise ValueError ( \"image argument should have attributes specified in \" \"ImageSchema.imageSchema [%s].\" % \", \" . join ( self . imageFields ) ) height = image . height width = image . width nChannels = image . nChannels return np . ndarray ( shape = ( height , width , nChannels ) , dtype = np . uint8 , buffer = image . data , strides = ( width * nChannels , nChannels , 1 ) )\n\n\n        Summary:", "input": "", "output": "Converts an image to an array with metadata ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toImage ( self , array , origin = \"\" ) : if not isinstance ( array , np . ndarray ) : raise TypeError ( \"array argument should be numpy.ndarray; however, it got [%s].\" % type ( array ) ) if array . ndim != 3 : raise ValueError ( \"Invalid array shape\" ) height , width , nChannels = array . shape ocvTypes = ImageSchema . ocvTypes if nChannels == 1 : mode = ocvTypes [ \"CV_8UC1\" ] elif nChannels == 3 : mode = ocvTypes [ \"CV_8UC3\" ] elif nChannels == 4 : mode = ocvTypes [ \"CV_8UC4\" ] else : raise ValueError ( \"Invalid number of channels\" ) # Running `bytearray(numpy.array([1]))` fails in specific Python versions # with a specific Numpy version, for example in Python 3.6.0 and NumPy 1.13.3. # Here, it avoids it by converting it to bytes. if LooseVersion ( np . __version__ ) >= LooseVersion ( '1.9' ) : data = bytearray ( array . astype ( dtype = np . uint8 ) . ravel ( ) . tobytes ( ) ) else : # Numpy prior to 1.9 don't have `tobytes` method. data = bytearray ( array . astype ( dtype = np . uint8 ) . ravel ( ) ) # Creating new Row with _create_row(), because Row(name = value, ... ) # orders fields by name, which conflicts with expected schema order # when the new DataFrame is created by UDF return _create_row ( self . imageFields , [ origin , height , width , nChannels , mode , data ] )\n\n\n        Summary:", "input": "", "output": "Converts an array with metadata to a two - dimensional image ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def readImages ( self , path , recursive = False , numPartitions = - 1 , dropImageFailures = False , sampleRatio = 1.0 , seed = 0 ) : warnings . warn ( \"`ImageSchema.readImage` is deprecated. \" + \"Use `spark.read.format(\\\"image\\\").load(path)` instead.\" , DeprecationWarning ) spark = SparkSession . builder . getOrCreate ( ) image_schema = spark . _jvm . org . apache . spark . ml . image . ImageSchema jsession = spark . _jsparkSession jresult = image_schema . readImages ( path , jsession , recursive , numPartitions , dropImageFailures , float ( sampleRatio ) , seed ) return DataFrame ( jresult , spark . _wrapped )\n\n\n        Summary:", "input": "", "output": "Reads the directory of images from the local or remote source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_from_java_class ( cls , java_class , * args ) : java_obj = JavaWrapper . _new_java_obj ( java_class , * args ) return cls ( java_obj )\n\n\n        Summary:", "input": "", "output": "Construct this object from given Java classname and arguments"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _new_java_array ( pylist , java_class ) : sc = SparkContext . _active_spark_context java_array = None if len ( pylist ) > 0 and isinstance ( pylist [ 0 ] , list ) : # If pylist is a 2D array, then a 2D java array will be created. # The 2D array is a square, non-jagged 2D array that is big enough for all elements. inner_array_length = 0 for i in xrange ( len ( pylist ) ) : inner_array_length = max ( inner_array_length , len ( pylist [ i ] ) ) java_array = sc . _gateway . new_array ( java_class , len ( pylist ) , inner_array_length ) for i in xrange ( len ( pylist ) ) : for j in xrange ( len ( pylist [ i ] ) ) : java_array [ i ] [ j ] = pylist [ i ] [ j ] else : java_array = sc . _gateway . new_array ( java_class , len ( pylist ) ) for i in xrange ( len ( pylist ) ) : java_array [ i ] = pylist [ i ] return java_array\n\n\n        Summary:", "input": "", "output": "Create a Java array of given java_class type . Useful for calling a method with a Scala Array from Python with Py4J . If the param pylist is a 2D array then a 2D java array will be returned . The returned 2D java array is a square non - jagged 2D array that is big enough for all elements . The empty slots in the inner Java arrays will be filled with null to make the non - jagged 2D array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_epytext ( line ) : line = line . replace ( '@' , ':' ) for p , sub in RULES : line = re . sub ( p , sub , line ) return line\n\n\n        Summary:", "input": "", "output": ">>> _convert_epytext ( L { A } ) : class : A"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rddToFileName ( prefix , suffix , timestamp ) : if isinstance ( timestamp , datetime ) : seconds = time . mktime ( timestamp . timetuple ( ) ) timestamp = int ( seconds * 1000 ) + timestamp . microsecond // 1000 if suffix is None : return prefix + \"-\" + str ( timestamp ) else : return prefix + \"-\" + str ( timestamp ) + \".\" + suffix\n\n\n        Summary:", "input": "", "output": "Return string prefix - time ( . suffix )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_profiler ( self , id , profiler ) : if not self . profilers : if self . profile_dump_path : atexit . register ( self . dump_profiles , self . profile_dump_path ) else : atexit . register ( self . show_profiles ) self . profilers . append ( [ id , profiler , False ] )\n\n\n        Summary:", "input": "", "output": "Add a profiler for RDD id"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dump_profiles ( self , path ) : for id , profiler , _ in self . profilers : profiler . dump ( id , path ) self . profilers = [ ]\n\n\n        Summary:", "input": "", "output": "Dump the profile stats into directory path"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def show_profiles ( self ) : for i , ( id , profiler , showed ) in enumerate ( self . profilers ) : if not showed and profiler : profiler . show ( id ) # mark it as showed self . profilers [ i ] [ 2 ] = True\n\n\n        Summary:", "input": "", "output": "Print the profile stats to stdout"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def show ( self , id ) : stats = self . stats ( ) if stats : print ( \"=\" * 60 ) print ( \"Profile of RDD<id=%d>\" % id ) print ( \"=\" * 60 ) stats . sort_stats ( \"time\" , \"cumulative\" ) . print_stats ( )\n\n\n        Summary:", "input": "", "output": "Print the profile stats to stdout id is the RDD id"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dump ( self , id , path ) : if not os . path . exists ( path ) : os . makedirs ( path ) stats = self . stats ( ) if stats : p = os . path . join ( path , \"rdd_%d.pstats\" % id ) stats . dump_stats ( p )\n\n\n        Summary:", "input": "", "output": "Dump the profile into path id is the RDD id"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def profile ( self , func ) : pr = cProfile . Profile ( ) pr . runcall ( func ) st = pstats . Stats ( pr ) st . stream = None # make it picklable st . strip_dirs ( ) # Adds a new profile to the existing accumulated value self . _accumulator . add ( st )\n\n\n        Summary:", "input": "", "output": "Runs and profiles the method to_profile passed in . A profile object is returned ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getOrCreate ( cls , sc ) : if cls . _instantiatedContext is None : jsqlContext = sc . _jvm . SQLContext . getOrCreate ( sc . _jsc . sc ( ) ) sparkSession = SparkSession ( sc , jsqlContext . sparkSession ( ) ) cls ( sc , sparkSession , jsqlContext ) return cls . _instantiatedContext\n\n\n        Summary:", "input": "", "output": "Get the existing SQLContext or create a new one with given SparkContext ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setConf ( self , key , value ) : self . sparkSession . conf . set ( key , value )\n\n\n        Summary:", "input": "", "output": "Sets the given Spark SQL configuration property ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getConf ( self , key , defaultValue = _NoValue ) : return self . sparkSession . conf . get ( key , defaultValue )\n\n\n        Summary:", "input": "", "output": "Returns the value of Spark SQL configuration property for the given key ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def range ( self , start , end = None , step = 1 , numPartitions = None ) : return self . sparkSession . range ( start , end , step , numPartitions )\n\n\n        Summary:", "input": "", "output": "Create a : class : DataFrame with single : class : pyspark . sql . types . LongType column named id containing elements in a range from start to end ( exclusive ) with step value step ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def registerFunction ( self , name , f , returnType = None ) : warnings . warn ( \"Deprecated in 2.3.0. Use spark.udf.register instead.\" , DeprecationWarning ) return self . sparkSession . udf . register ( name , f , returnType )\n\n\n        Summary:", "input": "", "output": "An alias for : func : spark . udf . register . See : meth : pyspark . sql . UDFRegistration . register ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def registerJavaFunction ( self , name , javaClassName , returnType = None ) : warnings . warn ( \"Deprecated in 2.3.0. Use spark.udf.registerJavaFunction instead.\" , DeprecationWarning ) return self . sparkSession . udf . registerJavaFunction ( name , javaClassName , returnType )\n\n\n        Summary:", "input": "", "output": "An alias for : func : spark . udf . registerJavaFunction . See : meth : pyspark . sql . UDFRegistration . registerJavaFunction ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def createDataFrame ( self , data , schema = None , samplingRatio = None , verifySchema = True ) : return self . sparkSession . createDataFrame ( data , schema , samplingRatio , verifySchema )\n\n\n        Summary:", "input": "", "output": "Creates a : class : DataFrame from an : class : RDD a list or a : class : pandas . DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def createExternalTable ( self , tableName , path = None , source = None , schema = None , * * options ) : return self . sparkSession . catalog . createExternalTable ( tableName , path , source , schema , * * options )\n\n\n        Summary:", "input": "", "output": "Creates an external table based on the dataset in a data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tables ( self , dbName = None ) : if dbName is None : return DataFrame ( self . _ssql_ctx . tables ( ) , self ) else : return DataFrame ( self . _ssql_ctx . tables ( dbName ) , self )\n\n\n        Summary:", "input": "", "output": "Returns a : class : DataFrame containing names of tables in the given database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tableNames ( self , dbName = None ) : if dbName is None : return [ name for name in self . _ssql_ctx . tableNames ( ) ] else : return [ name for name in self . _ssql_ctx . tableNames ( dbName ) ]\n\n\n        Summary:", "input": "", "output": "Returns a list of names of tables in the database dbName ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def streams ( self ) : from pyspark . sql . streaming import StreamingQueryManager return StreamingQueryManager ( self . _ssql_ctx . streams ( ) )\n\n\n        Summary:", "input": "", "output": "Returns a : class : StreamingQueryManager that allows managing all the : class : StreamingQuery StreamingQueries active on this context ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_avro ( data , jsonFormatSchema , options = { } ) : sc = SparkContext . _active_spark_context try : jc = sc . _jvm . org . apache . spark . sql . avro . functions . from_avro ( _to_java_column ( data ) , jsonFormatSchema , options ) except TypeError as e : if str ( e ) == \"'JavaPackage' object is not callable\" : _print_missing_jar ( \"Avro\" , \"avro\" , \"avro\" , sc . version ) raise return Column ( jc )\n\n\n        Summary:", "input": "", "output": "Converts a binary column of avro format into its corresponding catalyst value . The specified schema must match the read data otherwise the behavior is undefined : it may fail or return arbitrary result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( cls , filename ) : path = os . path . join ( SparkFiles . getRootDirectory ( ) , filename ) return os . path . abspath ( path )\n\n\n        Summary:", "input": "", "output": "Get the absolute path of a file added through C { SparkContext . addFile () } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getRootDirectory ( cls ) : if cls . _is_running_on_worker : return cls . _root_directory else : # This will have to change if we support multiple SparkContexts: return cls . _sc . _jvm . org . apache . spark . SparkFiles . getRootDirectory ( )\n\n\n        Summary:", "input": "", "output": "Get the root directory that contains files added through C { SparkContext . addFile () } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self ) : if self . hasSummary : if self . numClasses <= 2 : return BinaryLogisticRegressionTrainingSummary ( super ( LogisticRegressionModel , self ) . summary ) else : return LogisticRegressionTrainingSummary ( super ( LogisticRegressionModel , self ) . summary ) else : raise RuntimeError ( \"No training summary available for this %s\" % self . __class__ . __name__ )\n\n\n        Summary:", "input": "", "output": "Gets summary ( e . g . accuracy / precision / recall objective history total iterations ) of model trained on the training set . An exception is thrown if trainingSummary is None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def evaluate ( self , dataset ) : if not isinstance ( dataset , DataFrame ) : raise ValueError ( \"dataset must be a DataFrame but got %s.\" % type ( dataset ) ) java_blr_summary = self . _call_java ( \"evaluate\" , dataset ) return BinaryLogisticRegressionSummary ( java_blr_summary )\n\n\n        Summary:", "input": "", "output": "Evaluates the model on a test dataset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , extra = None ) : if extra is None : extra = dict ( ) newModel = Params . copy ( self , extra ) newModel . models = [ model . copy ( extra ) for model in self . models ] return newModel\n\n\n        Summary:", "input": "", "output": "Creates a copy of this instance with a randomly generated uid and some extra params . This creates a deep copy of the embedded paramMap and copies the embedded and extra parameters over ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_java ( cls , java_stage ) : featuresCol = java_stage . getFeaturesCol ( ) labelCol = java_stage . getLabelCol ( ) predictionCol = java_stage . getPredictionCol ( ) classifier = JavaParams . _from_java ( java_stage . getClassifier ( ) ) models = [ JavaParams . _from_java ( model ) for model in java_stage . models ( ) ] py_stage = cls ( models = models ) . setPredictionCol ( predictionCol ) . setLabelCol ( labelCol ) . setFeaturesCol ( featuresCol ) . setClassifier ( classifier ) py_stage . _resetUid ( java_stage . uid ( ) ) return py_stage\n\n\n        Summary:", "input": "", "output": "Given a Java OneVsRestModel create and return a Python wrapper of it . Used for ML persistence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java ( self ) : sc = SparkContext . _active_spark_context java_models = [ model . _to_java ( ) for model in self . models ] java_models_array = JavaWrapper . _new_java_array ( java_models , sc . _gateway . jvm . org . apache . spark . ml . classification . ClassificationModel ) metadata = JavaParams . _new_java_obj ( \"org.apache.spark.sql.types.Metadata\" ) _java_obj = JavaParams . _new_java_obj ( \"org.apache.spark.ml.classification.OneVsRestModel\" , self . uid , metadata . empty ( ) , java_models_array ) _java_obj . set ( \"classifier\" , self . getClassifier ( ) . _to_java ( ) ) _java_obj . set ( \"featuresCol\" , self . getFeaturesCol ( ) ) _java_obj . set ( \"labelCol\" , self . getLabelCol ( ) ) _java_obj . set ( \"predictionCol\" , self . getPredictionCol ( ) ) return _java_obj\n\n\n        Summary:", "input": "", "output": "Transfer this instance to a Java OneVsRestModel . Used for ML persistence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _exception_message ( excp ) : if isinstance ( excp , Py4JJavaError ) : # 'Py4JJavaError' doesn't contain the stack trace available on the Java side in 'message' # attribute in Python 2. We should call 'str' function on this exception in general but # 'Py4JJavaError' has an issue about addressing non-ascii strings. So, here we work # around by the direct call, '__str__()'. Please see SPARK-23517. return excp . __str__ ( ) if hasattr ( excp , \"message\" ) : return excp . message return str ( excp )\n\n\n        Summary:", "input": "", "output": "Return the message from an exception as either a str or unicode object . Supports both Python 2 and Python 3 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_argspec ( f ) : if sys . version_info [ 0 ] < 3 : argspec = inspect . getargspec ( f ) else : # `getargspec` is deprecated since python3.0 (incompatible with function annotations). # See SPARK-23569. argspec = inspect . getfullargspec ( f ) return argspec\n\n\n        Summary:", "input": "", "output": "Get argspec of a function . Supports both Python 2 and Python 3 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fail_on_stopiteration ( f ) : def wrapper ( * args , * * kwargs ) : try : return f ( * args , * * kwargs ) except StopIteration as exc : raise RuntimeError ( \"Caught StopIteration thrown from user's code; failing the task\" , exc ) return wrapper\n\n\n        Summary:", "input": "", "output": "Wraps the input function to fail on StopIteration by raising a RuntimeError prevents silent loss of data when f is used in a for loop in Spark code"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def majorMinorVersion ( sparkVersion ) : m = re . search ( r'^(\\d+)\\.(\\d+)(\\..*)?$' , sparkVersion ) if m is not None : return ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) ) else : raise ValueError ( \"Spark tried to parse '%s' as a Spark\" % sparkVersion + \" version string, but it could not find the major and minor\" + \" version numbers.\" )\n\n\n        Summary:", "input": "", "output": "Given a Spark version string return the ( major version number minor version number ) . E . g . for 2 . 0 . 1 - SNAPSHOT return ( 2 0 ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_initialized ( cls , instance = None , gateway = None , conf = None ) : with SparkContext . _lock : if not SparkContext . _gateway : SparkContext . _gateway = gateway or launch_gateway ( conf ) SparkContext . _jvm = SparkContext . _gateway . jvm if instance : if ( SparkContext . _active_spark_context and SparkContext . _active_spark_context != instance ) : currentMaster = SparkContext . _active_spark_context . master currentAppName = SparkContext . _active_spark_context . appName callsite = SparkContext . _active_spark_context . _callsite # Raise error if there is already a running Spark context raise ValueError ( \"Cannot run multiple SparkContexts at once; \" \"existing SparkContext(app=%s, master=%s)\" \" created by %s at %s:%s \" % ( currentAppName , currentMaster , callsite . function , callsite . file , callsite . linenum ) ) else : SparkContext . _active_spark_context = instance\n\n\n        Summary:", "input": "", "output": "Checks whether a SparkContext is initialized or not . Throws error if a SparkContext is already running ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getOrCreate ( cls , conf = None ) : with SparkContext . _lock : if SparkContext . _active_spark_context is None : SparkContext ( conf = conf or SparkConf ( ) ) return SparkContext . _active_spark_context\n\n\n        Summary:", "input": "", "output": "Get or instantiate a SparkContext and register it as a singleton object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setSystemProperty ( cls , key , value ) : SparkContext . _ensure_initialized ( ) SparkContext . _jvm . java . lang . System . setProperty ( key , value )\n\n\n        Summary:", "input": "", "output": "Set a Java system property such as spark . executor . memory . This must must be invoked before instantiating SparkContext ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stop ( self ) : if getattr ( self , \"_jsc\" , None ) : try : self . _jsc . stop ( ) except Py4JError : # Case: SPARK-18523 warnings . warn ( 'Unable to cleanly shutdown Spark JVM process.' ' It is possible that the process has crashed,' ' been killed or may also be in a zombie state.' , RuntimeWarning ) finally : self . _jsc = None if getattr ( self , \"_accumulatorServer\" , None ) : self . _accumulatorServer . shutdown ( ) self . _accumulatorServer = None with SparkContext . _lock : SparkContext . _active_spark_context = None\n\n\n        Summary:", "input": "", "output": "Shut down the SparkContext ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def range ( self , start , end = None , step = 1 , numSlices = None ) : if end is None : end = start start = 0 return self . parallelize ( xrange ( start , end , step ) , numSlices )\n\n\n        Summary:", "input": "", "output": "Create a new RDD of int containing elements from start to end ( exclusive ) increased by step every element . Can be called the same way as python s built - in range () function . If called with a single argument the argument is interpreted as end and start is set to 0 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parallelize ( self , c , numSlices = None ) : numSlices = int ( numSlices ) if numSlices is not None else self . defaultParallelism if isinstance ( c , xrange ) : size = len ( c ) if size == 0 : return self . parallelize ( [ ] , numSlices ) step = c [ 1 ] - c [ 0 ] if size > 1 else 1 start0 = c [ 0 ] def getStart ( split ) : return start0 + int ( ( split * size / numSlices ) ) * step def f ( split , iterator ) : # it's an empty iterator here but we need this line for triggering the # logic of signal handling in FramedSerializer.load_stream, for instance, # SpecialLengths.END_OF_DATA_SECTION in _read_with_length. Since # FramedSerializer.load_stream produces a generator, the control should # at least be in that function once. Here we do it by explicitly converting # the empty iterator to a list, thus make sure worker reuse takes effect. # See more details in SPARK-26549. assert len ( list ( iterator ) ) == 0 return xrange ( getStart ( split ) , getStart ( split + 1 ) , step ) return self . parallelize ( [ ] , numSlices ) . mapPartitionsWithIndex ( f ) # Make sure we distribute data evenly if it's smaller than self.batchSize if \"__len__\" not in dir ( c ) : c = list ( c ) # Make it a list so we can compute its length batchSize = max ( 1 , min ( len ( c ) // numSlices , self . _batchSize or 1024 ) ) serializer = BatchedSerializer ( self . _unbatched_serializer , batchSize ) def reader_func ( temp_filename ) : return self . _jvm . PythonRDD . readRDDFromFile ( self . _jsc , temp_filename , numSlices ) def createRDDServer ( ) : return self . _jvm . PythonParallelizeServer ( self . _jsc . sc ( ) , numSlices ) jrdd = self . _serialize_to_jvm ( c , serializer , reader_func , createRDDServer ) return RDD ( jrdd , self , serializer )\n\n\n        Summary:", "input": "", "output": "Distribute a local Python collection to form an RDD . Using xrange is recommended if the input represents a range for performance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _serialize_to_jvm ( self , data , serializer , reader_func , createRDDServer ) : if self . _encryption_enabled : # with encryption, we open a server in java and send the data directly server = createRDDServer ( ) ( sock_file , _ ) = local_connect_and_auth ( server . port ( ) , server . secret ( ) ) chunked_out = ChunkedStream ( sock_file , 8192 ) serializer . dump_stream ( data , chunked_out ) chunked_out . close ( ) # this call will block until the server has read all the data and processed it (or # throws an exception) r = server . getResult ( ) return r else : # without encryption, we serialize to a file, and we read the file in java and # parallelize from there. tempFile = NamedTemporaryFile ( delete = False , dir = self . _temp_dir ) try : try : serializer . dump_stream ( data , tempFile ) finally : tempFile . close ( ) return reader_func ( tempFile . name ) finally : # we eagerily reads the file so we can delete right after. os . unlink ( tempFile . name )\n\n\n        Summary:", "input": "", "output": "Using py4j to send a large dataset to the jvm is really slow so we use either a file or a socket if we have encryption enabled . : param data : : param serializer : : param reader_func : A function which takes a filename and reads in the data in the jvm and returns a JavaRDD . Only used when encryption is disabled . : param createRDDServer : A function which creates a PythonRDDServer in the jvm to accept the serialized data for use when encryption is enabled . : return :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pickleFile ( self , name , minPartitions = None ) : minPartitions = minPartitions or self . defaultMinPartitions return RDD ( self . _jsc . objectFile ( name , minPartitions ) , self )\n\n\n        Summary:", "input": "", "output": "Load an RDD previously saved using L { RDD . saveAsPickleFile } method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def textFile ( self , name , minPartitions = None , use_unicode = True ) : minPartitions = minPartitions or min ( self . defaultParallelism , 2 ) return RDD ( self . _jsc . textFile ( name , minPartitions ) , self , UTF8Deserializer ( use_unicode ) )\n\n\n        Summary:", "input": "", "output": "Read a text file from HDFS a local file system ( available on all nodes ) or any Hadoop - supported file system URI and return it as an RDD of Strings . The text files must be encoded as UTF - 8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def wholeTextFiles ( self , path , minPartitions = None , use_unicode = True ) : minPartitions = minPartitions or self . defaultMinPartitions return RDD ( self . _jsc . wholeTextFiles ( path , minPartitions ) , self , PairDeserializer ( UTF8Deserializer ( use_unicode ) , UTF8Deserializer ( use_unicode ) ) )\n\n\n        Summary:", "input": "", "output": "Read a directory of text files from HDFS a local file system ( available on all nodes ) or any Hadoop - supported file system URI . Each file is read as a single record and returned in a key - value pair where the key is the path of each file the value is the content of each file . The text files must be encoded as UTF - 8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def binaryFiles ( self , path , minPartitions = None ) : minPartitions = minPartitions or self . defaultMinPartitions return RDD ( self . _jsc . binaryFiles ( path , minPartitions ) , self , PairDeserializer ( UTF8Deserializer ( ) , NoOpSerializer ( ) ) )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def binaryRecords ( self , path , recordLength ) : return RDD ( self . _jsc . binaryRecords ( path , recordLength ) , self , NoOpSerializer ( ) )\n\n\n        Summary:", "input": "", "output": ".. note :: Experimental"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sequenceFile ( self , path , keyClass = None , valueClass = None , keyConverter = None , valueConverter = None , minSplits = None , batchSize = 0 ) : minSplits = minSplits or min ( self . defaultParallelism , 2 ) jrdd = self . _jvm . PythonRDD . sequenceFile ( self . _jsc , path , keyClass , valueClass , keyConverter , valueConverter , minSplits , batchSize ) return RDD ( jrdd , self )\n\n\n        Summary:", "input": "", "output": "Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS a local file system ( available on all nodes ) or any Hadoop - supported file system URI . The mechanism is as follows :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def newAPIHadoopFile ( self , path , inputFormatClass , keyClass , valueClass , keyConverter = None , valueConverter = None , conf = None , batchSize = 0 ) : jconf = self . _dictToJavaMap ( conf ) jrdd = self . _jvm . PythonRDD . newAPIHadoopFile ( self . _jsc , path , inputFormatClass , keyClass , valueClass , keyConverter , valueConverter , jconf , batchSize ) return RDD ( jrdd , self )\n\n\n        Summary:", "input": "", "output": "Read a new API Hadoop InputFormat with arbitrary key and value class from HDFS a local file system ( available on all nodes ) or any Hadoop - supported file system URI . The mechanism is the same as for sc . sequenceFile ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , rdds ) : first_jrdd_deserializer = rdds [ 0 ] . _jrdd_deserializer if any ( x . _jrdd_deserializer != first_jrdd_deserializer for x in rdds ) : rdds = [ x . _reserialize ( ) for x in rdds ] cls = SparkContext . _jvm . org . apache . spark . api . java . JavaRDD jrdds = SparkContext . _gateway . new_array ( cls , len ( rdds ) ) for i in range ( 0 , len ( rdds ) ) : jrdds [ i ] = rdds [ i ] . _jrdd return RDD ( self . _jsc . union ( jrdds ) , self , rdds [ 0 ] . _jrdd_deserializer )\n\n\n        Summary:", "input": "", "output": "Build the union of a list of RDDs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def accumulator ( self , value , accum_param = None ) : if accum_param is None : if isinstance ( value , int ) : accum_param = accumulators . INT_ACCUMULATOR_PARAM elif isinstance ( value , float ) : accum_param = accumulators . FLOAT_ACCUMULATOR_PARAM elif isinstance ( value , complex ) : accum_param = accumulators . COMPLEX_ACCUMULATOR_PARAM else : raise TypeError ( \"No default accumulator param for type %s\" % type ( value ) ) SparkContext . _next_accum_id += 1 return Accumulator ( SparkContext . _next_accum_id - 1 , value , accum_param )\n\n\n        Summary:", "input": "", "output": "Create an L { Accumulator } with the given initial value using a given L { AccumulatorParam } helper object to define how to add values of the data type if provided . Default AccumulatorParams are used for integers and floating - point numbers if you do not provide one . For other types a custom AccumulatorParam can be used ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def addFile ( self , path , recursive = False ) : self . _jsc . sc ( ) . addFile ( path , recursive )\n\n\n        Summary:", "input": "", "output": "Add a file to be downloaded with this Spark job on every node . The C { path } passed can be either a local file a file in HDFS ( or other Hadoop - supported filesystems ) or an HTTP HTTPS or FTP URI ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def addPyFile ( self , path ) : self . addFile ( path ) ( dirname , filename ) = os . path . split ( path ) # dirname may be directory or HDFS/S3 prefix if filename [ - 4 : ] . lower ( ) in self . PACKAGE_EXTENSIONS : self . _python_includes . append ( filename ) # for tests in local mode sys . path . insert ( 1 , os . path . join ( SparkFiles . getRootDirectory ( ) , filename ) ) if sys . version > '3' : import importlib importlib . invalidate_caches ( )\n\n\n        Summary:", "input": "", "output": "Add a . py or . zip dependency for all tasks to be executed on this SparkContext in the future . The C { path } passed can be either a local file a file in HDFS ( or other Hadoop - supported filesystems ) or an HTTP HTTPS or FTP URI ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _getJavaStorageLevel ( self , storageLevel ) : if not isinstance ( storageLevel , StorageLevel ) : raise Exception ( \"storageLevel must be of type pyspark.StorageLevel\" ) newStorageLevel = self . _jvm . org . apache . spark . storage . StorageLevel return newStorageLevel ( storageLevel . useDisk , storageLevel . useMemory , storageLevel . useOffHeap , storageLevel . deserialized , storageLevel . replication )\n\n\n        Summary:", "input": "", "output": "Returns a Java StorageLevel based on a pyspark . StorageLevel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setJobGroup ( self , groupId , description , interruptOnCancel = False ) : self . _jsc . setJobGroup ( groupId , description , interruptOnCancel )\n\n\n        Summary:", "input": "", "output": "Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def runJob ( self , rdd , partitionFunc , partitions = None , allowLocal = False ) : if partitions is None : partitions = range ( rdd . _jrdd . partitions ( ) . size ( ) ) # Implementation note: This is implemented as a mapPartitions followed # by runJob() in order to avoid having to pass a Python lambda into # SparkContext#runJob. mappedRDD = rdd . mapPartitions ( partitionFunc ) sock_info = self . _jvm . PythonRDD . runJob ( self . _jsc . sc ( ) , mappedRDD . _jrdd , partitions ) return list ( _load_from_socket ( sock_info , mappedRDD . _jrdd_deserializer ) )\n\n\n        Summary:", "input": "", "output": "Executes the given partitionFunc on the specified set of partitions returning the result as an array of elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dump_profiles ( self , path ) : if self . profiler_collector is not None : self . profiler_collector . dump_profiles ( path ) else : raise RuntimeError ( \"'spark.python.profile' configuration must be set \" \"to 'true' to enable Python profile.\" )\n\n\n        Summary:", "input": "", "output": "Dump the profile stats into directory path"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , ratings , rank , iterations = 5 , lambda_ = 0.01 , blocks = - 1 , nonnegative = False , seed = None ) : model = callMLlibFunc ( \"trainALSModel\" , cls . _prepare ( ratings ) , rank , iterations , lambda_ , blocks , nonnegative , seed ) return MatrixFactorizationModel ( model )\n\n\n        Summary:", "input": "", "output": "Train a matrix factorization model given an RDD of ratings by users for a subset of products . The ratings matrix is approximated as the product of two lower - rank matrices of a given rank ( number of features ) . To solve for these features ALS is run iteratively with a configurable level of parallelism ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , data , minSupport = 0.3 , numPartitions = - 1 ) : model = callMLlibFunc ( \"trainFPGrowthModel\" , data , float ( minSupport ) , int ( numPartitions ) ) return FPGrowthModel ( model )\n\n\n        Summary:", "input": "", "output": "Computes an FP - Growth model that contains frequent itemsets ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def train ( cls , data , minSupport = 0.1 , maxPatternLength = 10 , maxLocalProjDBSize = 32000000 ) : model = callMLlibFunc ( \"trainPrefixSpanModel\" , data , minSupport , maxPatternLength , maxLocalProjDBSize ) return PrefixSpanModel ( model )\n\n\n        Summary:", "input": "", "output": "Finds the complete set of frequent sequential patterns in the input sequences of itemsets ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setSample ( self , sample ) : if not isinstance ( sample , RDD ) : raise TypeError ( \"samples should be a RDD, received %s\" % type ( sample ) ) self . _sample = sample\n\n\n        Summary:", "input": "", "output": "Set sample points from the population . Should be a RDD"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def estimate ( self , points ) : points = list ( points ) densities = callMLlibFunc ( \"estimateKernelDensity\" , self . _sample , self . _bandwidth , points ) return np . asarray ( densities )\n\n\n        Summary:", "input": "", "output": "Estimate the probability density at points"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _start_update_server ( auth_token ) : server = AccumulatorServer ( ( \"localhost\" , 0 ) , _UpdateRequestHandler , auth_token ) thread = threading . Thread ( target = server . serve_forever ) thread . daemon = True thread . start ( ) return server\n\n\n        Summary:", "input": "", "output": "Start a TCP server to receive accumulator updates in a daemon thread and returns it"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add ( self , term ) : self . _value = self . accum_param . addInPlace ( self . _value , term )\n\n\n        Summary:", "input": "", "output": "Adds a term to this accumulator s value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def agg ( self , * exprs ) : assert exprs , \"exprs should not be empty\" if len ( exprs ) == 1 and isinstance ( exprs [ 0 ] , dict ) : jdf = self . _jgd . agg ( exprs [ 0 ] ) else : # Columns assert all ( isinstance ( c , Column ) for c in exprs ) , \"all exprs should be Column\" jdf = self . _jgd . agg ( exprs [ 0 ] . _jc , _to_seq ( self . sql_ctx . _sc , [ c . _jc for c in exprs [ 1 : ] ] ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Compute aggregates and returns the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pivot ( self , pivot_col , values = None ) : if values is None : jgd = self . _jgd . pivot ( pivot_col ) else : jgd = self . _jgd . pivot ( pivot_col , values ) return GroupedData ( jgd , self . _df )\n\n\n        Summary:", "input": "", "output": "Pivots a column of the current : class : DataFrame and perform the specified aggregation . There are two versions of pivot function : one that requires the caller to specify the list of distinct values to pivot on and one that does not . The latter is more concise but less efficient because Spark needs to first compute the list of distinct values internally ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , udf ) : # Columns are special because hasattr always return True if isinstance ( udf , Column ) or not hasattr ( udf , 'func' ) or udf . evalType != PythonEvalType . SQL_GROUPED_MAP_PANDAS_UDF : raise ValueError ( \"Invalid udf: the udf argument must be a pandas_udf of type \" \"GROUPED_MAP.\" ) df = self . _df udf_column = udf ( * [ df [ col ] for col in df . columns ] ) jdf = self . _jgd . flatMapGroupsInPandas ( udf_column . _jc . expr ( ) ) return DataFrame ( jdf , self . sql_ctx )\n\n\n        Summary:", "input": "", "output": "Maps each group of the current : class : DataFrame using a pandas udf and returns the result as a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def partitionBy ( * cols ) : sc = SparkContext . _active_spark_context jspec = sc . _jvm . org . apache . spark . sql . expressions . Window . partitionBy ( _to_java_cols ( cols ) ) return WindowSpec ( jspec )\n\n\n        Summary:", "input": "", "output": "Creates a : class : WindowSpec with the partitioning defined ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rowsBetween ( start , end ) : if start <= Window . _PRECEDING_THRESHOLD : start = Window . unboundedPreceding if end >= Window . _FOLLOWING_THRESHOLD : end = Window . unboundedFollowing sc = SparkContext . _active_spark_context jspec = sc . _jvm . org . apache . spark . sql . expressions . Window . rowsBetween ( start , end ) return WindowSpec ( jspec )\n\n\n        Summary:", "input": "", "output": "Creates a : class : WindowSpec with the frame boundaries defined from start ( inclusive ) to end ( inclusive ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rowsBetween ( self , start , end ) : if start <= Window . _PRECEDING_THRESHOLD : start = Window . unboundedPreceding if end >= Window . _FOLLOWING_THRESHOLD : end = Window . unboundedFollowing return WindowSpec ( self . _jspec . rowsBetween ( start , end ) )\n\n\n        Summary:", "input": "", "output": "Defines the frame boundaries from start ( inclusive ) to end ( inclusive ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def uniformRDD ( sc , size , numPartitions = None , seed = None ) : return callMLlibFunc ( \"uniformRDD\" , sc . _jsc , size , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of i . i . d . samples from the uniform distribution U ( 0 . 0 1 . 0 ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def normalRDD ( sc , size , numPartitions = None , seed = None ) : return callMLlibFunc ( \"normalRDD\" , sc . _jsc , size , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of i . i . d . samples from the standard normal distribution ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def logNormalRDD ( sc , mean , std , size , numPartitions = None , seed = None ) : return callMLlibFunc ( \"logNormalRDD\" , sc . _jsc , float ( mean ) , float ( std ) , size , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of i . i . d . samples from the log normal distribution with the input mean and standard distribution ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def exponentialRDD ( sc , mean , size , numPartitions = None , seed = None ) : return callMLlibFunc ( \"exponentialRDD\" , sc . _jsc , float ( mean ) , size , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of i . i . d . samples from the Exponential distribution with the input mean ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def gammaRDD ( sc , shape , scale , size , numPartitions = None , seed = None ) : return callMLlibFunc ( \"gammaRDD\" , sc . _jsc , float ( shape ) , float ( scale ) , size , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of i . i . d . samples from the Gamma distribution with the input shape and scale ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def uniformVectorRDD ( sc , numRows , numCols , numPartitions = None , seed = None ) : return callMLlibFunc ( \"uniformVectorRDD\" , sc . _jsc , numRows , numCols , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of vectors containing i . i . d . samples drawn from the uniform distribution U ( 0 . 0 1 . 0 ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def normalVectorRDD ( sc , numRows , numCols , numPartitions = None , seed = None ) : return callMLlibFunc ( \"normalVectorRDD\" , sc . _jsc , numRows , numCols , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of vectors containing i . i . d . samples drawn from the standard normal distribution ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def logNormalVectorRDD ( sc , mean , std , numRows , numCols , numPartitions = None , seed = None ) : return callMLlibFunc ( \"logNormalVectorRDD\" , sc . _jsc , float ( mean ) , float ( std ) , numRows , numCols , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of vectors containing i . i . d . samples drawn from the log normal distribution ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def poissonVectorRDD ( sc , mean , numRows , numCols , numPartitions = None , seed = None ) : return callMLlibFunc ( \"poissonVectorRDD\" , sc . _jsc , float ( mean ) , numRows , numCols , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of vectors containing i . i . d . samples drawn from the Poisson distribution with the input mean ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def gammaVectorRDD ( sc , shape , scale , numRows , numCols , numPartitions = None , seed = None ) : return callMLlibFunc ( \"gammaVectorRDD\" , sc . _jsc , float ( shape ) , float ( scale ) , numRows , numCols , numPartitions , seed )\n\n\n        Summary:", "input": "", "output": "Generates an RDD comprised of vectors containing i . i . d . samples drawn from the Gamma distribution ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getActiveSession ( cls ) : from pyspark import SparkContext sc = SparkContext . _active_spark_context if sc is None : return None else : if sc . _jvm . SparkSession . getActiveSession ( ) . isDefined ( ) : SparkSession ( sc , sc . _jvm . SparkSession . getActiveSession ( ) . get ( ) ) return SparkSession . _activeSession else : return None\n\n\n        Summary:", "input": "", "output": "Returns the active SparkSession for the current thread returned by the builder . >>> s = SparkSession . getActiveSession () >>> l = [ ( Alice 1 ) ] >>> rdd = s . sparkContext . parallelize ( l ) >>> df = s . createDataFrame ( rdd [ name age ] ) >>> df . select ( age ) . collect () [ Row ( age = 1 ) ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def conf ( self ) : if not hasattr ( self , \"_conf\" ) : self . _conf = RuntimeConfig ( self . _jsparkSession . conf ( ) ) return self . _conf\n\n\n        Summary:", "input": "", "output": "Runtime configuration interface for Spark ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def catalog ( self ) : from pyspark . sql . catalog import Catalog if not hasattr ( self , \"_catalog\" ) : self . _catalog = Catalog ( self ) return self . _catalog\n\n\n        Summary:", "input": "", "output": "Interface through which the user may create drop alter or query underlying databases tables functions etc ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def range ( self , start , end = None , step = 1 , numPartitions = None ) : if numPartitions is None : numPartitions = self . _sc . defaultParallelism if end is None : jdf = self . _jsparkSession . range ( 0 , int ( start ) , int ( step ) , int ( numPartitions ) ) else : jdf = self . _jsparkSession . range ( int ( start ) , int ( end ) , int ( step ) , int ( numPartitions ) ) return DataFrame ( jdf , self . _wrapped )\n\n\n        Summary:", "input": "", "output": "Create a : class : DataFrame with single : class : pyspark . sql . types . LongType column named id containing elements in a range from start to end ( exclusive ) with step value step ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _inferSchemaFromList ( self , data , names = None ) : if not data : raise ValueError ( \"can not infer schema from empty dataset\" ) first = data [ 0 ] if type ( first ) is dict : warnings . warn ( \"inferring schema from dict is deprecated,\" \"please use pyspark.sql.Row instead\" ) schema = reduce ( _merge_type , ( _infer_schema ( row , names ) for row in data ) ) if _has_nulltype ( schema ) : raise ValueError ( \"Some of types cannot be determined after inferring\" ) return schema\n\n\n        Summary:", "input": "", "output": "Infer schema from list of Row or tuple ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _inferSchema ( self , rdd , samplingRatio = None , names = None ) : first = rdd . first ( ) if not first : raise ValueError ( \"The first row in RDD is empty, \" \"can not infer schema\" ) if type ( first ) is dict : warnings . warn ( \"Using RDD of dict to inferSchema is deprecated. \" \"Use pyspark.sql.Row instead\" ) if samplingRatio is None : schema = _infer_schema ( first , names = names ) if _has_nulltype ( schema ) : for row in rdd . take ( 100 ) [ 1 : ] : schema = _merge_type ( schema , _infer_schema ( row , names = names ) ) if not _has_nulltype ( schema ) : break else : raise ValueError ( \"Some of types cannot be determined by the \" \"first 100 rows, please try again with sampling\" ) else : if samplingRatio < 0.99 : rdd = rdd . sample ( False , float ( samplingRatio ) ) schema = rdd . map ( lambda row : _infer_schema ( row , names ) ) . reduce ( _merge_type ) return schema\n\n\n        Summary:", "input": "", "output": "Infer schema from an RDD of Row or tuple ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _createFromRDD ( self , rdd , schema , samplingRatio ) : if schema is None or isinstance ( schema , ( list , tuple ) ) : struct = self . _inferSchema ( rdd , samplingRatio , names = schema ) converter = _create_converter ( struct ) rdd = rdd . map ( converter ) if isinstance ( schema , ( list , tuple ) ) : for i , name in enumerate ( schema ) : struct . fields [ i ] . name = name struct . names [ i ] = name schema = struct elif not isinstance ( schema , StructType ) : raise TypeError ( \"schema should be StructType or list or None, but got: %s\" % schema ) # convert python objects to sql data rdd = rdd . map ( schema . toInternal ) return rdd , schema\n\n\n        Summary:", "input": "", "output": "Create an RDD for DataFrame from an existing RDD returns the RDD and schema ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _createFromLocal ( self , data , schema ) : # make sure data could consumed multiple times if not isinstance ( data , list ) : data = list ( data ) if schema is None or isinstance ( schema , ( list , tuple ) ) : struct = self . _inferSchemaFromList ( data , names = schema ) converter = _create_converter ( struct ) data = map ( converter , data ) if isinstance ( schema , ( list , tuple ) ) : for i , name in enumerate ( schema ) : struct . fields [ i ] . name = name struct . names [ i ] = name schema = struct elif not isinstance ( schema , StructType ) : raise TypeError ( \"schema should be StructType or list or None, but got: %s\" % schema ) # convert python objects to sql data data = [ schema . toInternal ( row ) for row in data ] return self . _sc . parallelize ( data ) , schema\n\n\n        Summary:", "input": "", "output": "Create an RDD for DataFrame from a list or pandas . DataFrame returns the RDD and schema ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_numpy_record_dtype ( self , rec ) : import numpy as np cur_dtypes = rec . dtype col_names = cur_dtypes . names record_type_list = [ ] has_rec_fix = False for i in xrange ( len ( cur_dtypes ) ) : curr_type = cur_dtypes [ i ] # If type is a datetime64 timestamp, convert to microseconds # NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs, # conversion from [us] or lower will lead to py datetime objects, see SPARK-22417 if curr_type == np . dtype ( 'datetime64[ns]' ) : curr_type = 'datetime64[us]' has_rec_fix = True record_type_list . append ( ( str ( col_names [ i ] ) , curr_type ) ) return np . dtype ( record_type_list ) if has_rec_fix else None\n\n\n        Summary:", "input": "", "output": "Used when converting a pandas . DataFrame to Spark using to_records () this will correct the dtypes of fields in a record so they can be properly loaded into Spark . : param rec : a numpy record to check field dtypes : return corrected dtype for a numpy . record or None if no correction needed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_from_pandas ( self , pdf , schema , timezone ) : if timezone is not None : from pyspark . sql . types import _check_series_convert_timestamps_tz_local copied = False if isinstance ( schema , StructType ) : for field in schema : # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if isinstance ( field . dataType , TimestampType ) : s = _check_series_convert_timestamps_tz_local ( pdf [ field . name ] , timezone ) if s is not pdf [ field . name ] : if not copied : # Copy once if the series is modified to prevent the original # Pandas DataFrame from being updated pdf = pdf . copy ( ) copied = True pdf [ field . name ] = s else : for column , series in pdf . iteritems ( ) : s = _check_series_convert_timestamps_tz_local ( series , timezone ) if s is not series : if not copied : # Copy once if the series is modified to prevent the original # Pandas DataFrame from being updated pdf = pdf . copy ( ) copied = True pdf [ column ] = s # Convert pandas.DataFrame to list of numpy records np_records = pdf . to_records ( index = False ) # Check if any columns need to be fixed for Spark to infer properly if len ( np_records ) > 0 : record_dtype = self . _get_numpy_record_dtype ( np_records [ 0 ] ) if record_dtype is not None : return [ r . astype ( record_dtype ) . tolist ( ) for r in np_records ] # Convert list of numpy records to python lists return [ r . tolist ( ) for r in np_records ]\n\n\n        Summary:", "input": "", "output": "Convert a pandas . DataFrame to list of records that can be used to make a DataFrame : return list of records"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_from_pandas_with_arrow ( self , pdf , schema , timezone ) : from pyspark . serializers import ArrowStreamPandasSerializer from pyspark . sql . types import from_arrow_type , to_arrow_type , TimestampType from pyspark . sql . utils import require_minimum_pandas_version , require_minimum_pyarrow_version require_minimum_pandas_version ( ) require_minimum_pyarrow_version ( ) from pandas . api . types import is_datetime64_dtype , is_datetime64tz_dtype import pyarrow as pa # Create the Spark schema from list of names passed in with Arrow types if isinstance ( schema , ( list , tuple ) ) : arrow_schema = pa . Schema . from_pandas ( pdf , preserve_index = False ) struct = StructType ( ) for name , field in zip ( schema , arrow_schema ) : struct . add ( name , from_arrow_type ( field . type ) , nullable = field . nullable ) schema = struct # Determine arrow types to coerce data when creating batches if isinstance ( schema , StructType ) : arrow_types = [ to_arrow_type ( f . dataType ) for f in schema . fields ] elif isinstance ( schema , DataType ) : raise ValueError ( \"Single data type %s is not supported with Arrow\" % str ( schema ) ) else : # Any timestamps must be coerced to be compatible with Spark arrow_types = [ to_arrow_type ( TimestampType ( ) ) if is_datetime64_dtype ( t ) or is_datetime64tz_dtype ( t ) else None for t in pdf . dtypes ] # Slice the DataFrame to be batched step = - ( - len ( pdf ) // self . sparkContext . defaultParallelism ) # round int up pdf_slices = ( pdf [ start : start + step ] for start in xrange ( 0 , len ( pdf ) , step ) ) # Create list of Arrow (columns, type) for serializer dump_stream arrow_data = [ [ ( c , t ) for ( _ , c ) , t in zip ( pdf_slice . iteritems ( ) , arrow_types ) ] for pdf_slice in pdf_slices ] jsqlContext = self . _wrapped . _jsqlContext safecheck = self . _wrapped . _conf . arrowSafeTypeConversion ( ) col_by_name = True # col by name only applies to StructType columns, can't happen here ser = ArrowStreamPandasSerializer ( timezone , safecheck , col_by_name ) def reader_func ( temp_filename ) : return self . _jvm . PythonSQLUtils . readArrowStreamFromFile ( jsqlContext , temp_filename ) def create_RDD_server ( ) : return self . _jvm . ArrowRDDServer ( jsqlContext ) # Create Spark DataFrame from Arrow stream file, using one batch per partition jrdd = self . _sc . _serialize_to_jvm ( arrow_data , ser , reader_func , create_RDD_server ) jdf = self . _jvm . PythonSQLUtils . toDataFrame ( jrdd , schema . json ( ) , jsqlContext ) df = DataFrame ( jdf , self . _wrapped ) df . _schema = schema return df\n\n\n        Summary:", "input": "", "output": "Create a DataFrame from a given pandas . DataFrame by slicing it into partitions converting to Arrow data then sending to the JVM to parallelize . If a schema is passed in the data types will be used to coerce the data in Pandas to Arrow conversion ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_shell_session ( ) : import py4j from pyspark . conf import SparkConf from pyspark . context import SparkContext try : # Try to access HiveConf, it will raise exception if Hive is not added conf = SparkConf ( ) if conf . get ( 'spark.sql.catalogImplementation' , 'hive' ) . lower ( ) == 'hive' : SparkContext . _jvm . org . apache . hadoop . hive . conf . HiveConf ( ) return SparkSession . builder . enableHiveSupport ( ) . getOrCreate ( ) else : return SparkSession . builder . getOrCreate ( ) except ( py4j . protocol . Py4JError , TypeError ) : if conf . get ( 'spark.sql.catalogImplementation' , '' ) . lower ( ) == 'hive' : warnings . warn ( \"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\" ) return SparkSession . builder . getOrCreate ( )\n\n\n        Summary:", "input": "", "output": "Initialize a SparkSession for a pyspark shell session . This is called from shell . py to make error handling simpler without needing to declare local variables in that script which would expose those to users ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def createDataFrame ( self , data , schema = None , samplingRatio = None , verifySchema = True ) : SparkSession . _activeSession = self self . _jvm . SparkSession . setActiveSession ( self . _jsparkSession ) if isinstance ( data , DataFrame ) : raise TypeError ( \"data is already a DataFrame\" ) if isinstance ( schema , basestring ) : schema = _parse_datatype_string ( schema ) elif isinstance ( schema , ( list , tuple ) ) : # Must re-encode any unicode strings to be consistent with StructField names schema = [ x . encode ( 'utf-8' ) if not isinstance ( x , str ) else x for x in schema ] try : import pandas has_pandas = True except Exception : has_pandas = False if has_pandas and isinstance ( data , pandas . DataFrame ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) if self . _wrapped . _conf . pandasRespectSessionTimeZone ( ) : timezone = self . _wrapped . _conf . sessionLocalTimeZone ( ) else : timezone = None # If no schema supplied by user then get the names of columns only if schema is None : schema = [ str ( x ) if not isinstance ( x , basestring ) else ( x . encode ( 'utf-8' ) if not isinstance ( x , str ) else x ) for x in data . columns ] if self . _wrapped . _conf . arrowEnabled ( ) and len ( data ) > 0 : try : return self . _create_from_pandas_with_arrow ( data , schema , timezone ) except Exception as e : from pyspark . util import _exception_message if self . _wrapped . _conf . arrowFallbackEnabled ( ) : msg = ( \"createDataFrame attempted Arrow optimization because \" \"'spark.sql.execution.arrow.enabled' is set to true; however, \" \"failed by the reason below:\\n %s\\n\" \"Attempting non-optimization as \" \"'spark.sql.execution.arrow.fallback.enabled' is set to \" \"true.\" % _exception_message ( e ) ) warnings . warn ( msg ) else : msg = ( \"createDataFrame attempted Arrow optimization because \" \"'spark.sql.execution.arrow.enabled' is set to true, but has reached \" \"the error below and will not continue because automatic fallback \" \"with 'spark.sql.execution.arrow.fallback.enabled' has been set to \" \"false.\\n %s\" % _exception_message ( e ) ) warnings . warn ( msg ) raise data = self . _convert_from_pandas ( data , schema , timezone ) if isinstance ( schema , StructType ) : verify_func = _make_type_verifier ( schema ) if verifySchema else lambda _ : True def prepare ( obj ) : verify_func ( obj ) return obj elif isinstance ( schema , DataType ) : dataType = schema schema = StructType ( ) . add ( \"value\" , schema ) verify_func = _make_type_verifier ( dataType , name = \"field value\" ) if verifySchema else lambda _ : True def prepare ( obj ) : verify_func ( obj ) return obj , else : prepare = lambda obj : obj if isinstance ( data , RDD ) : rdd , schema = self . _createFromRDD ( data . map ( prepare ) , schema , samplingRatio ) else : rdd , schema = self . _createFromLocal ( map ( prepare , data ) , schema ) jrdd = self . _jvm . SerDeUtil . toJavaArray ( rdd . _to_java_object_rdd ( ) ) jdf = self . _jsparkSession . applySchemaToPythonRDD ( jrdd . rdd ( ) , schema . json ( ) ) df = DataFrame ( jdf , self . _wrapped ) df . _schema = schema return df\n\n\n        Summary:", "input": "", "output": "Creates a : class : DataFrame from an : class : RDD a list or a : class : pandas . DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sql ( self , sqlQuery ) : return DataFrame ( self . _jsparkSession . sql ( sqlQuery ) , self . _wrapped )\n\n\n        Summary:", "input": "", "output": "Returns a : class : DataFrame representing the result of the given query ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def table ( self , tableName ) : return DataFrame ( self . _jsparkSession . table ( tableName ) , self . _wrapped )\n\n\n        Summary:", "input": "", "output": "Returns the specified table as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def streams ( self ) : from pyspark . sql . streaming import StreamingQueryManager return StreamingQueryManager ( self . _jsparkSession . streams ( ) )\n\n\n        Summary:", "input": "", "output": "Returns a : class : StreamingQueryManager that allows managing all the : class : StreamingQuery StreamingQueries active on this context ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stop ( self ) : self . _sc . stop ( ) # We should clean the default session up. See SPARK-23228. self . _jvm . SparkSession . clearDefaultSession ( ) self . _jvm . SparkSession . clearActiveSession ( ) SparkSession . _instantiatedSession = None SparkSession . _activeSession = None\n\n\n        Summary:", "input": "", "output": "Stop the underlying : class : SparkContext ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getJobInfo ( self , jobId ) : job = self . _jtracker . getJobInfo ( jobId ) if job is not None : return SparkJobInfo ( jobId , job . stageIds ( ) , str ( job . status ( ) ) )\n\n\n        Summary:", "input": "", "output": "Returns a : class : SparkJobInfo object or None if the job info could not be found or was garbage collected ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getStageInfo ( self , stageId ) : stage = self . _jtracker . getStageInfo ( stageId ) if stage is not None : # TODO: fetch them in batch for better performance attrs = [ getattr ( stage , f ) ( ) for f in SparkStageInfo . _fields [ 1 : ] ] return SparkStageInfo ( stageId , * attrs )\n\n\n        Summary:", "input": "", "output": "Returns a : class : SparkStageInfo object or None if the stage info could not be found or was garbage collected ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _restore ( name , fields , value ) : k = ( name , fields ) cls = __cls . get ( k ) if cls is None : cls = collections . namedtuple ( name , fields ) __cls [ k ] = cls return cls ( * value )\n\n\n        Summary:", "input": "", "output": "Restore an object of namedtuple"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _hack_namedtuple ( cls ) : name = cls . __name__ fields = cls . _fields def __reduce__ ( self ) : return ( _restore , ( name , fields , tuple ( self ) ) ) cls . __reduce__ = __reduce__ cls . _is_namedtuple_ = True return cls\n\n\n        Summary:", "input": "", "output": "Make class generated by namedtuple picklable"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _hijack_namedtuple ( ) : # hijack only one time if hasattr ( collections . namedtuple , \"__hijack\" ) : return global _old_namedtuple # or it will put in closure global _old_namedtuple_kwdefaults # or it will put in closure too def _copy_func ( f ) : return types . FunctionType ( f . __code__ , f . __globals__ , f . __name__ , f . __defaults__ , f . __closure__ ) def _kwdefaults ( f ) : # __kwdefaults__ contains the default values of keyword-only arguments which are # introduced from Python 3. The possible cases for __kwdefaults__ in namedtuple # are as below: # # - Does not exist in Python 2. # - Returns None in <= Python 3.5.x. # - Returns a dictionary containing the default values to the keys from Python 3.6.x # (See https://bugs.python.org/issue25628). kargs = getattr ( f , \"__kwdefaults__\" , None ) if kargs is None : return { } else : return kargs _old_namedtuple = _copy_func ( collections . namedtuple ) _old_namedtuple_kwdefaults = _kwdefaults ( collections . namedtuple ) def namedtuple ( * args , * * kwargs ) : for k , v in _old_namedtuple_kwdefaults . items ( ) : kwargs [ k ] = kwargs . get ( k , v ) cls = _old_namedtuple ( * args , * * kwargs ) return _hack_namedtuple ( cls ) # replace namedtuple with the new one collections . namedtuple . __globals__ [ \"_old_namedtuple_kwdefaults\" ] = _old_namedtuple_kwdefaults collections . namedtuple . __globals__ [ \"_old_namedtuple\" ] = _old_namedtuple collections . namedtuple . __globals__ [ \"_hack_namedtuple\" ] = _hack_namedtuple collections . namedtuple . __code__ = namedtuple . __code__ collections . namedtuple . __hijack = 1 # hack the cls already generated by namedtuple. # Those created in other modules can be pickled as normal, # so only hack those in __main__ module for n , o in sys . modules [ \"__main__\" ] . __dict__ . items ( ) : if ( type ( o ) is type and o . __base__ is tuple and hasattr ( o , \"_fields\" ) and \"__reduce__\" not in o . __dict__ ) : _hack_namedtuple ( o )\n\n\n        Summary:", "input": "", "output": "Hack namedtuple () to make it picklable"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_stream ( self , stream ) : # load the batches for batch in self . serializer . load_stream ( stream ) : yield batch # load the batch order indices num = read_int ( stream ) batch_order = [ ] for i in xrange ( num ) : index = read_int ( stream ) batch_order . append ( index ) yield batch_order\n\n\n        Summary:", "input": "", "output": "Load a stream of un - ordered Arrow RecordBatches where the last iteration yields a list of indices that can be used to put the RecordBatches in the correct order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_batch ( self , series ) : import pandas as pd import pyarrow as pa from pyspark . sql . types import _check_series_convert_timestamps_internal # Make input conform to [(series1, type1), (series2, type2), ...] if not isinstance ( series , ( list , tuple ) ) or ( len ( series ) == 2 and isinstance ( series [ 1 ] , pa . DataType ) ) : series = [ series ] series = ( ( s , None ) if not isinstance ( s , ( list , tuple ) ) else s for s in series ) def create_array ( s , t ) : mask = s . isnull ( ) # Ensure timestamp series are in expected form for Spark internal representation if t is not None and pa . types . is_timestamp ( t ) : s = _check_series_convert_timestamps_internal ( s . fillna ( 0 ) , self . _timezone ) # TODO: need cast after Arrow conversion, ns values cause error with pandas 0.19.2 return pa . Array . from_pandas ( s , mask = mask ) . cast ( t , safe = False ) try : array = pa . Array . from_pandas ( s , mask = mask , type = t , safe = self . _safecheck ) except pa . ArrowException as e : error_msg = \"Exception thrown when converting pandas.Series (%s) to Arrow \" + \"Array (%s). It can be caused by overflows or other unsafe \" + \"conversions warned by Arrow. Arrow safe type check can be \" + \"disabled by using SQL config \" + \"`spark.sql.execution.pandas.arrowSafeTypeConversion`.\" raise RuntimeError ( error_msg % ( s . dtype , t ) , e ) return array arrs = [ ] for s , t in series : if t is not None and pa . types . is_struct ( t ) : if not isinstance ( s , pd . DataFrame ) : raise ValueError ( \"A field of type StructType expects a pandas.DataFrame, \" \"but got: %s\" % str ( type ( s ) ) ) # Input partition and result pandas.DataFrame empty, make empty Arrays with struct if len ( s ) == 0 and len ( s . columns ) == 0 : arrs_names = [ ( pa . array ( [ ] , type = field . type ) , field . name ) for field in t ] # Assign result columns by schema name if user labeled with strings elif self . _assign_cols_by_name and any ( isinstance ( name , basestring ) for name in s . columns ) : arrs_names = [ ( create_array ( s [ field . name ] , field . type ) , field . name ) for field in t ] # Assign result columns by position else : arrs_names = [ ( create_array ( s [ s . columns [ i ] ] , field . type ) , field . name ) for i , field in enumerate ( t ) ] struct_arrs , struct_names = zip ( * arrs_names ) arrs . append ( pa . StructArray . from_arrays ( struct_arrs , struct_names ) ) else : arrs . append ( create_array ( s , t ) ) return pa . RecordBatch . from_arrays ( arrs , [ \"_%d\" % i for i in xrange ( len ( arrs ) ) ] )\n\n\n        Summary:", "input": "", "output": "Create an Arrow record batch from the given pandas . Series or list of Series with optional type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dump_stream ( self , iterator , stream ) : batches = ( self . _create_batch ( series ) for series in iterator ) super ( ArrowStreamPandasSerializer , self ) . dump_stream ( batches , stream )\n\n\n        Summary:", "input": "", "output": "Make ArrowRecordBatches from Pandas Series and serialize . Input is a single series or a list of series accompanied by an optional pyarrow type to coerce the data to ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_stream ( self , stream ) : batches = super ( ArrowStreamPandasSerializer , self ) . load_stream ( stream ) import pyarrow as pa for batch in batches : yield [ self . arrow_to_pandas ( c ) for c in pa . Table . from_batches ( [ batch ] ) . itercolumns ( ) ]\n\n\n        Summary:", "input": "", "output": "Deserialize ArrowRecordBatches to an Arrow table and return as a list of pandas . Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dump_stream ( self , iterator , stream ) : def init_stream_yield_batches ( ) : should_write_start_length = True for series in iterator : batch = self . _create_batch ( series ) if should_write_start_length : write_int ( SpecialLengths . START_ARROW_STREAM , stream ) should_write_start_length = False yield batch return ArrowStreamSerializer . dump_stream ( self , init_stream_yield_batches ( ) , stream )\n\n\n        Summary:", "input": "", "output": "Override because Pandas UDFs require a START_ARROW_STREAM before the Arrow stream is sent . This should be sent after creating the first record batch so in case of an error it can be sent back to the JVM before the Arrow stream starts ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def awaitTermination ( self , timeout = None ) : if timeout is not None : if not isinstance ( timeout , ( int , float ) ) or timeout < 0 : raise ValueError ( \"timeout must be a positive integer or float. Got %s\" % timeout ) return self . _jsq . awaitTermination ( int ( timeout * 1000 ) ) else : return self . _jsq . awaitTermination ( )\n\n\n        Summary:", "input": "", "output": "Waits for the termination of this query either by : func : query . stop () or by an exception . If the query has terminated with an exception then the exception will be thrown . If timeout is set it returns whether the query has terminated or not within the timeout seconds ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def recentProgress ( self ) : return [ json . loads ( p . json ( ) ) for p in self . _jsq . recentProgress ( ) ]\n\n\n        Summary:", "input": "", "output": "Returns an array of the most recent [[ StreamingQueryProgress ]] updates for this query . The number of progress updates retained for each stream is configured by Spark session configuration spark . sql . streaming . numRecentProgressUpdates ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lastProgress ( self ) : lastProgress = self . _jsq . lastProgress ( ) if lastProgress : return json . loads ( lastProgress . json ( ) ) else : return None\n\n\n        Summary:", "input": "", "output": "Returns the most recent : class : StreamingQueryProgress update of this streaming query or None if there were no progress updates : return : a map"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def exception ( self ) : if self . _jsq . exception ( ) . isDefined ( ) : je = self . _jsq . exception ( ) . get ( ) msg = je . toString ( ) . split ( ': ' , 1 ) [ 1 ] # Drop the Java StreamingQueryException type info stackTrace = '\\n\\t at ' . join ( map ( lambda x : x . toString ( ) , je . getStackTrace ( ) ) ) return StreamingQueryException ( msg , stackTrace , je . getCause ( ) ) else : return None\n\n\n        Summary:", "input": "", "output": ": return : the StreamingQueryException if the query was terminated by an exception or None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def awaitAnyTermination ( self , timeout = None ) : if timeout is not None : if not isinstance ( timeout , ( int , float ) ) or timeout < 0 : raise ValueError ( \"timeout must be a positive integer or float. Got %s\" % timeout ) return self . _jsqm . awaitAnyTermination ( int ( timeout * 1000 ) ) else : return self . _jsqm . awaitAnyTermination ( )\n\n\n        Summary:", "input": "", "output": "Wait until any of the queries on the associated SQLContext has terminated since the creation of the context or since : func : resetTerminated () was called . If any query was terminated with an exception then the exception will be thrown . If timeout is set it returns whether the query has terminated or not within the timeout seconds ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( self , path = None , format = None , schema = None , * * options ) : if format is not None : self . format ( format ) if schema is not None : self . schema ( schema ) self . options ( * * options ) if path is not None : if type ( path ) != str or len ( path . strip ( ) ) == 0 : raise ValueError ( \"If the path is provided for stream, it needs to be a \" + \"non-empty string. List of paths are not supported.\" ) return self . _df ( self . _jreader . load ( path ) ) else : return self . _df ( self . _jreader . load ( ) )\n\n\n        Summary:", "input": "", "output": "Loads a data stream from a data source and returns it as a : class DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def json ( self , path , schema = None , primitivesAsString = None , prefersDecimal = None , allowComments = None , allowUnquotedFieldNames = None , allowSingleQuotes = None , allowNumericLeadingZero = None , allowBackslashEscapingAnyCharacter = None , mode = None , columnNameOfCorruptRecord = None , dateFormat = None , timestampFormat = None , multiLine = None , allowUnquotedControlChars = None , lineSep = None , locale = None , dropFieldIfAllNull = None , encoding = None ) : self . _set_opts ( schema = schema , primitivesAsString = primitivesAsString , prefersDecimal = prefersDecimal , allowComments = allowComments , allowUnquotedFieldNames = allowUnquotedFieldNames , allowSingleQuotes = allowSingleQuotes , allowNumericLeadingZero = allowNumericLeadingZero , allowBackslashEscapingAnyCharacter = allowBackslashEscapingAnyCharacter , mode = mode , columnNameOfCorruptRecord = columnNameOfCorruptRecord , dateFormat = dateFormat , timestampFormat = timestampFormat , multiLine = multiLine , allowUnquotedControlChars = allowUnquotedControlChars , lineSep = lineSep , locale = locale , dropFieldIfAllNull = dropFieldIfAllNull , encoding = encoding ) if isinstance ( path , basestring ) : return self . _df ( self . _jreader . json ( path ) ) else : raise TypeError ( \"path can be only a single string\" )\n\n\n        Summary:", "input": "", "output": "Loads a JSON file stream and returns the results as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def orc ( self , path ) : if isinstance ( path , basestring ) : return self . _df ( self . _jreader . orc ( path ) ) else : raise TypeError ( \"path can be only a single string\" )\n\n\n        Summary:", "input": "", "output": "Loads a ORC file stream returning the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parquet ( self , path ) : if isinstance ( path , basestring ) : return self . _df ( self . _jreader . parquet ( path ) ) else : raise TypeError ( \"path can be only a single string\" )\n\n\n        Summary:", "input": "", "output": "Loads a Parquet file stream returning the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def text ( self , path , wholetext = False , lineSep = None ) : self . _set_opts ( wholetext = wholetext , lineSep = lineSep ) if isinstance ( path , basestring ) : return self . _df ( self . _jreader . text ( path ) ) else : raise TypeError ( \"path can be only a single string\" )\n\n\n        Summary:", "input": "", "output": "Loads a text file stream and returns a : class : DataFrame whose schema starts with a string column named value and followed by partitioned columns if there are any . The text files must be encoded as UTF - 8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def csv ( self , path , schema = None , sep = None , encoding = None , quote = None , escape = None , comment = None , header = None , inferSchema = None , ignoreLeadingWhiteSpace = None , ignoreTrailingWhiteSpace = None , nullValue = None , nanValue = None , positiveInf = None , negativeInf = None , dateFormat = None , timestampFormat = None , maxColumns = None , maxCharsPerColumn = None , maxMalformedLogPerPartition = None , mode = None , columnNameOfCorruptRecord = None , multiLine = None , charToEscapeQuoteEscaping = None , enforceSchema = None , emptyValue = None , locale = None , lineSep = None ) : self . _set_opts ( schema = schema , sep = sep , encoding = encoding , quote = quote , escape = escape , comment = comment , header = header , inferSchema = inferSchema , ignoreLeadingWhiteSpace = ignoreLeadingWhiteSpace , ignoreTrailingWhiteSpace = ignoreTrailingWhiteSpace , nullValue = nullValue , nanValue = nanValue , positiveInf = positiveInf , negativeInf = negativeInf , dateFormat = dateFormat , timestampFormat = timestampFormat , maxColumns = maxColumns , maxCharsPerColumn = maxCharsPerColumn , maxMalformedLogPerPartition = maxMalformedLogPerPartition , mode = mode , columnNameOfCorruptRecord = columnNameOfCorruptRecord , multiLine = multiLine , charToEscapeQuoteEscaping = charToEscapeQuoteEscaping , enforceSchema = enforceSchema , emptyValue = emptyValue , locale = locale , lineSep = lineSep ) if isinstance ( path , basestring ) : return self . _df ( self . _jreader . csv ( path ) ) else : raise TypeError ( \"path can be only a single string\" )\n\n\n        Summary:", "input": "", "output": "r Loads a CSV file stream and returns the result as a : class : DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def outputMode ( self , outputMode ) : if not outputMode or type ( outputMode ) != str or len ( outputMode . strip ( ) ) == 0 : raise ValueError ( 'The output mode must be a non-empty string. Got: %s' % outputMode ) self . _jwrite = self . _jwrite . outputMode ( outputMode ) return self\n\n\n        Summary:", "input": "", "output": "Specifies how data of a streaming DataFrame / Dataset is written to a streaming sink ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def queryName ( self , queryName ) : if not queryName or type ( queryName ) != str or len ( queryName . strip ( ) ) == 0 : raise ValueError ( 'The queryName must be a non-empty string. Got: %s' % queryName ) self . _jwrite = self . _jwrite . queryName ( queryName ) return self\n\n\n        Summary:", "input": "", "output": "Specifies the name of the : class : StreamingQuery that can be started with : func : start . This name must be unique among all the currently active queries in the associated SparkSession ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trigger ( self , processingTime = None , once = None , continuous = None ) : params = [ processingTime , once , continuous ] if params . count ( None ) == 3 : raise ValueError ( 'No trigger provided' ) elif params . count ( None ) < 2 : raise ValueError ( 'Multiple triggers not allowed.' ) jTrigger = None if processingTime is not None : if type ( processingTime ) != str or len ( processingTime . strip ( ) ) == 0 : raise ValueError ( 'Value for processingTime must be a non empty string. Got: %s' % processingTime ) interval = processingTime . strip ( ) jTrigger = self . _spark . _sc . _jvm . org . apache . spark . sql . streaming . Trigger . ProcessingTime ( interval ) elif once is not None : if once is not True : raise ValueError ( 'Value for once must be True. Got: %s' % once ) jTrigger = self . _spark . _sc . _jvm . org . apache . spark . sql . streaming . Trigger . Once ( ) else : if type ( continuous ) != str or len ( continuous . strip ( ) ) == 0 : raise ValueError ( 'Value for continuous must be a non empty string. Got: %s' % continuous ) interval = continuous . strip ( ) jTrigger = self . _spark . _sc . _jvm . org . apache . spark . sql . streaming . Trigger . Continuous ( interval ) self . _jwrite = self . _jwrite . trigger ( jTrigger ) return self\n\n\n        Summary:", "input": "", "output": "Set the trigger for the stream query . If this is not set it will run the query as fast as possible which is equivalent to setting the trigger to processingTime = 0 seconds ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def foreach ( self , f ) : from pyspark . rdd import _wrap_function from pyspark . serializers import PickleSerializer , AutoBatchedSerializer from pyspark . taskcontext import TaskContext if callable ( f ) : # The provided object is a callable function that is supposed to be called on each row. # Construct a function that takes an iterator and calls the provided function on each # row. def func_without_process ( _ , iterator ) : for x in iterator : f ( x ) return iter ( [ ] ) func = func_without_process else : # The provided object is not a callable function. Then it is expected to have a # 'process(row)' method, and optional 'open(partition_id, epoch_id)' and # 'close(error)' methods. if not hasattr ( f , 'process' ) : raise Exception ( \"Provided object does not have a 'process' method\" ) if not callable ( getattr ( f , 'process' ) ) : raise Exception ( \"Attribute 'process' in provided object is not callable\" ) def doesMethodExist ( method_name ) : exists = hasattr ( f , method_name ) if exists and not callable ( getattr ( f , method_name ) ) : raise Exception ( \"Attribute '%s' in provided object is not callable\" % method_name ) return exists open_exists = doesMethodExist ( 'open' ) close_exists = doesMethodExist ( 'close' ) def func_with_open_process_close ( partition_id , iterator ) : epoch_id = TaskContext . get ( ) . getLocalProperty ( 'streaming.sql.batchId' ) if epoch_id : epoch_id = int ( epoch_id ) else : raise Exception ( \"Could not get batch id from TaskContext\" ) # Check if the data should be processed should_process = True if open_exists : should_process = f . open ( partition_id , epoch_id ) error = None try : if should_process : for x in iterator : f . process ( x ) except Exception as ex : error = ex finally : if close_exists : f . close ( error ) if error : raise error return iter ( [ ] ) func = func_with_open_process_close serializer = AutoBatchedSerializer ( PickleSerializer ( ) ) wrapped_func = _wrap_function ( self . _spark . _sc , func , serializer , serializer ) jForeachWriter = self . _spark . _sc . _jvm . org . apache . spark . sql . execution . python . PythonForeachWriter ( wrapped_func , self . _df . _jdf . schema ( ) ) self . _jwrite . foreach ( jForeachWriter ) return self\n\n\n        Summary:", "input": "", "output": "Sets the output of the streaming query to be processed using the provided writer f . This is often used to write the output of a streaming query to arbitrary storage systems . The processing logic can be specified in two ways ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def foreachBatch ( self , func ) : from pyspark . java_gateway import ensure_callback_server_started gw = self . _spark . _sc . _gateway java_import ( gw . jvm , \"org.apache.spark.sql.execution.streaming.sources.*\" ) wrapped_func = ForeachBatchFunction ( self . _spark , func ) gw . jvm . PythonForeachBatchHelper . callForeachBatch ( self . _jwrite , wrapped_func ) ensure_callback_server_started ( gw ) return self\n\n\n        Summary:", "input": "", "output": "Sets the output of the streaming query to be processed using the provided function . This is supported only the in the micro - batch execution modes ( that is when the trigger is not continuous ) . In every micro - batch the provided function will be called in every micro - batch with ( i ) the output rows as a DataFrame and ( ii ) the batch identifier . The batchId can be used deduplicate and transactionally write the output ( that is the provided Dataset ) to external systems . The output DataFrame is guaranteed to exactly same for the same batchId ( assuming all operations are deterministic in the query ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def start ( self , path = None , format = None , outputMode = None , partitionBy = None , queryName = None , * * options ) : self . options ( * * options ) if outputMode is not None : self . outputMode ( outputMode ) if partitionBy is not None : self . partitionBy ( partitionBy ) if format is not None : self . format ( format ) if queryName is not None : self . queryName ( queryName ) if path is None : return self . _sq ( self . _jwrite . start ( ) ) else : return self . _sq ( self . _jwrite . start ( path ) )\n\n\n        Summary:", "input": "", "output": "Streams the contents of the : class : DataFrame to a data source ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_cell_set_template_code ( ) : def inner ( value ) : lambda : cell # make ``cell`` a closure so that we get a STORE_DEREF cell = value co = inner . __code__ # NOTE: we are marking the cell variable as a free variable intentionally # so that we simulate an inner function instead of the outer function. This # is what gives us the ``nonlocal`` behavior in a Python 2 compatible way. if not PY3 : # pragma: no branch return types . CodeType ( co . co_argcount , co . co_nlocals , co . co_stacksize , co . co_flags , co . co_code , co . co_consts , co . co_names , co . co_varnames , co . co_filename , co . co_name , co . co_firstlineno , co . co_lnotab , co . co_cellvars , # this is the trickery ( ) , ) else : return types . CodeType ( co . co_argcount , co . co_kwonlyargcount , co . co_nlocals , co . co_stacksize , co . co_flags , co . co_code , co . co_consts , co . co_names , co . co_varnames , co . co_filename , co . co_name , co . co_firstlineno , co . co_lnotab , co . co_cellvars , # this is the trickery ( ) , )\n\n\n        Summary:", "input": "", "output": "Get the Python compiler to emit LOAD_FAST ( arg ) ; STORE_DEREF"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_tornado_coroutine ( func ) : if 'tornado.gen' not in sys . modules : return False gen = sys . modules [ 'tornado.gen' ] if not hasattr ( gen , \"is_coroutine_function\" ) : # Tornado version is too old return False return gen . is_coroutine_function ( func )\n\n\n        Summary:", "input": "", "output": "Return whether * func * is a Tornado coroutine function . Running coroutines are not supported ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dump ( obj , file , protocol = None ) : CloudPickler ( file , protocol = protocol ) . dump ( obj )\n\n\n        Summary:", "input": "", "output": "Serialize obj as bytes streamed into file"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dumps ( obj , protocol = None ) : file = StringIO ( ) try : cp = CloudPickler ( file , protocol = protocol ) cp . dump ( obj ) return file . getvalue ( ) finally : file . close ( )\n\n\n        Summary:", "input": "", "output": "Serialize obj as a string of bytes allocated in memory"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _fill_function ( * args ) : if len ( args ) == 2 : func = args [ 0 ] state = args [ 1 ] elif len ( args ) == 5 : # Backwards compat for cloudpickle v0.4.0, after which the `module` # argument was introduced func = args [ 0 ] keys = [ 'globals' , 'defaults' , 'dict' , 'closure_values' ] state = dict ( zip ( keys , args [ 1 : ] ) ) elif len ( args ) == 6 : # Backwards compat for cloudpickle v0.4.1, after which the function # state was passed as a dict to the _fill_function it-self. func = args [ 0 ] keys = [ 'globals' , 'defaults' , 'dict' , 'module' , 'closure_values' ] state = dict ( zip ( keys , args [ 1 : ] ) ) else : raise ValueError ( 'Unexpected _fill_value arguments: %r' % ( args , ) ) # - At pickling time, any dynamic global variable used by func is # serialized by value (in state['globals']). # - At unpickling time, func's __globals__ attribute is initialized by # first retrieving an empty isolated namespace that will be shared # with other functions pickled from the same original module # by the same CloudPickler instance and then updated with the # content of state['globals'] to populate the shared isolated # namespace with all the global variables that are specifically # referenced for this function. func . __globals__ . update ( state [ 'globals' ] ) func . __defaults__ = state [ 'defaults' ] func . __dict__ = state [ 'dict' ] if 'annotations' in state : func . __annotations__ = state [ 'annotations' ] if 'doc' in state : func . __doc__ = state [ 'doc' ] if 'name' in state : func . __name__ = state [ 'name' ] if 'module' in state : func . __module__ = state [ 'module' ] if 'qualname' in state : func . __qualname__ = state [ 'qualname' ] cells = func . __closure__ if cells is not None : for cell , value in zip ( cells , state [ 'closure_values' ] ) : if value is not _empty_cell_value : cell_set ( cell , value ) return func\n\n\n        Summary:", "input": "", "output": "Fills in the rest of function data into the skeleton function object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _rehydrate_skeleton_class ( skeleton_class , class_dict ) : registry = None for attrname , attr in class_dict . items ( ) : if attrname == \"_abc_impl\" : registry = attr else : setattr ( skeleton_class , attrname , attr ) if registry is not None : for subclass in registry : skeleton_class . register ( subclass ) return skeleton_class\n\n\n        Summary:", "input": "", "output": "Put attributes from class_dict back on skeleton_class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_dynamic ( module ) : # Quick check: module that have __file__ attribute are not dynamic modules. if hasattr ( module , '__file__' ) : return False if hasattr ( module , '__spec__' ) : return module . __spec__ is None else : # Backward compat for Python 2 import imp try : path = None for part in module . __name__ . split ( '.' ) : if path is not None : path = [ path ] f , path , description = imp . find_module ( part , path ) if f is not None : f . close ( ) except ImportError : return True return False\n\n\n        Summary:", "input": "", "output": "Return True if the module is special module that cannot be imported by its name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_codeobject ( self , obj ) : if PY3 : # pragma: no branch args = ( obj . co_argcount , obj . co_kwonlyargcount , obj . co_nlocals , obj . co_stacksize , obj . co_flags , obj . co_code , obj . co_consts , obj . co_names , obj . co_varnames , obj . co_filename , obj . co_name , obj . co_firstlineno , obj . co_lnotab , obj . co_freevars , obj . co_cellvars ) else : args = ( obj . co_argcount , obj . co_nlocals , obj . co_stacksize , obj . co_flags , obj . co_code , obj . co_consts , obj . co_names , obj . co_varnames , obj . co_filename , obj . co_name , obj . co_firstlineno , obj . co_lnotab , obj . co_freevars , obj . co_cellvars ) self . save_reduce ( types . CodeType , args , obj = obj )\n\n\n        Summary:", "input": "", "output": "Save a code object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_function ( self , obj , name = None ) : try : should_special_case = obj in _BUILTIN_TYPE_CONSTRUCTORS except TypeError : # Methods of builtin types aren't hashable in python 2. should_special_case = False if should_special_case : # We keep a special-cased cache of built-in type constructors at # global scope, because these functions are structured very # differently in different python versions and implementations (for # example, they're instances of types.BuiltinFunctionType in # CPython, but they're ordinary types.FunctionType instances in # PyPy). # # If the function we've received is in that cache, we just # serialize it as a lookup into the cache. return self . save_reduce ( _BUILTIN_TYPE_CONSTRUCTORS [ obj ] , ( ) , obj = obj ) write = self . write if name is None : name = obj . __name__ try : # whichmodule() could fail, see # https://bitbucket.org/gutworth/six/issues/63/importing-six-breaks-pickling modname = pickle . whichmodule ( obj , name ) except Exception : modname = None # print('which gives %s %s %s' % (modname, obj, name)) try : themodule = sys . modules [ modname ] except KeyError : # eval'd items such as namedtuple give invalid items for their function __module__ modname = '__main__' if modname == '__main__' : themodule = None try : lookedup_by_name = getattr ( themodule , name , None ) except Exception : lookedup_by_name = None if themodule : if lookedup_by_name is obj : return self . save_global ( obj , name ) # a builtin_function_or_method which comes in as an attribute of some # object (e.g., itertools.chain.from_iterable) will end # up with modname \"__main__\" and so end up here. But these functions # have no __code__ attribute in CPython, so the handling for # user-defined functions below will fail. # So we pickle them here using save_reduce; have to do it differently # for different python versions. if not hasattr ( obj , '__code__' ) : if PY3 : # pragma: no branch rv = obj . __reduce_ex__ ( self . proto ) else : if hasattr ( obj , '__self__' ) : rv = ( getattr , ( obj . __self__ , name ) ) else : raise pickle . PicklingError ( \"Can't pickle %r\" % obj ) return self . save_reduce ( obj = obj , * rv ) # if func is lambda, def'ed at prompt, is in main, or is nested, then # we'll pickle the actual function object rather than simply saving a # reference (as is done in default pickler), via save_function_tuple. if ( islambda ( obj ) or getattr ( obj . __code__ , 'co_filename' , None ) == '<stdin>' or themodule is None ) : self . save_function_tuple ( obj ) return else : # func is nested if lookedup_by_name is None or lookedup_by_name is not obj : self . save_function_tuple ( obj ) return if obj . __dict__ : # essentially save_reduce, but workaround needed to avoid recursion self . save ( _restore_attr ) write ( pickle . MARK + pickle . GLOBAL + modname + '\\n' + name + '\\n' ) self . memoize ( obj ) self . save ( obj . __dict__ ) write ( pickle . TUPLE + pickle . REDUCE ) else : write ( pickle . GLOBAL + modname + '\\n' + name + '\\n' ) self . memoize ( obj )\n\n\n        Summary:", "input": "", "output": "Registered with the dispatch to handle all function types ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_dynamic_class ( self , obj ) : clsdict = dict ( obj . __dict__ ) # copy dict proxy to a dict clsdict . pop ( '__weakref__' , None ) # For ABCMeta in python3.7+, remove _abc_impl as it is not picklable. # This is a fix which breaks the cache but this only makes the first # calls to issubclass slower. if \"_abc_impl\" in clsdict : import abc ( registry , _ , _ , _ ) = abc . _get_dump ( obj ) clsdict [ \"_abc_impl\" ] = [ subclass_weakref ( ) for subclass_weakref in registry ] # On PyPy, __doc__ is a readonly attribute, so we need to include it in # the initial skeleton class. This is safe because we know that the # doc can't participate in a cycle with the original class. type_kwargs = { '__doc__' : clsdict . pop ( '__doc__' , None ) } if hasattr ( obj , \"__slots__\" ) : type_kwargs [ '__slots__' ] = obj . __slots__ # pickle string length optimization: member descriptors of obj are # created automatically from obj's __slots__ attribute, no need to # save them in obj's state if isinstance ( obj . __slots__ , string_types ) : clsdict . pop ( obj . __slots__ ) else : for k in obj . __slots__ : clsdict . pop ( k , None ) # If type overrides __dict__ as a property, include it in the type kwargs. # In Python 2, we can't set this attribute after construction. __dict__ = clsdict . pop ( '__dict__' , None ) if isinstance ( __dict__ , property ) : type_kwargs [ '__dict__' ] = __dict__ save = self . save write = self . write # We write pickle instructions explicitly here to handle the # possibility that the type object participates in a cycle with its own # __dict__. We first write an empty \"skeleton\" version of the class and # memoize it before writing the class' __dict__ itself. We then write # instructions to \"rehydrate\" the skeleton class by restoring the # attributes from the __dict__. # # A type can appear in a cycle with its __dict__ if an instance of the # type appears in the type's __dict__ (which happens for the stdlib # Enum class), or if the type defines methods that close over the name # of the type, (which is common for Python 2-style super() calls). # Push the rehydration function. save ( _rehydrate_skeleton_class ) # Mark the start of the args tuple for the rehydration function. write ( pickle . MARK ) # Create and memoize an skeleton class with obj's name and bases. tp = type ( obj ) self . save_reduce ( tp , ( obj . __name__ , obj . __bases__ , type_kwargs ) , obj = obj ) # Now save the rest of obj's __dict__. Any references to obj # encountered while saving will point to the skeleton class. save ( clsdict ) # Write a tuple of (skeleton_class, clsdict). write ( pickle . TUPLE ) # Call _rehydrate_skeleton_class(skeleton_class, clsdict) write ( pickle . REDUCE )\n\n\n        Summary:", "input": "", "output": "Save a class that can t be stored as module global ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_function_tuple ( self , func ) : if is_tornado_coroutine ( func ) : self . save_reduce ( _rebuild_tornado_coroutine , ( func . __wrapped__ , ) , obj = func ) return save = self . save write = self . write code , f_globals , defaults , closure_values , dct , base_globals = self . extract_func_data ( func ) save ( _fill_function ) # skeleton function updater write ( pickle . MARK ) # beginning of tuple that _fill_function expects self . _save_subimports ( code , itertools . chain ( f_globals . values ( ) , closure_values or ( ) ) , ) # create a skeleton function object and memoize it save ( _make_skel_func ) save ( ( code , len ( closure_values ) if closure_values is not None else - 1 , base_globals , ) ) write ( pickle . REDUCE ) self . memoize ( func ) # save the rest of the func data needed by _fill_function state = { 'globals' : f_globals , 'defaults' : defaults , 'dict' : dct , 'closure_values' : closure_values , 'module' : func . __module__ , 'name' : func . __name__ , 'doc' : func . __doc__ , } if hasattr ( func , '__annotations__' ) and sys . version_info >= ( 3 , 7 ) : state [ 'annotations' ] = func . __annotations__ if hasattr ( func , '__qualname__' ) : state [ 'qualname' ] = func . __qualname__ save ( state ) write ( pickle . TUPLE ) write ( pickle . REDUCE )\n\n\n        Summary:", "input": "", "output": "Pickles an actual func object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_global ( self , obj , name = None , pack = struct . pack ) : if obj is type ( None ) : return self . save_reduce ( type , ( None , ) , obj = obj ) elif obj is type ( Ellipsis ) : return self . save_reduce ( type , ( Ellipsis , ) , obj = obj ) elif obj is type ( NotImplemented ) : return self . save_reduce ( type , ( NotImplemented , ) , obj = obj ) if obj . __module__ == \"__main__\" : return self . save_dynamic_class ( obj ) try : return Pickler . save_global ( self , obj , name = name ) except Exception : if obj . __module__ == \"__builtin__\" or obj . __module__ == \"builtins\" : if obj in _BUILTIN_TYPE_NAMES : return self . save_reduce ( _builtin_type , ( _BUILTIN_TYPE_NAMES [ obj ] , ) , obj = obj ) typ = type ( obj ) if typ is not obj and isinstance ( obj , ( type , types . ClassType ) ) : return self . save_dynamic_class ( obj ) raise\n\n\n        Summary:", "input": "", "output": "Save a global ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_inst ( self , obj ) : cls = obj . __class__ # Try the dispatch table (pickle module doesn't do it) f = self . dispatch . get ( cls ) if f : f ( self , obj ) # Call unbound method with explicit self return memo = self . memo write = self . write save = self . save if hasattr ( obj , '__getinitargs__' ) : args = obj . __getinitargs__ ( ) len ( args ) # XXX Assert it's a sequence pickle . _keep_alive ( args , memo ) else : args = ( ) write ( pickle . MARK ) if self . bin : save ( cls ) for arg in args : save ( arg ) write ( pickle . OBJ ) else : for arg in args : save ( arg ) write ( pickle . INST + cls . __module__ + '\\n' + cls . __name__ + '\\n' ) self . memoize ( obj ) try : getstate = obj . __getstate__ except AttributeError : stuff = obj . __dict__ else : stuff = getstate ( ) pickle . _keep_alive ( stuff , memo ) save ( stuff ) write ( pickle . BUILD )\n\n\n        Summary:", "input": "", "output": "Inner logic to save instance . Based off pickle . save_inst"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_itemgetter ( self , obj ) : class Dummy : def __getitem__ ( self , item ) : return item items = obj ( Dummy ( ) ) if not isinstance ( items , tuple ) : items = ( items , ) return self . save_reduce ( operator . itemgetter , items )\n\n\n        Summary:", "input": "", "output": "itemgetter serializer ( needed for namedtuple support )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_attrgetter ( self , obj ) : class Dummy ( object ) : def __init__ ( self , attrs , index = None ) : self . attrs = attrs self . index = index def __getattribute__ ( self , item ) : attrs = object . __getattribute__ ( self , \"attrs\" ) index = object . __getattribute__ ( self , \"index\" ) if index is None : index = len ( attrs ) attrs . append ( item ) else : attrs [ index ] = \".\" . join ( [ attrs [ index ] , item ] ) return type ( self ) ( attrs , index ) attrs = [ ] obj ( Dummy ( attrs ) ) return self . save_reduce ( operator . attrgetter , tuple ( attrs ) )\n\n\n        Summary:", "input": "", "output": "attrgetter serializer"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _copy_new_parent ( self , parent ) : if self . parent == \"undefined\" : param = copy . copy ( self ) param . parent = parent . uid return param else : raise ValueError ( \"Cannot copy from non-dummy parent %s.\" % parent )\n\n\n        Summary:", "input": "", "output": "Copy the current param to a new parent must be a dummy param ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toList ( value ) : if type ( value ) == list : return value elif type ( value ) in [ np . ndarray , tuple , xrange , array . array ] : return list ( value ) elif isinstance ( value , Vector ) : return list ( value . toArray ( ) ) else : raise TypeError ( \"Could not convert %s to list\" % value )\n\n\n        Summary:", "input": "", "output": "Convert a value to a list if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toListFloat ( value ) : if TypeConverters . _can_convert_to_list ( value ) : value = TypeConverters . toList ( value ) if all ( map ( lambda v : TypeConverters . _is_numeric ( v ) , value ) ) : return [ float ( v ) for v in value ] raise TypeError ( \"Could not convert %s to list of floats\" % value )\n\n\n        Summary:", "input": "", "output": "Convert a value to list of floats if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toListInt ( value ) : if TypeConverters . _can_convert_to_list ( value ) : value = TypeConverters . toList ( value ) if all ( map ( lambda v : TypeConverters . _is_integer ( v ) , value ) ) : return [ int ( v ) for v in value ] raise TypeError ( \"Could not convert %s to list of ints\" % value )\n\n\n        Summary:", "input": "", "output": "Convert a value to list of ints if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toListString ( value ) : if TypeConverters . _can_convert_to_list ( value ) : value = TypeConverters . toList ( value ) if all ( map ( lambda v : TypeConverters . _can_convert_to_string ( v ) , value ) ) : return [ TypeConverters . toString ( v ) for v in value ] raise TypeError ( \"Could not convert %s to list of strings\" % value )\n\n\n        Summary:", "input": "", "output": "Convert a value to list of strings if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toVector ( value ) : if isinstance ( value , Vector ) : return value elif TypeConverters . _can_convert_to_list ( value ) : value = TypeConverters . toList ( value ) if all ( map ( lambda v : TypeConverters . _is_numeric ( v ) , value ) ) : return DenseVector ( value ) raise TypeError ( \"Could not convert %s to vector\" % value )\n\n\n        Summary:", "input": "", "output": "Convert a value to a MLlib Vector if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def toString ( value ) : if isinstance ( value , basestring ) : return value elif type ( value ) in [ np . string_ , np . str_ ] : return str ( value ) elif type ( value ) == np . unicode_ : return unicode ( value ) else : raise TypeError ( \"Could not convert %s to string type\" % type ( value ) )\n\n\n        Summary:", "input": "", "output": "Convert a value to a string if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _copy_params ( self ) : cls = type ( self ) src_name_attrs = [ ( x , getattr ( cls , x ) ) for x in dir ( cls ) ] src_params = list ( filter ( lambda nameAttr : isinstance ( nameAttr [ 1 ] , Param ) , src_name_attrs ) ) for name , param in src_params : setattr ( self , name , param . _copy_new_parent ( self ) )\n\n\n        Summary:", "input": "", "output": "Copy all params defined on the class to current object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def params ( self ) : if self . _params is None : self . _params = list ( filter ( lambda attr : isinstance ( attr , Param ) , [ getattr ( self , x ) for x in dir ( self ) if x != \"params\" and not isinstance ( getattr ( type ( self ) , x , None ) , property ) ] ) ) return self . _params\n\n\n        Summary:", "input": "", "output": "Returns all params ordered by name . The default implementation uses : py : func : dir to get all attributes of type : py : class : Param ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def explainParam ( self , param ) : param = self . _resolveParam ( param ) values = [ ] if self . isDefined ( param ) : if param in self . _defaultParamMap : values . append ( \"default: %s\" % self . _defaultParamMap [ param ] ) if param in self . _paramMap : values . append ( \"current: %s\" % self . _paramMap [ param ] ) else : values . append ( \"undefined\" ) valueStr = \"(\" + \", \" . join ( values ) + \")\" return \"%s: %s %s\" % ( param . name , param . doc , valueStr )\n\n\n        Summary:", "input": "", "output": "Explains a single param and returns its name doc and optional default value and user - supplied value in a string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getParam ( self , paramName ) : param = getattr ( self , paramName ) if isinstance ( param , Param ) : return param else : raise ValueError ( \"Cannot find param with name %s.\" % paramName )\n\n\n        Summary:", "input": "", "output": "Gets a param by its name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isSet ( self , param ) : param = self . _resolveParam ( param ) return param in self . _paramMap\n\n\n        Summary:", "input": "", "output": "Checks whether a param is explicitly set by user ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hasDefault ( self , param ) : param = self . _resolveParam ( param ) return param in self . _defaultParamMap\n\n\n        Summary:", "input": "", "output": "Checks whether a param has a default value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hasParam ( self , paramName ) : if isinstance ( paramName , basestring ) : p = getattr ( self , paramName , None ) return isinstance ( p , Param ) else : raise TypeError ( \"hasParam(): paramName must be a string\" )\n\n\n        Summary:", "input": "", "output": "Tests whether this instance contains a param with a given ( string ) name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getOrDefault ( self , param ) : param = self . _resolveParam ( param ) if param in self . _paramMap : return self . _paramMap [ param ] else : return self . _defaultParamMap [ param ]\n\n\n        Summary:", "input": "", "output": "Gets the value of a param in the user - supplied param map or its default value . Raises an error if neither is set ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def extractParamMap ( self , extra = None ) : if extra is None : extra = dict ( ) paramMap = self . _defaultParamMap . copy ( ) paramMap . update ( self . _paramMap ) paramMap . update ( extra ) return paramMap\n\n\n        Summary:", "input": "", "output": "Extracts the embedded default param values and user - supplied values and then merges them with extra values from input into a flat param map where the latter value is used if there exist conflicts i . e . with ordering : default param values < user - supplied values < extra ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , extra = None ) : if extra is None : extra = dict ( ) that = copy . copy ( self ) that . _paramMap = { } that . _defaultParamMap = { } return self . _copyValues ( that , extra )\n\n\n        Summary:", "input": "", "output": "Creates a copy of this instance with the same uid and some extra params . The default implementation creates a shallow copy using : py : func : copy . copy and then copies the embedded and extra parameters over and returns the copy . Subclasses should override this method if the default approach is not sufficient ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set ( self , param , value ) : self . _shouldOwn ( param ) try : value = param . typeConverter ( value ) except ValueError as e : raise ValueError ( 'Invalid param value given for param \"%s\". %s' % ( param . name , e ) ) self . _paramMap [ param ] = value\n\n\n        Summary:", "input": "", "output": "Sets a parameter in the embedded param map ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _shouldOwn ( self , param ) : if not ( self . uid == param . parent and self . hasParam ( param . name ) ) : raise ValueError ( \"Param %r does not belong to %r.\" % ( param , self ) )\n\n\n        Summary:", "input": "", "output": "Validates that the input param belongs to this Params instance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _resolveParam ( self , param ) : if isinstance ( param , Param ) : self . _shouldOwn ( param ) return param elif isinstance ( param , basestring ) : return self . getParam ( param ) else : raise ValueError ( \"Cannot resolve %r as a param.\" % param )\n\n\n        Summary:", "input": "", "output": "Resolves a param and validates the ownership ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set ( self , * * kwargs ) : for param , value in kwargs . items ( ) : p = getattr ( self , param ) if value is not None : try : value = p . typeConverter ( value ) except TypeError as e : raise TypeError ( 'Invalid param value given for param \"%s\". %s' % ( p . name , e ) ) self . _paramMap [ p ] = value return self\n\n\n        Summary:", "input": "", "output": "Sets user - supplied params ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _setDefault ( self , * * kwargs ) : for param , value in kwargs . items ( ) : p = getattr ( self , param ) if value is not None and not isinstance ( value , JavaObject ) : try : value = p . typeConverter ( value ) except TypeError as e : raise TypeError ( 'Invalid default param value given for param \"%s\". %s' % ( p . name , e ) ) self . _defaultParamMap [ p ] = value return self\n\n\n        Summary:", "input": "", "output": "Sets default params ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _copyValues ( self , to , extra = None ) : paramMap = self . _paramMap . copy ( ) if extra is not None : paramMap . update ( extra ) for param in self . params : # copy default params if param in self . _defaultParamMap and to . hasParam ( param . name ) : to . _defaultParamMap [ to . getParam ( param . name ) ] = self . _defaultParamMap [ param ] # copy explicitly set params if param in paramMap and to . hasParam ( param . name ) : to . _set ( * * { param . name : paramMap [ param ] } ) return to\n\n\n        Summary:", "input": "", "output": "Copies param values from this instance to another instance for params shared by them ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _resetUid ( self , newUid ) : newUid = unicode ( newUid ) self . uid = newUid newDefaultParamMap = dict ( ) newParamMap = dict ( ) for param in self . params : newParam = copy . copy ( param ) newParam . parent = newUid if param in self . _defaultParamMap : newDefaultParamMap [ newParam ] = self . _defaultParamMap [ param ] if param in self . _paramMap : newParamMap [ newParam ] = self . _paramMap [ param ] param . parent = newUid self . _defaultParamMap = newDefaultParamMap self . _paramMap = newParamMap return self\n\n\n        Summary:", "input": "", "output": "Changes the uid of this instance . This updates both the stored uid and the parent uid of params and param maps . This is used by persistence ( loading ) . : param newUid : new uid to use which is converted to unicode : return : same instance but with the uid and Param . parent values updated including within param maps"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_java_object_rdd ( rdd ) : rdd = rdd . _reserialize ( AutoBatchedSerializer ( PickleSerializer ( ) ) ) return rdd . ctx . _jvm . org . apache . spark . ml . python . MLSerDe . pythonToJava ( rdd . _jrdd , True )\n\n\n        Summary:", "input": "", "output": "Return an JavaRDD of Object by unpickling"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def value ( self ) : if not hasattr ( self , \"_value\" ) and self . _path is not None : # we only need to decrypt it here when encryption is enabled and # if its on the driver, since executor decryption is handled already if self . _sc is not None and self . _sc . _encryption_enabled : port , auth_secret = self . _python_broadcast . setupDecryptionServer ( ) ( decrypted_sock_file , _ ) = local_connect_and_auth ( port , auth_secret ) self . _python_broadcast . waitTillBroadcastDataSent ( ) return self . load ( decrypted_sock_file ) else : self . _value = self . load_from_path ( self . _path ) return self . _value\n\n\n        Summary:", "input": "", "output": "Return the broadcasted value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unpersist ( self , blocking = False ) : if self . _jbroadcast is None : raise Exception ( \"Broadcast can only be unpersisted in driver\" ) self . _jbroadcast . unpersist ( blocking )\n\n\n        Summary:", "input": "", "output": "Delete cached copies of this broadcast on the executors . If the broadcast is used after this is called it will need to be re - sent to each executor ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def destroy ( self , blocking = False ) : if self . _jbroadcast is None : raise Exception ( \"Broadcast can only be destroyed in driver\" ) self . _jbroadcast . destroy ( blocking ) os . unlink ( self . _path )\n\n\n        Summary:", "input": "", "output": "Destroy all data and metadata related to this broadcast variable . Use this with caution ; once a broadcast variable has been destroyed it cannot be used again ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrapped ( self ) : # It is possible for a callable instance without __name__ attribute or/and # __module__ attribute to be wrapped here. For example, functools.partial. In this case, # we should avoid wrapping the attributes from the wrapped function to the wrapper # function. So, we take out these attribute names from the default names to set and # then manually assign it after being wrapped. assignments = tuple ( a for a in functools . WRAPPER_ASSIGNMENTS if a != '__name__' and a != '__module__' ) @ functools . wraps ( self . func , assigned = assignments ) def wrapper ( * args ) : return self ( * args ) wrapper . __name__ = self . _name wrapper . __module__ = ( self . func . __module__ if hasattr ( self . func , '__module__' ) else self . func . __class__ . __module__ ) wrapper . func = self . func wrapper . returnType = self . returnType wrapper . evalType = self . evalType wrapper . deterministic = self . deterministic wrapper . asNondeterministic = functools . wraps ( self . asNondeterministic ) ( lambda : self . asNondeterministic ( ) . _wrapped ( ) ) return wrapper\n\n\n        Summary:", "input": "", "output": "Wrap this udf with a function and attach docstring from func"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def register ( self , name , f , returnType = None ) : # This is to check whether the input function is from a user-defined function or # Python function. if hasattr ( f , 'asNondeterministic' ) : if returnType is not None : raise TypeError ( \"Invalid returnType: data type can not be specified when f is\" \"a user-defined function, but got %s.\" % returnType ) if f . evalType not in [ PythonEvalType . SQL_BATCHED_UDF , PythonEvalType . SQL_SCALAR_PANDAS_UDF , PythonEvalType . SQL_GROUPED_AGG_PANDAS_UDF ] : raise ValueError ( \"Invalid f: f must be SQL_BATCHED_UDF, SQL_SCALAR_PANDAS_UDF or \" \"SQL_GROUPED_AGG_PANDAS_UDF\" ) register_udf = UserDefinedFunction ( f . func , returnType = f . returnType , name = name , evalType = f . evalType , deterministic = f . deterministic ) return_udf = f else : if returnType is None : returnType = StringType ( ) register_udf = UserDefinedFunction ( f , returnType = returnType , name = name , evalType = PythonEvalType . SQL_BATCHED_UDF ) return_udf = register_udf . _wrapped ( ) self . sparkSession . _jsparkSession . udf ( ) . registerPython ( name , register_udf . _judf ) return return_udf\n\n\n        Summary:", "input": "", "output": "Register a Python function ( including lambda function ) or a user - defined function as a SQL function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def registerJavaFunction ( self , name , javaClassName , returnType = None ) : jdt = None if returnType is not None : if not isinstance ( returnType , DataType ) : returnType = _parse_datatype_string ( returnType ) jdt = self . sparkSession . _jsparkSession . parseDataType ( returnType . json ( ) ) self . sparkSession . _jsparkSession . udf ( ) . registerJava ( name , javaClassName , jdt )\n\n\n        Summary:", "input": "", "output": "Register a Java user - defined function as a SQL function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def registerJavaUDAF ( self , name , javaClassName ) : self . sparkSession . _jsparkSession . udf ( ) . registerJavaUDAF ( name , javaClassName )\n\n\n        Summary:", "input": "", "output": "Register a Java user - defined aggregate function as a SQL function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getOrCreate ( cls , checkpointPath , setupFunc ) : cls . _ensure_initialized ( ) gw = SparkContext . _gateway # Check whether valid checkpoint information exists in the given path ssc_option = gw . jvm . StreamingContextPythonHelper ( ) . tryRecoverFromCheckpoint ( checkpointPath ) if ssc_option . isEmpty ( ) : ssc = setupFunc ( ) ssc . checkpoint ( checkpointPath ) return ssc jssc = gw . jvm . JavaStreamingContext ( ssc_option . get ( ) ) # If there is already an active instance of Python SparkContext use it, or create a new one if not SparkContext . _active_spark_context : jsc = jssc . sparkContext ( ) conf = SparkConf ( _jconf = jsc . getConf ( ) ) SparkContext ( conf = conf , gateway = gw , jsc = jsc ) sc = SparkContext . _active_spark_context # update ctx in serializer cls . _transformerSerializer . ctx = sc return StreamingContext ( sc , None , jssc )\n\n\n        Summary:", "input": "", "output": "Either recreate a StreamingContext from checkpoint data or create a new StreamingContext . If checkpoint data exists in the provided checkpointPath then StreamingContext will be recreated from the checkpoint data . If the data does not exist then the provided setupFunc will be used to create a new context ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getActive ( cls ) : activePythonContext = cls . _activeContext if activePythonContext is not None : # Verify that the current running Java StreamingContext is active and is the same one # backing the supposedly active Python context activePythonContextJavaId = activePythonContext . _jssc . ssc ( ) . hashCode ( ) activeJvmContextOption = activePythonContext . _jvm . StreamingContext . getActive ( ) if activeJvmContextOption . isEmpty ( ) : cls . _activeContext = None elif activeJvmContextOption . get ( ) . hashCode ( ) != activePythonContextJavaId : cls . _activeContext = None raise Exception ( \"JVM's active JavaStreamingContext is not the JavaStreamingContext \" \"backing the action Python StreamingContext. This is unexpected.\" ) return cls . _activeContext\n\n\n        Summary:", "input": "", "output": "Return either the currently active StreamingContext ( i . e . if there is a context started but not stopped ) or None ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getActiveOrCreate ( cls , checkpointPath , setupFunc ) : if setupFunc is None : raise Exception ( \"setupFunc cannot be None\" ) activeContext = cls . getActive ( ) if activeContext is not None : return activeContext elif checkpointPath is not None : return cls . getOrCreate ( checkpointPath , setupFunc ) else : return setupFunc ( )\n\n\n        Summary:", "input": "", "output": "Either return the active StreamingContext ( i . e . currently started but not stopped ) or recreate a StreamingContext from checkpoint data or create a new StreamingContext using the provided setupFunc function . If the checkpointPath is None or does not contain valid checkpoint data then setupFunc will be called to create a new context and setup DStreams ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def awaitTermination ( self , timeout = None ) : if timeout is None : self . _jssc . awaitTermination ( ) else : self . _jssc . awaitTerminationOrTimeout ( int ( timeout * 1000 ) )\n\n\n        Summary:", "input": "", "output": "Wait for the execution to stop ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stop ( self , stopSparkContext = True , stopGraceFully = False ) : self . _jssc . stop ( stopSparkContext , stopGraceFully ) StreamingContext . _activeContext = None if stopSparkContext : self . _sc . stop ( )\n\n\n        Summary:", "input": "", "output": "Stop the execution of the streams with option of ensuring all received data has been processed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def socketTextStream ( self , hostname , port , storageLevel = StorageLevel . MEMORY_AND_DISK_2 ) : jlevel = self . _sc . _getJavaStorageLevel ( storageLevel ) return DStream ( self . _jssc . socketTextStream ( hostname , port , jlevel ) , self , UTF8Deserializer ( ) )\n\n\n        Summary:", "input": "", "output": "Create an input from TCP source hostname : port . Data is received using a TCP socket and receive byte is interpreted as UTF8 encoded \\\\ n delimited lines ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def textFileStream ( self , directory ) : return DStream ( self . _jssc . textFileStream ( directory ) , self , UTF8Deserializer ( ) )\n\n\n        Summary:", "input": "", "output": "Create an input stream that monitors a Hadoop - compatible file system for new files and reads them as text files . Files must be wrriten to the monitored directory by moving them from another location within the same file system . File names starting with . are ignored . The text files must be encoded as UTF - 8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def binaryRecordsStream ( self , directory , recordLength ) : return DStream ( self . _jssc . binaryRecordsStream ( directory , recordLength ) , self , NoOpSerializer ( ) )\n\n\n        Summary:", "input": "", "output": "Create an input stream that monitors a Hadoop - compatible file system for new files and reads them as flat binary files with records of fixed length . Files must be written to the monitored directory by moving them from another location within the same file system . File names starting with . are ignored ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def queueStream ( self , rdds , oneAtATime = True , default = None ) : if default and not isinstance ( default , RDD ) : default = self . _sc . parallelize ( default ) if not rdds and default : rdds = [ rdds ] if rdds and not isinstance ( rdds [ 0 ] , RDD ) : rdds = [ self . _sc . parallelize ( input ) for input in rdds ] self . _check_serializers ( rdds ) queue = self . _jvm . PythonDStream . toRDDQueue ( [ r . _jrdd for r in rdds ] ) if default : default = default . _reserialize ( rdds [ 0 ] . _jrdd_deserializer ) jdstream = self . _jssc . queueStream ( queue , oneAtATime , default . _jrdd ) else : jdstream = self . _jssc . queueStream ( queue , oneAtATime ) return DStream ( jdstream , self , rdds [ 0 ] . _jrdd_deserializer )\n\n\n        Summary:", "input": "", "output": "Create an input stream from a queue of RDDs or list . In each batch it will process either one or all of the RDDs returned by the queue ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , dstreams , transformFunc ) : jdstreams = [ d . _jdstream for d in dstreams ] # change the final serializer to sc.serializer func = TransformFunction ( self . _sc , lambda t , * rdds : transformFunc ( rdds ) , * [ d . _jrdd_deserializer for d in dstreams ] ) jfunc = self . _jvm . TransformFunction ( func ) jdstream = self . _jssc . transform ( jdstreams , jfunc ) return DStream ( jdstream , self , self . _sc . serializer )\n\n\n        Summary:", "input": "", "output": "Create a new DStream in which each RDD is generated by applying a function on RDDs of the DStreams . The order of the JavaRDDs in the transform function parameter will be the same as the order of corresponding DStreams in the list ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , * dstreams ) : if not dstreams : raise ValueError ( \"should have at least one DStream to union\" ) if len ( dstreams ) == 1 : return dstreams [ 0 ] if len ( set ( s . _jrdd_deserializer for s in dstreams ) ) > 1 : raise ValueError ( \"All DStreams should have same serializer\" ) if len ( set ( s . _slideDuration for s in dstreams ) ) > 1 : raise ValueError ( \"All DStreams should have same slide duration\" ) cls = SparkContext . _jvm . org . apache . spark . streaming . api . java . JavaDStream jdstreams = SparkContext . _gateway . new_array ( cls , len ( dstreams ) ) for i in range ( 0 , len ( dstreams ) ) : jdstreams [ i ] = dstreams [ i ] . _jdstream return DStream ( self . _jssc . union ( jdstreams ) , self , dstreams [ 0 ] . _jrdd_deserializer )\n\n\n        Summary:", "input": "", "output": "Create a unified DStream from multiple DStreams of the same type and same slide duration ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def addStreamingListener ( self , streamingListener ) : self . _jssc . addStreamingListener ( self . _jvm . JavaStreamingListenerWrapper ( self . _jvm . PythonStreamingListenerWrapper ( streamingListener ) ) )\n\n\n        Summary:", "input": "", "output": "Add a [[ org . apache . spark . streaming . scheduler . StreamingListener ]] object for receiving system events related to streaming ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_tf_weights_in_gpt2 ( model , gpt2_checkpoint_path ) : try : import re import numpy as np import tensorflow as tf except ImportError : print ( \"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \" \"https://www.tensorflow.org/install/ for installation instructions.\" ) raise tf_path = os . path . abspath ( gpt2_checkpoint_path ) print ( \"Converting TensorFlow checkpoint from {}\" . format ( tf_path ) ) # Load weights from TF model init_vars = tf . train . list_variables ( tf_path ) names = [ ] arrays = [ ] for name , shape in init_vars : print ( \"Loading TF weight {} with shape {}\" . format ( name , shape ) ) array = tf . train . load_variable ( tf_path , name ) names . append ( name ) arrays . append ( array . squeeze ( ) ) for name , array in zip ( names , arrays ) : name = name [ 6 : ] # skip \"model/\" name = name . split ( '/' ) pointer = model for m_name in name : if re . fullmatch ( r'[A-Za-z]+\\d+' , m_name ) : l = re . split ( r'(\\d+)' , m_name ) else : l = [ m_name ] if l [ 0 ] == 'w' or l [ 0 ] == 'g' : pointer = getattr ( pointer , 'weight' ) elif l [ 0 ] == 'b' : pointer = getattr ( pointer , 'bias' ) elif l [ 0 ] == 'wpe' or l [ 0 ] == 'wte' : pointer = getattr ( pointer , l [ 0 ] ) pointer = getattr ( pointer , 'weight' ) else : pointer = getattr ( pointer , l [ 0 ] ) if len ( l ) >= 2 : num = int ( l [ 1 ] ) pointer = pointer [ num ] try : assert pointer . shape == array . shape except AssertionError as e : e . args += ( pointer . shape , array . shape ) raise print ( \"Initialize PyTorch weight {}\" . format ( name ) ) pointer . data = torch . from_numpy ( array ) return model\n\n\n        Summary:", "input": "", "output": "Load tf checkpoints in a pytorch model"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_json_file ( cls , json_file ) : with open ( json_file , \"r\" , encoding = \"utf-8\" ) as reader : text = reader . read ( ) return cls . from_dict ( json . loads ( text ) )\n\n\n        Summary:", "input": "", "output": "Constructs a GPT2Config from a json file of parameters ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_json_file ( self , json_file_path ) : with open ( json_file_path , \"w\" , encoding = 'utf-8' ) as writer : writer . write ( self . to_json_string ( ) )\n\n\n        Summary:", "input": "", "output": "Save this instance to a json file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def init_weights ( self , module ) : if isinstance ( module , ( nn . Linear , nn . Embedding ) ) : # Slightly different from the TF version which uses truncated_normal for initialization # cf https://github.com/pytorch/pytorch/pull/5617 module . weight . data . normal_ ( mean = 0.0 , std = self . config . initializer_range ) elif isinstance ( module , LayerNorm ) : module . bias . data . zero_ ( ) module . weight . data . fill_ ( 1.0 ) if isinstance ( module , nn . Linear ) and module . bias is not None : module . bias . data . zero_ ( )\n\n\n        Summary:", "input": "", "output": "Initialize the weights ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_pretrained ( cls , pretrained_model_name_or_path , state_dict = None , cache_dir = None , from_tf = False , * inputs , * * kwargs ) : if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP : archive_file = PRETRAINED_MODEL_ARCHIVE_MAP [ pretrained_model_name_or_path ] config_file = PRETRAINED_CONFIG_ARCHIVE_MAP [ pretrained_model_name_or_path ] else : archive_file = os . path . join ( pretrained_model_name_or_path , WEIGHTS_NAME ) config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) # redirect to the cache, if necessary try : resolved_archive_file = cached_path ( archive_file , cache_dir = cache_dir ) resolved_config_file = cached_path ( config_file , cache_dir = cache_dir ) except EnvironmentError : logger . error ( \"Model name '{}' was not found in model name list ({}). \" \"We assumed '{}' was a path or url but couldn't find files {} and {} \" \"at this path or url.\" . format ( pretrained_model_name_or_path , \", \" . join ( PRETRAINED_MODEL_ARCHIVE_MAP . keys ( ) ) , pretrained_model_name_or_path , archive_file , config_file ) ) return None if resolved_archive_file == archive_file and resolved_config_file == config_file : logger . info ( \"loading weights file {}\" . format ( archive_file ) ) logger . info ( \"loading configuration file {}\" . format ( config_file ) ) else : logger . info ( \"loading weights file {} from cache at {}\" . format ( archive_file , resolved_archive_file ) ) logger . info ( \"loading configuration file {} from cache at {}\" . format ( config_file , resolved_config_file ) ) # Load config config = GPT2Config . from_json_file ( resolved_config_file ) logger . info ( \"Model config {}\" . format ( config ) ) # Instantiate model. model = cls ( config , * inputs , * * kwargs ) if state_dict is None and not from_tf : state_dict = torch . load ( resolved_archive_file , map_location = 'cpu' ) if from_tf : # Directly load from a TensorFlow checkpoint (stored as NumPy array) return load_tf_weights_in_gpt2 ( model , resolved_archive_file ) old_keys = [ ] new_keys = [ ] for key in state_dict . keys ( ) : new_key = None if key . endswith ( \".g\" ) : new_key = key [ : - 2 ] + \".weight\" elif key . endswith ( \".b\" ) : new_key = key [ : - 2 ] + \".bias\" elif key . endswith ( \".w\" ) : new_key = key [ : - 2 ] + \".weight\" if new_key : old_keys . append ( key ) new_keys . append ( new_key ) for old_key , new_key in zip ( old_keys , new_keys ) : state_dict [ new_key ] = state_dict . pop ( old_key ) missing_keys = [ ] unexpected_keys = [ ] error_msgs = [ ] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , \"_metadata\" , None ) state_dict = state_dict . copy ( ) if metadata is not None : state_dict . _metadata = metadata def load ( module , prefix = \"\" ) : local_metadata = { } if metadata is None else metadata . get ( prefix [ : - 1 ] , { } ) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items ( ) : if child is not None : load ( child , prefix + name + \".\" ) start_model = model if hasattr ( model , \"transformer\" ) and all ( not s . startswith ( 'transformer.' ) for s in state_dict . keys ( ) ) : start_model = model . transformer load ( start_model , prefix = \"\" ) if len ( missing_keys ) > 0 : logger . info ( \"Weights of {} not initialized from pretrained model: {}\" . format ( model . __class__ . __name__ , missing_keys ) ) if len ( unexpected_keys ) > 0 : logger . info ( \"Weights from pretrained model not used in {}: {}\" . format ( model . __class__ . __name__ , unexpected_keys ) ) if len ( error_msgs ) > 0 : raise RuntimeError ( \"Error(s) in loading state_dict for {}:\\n\\t{}\" . format ( model . __class__ . __name__ , \"\\n\\t\" . join ( error_msgs ) ) ) # Make sure we are still sharing the output and input embeddings after loading weights model . set_tied ( ) return model\n\n\n        Summary:", "input": "", "output": "Instantiate a GPT2PreTrainedModel from a pre - trained model file or a pytorch state dict . Download and cache the pre - trained model file if needed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_examples_to_features ( examples , seq_length , tokenizer ) : features = [ ] for ( ex_index , example ) in enumerate ( examples ) : tokens_a = tokenizer . tokenize ( example . text_a ) tokens_b = None if example . text_b : tokens_b = tokenizer . tokenize ( example . text_b ) if tokens_b : # Modifies `tokens_a` and `tokens_b` in place so that the total # length is less than the specified length. # Account for [CLS], [SEP], [SEP] with \"- 3\" _truncate_seq_pair ( tokens_a , tokens_b , seq_length - 3 ) else : # Account for [CLS] and [SEP] with \"- 2\" if len ( tokens_a ) > seq_length - 2 : tokens_a = tokens_a [ 0 : ( seq_length - 2 ) ] # The convention in BERT is: # (a) For sequence pairs: # tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP] # type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 # (b) For single sequences: # tokens: [CLS] the dog is hairy . [SEP] # type_ids: 0 0 0 0 0 0 0 # # Where \"type_ids\" are used to indicate whether this is the first # sequence or the second sequence. The embedding vectors for `type=0` and # `type=1` were learned during pre-training and are added to the wordpiece # embedding vector (and position vector). This is not *strictly* necessary # since the [SEP] token unambigiously separates the sequences, but it makes # it easier for the model to learn the concept of sequences. # # For classification tasks, the first vector (corresponding to [CLS]) is # used as as the \"sentence vector\". Note that this only makes sense because # the entire model is fine-tuned. tokens = [ ] input_type_ids = [ ] tokens . append ( \"[CLS]\" ) input_type_ids . append ( 0 ) for token in tokens_a : tokens . append ( token ) input_type_ids . append ( 0 ) tokens . append ( \"[SEP]\" ) input_type_ids . append ( 0 ) if tokens_b : for token in tokens_b : tokens . append ( token ) input_type_ids . append ( 1 ) tokens . append ( \"[SEP]\" ) input_type_ids . append ( 1 ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) # The mask has 1 for real tokens and 0 for padding tokens. Only real # tokens are attended to. input_mask = [ 1 ] * len ( input_ids ) # Zero-pad up to the sequence length. while len ( input_ids ) < seq_length : input_ids . append ( 0 ) input_mask . append ( 0 ) input_type_ids . append ( 0 ) assert len ( input_ids ) == seq_length assert len ( input_mask ) == seq_length assert len ( input_type_ids ) == seq_length if ex_index < 5 : logger . info ( \"*** Example ***\" ) logger . info ( \"unique_id: %s\" % ( example . unique_id ) ) logger . info ( \"tokens: %s\" % \" \" . join ( [ str ( x ) for x in tokens ] ) ) logger . info ( \"input_ids: %s\" % \" \" . join ( [ str ( x ) for x in input_ids ] ) ) logger . info ( \"input_mask: %s\" % \" \" . join ( [ str ( x ) for x in input_mask ] ) ) logger . info ( \"input_type_ids: %s\" % \" \" . join ( [ str ( x ) for x in input_type_ids ] ) ) features . append ( InputFeatures ( unique_id = example . unique_id , tokens = tokens , input_ids = input_ids , input_mask = input_mask , input_type_ids = input_type_ids ) ) return features\n\n\n        Summary:", "input": "", "output": "Loads a data file into a list of InputFeature s ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_examples ( input_file ) : examples = [ ] unique_id = 0 with open ( input_file , \"r\" , encoding = 'utf-8' ) as reader : while True : line = reader . readline ( ) if not line : break line = line . strip ( ) text_a = None text_b = None m = re . match ( r\"^(.*) \\|\\|\\| (.*)$\" , line ) if m is None : text_a = line else : text_a = m . group ( 1 ) text_b = m . group ( 2 ) examples . append ( InputExample ( unique_id = unique_id , text_a = text_a , text_b = text_b ) ) unique_id += 1 return examples\n\n\n        Summary:", "input": "", "output": "Read a list of InputExample s from an input file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_squad_examples ( input_file , is_training , version_2_with_negative ) : with open ( input_file , \"r\" , encoding = 'utf-8' ) as reader : input_data = json . load ( reader ) [ \"data\" ] def is_whitespace ( c ) : if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord ( c ) == 0x202F : return True return False examples = [ ] for entry in input_data : for paragraph in entry [ \"paragraphs\" ] : paragraph_text = paragraph [ \"context\" ] doc_tokens = [ ] char_to_word_offset = [ ] prev_is_whitespace = True for c in paragraph_text : if is_whitespace ( c ) : prev_is_whitespace = True else : if prev_is_whitespace : doc_tokens . append ( c ) else : doc_tokens [ - 1 ] += c prev_is_whitespace = False char_to_word_offset . append ( len ( doc_tokens ) - 1 ) for qa in paragraph [ \"qas\" ] : qas_id = qa [ \"id\" ] question_text = qa [ \"question\" ] start_position = None end_position = None orig_answer_text = None is_impossible = False if is_training : if version_2_with_negative : is_impossible = qa [ \"is_impossible\" ] if ( len ( qa [ \"answers\" ] ) != 1 ) and ( not is_impossible ) : raise ValueError ( \"For training, each question should have exactly 1 answer.\" ) if not is_impossible : answer = qa [ \"answers\" ] [ 0 ] orig_answer_text = answer [ \"text\" ] answer_offset = answer [ \"answer_start\" ] answer_length = len ( orig_answer_text ) start_position = char_to_word_offset [ answer_offset ] end_position = char_to_word_offset [ answer_offset + answer_length - 1 ] # Only add answers where the text can be exactly recovered from the # document. If this CAN'T happen it's likely due to weird Unicode # stuff so we will just skip the example. # # Note that this means for training mode, every example is NOT # guaranteed to be preserved. actual_text = \" \" . join ( doc_tokens [ start_position : ( end_position + 1 ) ] ) cleaned_answer_text = \" \" . join ( whitespace_tokenize ( orig_answer_text ) ) if actual_text . find ( cleaned_answer_text ) == - 1 : logger . warning ( \"Could not find answer: '%s' vs. '%s'\" , actual_text , cleaned_answer_text ) continue else : start_position = - 1 end_position = - 1 orig_answer_text = \"\" example = SquadExample ( qas_id = qas_id , question_text = question_text , doc_tokens = doc_tokens , orig_answer_text = orig_answer_text , start_position = start_position , end_position = end_position , is_impossible = is_impossible ) examples . append ( example ) return examples\n\n\n        Summary:", "input": "", "output": "Read a SQuAD json file into a list of SquadExample ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_examples_to_features ( examples , tokenizer , max_seq_length , doc_stride , max_query_length , is_training ) : unique_id = 1000000000 features = [ ] for ( example_index , example ) in enumerate ( examples ) : query_tokens = tokenizer . tokenize ( example . question_text ) if len ( query_tokens ) > max_query_length : query_tokens = query_tokens [ 0 : max_query_length ] tok_to_orig_index = [ ] orig_to_tok_index = [ ] all_doc_tokens = [ ] for ( i , token ) in enumerate ( example . doc_tokens ) : orig_to_tok_index . append ( len ( all_doc_tokens ) ) sub_tokens = tokenizer . tokenize ( token ) for sub_token in sub_tokens : tok_to_orig_index . append ( i ) all_doc_tokens . append ( sub_token ) tok_start_position = None tok_end_position = None if is_training and example . is_impossible : tok_start_position = - 1 tok_end_position = - 1 if is_training and not example . is_impossible : tok_start_position = orig_to_tok_index [ example . start_position ] if example . end_position < len ( example . doc_tokens ) - 1 : tok_end_position = orig_to_tok_index [ example . end_position + 1 ] - 1 else : tok_end_position = len ( all_doc_tokens ) - 1 ( tok_start_position , tok_end_position ) = _improve_answer_span ( all_doc_tokens , tok_start_position , tok_end_position , tokenizer , example . orig_answer_text ) # The -3 accounts for [CLS], [SEP] and [SEP] max_tokens_for_doc = max_seq_length - len ( query_tokens ) - 3 # We can have documents that are longer than the maximum sequence length. # To deal with this we do a sliding window approach, where we take chunks # of the up to our max length with a stride of `doc_stride`. _DocSpan = collections . namedtuple ( # pylint: disable=invalid-name \"DocSpan\" , [ \"start\" , \"length\" ] ) doc_spans = [ ] start_offset = 0 while start_offset < len ( all_doc_tokens ) : length = len ( all_doc_tokens ) - start_offset if length > max_tokens_for_doc : length = max_tokens_for_doc doc_spans . append ( _DocSpan ( start = start_offset , length = length ) ) if start_offset + length == len ( all_doc_tokens ) : break start_offset += min ( length , doc_stride ) for ( doc_span_index , doc_span ) in enumerate ( doc_spans ) : tokens = [ ] token_to_orig_map = { } token_is_max_context = { } segment_ids = [ ] tokens . append ( \"[CLS]\" ) segment_ids . append ( 0 ) for token in query_tokens : tokens . append ( token ) segment_ids . append ( 0 ) tokens . append ( \"[SEP]\" ) segment_ids . append ( 0 ) for i in range ( doc_span . length ) : split_token_index = doc_span . start + i token_to_orig_map [ len ( tokens ) ] = tok_to_orig_index [ split_token_index ] is_max_context = _check_is_max_context ( doc_spans , doc_span_index , split_token_index ) token_is_max_context [ len ( tokens ) ] = is_max_context tokens . append ( all_doc_tokens [ split_token_index ] ) segment_ids . append ( 1 ) tokens . append ( \"[SEP]\" ) segment_ids . append ( 1 ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) # The mask has 1 for real tokens and 0 for padding tokens. Only real # tokens are attended to. input_mask = [ 1 ] * len ( input_ids ) # Zero-pad up to the sequence length. while len ( input_ids ) < max_seq_length : input_ids . append ( 0 ) input_mask . append ( 0 ) segment_ids . append ( 0 ) assert len ( input_ids ) == max_seq_length assert len ( input_mask ) == max_seq_length assert len ( segment_ids ) == max_seq_length start_position = None end_position = None if is_training and not example . is_impossible : # For training, if our document chunk does not contain an annotation # we throw it out, since there is nothing to predict. doc_start = doc_span . start doc_end = doc_span . start + doc_span . length - 1 out_of_span = False if not ( tok_start_position >= doc_start and tok_end_position <= doc_end ) : out_of_span = True if out_of_span : start_position = 0 end_position = 0 else : doc_offset = len ( query_tokens ) + 2 start_position = tok_start_position - doc_start + doc_offset end_position = tok_end_position - doc_start + doc_offset if is_training and example . is_impossible : start_position = 0 end_position = 0 if example_index < 20 : logger . info ( \"*** Example ***\" ) logger . info ( \"unique_id: %s\" % ( unique_id ) ) logger . info ( \"example_index: %s\" % ( example_index ) ) logger . info ( \"doc_span_index: %s\" % ( doc_span_index ) ) logger . info ( \"tokens: %s\" % \" \" . join ( tokens ) ) logger . info ( \"token_to_orig_map: %s\" % \" \" . join ( [ \"%d:%d\" % ( x , y ) for ( x , y ) in token_to_orig_map . items ( ) ] ) ) logger . info ( \"token_is_max_context: %s\" % \" \" . join ( [ \"%d:%s\" % ( x , y ) for ( x , y ) in token_is_max_context . items ( ) ] ) ) logger . info ( \"input_ids: %s\" % \" \" . join ( [ str ( x ) for x in input_ids ] ) ) logger . info ( \"input_mask: %s\" % \" \" . join ( [ str ( x ) for x in input_mask ] ) ) logger . info ( \"segment_ids: %s\" % \" \" . join ( [ str ( x ) for x in segment_ids ] ) ) if is_training and example . is_impossible : logger . info ( \"impossible example\" ) if is_training and not example . is_impossible : answer_text = \" \" . join ( tokens [ start_position : ( end_position + 1 ) ] ) logger . info ( \"start_position: %d\" % ( start_position ) ) logger . info ( \"end_position: %d\" % ( end_position ) ) logger . info ( \"answer: %s\" % ( answer_text ) ) features . append ( InputFeatures ( unique_id = unique_id , example_index = example_index , doc_span_index = doc_span_index , tokens = tokens , token_to_orig_map = token_to_orig_map , token_is_max_context = token_is_max_context , input_ids = input_ids , input_mask = input_mask , segment_ids = segment_ids , start_position = start_position , end_position = end_position , is_impossible = example . is_impossible ) ) unique_id += 1 return features\n\n\n        Summary:", "input": "", "output": "Loads a data file into a list of InputBatch s ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _improve_answer_span ( doc_tokens , input_start , input_end , tokenizer , orig_answer_text ) : # The SQuAD annotations are character based. We first project them to # whitespace-tokenized words. But then after WordPiece tokenization, we can # often find a \"better match\". For example: # # Question: What year was John Smith born? # Context: The leader was John Smith (1895-1943). # Answer: 1895 # # The original whitespace-tokenized answer will be \"(1895-1943).\". However # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match # the exact answer, 1895. # # However, this is not always possible. Consider the following: # # Question: What country is the top exporter of electornics? # Context: The Japanese electronics industry is the lagest in the world. # Answer: Japan # # In this case, the annotator chose \"Japan\" as a character sub-span of # the word \"Japanese\". Since our WordPiece tokenizer does not split # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare # in SQuAD, but does happen. tok_answer_text = \" \" . join ( tokenizer . tokenize ( orig_answer_text ) ) for new_start in range ( input_start , input_end + 1 ) : for new_end in range ( input_end , new_start - 1 , - 1 ) : text_span = \" \" . join ( doc_tokens [ new_start : ( new_end + 1 ) ] ) if text_span == tok_answer_text : return ( new_start , new_end ) return ( input_start , input_end )\n\n\n        Summary:", "input": "", "output": "Returns tokenized answer spans that better match the annotated answer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_is_max_context ( doc_spans , cur_span_index , position ) : # Because of the sliding window approach taken to scoring documents, a single # token can appear in multiple documents. E.g. # Doc: the man went to the store and bought a gallon of milk # Span A: the man went to the # Span B: to the store and bought # Span C: and bought a gallon of # ... # # Now the word 'bought' will have two scores from spans B and C. We only # want to consider the score with \"maximum context\", which we define as # the *minimum* of its left and right context (the *sum* of left and # right context will always be the same, of course). # # In the example the maximum context for 'bought' would be span C since # it has 1 left context and 3 right context, while span B has 4 left context # and 0 right context. best_score = None best_span_index = None for ( span_index , doc_span ) in enumerate ( doc_spans ) : end = doc_span . start + doc_span . length - 1 if position < doc_span . start : continue if position > end : continue num_left_context = position - doc_span . start num_right_context = end - position score = min ( num_left_context , num_right_context ) + 0.01 * doc_span . length if best_score is None or score > best_score : best_score = score best_span_index = span_index return cur_span_index == best_span_index\n\n\n        Summary:", "input": "", "output": "Check if this is the max context doc span for the token ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_predictions ( all_examples , all_features , all_results , n_best_size , max_answer_length , do_lower_case , output_prediction_file , output_nbest_file , output_null_log_odds_file , verbose_logging , version_2_with_negative , null_score_diff_threshold ) : logger . info ( \"Writing predictions to: %s\" % ( output_prediction_file ) ) logger . info ( \"Writing nbest to: %s\" % ( output_nbest_file ) ) example_index_to_features = collections . defaultdict ( list ) for feature in all_features : example_index_to_features [ feature . example_index ] . append ( feature ) unique_id_to_result = { } for result in all_results : unique_id_to_result [ result . unique_id ] = result _PrelimPrediction = collections . namedtuple ( # pylint: disable=invalid-name \"PrelimPrediction\" , [ \"feature_index\" , \"start_index\" , \"end_index\" , \"start_logit\" , \"end_logit\" ] ) all_predictions = collections . OrderedDict ( ) all_nbest_json = collections . OrderedDict ( ) scores_diff_json = collections . OrderedDict ( ) for ( example_index , example ) in enumerate ( all_examples ) : features = example_index_to_features [ example_index ] prelim_predictions = [ ] # keep track of the minimum score of null start+end of position 0 score_null = 1000000 # large and positive min_null_feature_index = 0 # the paragraph slice with min null score null_start_logit = 0 # the start logit at the slice with min null score null_end_logit = 0 # the end logit at the slice with min null score for ( feature_index , feature ) in enumerate ( features ) : result = unique_id_to_result [ feature . unique_id ] start_indexes = _get_best_indexes ( result . start_logits , n_best_size ) end_indexes = _get_best_indexes ( result . end_logits , n_best_size ) # if we could have irrelevant answers, get the min score of irrelevant if version_2_with_negative : feature_null_score = result . start_logits [ 0 ] + result . end_logits [ 0 ] if feature_null_score < score_null : score_null = feature_null_score min_null_feature_index = feature_index null_start_logit = result . start_logits [ 0 ] null_end_logit = result . end_logits [ 0 ] for start_index in start_indexes : for end_index in end_indexes : # We could hypothetically create invalid predictions, e.g., predict # that the start of the span is in the question. We throw out all # invalid predictions. if start_index >= len ( feature . tokens ) : continue if end_index >= len ( feature . tokens ) : continue if start_index not in feature . token_to_orig_map : continue if end_index not in feature . token_to_orig_map : continue if not feature . token_is_max_context . get ( start_index , False ) : continue if end_index < start_index : continue length = end_index - start_index + 1 if length > max_answer_length : continue prelim_predictions . append ( _PrelimPrediction ( feature_index = feature_index , start_index = start_index , end_index = end_index , start_logit = result . start_logits [ start_index ] , end_logit = result . end_logits [ end_index ] ) ) if version_2_with_negative : prelim_predictions . append ( _PrelimPrediction ( feature_index = min_null_feature_index , start_index = 0 , end_index = 0 , start_logit = null_start_logit , end_logit = null_end_logit ) ) prelim_predictions = sorted ( prelim_predictions , key = lambda x : ( x . start_logit + x . end_logit ) , reverse = True ) _NbestPrediction = collections . namedtuple ( # pylint: disable=invalid-name \"NbestPrediction\" , [ \"text\" , \"start_logit\" , \"end_logit\" ] ) seen_predictions = { } nbest = [ ] for pred in prelim_predictions : if len ( nbest ) >= n_best_size : break feature = features [ pred . feature_index ] if pred . start_index > 0 : # this is a non-null prediction tok_tokens = feature . tokens [ pred . start_index : ( pred . end_index + 1 ) ] orig_doc_start = feature . token_to_orig_map [ pred . start_index ] orig_doc_end = feature . token_to_orig_map [ pred . end_index ] orig_tokens = example . doc_tokens [ orig_doc_start : ( orig_doc_end + 1 ) ] tok_text = \" \" . join ( tok_tokens ) # De-tokenize WordPieces that have been split off. tok_text = tok_text . replace ( \" ##\" , \"\" ) tok_text = tok_text . replace ( \"##\" , \"\" ) # Clean whitespace tok_text = tok_text . strip ( ) tok_text = \" \" . join ( tok_text . split ( ) ) orig_text = \" \" . join ( orig_tokens ) final_text = get_final_text ( tok_text , orig_text , do_lower_case , verbose_logging ) if final_text in seen_predictions : continue seen_predictions [ final_text ] = True else : final_text = \"\" seen_predictions [ final_text ] = True nbest . append ( _NbestPrediction ( text = final_text , start_logit = pred . start_logit , end_logit = pred . end_logit ) ) # if we didn't include the empty option in the n-best, include it if version_2_with_negative : if \"\" not in seen_predictions : nbest . append ( _NbestPrediction ( text = \"\" , start_logit = null_start_logit , end_logit = null_end_logit ) ) # In very rare edge cases we could only have single null prediction. # So we just create a nonce prediction in this case to avoid failure. if len ( nbest ) == 1 : nbest . insert ( 0 , _NbestPrediction ( text = \"empty\" , start_logit = 0.0 , end_logit = 0.0 ) ) # In very rare edge cases we could have no valid predictions. So we # just create a nonce prediction in this case to avoid failure. if not nbest : nbest . append ( _NbestPrediction ( text = \"empty\" , start_logit = 0.0 , end_logit = 0.0 ) ) assert len ( nbest ) >= 1 total_scores = [ ] best_non_null_entry = None for entry in nbest : total_scores . append ( entry . start_logit + entry . end_logit ) if not best_non_null_entry : if entry . text : best_non_null_entry = entry probs = _compute_softmax ( total_scores ) nbest_json = [ ] for ( i , entry ) in enumerate ( nbest ) : output = collections . OrderedDict ( ) output [ \"text\" ] = entry . text output [ \"probability\" ] = probs [ i ] output [ \"start_logit\" ] = entry . start_logit output [ \"end_logit\" ] = entry . end_logit nbest_json . append ( output ) assert len ( nbest_json ) >= 1 if not version_2_with_negative : all_predictions [ example . qas_id ] = nbest_json [ 0 ] [ \"text\" ] else : # predict \"\" iff the null score - the score of best non-null > threshold score_diff = score_null - best_non_null_entry . start_logit - ( best_non_null_entry . end_logit ) scores_diff_json [ example . qas_id ] = score_diff if score_diff > null_score_diff_threshold : all_predictions [ example . qas_id ] = \"\" else : all_predictions [ example . qas_id ] = best_non_null_entry . text all_nbest_json [ example . qas_id ] = nbest_json with open ( output_prediction_file , \"w\" ) as writer : writer . write ( json . dumps ( all_predictions , indent = 4 ) + \"\\n\" ) with open ( output_nbest_file , \"w\" ) as writer : writer . write ( json . dumps ( all_nbest_json , indent = 4 ) + \"\\n\" ) if version_2_with_negative : with open ( output_null_log_odds_file , \"w\" ) as writer : writer . write ( json . dumps ( scores_diff_json , indent = 4 ) + \"\\n\" )\n\n\n        Summary:", "input": "", "output": "Write final predictions to the json file and log - odds of null if needed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_final_text ( pred_text , orig_text , do_lower_case , verbose_logging = False ) : # When we created the data, we kept track of the alignment between original # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So # now `orig_text` contains the span of our original text corresponding to the # span that we predicted. # # However, `orig_text` may contain extra characters that we don't want in # our prediction. # # For example, let's say: # pred_text = steve smith # orig_text = Steve Smith's # # We don't want to return `orig_text` because it contains the extra \"'s\". # # We don't want to return `pred_text` because it's already been normalized # (the SQuAD eval script also does punctuation stripping/lower casing but # our tokenizer does additional normalization like stripping accent # characters). # # What we really want to return is \"Steve Smith\". # # Therefore, we have to apply a semi-complicated alignment heuristic between # `pred_text` and `orig_text` to get a character-to-character alignment. This # can fail in certain cases in which case we just return `orig_text`. def _strip_spaces ( text ) : ns_chars = [ ] ns_to_s_map = collections . OrderedDict ( ) for ( i , c ) in enumerate ( text ) : if c == \" \" : continue ns_to_s_map [ len ( ns_chars ) ] = i ns_chars . append ( c ) ns_text = \"\" . join ( ns_chars ) return ( ns_text , ns_to_s_map ) # We first tokenize `orig_text`, strip whitespace from the result # and `pred_text`, and check if they are the same length. If they are # NOT the same length, the heuristic has failed. If they are the same # length, we assume the characters are one-to-one aligned. tokenizer = BasicTokenizer ( do_lower_case = do_lower_case ) tok_text = \" \" . join ( tokenizer . tokenize ( orig_text ) ) start_position = tok_text . find ( pred_text ) if start_position == - 1 : if verbose_logging : logger . info ( \"Unable to find text: '%s' in '%s'\" % ( pred_text , orig_text ) ) return orig_text end_position = start_position + len ( pred_text ) - 1 ( orig_ns_text , orig_ns_to_s_map ) = _strip_spaces ( orig_text ) ( tok_ns_text , tok_ns_to_s_map ) = _strip_spaces ( tok_text ) if len ( orig_ns_text ) != len ( tok_ns_text ) : if verbose_logging : logger . info ( \"Length not equal after stripping spaces: '%s' vs '%s'\" , orig_ns_text , tok_ns_text ) return orig_text # We then project the characters in `pred_text` back to `orig_text` using # the character-to-character alignment. tok_s_to_ns_map = { } for ( i , tok_index ) in tok_ns_to_s_map . items ( ) : tok_s_to_ns_map [ tok_index ] = i orig_start_position = None if start_position in tok_s_to_ns_map : ns_start_position = tok_s_to_ns_map [ start_position ] if ns_start_position in orig_ns_to_s_map : orig_start_position = orig_ns_to_s_map [ ns_start_position ] if orig_start_position is None : if verbose_logging : logger . info ( \"Couldn't map start position\" ) return orig_text orig_end_position = None if end_position in tok_s_to_ns_map : ns_end_position = tok_s_to_ns_map [ end_position ] if ns_end_position in orig_ns_to_s_map : orig_end_position = orig_ns_to_s_map [ ns_end_position ] if orig_end_position is None : if verbose_logging : logger . info ( \"Couldn't map end position\" ) return orig_text output_text = orig_text [ orig_start_position : ( orig_end_position + 1 ) ] return output_text\n\n\n        Summary:", "input": "", "output": "Project the tokenized prediction back to the original text ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_best_indexes ( logits , n_best_size ) : index_and_score = sorted ( enumerate ( logits ) , key = lambda x : x [ 1 ] , reverse = True ) best_indexes = [ ] for i in range ( len ( index_and_score ) ) : if i >= n_best_size : break best_indexes . append ( index_and_score [ i ] [ 0 ] ) return best_indexes\n\n\n        Summary:", "input": "", "output": "Get the n - best logits from a list ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _compute_softmax ( scores ) : if not scores : return [ ] max_score = None for score in scores : if max_score is None or score > max_score : max_score = score exp_scores = [ ] total_sum = 0.0 for score in scores : x = math . exp ( score - max_score ) exp_scores . append ( x ) total_sum += x probs = [ ] for score in exp_scores : probs . append ( score / total_sum ) return probs\n\n\n        Summary:", "input": "", "output": "Compute softmax probability over raw logits ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_examples_to_features ( examples , tokenizer , max_seq_length , is_training ) : # Swag is a multiple choice task. To perform this task using Bert, # we will use the formatting proposed in \"Improving Language # Understanding by Generative Pre-Training\" and suggested by # @jacobdevlin-google in this issue # https://github.com/google-research/bert/issues/38. # # Each choice will correspond to a sample on which we run the # inference. For a given Swag example, we will create the 4 # following inputs: # - [CLS] context [SEP] choice_1 [SEP] # - [CLS] context [SEP] choice_2 [SEP] # - [CLS] context [SEP] choice_3 [SEP] # - [CLS] context [SEP] choice_4 [SEP] # The model will output a single value for each input. To get the # final decision of the model, we will run a softmax over these 4 # outputs. features = [ ] for example_index , example in enumerate ( examples ) : context_tokens = tokenizer . tokenize ( example . context_sentence ) start_ending_tokens = tokenizer . tokenize ( example . start_ending ) choices_features = [ ] for ending_index , ending in enumerate ( example . endings ) : # We create a copy of the context tokens in order to be # able to shrink it according to ending_tokens context_tokens_choice = context_tokens [ : ] ending_tokens = start_ending_tokens + tokenizer . tokenize ( ending ) # Modifies `context_tokens_choice` and `ending_tokens` in # place so that the total length is less than the # specified length. Account for [CLS], [SEP], [SEP] with # \"- 3\" _truncate_seq_pair ( context_tokens_choice , ending_tokens , max_seq_length - 3 ) tokens = [ \"[CLS]\" ] + context_tokens_choice + [ \"[SEP]\" ] + ending_tokens + [ \"[SEP]\" ] segment_ids = [ 0 ] * ( len ( context_tokens_choice ) + 2 ) + [ 1 ] * ( len ( ending_tokens ) + 1 ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) input_mask = [ 1 ] * len ( input_ids ) # Zero-pad up to the sequence length. padding = [ 0 ] * ( max_seq_length - len ( input_ids ) ) input_ids += padding input_mask += padding segment_ids += padding assert len ( input_ids ) == max_seq_length assert len ( input_mask ) == max_seq_length assert len ( segment_ids ) == max_seq_length choices_features . append ( ( tokens , input_ids , input_mask , segment_ids ) ) label = example . label if example_index < 5 : logger . info ( \"*** Example ***\" ) logger . info ( \"swag_id: {}\" . format ( example . swag_id ) ) for choice_idx , ( tokens , input_ids , input_mask , segment_ids ) in enumerate ( choices_features ) : logger . info ( \"choice: {}\" . format ( choice_idx ) ) logger . info ( \"tokens: {}\" . format ( ' ' . join ( tokens ) ) ) logger . info ( \"input_ids: {}\" . format ( ' ' . join ( map ( str , input_ids ) ) ) ) logger . info ( \"input_mask: {}\" . format ( ' ' . join ( map ( str , input_mask ) ) ) ) logger . info ( \"segment_ids: {}\" . format ( ' ' . join ( map ( str , segment_ids ) ) ) ) if is_training : logger . info ( \"label: {}\" . format ( label ) ) features . append ( InputFeatures ( example_id = example . swag_id , choices_features = choices_features , label = label ) ) return features\n\n\n        Summary:", "input": "", "output": "Loads a data file into a list of InputBatch s ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_examples_to_features ( examples , label_list , max_seq_length , tokenizer , output_mode ) : label_map = { label : i for i , label in enumerate ( label_list ) } features = [ ] for ( ex_index , example ) in enumerate ( examples ) : if ex_index % 10000 == 0 : logger . info ( \"Writing example %d of %d\" % ( ex_index , len ( examples ) ) ) tokens_a = tokenizer . tokenize ( example . text_a ) tokens_b = None if example . text_b : tokens_b = tokenizer . tokenize ( example . text_b ) # Modifies `tokens_a` and `tokens_b` in place so that the total # length is less than the specified length. # Account for [CLS], [SEP], [SEP] with \"- 3\" _truncate_seq_pair ( tokens_a , tokens_b , max_seq_length - 3 ) else : # Account for [CLS] and [SEP] with \"- 2\" if len ( tokens_a ) > max_seq_length - 2 : tokens_a = tokens_a [ : ( max_seq_length - 2 ) ] # The convention in BERT is: # (a) For sequence pairs: # tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP] # type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 # (b) For single sequences: # tokens: [CLS] the dog is hairy . [SEP] # type_ids: 0 0 0 0 0 0 0 # # Where \"type_ids\" are used to indicate whether this is the first # sequence or the second sequence. The embedding vectors for `type=0` and # `type=1` were learned during pre-training and are added to the wordpiece # embedding vector (and position vector). This is not *strictly* necessary # since the [SEP] token unambiguously separates the sequences, but it makes # it easier for the model to learn the concept of sequences. # # For classification tasks, the first vector (corresponding to [CLS]) is # used as as the \"sentence vector\". Note that this only makes sense because # the entire model is fine-tuned. tokens = [ \"[CLS]\" ] + tokens_a + [ \"[SEP]\" ] segment_ids = [ 0 ] * len ( tokens ) if tokens_b : tokens += tokens_b + [ \"[SEP]\" ] segment_ids += [ 1 ] * ( len ( tokens_b ) + 1 ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) # The mask has 1 for real tokens and 0 for padding tokens. Only real # tokens are attended to. input_mask = [ 1 ] * len ( input_ids ) # Zero-pad up to the sequence length. padding = [ 0 ] * ( max_seq_length - len ( input_ids ) ) input_ids += padding input_mask += padding segment_ids += padding assert len ( input_ids ) == max_seq_length assert len ( input_mask ) == max_seq_length assert len ( segment_ids ) == max_seq_length if output_mode == \"classification\" : label_id = label_map [ example . label ] elif output_mode == \"regression\" : label_id = float ( example . label ) else : raise KeyError ( output_mode ) if ex_index < 5 : logger . info ( \"*** Example ***\" ) logger . info ( \"guid: %s\" % ( example . guid ) ) logger . info ( \"tokens: %s\" % \" \" . join ( [ str ( x ) for x in tokens ] ) ) logger . info ( \"input_ids: %s\" % \" \" . join ( [ str ( x ) for x in input_ids ] ) ) logger . info ( \"input_mask: %s\" % \" \" . join ( [ str ( x ) for x in input_mask ] ) ) logger . info ( \"segment_ids: %s\" % \" \" . join ( [ str ( x ) for x in segment_ids ] ) ) logger . info ( \"label: %s (id = %d)\" % ( example . label , label_id ) ) features . append ( InputFeatures ( input_ids = input_ids , input_mask = input_mask , segment_ids = segment_ids , label_id = label_id ) ) return features\n\n\n        Summary:", "input": "", "output": "Loads a data file into a list of InputBatch s ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _read_tsv ( cls , input_file , quotechar = None ) : with open ( input_file , \"r\" , encoding = \"utf-8\" ) as f : reader = csv . reader ( f , delimiter = \"\\t\" , quotechar = quotechar ) lines = [ ] for line in reader : if sys . version_info [ 0 ] == 2 : line = list ( unicode ( cell , 'utf-8' ) for cell in line ) lines . append ( line ) return lines\n\n\n        Summary:", "input": "", "output": "Reads a tab separated value file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_train_examples ( self , data_dir ) : logger . info ( \"LOOKING AT {}\" . format ( os . path . join ( data_dir , \"train.tsv\" ) ) ) return self . _create_examples ( self . _read_tsv ( os . path . join ( data_dir , \"train.tsv\" ) ) , \"train\" )\n\n\n        Summary:", "input": "", "output": "See base class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_examples ( self , lines , set_type ) : examples = [ ] for ( i , line ) in enumerate ( lines ) : if i == 0 : continue guid = \"%s-%s\" % ( set_type , i ) text_a = line [ 3 ] text_b = line [ 4 ] label = line [ 0 ] examples . append ( InputExample ( guid = guid , text_a = text_a , text_b = text_b , label = label ) ) return examples\n\n\n        Summary:", "input": "", "output": "Creates examples for the training and dev sets ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_train_examples ( self , data_dir ) : return self . _create_examples ( self . _read_tsv ( os . path . join ( data_dir , \"train.tsv\" ) ) , \"train\" )\n\n\n        Summary:", "input": "", "output": "See base class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_dev_examples ( self , data_dir ) : return self . _create_examples ( self . _read_tsv ( os . path . join ( data_dir , \"dev_matched.tsv\" ) ) , \"dev_matched\" )\n\n\n        Summary:", "input": "", "output": "See base class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def top_k_logits ( logits , k ) : if k == 0 : return logits else : values = torch . topk ( logits , k ) [ 0 ] batch_mins = values [ : , - 1 ] . view ( - 1 , 1 ) . expand_as ( logits ) return torch . where ( logits < batch_mins , torch . ones_like ( logits ) * - 1e10 , logits )\n\n\n        Summary:", "input": "", "output": "Masks everything but the k top entries as - infinity ( 1e10 ) . Used to mask logits such that e^ - infinity - > 0 won t contribute to the sum of the denominator ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_tf_weights_in_bert ( model , tf_checkpoint_path ) : try : import re import numpy as np import tensorflow as tf except ImportError : print ( \"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \" \"https://www.tensorflow.org/install/ for installation instructions.\" ) raise tf_path = os . path . abspath ( tf_checkpoint_path ) print ( \"Converting TensorFlow checkpoint from {}\" . format ( tf_path ) ) # Load weights from TF model init_vars = tf . train . list_variables ( tf_path ) names = [ ] arrays = [ ] for name , shape in init_vars : print ( \"Loading TF weight {} with shape {}\" . format ( name , shape ) ) array = tf . train . load_variable ( tf_path , name ) names . append ( name ) arrays . append ( array ) for name , array in zip ( names , arrays ) : name = name . split ( '/' ) # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v # which are not required for using pretrained model if any ( n in [ \"adam_v\" , \"adam_m\" , \"global_step\" ] for n in name ) : print ( \"Skipping {}\" . format ( \"/\" . join ( name ) ) ) continue pointer = model for m_name in name : if re . fullmatch ( r'[A-Za-z]+_\\d+' , m_name ) : l = re . split ( r'_(\\d+)' , m_name ) else : l = [ m_name ] if l [ 0 ] == 'kernel' or l [ 0 ] == 'gamma' : pointer = getattr ( pointer , 'weight' ) elif l [ 0 ] == 'output_bias' or l [ 0 ] == 'beta' : pointer = getattr ( pointer , 'bias' ) elif l [ 0 ] == 'output_weights' : pointer = getattr ( pointer , 'weight' ) elif l [ 0 ] == 'squad' : pointer = getattr ( pointer , 'classifier' ) else : try : pointer = getattr ( pointer , l [ 0 ] ) except AttributeError : print ( \"Skipping {}\" . format ( \"/\" . join ( name ) ) ) continue if len ( l ) >= 2 : num = int ( l [ 1 ] ) pointer = pointer [ num ] if m_name [ - 11 : ] == '_embeddings' : pointer = getattr ( pointer , 'weight' ) elif m_name == 'kernel' : array = np . transpose ( array ) try : assert pointer . shape == array . shape except AssertionError as e : e . args += ( pointer . shape , array . shape ) raise print ( \"Initialize PyTorch weight {}\" . format ( name ) ) pointer . data = torch . from_numpy ( array ) return model\n\n\n        Summary:", "input": "", "output": "Load tf checkpoints in a pytorch model"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_pretrained ( cls , pretrained_model_name_or_path , * inputs , * * kwargs ) : state_dict = kwargs . get ( 'state_dict' , None ) kwargs . pop ( 'state_dict' , None ) cache_dir = kwargs . get ( 'cache_dir' , None ) kwargs . pop ( 'cache_dir' , None ) from_tf = kwargs . get ( 'from_tf' , False ) kwargs . pop ( 'from_tf' , None ) if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP : archive_file = PRETRAINED_MODEL_ARCHIVE_MAP [ pretrained_model_name_or_path ] else : archive_file = pretrained_model_name_or_path # redirect to the cache, if necessary try : resolved_archive_file = cached_path ( archive_file , cache_dir = cache_dir ) except EnvironmentError : logger . error ( \"Model name '{}' was not found in model name list ({}). \" \"We assumed '{}' was a path or url but couldn't find any file \" \"associated to this path or url.\" . format ( pretrained_model_name_or_path , ', ' . join ( PRETRAINED_MODEL_ARCHIVE_MAP . keys ( ) ) , archive_file ) ) return None if resolved_archive_file == archive_file : logger . info ( \"loading archive file {}\" . format ( archive_file ) ) else : logger . info ( \"loading archive file {} from cache at {}\" . format ( archive_file , resolved_archive_file ) ) tempdir = None if os . path . isdir ( resolved_archive_file ) or from_tf : serialization_dir = resolved_archive_file else : # Extract archive to temp dir tempdir = tempfile . mkdtemp ( ) logger . info ( \"extracting archive file {} to temp dir {}\" . format ( resolved_archive_file , tempdir ) ) with tarfile . open ( resolved_archive_file , 'r:gz' ) as archive : archive . extractall ( tempdir ) serialization_dir = tempdir # Load config config_file = os . path . join ( serialization_dir , CONFIG_NAME ) if not os . path . exists ( config_file ) : # Backward compatibility with old naming format config_file = os . path . join ( serialization_dir , BERT_CONFIG_NAME ) config = BertConfig . from_json_file ( config_file ) logger . info ( \"Model config {}\" . format ( config ) ) # Instantiate model. model = cls ( config , * inputs , * * kwargs ) if state_dict is None and not from_tf : weights_path = os . path . join ( serialization_dir , WEIGHTS_NAME ) state_dict = torch . load ( weights_path , map_location = 'cpu' ) if tempdir : # Clean up temp dir shutil . rmtree ( tempdir ) if from_tf : # Directly load from a TensorFlow checkpoint weights_path = os . path . join ( serialization_dir , TF_WEIGHTS_NAME ) return load_tf_weights_in_bert ( model , weights_path ) # Load from a PyTorch state_dict old_keys = [ ] new_keys = [ ] for key in state_dict . keys ( ) : new_key = None if 'gamma' in key : new_key = key . replace ( 'gamma' , 'weight' ) if 'beta' in key : new_key = key . replace ( 'beta' , 'bias' ) if new_key : old_keys . append ( key ) new_keys . append ( new_key ) for old_key , new_key in zip ( old_keys , new_keys ) : state_dict [ new_key ] = state_dict . pop ( old_key ) missing_keys = [ ] unexpected_keys = [ ] error_msgs = [ ] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy ( ) if metadata is not None : state_dict . _metadata = metadata def load ( module , prefix = '' ) : local_metadata = { } if metadata is None else metadata . get ( prefix [ : - 1 ] , { } ) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items ( ) : if child is not None : load ( child , prefix + name + '.' ) start_prefix = '' if not hasattr ( model , 'bert' ) and any ( s . startswith ( 'bert.' ) for s in state_dict . keys ( ) ) : start_prefix = 'bert.' load ( model , prefix = start_prefix ) if len ( missing_keys ) > 0 : logger . info ( \"Weights of {} not initialized from pretrained model: {}\" . format ( model . __class__ . __name__ , missing_keys ) ) if len ( unexpected_keys ) > 0 : logger . info ( \"Weights from pretrained model not used in {}: {}\" . format ( model . __class__ . __name__ , unexpected_keys ) ) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( model . __class__ . __name__ , \"\\n\\t\" . join ( error_msgs ) ) ) return model\n\n\n        Summary:", "input": "", "output": "Instantiate a BertPreTrainedModel from a pre - trained model file or a pytorch state dict . Download and cache the pre - trained model file if needed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_tf_weights_in_openai_gpt ( model , openai_checkpoint_folder_path ) : import re import numpy as np print ( \"Loading weights...\" ) names = json . load ( open ( openai_checkpoint_folder_path + '/parameters_names.json' , \"r\" , encoding = 'utf-8' ) ) shapes = json . load ( open ( openai_checkpoint_folder_path + '/params_shapes.json' , \"r\" , encoding = 'utf-8' ) ) offsets = np . cumsum ( [ np . prod ( shape ) for shape in shapes ] ) init_params = [ np . load ( openai_checkpoint_folder_path + '/params_{}.npy' . format ( n ) ) for n in range ( 10 ) ] init_params = np . split ( np . concatenate ( init_params , 0 ) , offsets ) [ : - 1 ] init_params = [ param . reshape ( shape ) for param , shape in zip ( init_params , shapes ) ] # This was used when we had a single embedding matrix for positions and tokens # init_params[0] = np.concatenate([init_params[1], init_params[0]], 0) # del init_params[1] init_params = [ arr . squeeze ( ) for arr in init_params ] try : assert model . tokens_embed . weight . shape == init_params [ 1 ] . shape assert model . positions_embed . weight . shape == init_params [ 0 ] . shape except AssertionError as e : e . args += ( model . tokens_embed . weight . shape , init_params [ 1 ] . shape ) e . args += ( model . positions_embed . weight . shape , init_params [ 0 ] . shape ) raise model . tokens_embed . weight . data = torch . from_numpy ( init_params [ 1 ] ) model . positions_embed . weight . data = torch . from_numpy ( init_params [ 0 ] ) names . pop ( 0 ) # Pop position and token embedding arrays init_params . pop ( 0 ) init_params . pop ( 0 ) for name , array in zip ( names , init_params ) : # names[1:n_transfer], init_params[1:n_transfer]): name = name [ 6 : ] # skip \"model/\" assert name [ - 2 : ] == \":0\" name = name [ : - 2 ] name = name . split ( '/' ) pointer = model for m_name in name : if re . fullmatch ( r'[A-Za-z]+\\d+' , m_name ) : l = re . split ( r'(\\d+)' , m_name ) else : l = [ m_name ] if l [ 0 ] == 'g' : pointer = getattr ( pointer , 'weight' ) elif l [ 0 ] == 'b' : pointer = getattr ( pointer , 'bias' ) elif l [ 0 ] == 'w' : pointer = getattr ( pointer , 'weight' ) else : pointer = getattr ( pointer , l [ 0 ] ) if len ( l ) >= 2 : num = int ( l [ 1 ] ) pointer = pointer [ num ] try : assert pointer . shape == array . shape except AssertionError as e : e . args += ( pointer . shape , array . shape ) raise try : assert pointer . shape == array . shape except AssertionError as e : e . args += ( pointer . shape , array . shape ) raise print ( \"Initialize PyTorch weight {}\" . format ( name ) ) pointer . data = torch . from_numpy ( array ) return model\n\n\n        Summary:", "input": "", "output": "Load tf pre - trained weights in a pytorch model ( from NumPy arrays here )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_dict ( cls , json_object ) : config = OpenAIGPTConfig ( vocab_size_or_config_json_file = - 1 ) for key , value in json_object . items ( ) : config . __dict__ [ key ] = value return config\n\n\n        Summary:", "input": "", "output": "Constructs a OpenAIGPTConfig from a Python dictionary of parameters ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_num_special_tokens ( self , num_special_tokens ) : if self . config . n_special == num_special_tokens : return # Update config self . config . n_special = num_special_tokens # Build new embeddings and initialize all new embeddings (in particular the special tokens) old_embed = self . tokens_embed self . tokens_embed = nn . Embedding ( self . config . total_tokens_embeddings , self . config . n_embd ) self . tokens_embed . to ( old_embed . weight . device ) self . init_weights ( self . tokens_embed ) # Copy word embeddings from the previous weights self . tokens_embed . weight . data [ : self . config . vocab_size , : ] = old_embed . weight . data [ : self . config . vocab_size , : ]\n\n\n        Summary:", "input": "", "output": "Update input embeddings with new embedding matrice if needed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_num_special_tokens ( self , num_special_tokens ) : self . transformer . set_num_special_tokens ( num_special_tokens ) self . lm_head . set_embeddings_weights ( self . transformer . tokens_embed . weight )\n\n\n        Summary:", "input": "", "output": "Update input and output embeddings with new embedding matrice Make sure we are sharing the embeddings"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def step ( self , closure = None ) : loss = None if closure is not None : loss = closure ( ) for group in self . param_groups : for p in group [ 'params' ] : if p . grad is None : continue grad = p . grad . data if grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) state = self . state [ p ] # State initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p . data ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p . data ) exp_avg , exp_avg_sq = state [ 'exp_avg' ] , state [ 'exp_avg_sq' ] beta1 , beta2 = group [ 'b1' ] , group [ 'b2' ] state [ 'step' ] += 1 # Add grad clipping if group [ 'max_grad_norm' ] > 0 : clip_grad_norm_ ( p , group [ 'max_grad_norm' ] ) # Decay the first and second moment running average coefficient exp_avg . mul_ ( beta1 ) . add_ ( 1 - beta1 , grad ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( 1 - beta2 , grad , grad ) denom = exp_avg_sq . sqrt ( ) . add_ ( group [ 'e' ] ) bias_correction1 = 1 - beta1 ** state [ 'step' ] bias_correction2 = 1 - beta2 ** state [ 'step' ] lr_scheduled = group [ 'lr' ] lr_scheduled *= group [ 'schedule' ] . get_lr ( state [ 'step' ] ) step_size = lr_scheduled * math . sqrt ( bias_correction2 ) / bias_correction1 p . data . addcdiv_ ( - step_size , exp_avg , denom ) # Add weight decay at the end (fixed version) if ( len ( p . size ( ) ) > 1 or group [ 'vector_l2' ] ) and group [ 'weight_decay' ] > 0 : p . data . add_ ( - lr_scheduled * group [ 'weight_decay' ] , p . data ) return loss\n\n\n        Summary:", "input": "", "output": "Performs a single optimization step ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_lr ( self , step , nowarn = False ) : if self . t_total < 0 : return 1. progress = float ( step ) / self . t_total ret = self . get_lr_ ( progress ) # warning for exceeding t_total (only active with warmup_linear if not nowarn and self . warn_t_total and progress > 1. and progress > self . warned_for_t_total_at_progress : logger . warning ( \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\" . format ( ret , self . __class__ . __name__ ) ) self . warned_for_t_total_at_progress = progress # end warning return ret\n\n\n        Summary:", "input": "", "output": ": param step : which of t_total steps we re on : param nowarn : set to True to suppress warning regarding training beyond specified t_total steps : return : learning rate multiplier for current update"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def step ( self , closure = None ) : loss = None if closure is not None : loss = closure ( ) for group in self . param_groups : for p in group [ 'params' ] : if p . grad is None : continue grad = p . grad . data if grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) state = self . state [ p ] # State initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'next_m' ] = torch . zeros_like ( p . data ) # Exponential moving average of squared gradient values state [ 'next_v' ] = torch . zeros_like ( p . data ) next_m , next_v = state [ 'next_m' ] , state [ 'next_v' ] beta1 , beta2 = group [ 'b1' ] , group [ 'b2' ] # Add grad clipping if group [ 'max_grad_norm' ] > 0 : clip_grad_norm_ ( p , group [ 'max_grad_norm' ] ) # Decay the first and second moment running average coefficient # In-place operations to update the averages at the same time next_m . mul_ ( beta1 ) . add_ ( 1 - beta1 , grad ) next_v . mul_ ( beta2 ) . addcmul_ ( 1 - beta2 , grad , grad ) update = next_m / ( next_v . sqrt ( ) + group [ 'e' ] ) # Just adding the square of the weights to the loss function is *not* # the correct way of using L2 regularization/weight decay with Adam, # since that will interact with the m and v parameters in strange ways. # # Instead we want to decay the weights in a manner that doesn't interact # with the m/v parameters. This is equivalent to adding the square # of the weights to the loss with plain (non-momentum) SGD. if group [ 'weight_decay' ] > 0.0 : update += group [ 'weight_decay' ] * p . data lr_scheduled = group [ 'lr' ] lr_scheduled *= group [ 'schedule' ] . get_lr ( state [ 'step' ] ) update_with_lr = lr_scheduled * update p . data . add_ ( - update_with_lr ) state [ 'step' ] += 1 # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1 # No bias correction # bias_correction1 = 1 - beta1 ** state['step'] # bias_correction2 = 1 - beta2 ** state['step'] return loss\n\n\n        Summary:", "input": "", "output": "Performs a single optimization step ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def whitespace_tokenize ( text ) : text = text . strip ( ) if not text : return [ ] tokens = text . split ( ) return tokens\n\n\n        Summary:", "input": "", "output": "Runs basic whitespace cleaning and splitting on a piece of text ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_punctuation ( char ) : cp = ord ( char ) # We treat all non-letter/number ASCII as punctuation. # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode # Punctuation class but we treat them as punctuation anyways, for # consistency. if ( ( cp >= 33 and cp <= 47 ) or ( cp >= 58 and cp <= 64 ) or ( cp >= 91 and cp <= 96 ) or ( cp >= 123 and cp <= 126 ) ) : return True cat = unicodedata . category ( char ) if cat . startswith ( \"P\" ) : return True return False\n\n\n        Summary:", "input": "", "output": "Checks whether chars is a punctuation character ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_tokens_to_ids ( self , tokens ) : ids = [ ] for token in tokens : ids . append ( self . vocab [ token ] ) if len ( ids ) > self . max_len : logger . warning ( \"Token indices sequence length is longer than the specified maximum \" \" sequence length for this BERT model ({} > {}). Running this\" \" sequence through BERT will result in indexing errors\" . format ( len ( ids ) , self . max_len ) ) return ids\n\n\n        Summary:", "input": "", "output": "Converts a sequence of tokens into ids using the vocab ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_ids_to_tokens ( self , ids ) : tokens = [ ] for i in ids : tokens . append ( self . ids_to_tokens [ i ] ) return tokens\n\n\n        Summary:", "input": "", "output": "Converts a sequence of ids in wordpiece tokens using the vocab ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def save_vocabulary ( self , vocab_path ) : index = 0 if os . path . isdir ( vocab_path ) : vocab_file = os . path . join ( vocab_path , VOCAB_NAME ) with open ( vocab_file , \"w\" , encoding = \"utf-8\" ) as writer : for token , token_index in sorted ( self . vocab . items ( ) , key = lambda kv : kv [ 1 ] ) : if index != token_index : logger . warning ( \"Saving vocabulary to {}: vocabulary indices are not consecutive.\" \" Please check that the vocabulary is not corrupted!\" . format ( vocab_file ) ) index = token_index writer . write ( token + u'\\n' ) index += 1 return vocab_file\n\n\n        Summary:", "input": "", "output": "Save the tokenizer vocabulary to a directory or file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_pretrained ( cls , pretrained_model_name_or_path , cache_dir = None , * inputs , * * kwargs ) : if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP : vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP [ pretrained_model_name_or_path ] if '-cased' in pretrained_model_name_or_path and kwargs . get ( 'do_lower_case' , True ) : logger . warning ( \"The pre-trained model you are loading is a cased model but you have not set \" \"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \" \"you may want to check this behavior.\" ) kwargs [ 'do_lower_case' ] = False elif '-cased' not in pretrained_model_name_or_path and not kwargs . get ( 'do_lower_case' , True ) : logger . warning ( \"The pre-trained model you are loading is an uncased model but you have set \" \"`do_lower_case` to False. We are setting `do_lower_case=True` for you \" \"but you may want to check this behavior.\" ) kwargs [ 'do_lower_case' ] = True else : vocab_file = pretrained_model_name_or_path if os . path . isdir ( vocab_file ) : vocab_file = os . path . join ( vocab_file , VOCAB_NAME ) # redirect to the cache, if necessary try : resolved_vocab_file = cached_path ( vocab_file , cache_dir = cache_dir ) except EnvironmentError : logger . error ( \"Model name '{}' was not found in model name list ({}). \" \"We assumed '{}' was a path or url but couldn't find any file \" \"associated to this path or url.\" . format ( pretrained_model_name_or_path , ', ' . join ( PRETRAINED_VOCAB_ARCHIVE_MAP . keys ( ) ) , vocab_file ) ) return None if resolved_vocab_file == vocab_file : logger . info ( \"loading vocabulary file {}\" . format ( vocab_file ) ) else : logger . info ( \"loading vocabulary file {} from cache at {}\" . format ( vocab_file , resolved_vocab_file ) ) if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP : # if we're using a pretrained model, ensure the tokenizer wont index sequences longer # than the number of positional embeddings max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP [ pretrained_model_name_or_path ] kwargs [ 'max_len' ] = min ( kwargs . get ( 'max_len' , int ( 1e12 ) ) , max_len ) # Instantiate tokenizer. tokenizer = cls ( resolved_vocab_file , * inputs , * * kwargs ) return tokenizer\n\n\n        Summary:", "input": "", "output": "Instantiate a PreTrainedBertModel from a pre - trained model file . Download and cache the pre - trained model file if needed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tokenize ( self , text ) : text = self . _clean_text ( text ) # This was added on November 1st, 2018 for the multilingual and Chinese # models. This is also applied to the English models now, but it doesn't # matter since the English models were not trained on any Chinese data # and generally don't have any Chinese data in them (there are Chinese # characters in the vocabulary because Wikipedia does have some Chinese # words in the English Wikipedia.). text = self . _tokenize_chinese_chars ( text ) orig_tokens = whitespace_tokenize ( text ) split_tokens = [ ] for token in orig_tokens : if self . do_lower_case and token not in self . never_split : token = token . lower ( ) token = self . _run_strip_accents ( token ) split_tokens . extend ( self . _run_split_on_punc ( token ) ) output_tokens = whitespace_tokenize ( \" \" . join ( split_tokens ) ) return output_tokens\n\n\n        Summary:", "input": "", "output": "Tokenizes a piece of text ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _run_strip_accents ( self , text ) : text = unicodedata . normalize ( \"NFD\" , text ) output = [ ] for char in text : cat = unicodedata . category ( char ) if cat == \"Mn\" : continue output . append ( char ) return \"\" . join ( output )\n\n\n        Summary:", "input": "", "output": "Strips accents from a piece of text ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _tokenize_chinese_chars ( self , text ) : output = [ ] for char in text : cp = ord ( char ) if self . _is_chinese_char ( cp ) : output . append ( \" \" ) output . append ( char ) output . append ( \" \" ) else : output . append ( char ) return \"\" . join ( output )\n\n\n        Summary:", "input": "", "output": "Adds whitespace around any CJK character ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_chinese_char ( self , cp ) : # This defines a \"chinese character\" as anything in the CJK Unicode block: # https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block) # # Note that the CJK Unicode block is NOT all Japanese and Korean characters, # despite its name. The modern Korean Hangul alphabet is a different block, # as is Japanese Hiragana and Katakana. Those alphabets are used to write # space-separated words, so they are not treated specially and handled # like the all of the other languages. if ( ( cp >= 0x4E00 and cp <= 0x9FFF ) or # ( cp >= 0x3400 and cp <= 0x4DBF ) or # ( cp >= 0x20000 and cp <= 0x2A6DF ) or # ( cp >= 0x2A700 and cp <= 0x2B73F ) or # ( cp >= 0x2B740 and cp <= 0x2B81F ) or # ( cp >= 0x2B820 and cp <= 0x2CEAF ) or ( cp >= 0xF900 and cp <= 0xFAFF ) or # ( cp >= 0x2F800 and cp <= 0x2FA1F ) ) : # return True return False\n\n\n        Summary:", "input": "", "output": "Checks whether CP is the codepoint of a CJK character ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tokenize ( self , text ) : output_tokens = [ ] for token in whitespace_tokenize ( text ) : chars = list ( token ) if len ( chars ) > self . max_input_chars_per_word : output_tokens . append ( self . unk_token ) continue is_bad = False start = 0 sub_tokens = [ ] while start < len ( chars ) : end = len ( chars ) cur_substr = None while start < end : substr = \"\" . join ( chars [ start : end ] ) if start > 0 : substr = \"##\" + substr if substr in self . vocab : cur_substr = substr break end -= 1 if cur_substr is None : is_bad = True break sub_tokens . append ( cur_substr ) start = end if is_bad : output_tokens . append ( self . unk_token ) else : output_tokens . extend ( sub_tokens ) return output_tokens\n\n\n        Summary:", "input": "", "output": "Tokenizes a piece of text into its word pieces ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_rocstories_dataset ( dataset_path ) : with open ( dataset_path , encoding = 'utf_8' ) as f : f = csv . reader ( f ) output = [ ] next ( f ) # skip the first line for line in tqdm ( f ) : output . append ( ( ' ' . join ( line [ 1 : 5 ] ) , line [ 5 ] , line [ 6 ] , int ( line [ - 1 ] ) - 1 ) ) return output\n\n\n        Summary:", "input": "", "output": "Output a list of tuples ( story 1st continuation 2nd continuation label )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pre_process_datasets ( encoded_datasets , input_len , cap_length , start_token , delimiter_token , clf_token ) : tensor_datasets = [ ] for dataset in encoded_datasets : n_batch = len ( dataset ) input_ids = np . zeros ( ( n_batch , 2 , input_len ) , dtype = np . int64 ) mc_token_ids = np . zeros ( ( n_batch , 2 ) , dtype = np . int64 ) lm_labels = np . full ( ( n_batch , 2 , input_len ) , fill_value = - 1 , dtype = np . int64 ) mc_labels = np . zeros ( ( n_batch , ) , dtype = np . int64 ) for i , ( story , cont1 , cont2 , mc_label ) , in enumerate ( dataset ) : with_cont1 = [ start_token ] + story [ : cap_length ] + [ delimiter_token ] + cont1 [ : cap_length ] + [ clf_token ] with_cont2 = [ start_token ] + story [ : cap_length ] + [ delimiter_token ] + cont2 [ : cap_length ] + [ clf_token ] input_ids [ i , 0 , : len ( with_cont1 ) ] = with_cont1 input_ids [ i , 1 , : len ( with_cont2 ) ] = with_cont2 mc_token_ids [ i , 0 ] = len ( with_cont1 ) - 1 mc_token_ids [ i , 1 ] = len ( with_cont2 ) - 1 lm_labels [ i , 0 , : len ( with_cont1 ) - 1 ] = with_cont1 [ 1 : ] lm_labels [ i , 1 , : len ( with_cont2 ) - 1 ] = with_cont2 [ 1 : ] mc_labels [ i ] = mc_label all_inputs = ( input_ids , mc_token_ids , lm_labels , mc_labels ) tensor_datasets . append ( tuple ( torch . tensor ( t ) for t in all_inputs ) ) return tensor_datasets\n\n\n        Summary:", "input": "", "output": "Pre - process datasets containing lists of tuples ( story 1st continuation 2nd continuation label )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def random_word ( tokens , tokenizer ) : output_label = [ ] for i , token in enumerate ( tokens ) : prob = random . random ( ) # mask token with 15% probability if prob < 0.15 : prob /= 0.15 # 80% randomly change token to mask token if prob < 0.8 : tokens [ i ] = \"[MASK]\" # 10% randomly change token to random token elif prob < 0.9 : tokens [ i ] = random . choice ( list ( tokenizer . vocab . items ( ) ) ) [ 0 ] # -> rest 10% randomly keep current token # append current token to output (we will predict these later) try : output_label . append ( tokenizer . vocab [ token ] ) except KeyError : # For unknown words (should not occur with BPE vocab) output_label . append ( tokenizer . vocab [ \"[UNK]\" ] ) logger . warning ( \"Cannot find token '{}' in vocab. Using [UNK] insetad\" . format ( token ) ) else : # no masking token (will be ignored by loss function later) output_label . append ( - 1 ) return tokens , output_label\n\n\n        Summary:", "input": "", "output": "Masking some random tokens for Language Model task with probabilities as in the original BERT paper . : param tokens : list of str tokenized sentence . : param tokenizer : Tokenizer object used for tokenization ( we need it s vocab here ) : return : ( list of str list of int ) masked tokens and related labels for LM prediction"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_example_to_features ( example , max_seq_length , tokenizer ) : tokens_a = example . tokens_a tokens_b = example . tokens_b # Modifies `tokens_a` and `tokens_b` in place so that the total # length is less than the specified length. # Account for [CLS], [SEP], [SEP] with \"- 3\" _truncate_seq_pair ( tokens_a , tokens_b , max_seq_length - 3 ) tokens_a , t1_label = random_word ( tokens_a , tokenizer ) tokens_b , t2_label = random_word ( tokens_b , tokenizer ) # concatenate lm labels and account for CLS, SEP, SEP lm_label_ids = ( [ - 1 ] + t1_label + [ - 1 ] + t2_label + [ - 1 ] ) # The convention in BERT is: # (a) For sequence pairs: # tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP] # type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 # (b) For single sequences: # tokens: [CLS] the dog is hairy . [SEP] # type_ids: 0 0 0 0 0 0 0 # # Where \"type_ids\" are used to indicate whether this is the first # sequence or the second sequence. The embedding vectors for `type=0` and # `type=1` were learned during pre-training and are added to the wordpiece # embedding vector (and position vector). This is not *strictly* necessary # since the [SEP] token unambigiously separates the sequences, but it makes # it easier for the model to learn the concept of sequences. # # For classification tasks, the first vector (corresponding to [CLS]) is # used as as the \"sentence vector\". Note that this only makes sense because # the entire model is fine-tuned. tokens = [ ] segment_ids = [ ] tokens . append ( \"[CLS]\" ) segment_ids . append ( 0 ) for token in tokens_a : tokens . append ( token ) segment_ids . append ( 0 ) tokens . append ( \"[SEP]\" ) segment_ids . append ( 0 ) assert len ( tokens_b ) > 0 for token in tokens_b : tokens . append ( token ) segment_ids . append ( 1 ) tokens . append ( \"[SEP]\" ) segment_ids . append ( 1 ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) # The mask has 1 for real tokens and 0 for padding tokens. Only real # tokens are attended to. input_mask = [ 1 ] * len ( input_ids ) # Zero-pad up to the sequence length. while len ( input_ids ) < max_seq_length : input_ids . append ( 0 ) input_mask . append ( 0 ) segment_ids . append ( 0 ) lm_label_ids . append ( - 1 ) assert len ( input_ids ) == max_seq_length assert len ( input_mask ) == max_seq_length assert len ( segment_ids ) == max_seq_length assert len ( lm_label_ids ) == max_seq_length if example . guid < 5 : logger . info ( \"*** Example ***\" ) logger . info ( \"guid: %s\" % ( example . guid ) ) logger . info ( \"tokens: %s\" % \" \" . join ( [ str ( x ) for x in tokens ] ) ) logger . info ( \"input_ids: %s\" % \" \" . join ( [ str ( x ) for x in input_ids ] ) ) logger . info ( \"input_mask: %s\" % \" \" . join ( [ str ( x ) for x in input_mask ] ) ) logger . info ( \"segment_ids: %s\" % \" \" . join ( [ str ( x ) for x in segment_ids ] ) ) logger . info ( \"LM label: %s \" % ( lm_label_ids ) ) logger . info ( \"Is next sentence label: %s \" % ( example . is_next ) ) features = InputFeatures ( input_ids = input_ids , input_mask = input_mask , segment_ids = segment_ids , lm_label_ids = lm_label_ids , is_next = example . is_next ) return features\n\n\n        Summary:", "input": "", "output": "Convert a raw sample ( pair of sentences as tokenized strings ) into a proper training sample with IDs LM labels input_mask CLS and SEP tokens etc . : param example : InputExample containing sentence input as strings and is_next label : param max_seq_length : int maximum length of sequence . : param tokenizer : Tokenizer : return : InputFeatures containing all inputs and labels of one sample as IDs ( as used for model training )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def random_sent ( self , index ) : t1 , t2 = self . get_corpus_line ( index ) if random . random ( ) > 0.5 : label = 0 else : t2 = self . get_random_line ( ) label = 1 assert len ( t1 ) > 0 assert len ( t2 ) > 0 return t1 , t2 , label\n\n\n        Summary:", "input": "", "output": "Get one sample from corpus consisting of two sentences . With prob . 50% these are two subsequent sentences from one doc . With 50% the second sentence will be a random one from another doc . : param index : int index of sample . : return : ( str str int ) sentence 1 sentence 2 isNextSentence Label"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_corpus_line ( self , item ) : t1 = \"\" t2 = \"\" assert item < self . corpus_lines if self . on_memory : sample = self . sample_to_doc [ item ] t1 = self . all_docs [ sample [ \"doc_id\" ] ] [ sample [ \"line\" ] ] t2 = self . all_docs [ sample [ \"doc_id\" ] ] [ sample [ \"line\" ] + 1 ] # used later to avoid random nextSentence from same doc self . current_doc = sample [ \"doc_id\" ] return t1 , t2 else : if self . line_buffer is None : # read first non-empty line of file while t1 == \"\" : t1 = next ( self . file ) . strip ( ) t2 = next ( self . file ) . strip ( ) else : # use t2 from previous iteration as new t1 t1 = self . line_buffer t2 = next ( self . file ) . strip ( ) # skip empty rows that are used for separating documents and keep track of current doc id while t2 == \"\" or t1 == \"\" : t1 = next ( self . file ) . strip ( ) t2 = next ( self . file ) . strip ( ) self . current_doc = self . current_doc + 1 self . line_buffer = t2 assert t1 != \"\" assert t2 != \"\" return t1 , t2\n\n\n        Summary:", "input": "", "output": "Get one sample from corpus consisting of a pair of two subsequent lines from the same doc . : param item : int index of sample . : return : ( str str ) two subsequent sentences from corpus"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_random_line ( self ) : # Similar to original tf repo: This outer loop should rarely go for more than one iteration for large # corpora. However, just to be careful, we try to make sure that # the random document is not the same as the document we're processing. for _ in range ( 10 ) : if self . on_memory : rand_doc_idx = random . randint ( 0 , len ( self . all_docs ) - 1 ) rand_doc = self . all_docs [ rand_doc_idx ] line = rand_doc [ random . randrange ( len ( rand_doc ) ) ] else : rand_index = random . randint ( 1 , self . corpus_lines if self . corpus_lines < 1000 else 1000 ) #pick random line for _ in range ( rand_index ) : line = self . get_next_line ( ) #check if our picked random line is really from another doc like we want it to be if self . current_random_doc != self . current_doc : break return line\n\n\n        Summary:", "input": "", "output": "Get random line from another document for nextSentence task . : return : str content of one line"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_next_line ( self ) : try : line = next ( self . random_file ) . strip ( ) #keep track of which document we are currently looking at to later avoid having the same doc as t1 if line == \"\" : self . current_random_doc = self . current_random_doc + 1 line = next ( self . random_file ) . strip ( ) except StopIteration : self . random_file . close ( ) self . random_file = open ( self . corpus_path , \"r\" , encoding = self . encoding ) line = next ( self . random_file ) . strip ( ) return line\n\n\n        Summary:", "input": "", "output": "Gets next line of random_file and starts over when reaching end of file"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_masked_lm_predictions ( tokens , masked_lm_prob , max_predictions_per_seq , vocab_list ) : cand_indices = [ ] for ( i , token ) in enumerate ( tokens ) : if token == \"[CLS]\" or token == \"[SEP]\" : continue cand_indices . append ( i ) num_to_mask = min ( max_predictions_per_seq , max ( 1 , int ( round ( len ( tokens ) * masked_lm_prob ) ) ) ) shuffle ( cand_indices ) mask_indices = sorted ( sample ( cand_indices , num_to_mask ) ) masked_token_labels = [ ] for index in mask_indices : # 80% of the time, replace with [MASK] if random ( ) < 0.8 : masked_token = \"[MASK]\" else : # 10% of the time, keep original if random ( ) < 0.5 : masked_token = tokens [ index ] # 10% of the time, replace with random word else : masked_token = choice ( vocab_list ) masked_token_labels . append ( tokens [ index ] ) # Once we've saved the true label for that token, we can overwrite it with the masked version tokens [ index ] = masked_token return tokens , mask_indices , masked_token_labels\n\n\n        Summary:", "input": "", "output": "Creates the predictions for the masked LM objective . This is mostly copied from the Google BERT repo but with several refactors to clean it up and remove a lot of unnecessary variables ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_instances_from_document ( doc_database , doc_idx , max_seq_length , short_seq_prob , masked_lm_prob , max_predictions_per_seq , vocab_list ) : document = doc_database [ doc_idx ] # Account for [CLS], [SEP], [SEP] max_num_tokens = max_seq_length - 3 # We *usually* want to fill up the entire sequence since we are padding # to `max_seq_length` anyways, so short sequences are generally wasted # computation. However, we *sometimes* # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter # sequences to minimize the mismatch between pre-training and fine-tuning. # The `target_seq_length` is just a rough target however, whereas # `max_seq_length` is a hard limit. target_seq_length = max_num_tokens if random ( ) < short_seq_prob : target_seq_length = randint ( 2 , max_num_tokens ) # We DON'T just concatenate all of the tokens from a document into a long # sequence and choose an arbitrary split point because this would make the # next sentence prediction task too easy. Instead, we split the input into # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user # input. instances = [ ] current_chunk = [ ] current_length = 0 i = 0 while i < len ( document ) : segment = document [ i ] current_chunk . append ( segment ) current_length += len ( segment ) if i == len ( document ) - 1 or current_length >= target_seq_length : if current_chunk : # `a_end` is how many segments from `current_chunk` go into the `A` # (first) sentence. a_end = 1 if len ( current_chunk ) >= 2 : a_end = randrange ( 1 , len ( current_chunk ) ) tokens_a = [ ] for j in range ( a_end ) : tokens_a . extend ( current_chunk [ j ] ) tokens_b = [ ] # Random next if len ( current_chunk ) == 1 or random ( ) < 0.5 : is_random_next = True target_b_length = target_seq_length - len ( tokens_a ) # Sample a random document, with longer docs being sampled more frequently random_document = doc_database . sample_doc ( current_idx = doc_idx , sentence_weighted = True ) random_start = randrange ( 0 , len ( random_document ) ) for j in range ( random_start , len ( random_document ) ) : tokens_b . extend ( random_document [ j ] ) if len ( tokens_b ) >= target_b_length : break # We didn't actually use these segments so we \"put them back\" so # they don't go to waste. num_unused_segments = len ( current_chunk ) - a_end i -= num_unused_segments # Actual next else : is_random_next = False for j in range ( a_end , len ( current_chunk ) ) : tokens_b . extend ( current_chunk [ j ] ) truncate_seq_pair ( tokens_a , tokens_b , max_num_tokens ) assert len ( tokens_a ) >= 1 assert len ( tokens_b ) >= 1 tokens = [ \"[CLS]\" ] + tokens_a + [ \"[SEP]\" ] + tokens_b + [ \"[SEP]\" ] # The segment IDs are 0 for the [CLS] token, the A tokens and the first [SEP] # They are 1 for the B tokens and the final [SEP] segment_ids = [ 0 for _ in range ( len ( tokens_a ) + 2 ) ] + [ 1 for _ in range ( len ( tokens_b ) + 1 ) ] tokens , masked_lm_positions , masked_lm_labels = create_masked_lm_predictions ( tokens , masked_lm_prob , max_predictions_per_seq , vocab_list ) instance = { \"tokens\" : tokens , \"segment_ids\" : segment_ids , \"is_random_next\" : is_random_next , \"masked_lm_positions\" : masked_lm_positions , \"masked_lm_labels\" : masked_lm_labels } instances . append ( instance ) current_chunk = [ ] current_length = 0 i += 1 return instances\n\n\n        Summary:", "input": "", "output": "This code is mostly a duplicate of the equivalent function from Google BERT s repo . However we make some changes and improvements . Sampling is improved and no longer requires a loop in this function . Also documents are sampled proportionally to the number of sentences they contain which means each sentence ( rather than each document ) has an equal chance of being sampled as a false example for the NextSentence task ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sample_logits ( embedding , bias , labels , inputs , sampler ) : true_log_probs , samp_log_probs , neg_samples = sampler . sample ( labels ) n_sample = neg_samples . size ( 0 ) b1 , b2 = labels . size ( 0 ) , labels . size ( 1 ) all_ids = torch . cat ( [ labels . view ( - 1 ) , neg_samples ] ) all_w = embedding ( all_ids ) true_w = all_w [ : - n_sample ] . view ( b1 , b2 , - 1 ) sample_w = all_w [ - n_sample : ] . view ( n_sample , - 1 ) all_b = bias [ all_ids ] true_b = all_b [ : - n_sample ] . view ( b1 , b2 ) sample_b = all_b [ - n_sample : ] hit = ( labels [ : , : , None ] == neg_samples ) . detach ( ) true_logits = torch . einsum ( 'ijk,ijk->ij' , [ true_w , inputs ] ) + true_b - true_log_probs sample_logits = torch . einsum ( 'lk,ijk->ijl' , [ sample_w , inputs ] ) + sample_b - samp_log_probs sample_logits . masked_fill_ ( hit , - 1e30 ) logits = torch . cat ( [ true_logits [ : , : , None ] , sample_logits ] , - 1 ) return logits\n\n\n        Summary:", "input": "", "output": "embedding : an nn . Embedding layer bias : [ n_vocab ] labels : [ b1 b2 ] inputs : [ b1 b2 n_emb ] sampler : you may use a LogUniformSampler Return logits : [ b1 b2 1 + n_sample ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def forward ( self , hidden , target = None , keep_order = False ) : if target is not None : target = target . view ( - 1 ) if hidden . size ( 0 ) != target . size ( 0 ) : raise RuntimeError ( 'Input and target should have the same size ' 'in the batch dimension.' ) if self . n_clusters == 0 : logit = self . _compute_logit ( hidden , self . out_layers [ 0 ] . weight , self . out_layers [ 0 ] . bias , self . out_projs [ 0 ] ) if target is not None : output = - F . log_softmax ( logit , dim = - 1 ) . gather ( 1 , target . unsqueeze ( 1 ) ) . squeeze ( 1 ) else : output = F . log_softmax ( logit , dim = - 1 ) else : # construct weights and biases weights , biases = [ ] , [ ] for i in range ( len ( self . cutoffs ) ) : if self . div_val == 1 : l_idx , r_idx = self . cutoff_ends [ i ] , self . cutoff_ends [ i + 1 ] weight_i = self . out_layers [ 0 ] . weight [ l_idx : r_idx ] bias_i = self . out_layers [ 0 ] . bias [ l_idx : r_idx ] else : weight_i = self . out_layers [ i ] . weight bias_i = self . out_layers [ i ] . bias if i == 0 : weight_i = torch . cat ( [ weight_i , self . cluster_weight ] , dim = 0 ) bias_i = torch . cat ( [ bias_i , self . cluster_bias ] , dim = 0 ) weights . append ( weight_i ) biases . append ( bias_i ) head_weight , head_bias , head_proj = weights [ 0 ] , biases [ 0 ] , self . out_projs [ 0 ] head_logit = self . _compute_logit ( hidden , head_weight , head_bias , head_proj ) head_logprob = F . log_softmax ( head_logit , dim = 1 ) if target is None : out = hidden . new_empty ( ( head_logit . size ( 0 ) , self . n_token ) ) else : out = torch . zeros_like ( target , dtype = hidden . dtype , device = hidden . device ) offset = 0 cutoff_values = [ 0 ] + self . cutoffs for i in range ( len ( cutoff_values ) - 1 ) : l_idx , r_idx = cutoff_values [ i ] , cutoff_values [ i + 1 ] if target is not None : mask_i = ( target >= l_idx ) & ( target < r_idx ) indices_i = mask_i . nonzero ( ) . squeeze ( ) if indices_i . numel ( ) == 0 : continue target_i = target . index_select ( 0 , indices_i ) - l_idx head_logprob_i = head_logprob . index_select ( 0 , indices_i ) hidden_i = hidden . index_select ( 0 , indices_i ) else : hidden_i = hidden if i == 0 : if target is not None : logprob_i = head_logprob_i . gather ( 1 , target_i [ : , None ] ) . squeeze ( 1 ) else : out [ : , : self . cutoffs [ 0 ] ] = head_logprob [ : , : self . cutoffs [ 0 ] ] else : weight_i , bias_i , proj_i = weights [ i ] , biases [ i ] , self . out_projs [ i ] tail_logit_i = self . _compute_logit ( hidden_i , weight_i , bias_i , proj_i ) tail_logprob_i = F . log_softmax ( tail_logit_i , dim = 1 ) cluster_prob_idx = self . cutoffs [ 0 ] + i - 1 # No probability for the head cluster if target is not None : logprob_i = head_logprob_i [ : , cluster_prob_idx ] + tail_logprob_i . gather ( 1 , target_i [ : , None ] ) . squeeze ( 1 ) else : logprob_i = head_logprob [ : , cluster_prob_idx , None ] + tail_logprob_i out [ : , l_idx : r_idx ] = logprob_i if target is not None : if ( hasattr ( self , 'keep_order' ) and self . keep_order ) or keep_order : out . index_copy_ ( 0 , indices_i , - logprob_i ) else : out [ offset : offset + logprob_i . size ( 0 ) ] . copy_ ( - logprob_i ) offset += logprob_i . size ( 0 ) return out\n\n\n        Summary:", "input": "", "output": "Params : hidden :: [ len * bsz x d_proj ] target :: [ len * bsz ] Return : if target is None : out :: [ len * bsz ] Negative log likelihood else : out :: [ len * bsz x n_tokens ] log probabilities of tokens over the vocabulary We could replace this implementation by the native PyTorch one if their s had an option to set bias on all clusters in the native one . here : https : // github . com / pytorch / pytorch / blob / dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da / torch / nn / modules / adaptive . py#L138"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def log_prob ( self , hidden ) : if self . n_clusters == 0 : logit = self . _compute_logit ( hidden , self . out_layers [ 0 ] . weight , self . out_layers [ 0 ] . bias , self . out_projs [ 0 ] ) return F . log_softmax ( logit , dim = - 1 ) else : # construct weights and biases weights , biases = [ ] , [ ] for i in range ( len ( self . cutoffs ) ) : if self . div_val == 1 : l_idx , r_idx = self . cutoff_ends [ i ] , self . cutoff_ends [ i + 1 ] weight_i = self . out_layers [ 0 ] . weight [ l_idx : r_idx ] bias_i = self . out_layers [ 0 ] . bias [ l_idx : r_idx ] else : weight_i = self . out_layers [ i ] . weight bias_i = self . out_layers [ i ] . bias if i == 0 : weight_i = torch . cat ( [ weight_i , self . cluster_weight ] , dim = 0 ) bias_i = torch . cat ( [ bias_i , self . cluster_bias ] , dim = 0 ) weights . append ( weight_i ) biases . append ( bias_i ) head_weight , head_bias , head_proj = weights [ 0 ] , biases [ 0 ] , self . out_projs [ 0 ] head_logit = self . _compute_logit ( hidden , head_weight , head_bias , head_proj ) out = hidden . new_empty ( ( head_logit . size ( 0 ) , self . n_token ) ) head_logprob = F . log_softmax ( head_logit , dim = 1 ) cutoff_values = [ 0 ] + self . cutoffs for i in range ( len ( cutoff_values ) - 1 ) : start_idx , stop_idx = cutoff_values [ i ] , cutoff_values [ i + 1 ] if i == 0 : out [ : , : self . cutoffs [ 0 ] ] = head_logprob [ : , : self . cutoffs [ 0 ] ] else : weight_i , bias_i , proj_i = weights [ i ] , biases [ i ] , self . out_projs [ i ] tail_logit_i = self . _compute_logit ( hidden , weight_i , bias_i , proj_i ) tail_logprob_i = F . log_softmax ( tail_logit_i , dim = 1 ) logprob_i = head_logprob [ : , - i ] + tail_logprob_i out [ : , start_idx , stop_idx ] = logprob_i return out\n\n\n        Summary:", "input": "", "output": "r Computes log probabilities for all : math : n \\ _classes From : https : // github . com / pytorch / pytorch / blob / master / torch / nn / modules / adaptive . py Args : hidden ( Tensor ) : a minibatch of examples Returns : log - probabilities of for each class : math : c in range : math : 0 < = c < = n \\ _classes where : math : n \\ _classes is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor . Shape : - Input : : math : ( N in \\ _features ) - Output : : math : ( N n \\ _classes )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sample ( self , labels ) : # neg_samples = torch.empty(0).long() n_sample = self . n_sample n_tries = 2 * n_sample with torch . no_grad ( ) : neg_samples = torch . multinomial ( self . dist , n_tries , replacement = True ) . unique ( ) device = labels . device neg_samples = neg_samples . to ( device ) true_log_probs = self . log_q [ labels ] . to ( device ) samp_log_probs = self . log_q [ neg_samples ] . to ( device ) return true_log_probs , samp_log_probs , neg_samples\n\n\n        Summary:", "input": "", "output": "labels : [ b1 b2 ] Return true_log_probs : [ b1 b2 ] samp_log_probs : [ n_sample ] neg_samples : [ n_sample ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def build_tf_to_pytorch_map ( model , config ) : tf_to_pt_map = { } if hasattr ( model , 'transformer' ) : # We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax tf_to_pt_map . update ( { \"transformer/adaptive_softmax/cutoff_0/cluster_W\" : model . crit . cluster_weight , \"transformer/adaptive_softmax/cutoff_0/cluster_b\" : model . crit . cluster_bias } ) for i , ( out_l , proj_l , tie_proj ) in enumerate ( zip ( model . crit . out_layers , model . crit . out_projs , config . tie_projs ) ) : layer_str = \"transformer/adaptive_softmax/cutoff_%d/\" % i if config . tie_weight : tf_to_pt_map . update ( { layer_str + 'b' : out_l . bias } ) else : raise NotImplementedError # I don't think this is implemented in the TF code tf_to_pt_map . update ( { layer_str + 'lookup_table' : out_l . weight , layer_str + 'b' : out_l . bias } ) if not tie_proj : tf_to_pt_map . update ( { layer_str + 'proj' : proj_l } ) # Now load the rest of the transformer model = model . transformer # Embeddings for i , ( embed_l , proj_l ) in enumerate ( zip ( model . word_emb . emb_layers , model . word_emb . emb_projs ) ) : layer_str = \"transformer/adaptive_embed/cutoff_%d/\" % i tf_to_pt_map . update ( { layer_str + 'lookup_table' : embed_l . weight , layer_str + 'proj_W' : proj_l } ) # Transformer blocks for i , b in enumerate ( model . layers ) : layer_str = \"transformer/layer_%d/\" % i tf_to_pt_map . update ( { layer_str + \"rel_attn/LayerNorm/gamma\" : b . dec_attn . layer_norm . weight , layer_str + \"rel_attn/LayerNorm/beta\" : b . dec_attn . layer_norm . bias , layer_str + \"rel_attn/o/kernel\" : b . dec_attn . o_net . weight , layer_str + \"rel_attn/qkv/kernel\" : b . dec_attn . qkv_net . weight , layer_str + \"rel_attn/r/kernel\" : b . dec_attn . r_net . weight , layer_str + \"ff/LayerNorm/gamma\" : b . pos_ff . layer_norm . weight , layer_str + \"ff/LayerNorm/beta\" : b . pos_ff . layer_norm . bias , layer_str + \"ff/layer_1/kernel\" : b . pos_ff . CoreNet [ 0 ] . weight , layer_str + \"ff/layer_1/bias\" : b . pos_ff . CoreNet [ 0 ] . bias , layer_str + \"ff/layer_2/kernel\" : b . pos_ff . CoreNet [ 3 ] . weight , layer_str + \"ff/layer_2/bias\" : b . pos_ff . CoreNet [ 3 ] . bias , } ) # Relative positioning biases if config . untie_r : r_r_list = [ ] r_w_list = [ ] for b in model . layers : r_r_list . append ( b . dec_attn . r_r_bias ) r_w_list . append ( b . dec_attn . r_w_bias ) else : r_r_list = [ model . r_r_bias ] r_w_list = [ model . r_w_bias ] tf_to_pt_map . update ( { 'transformer/r_r_bias' : r_r_list , 'transformer/r_w_bias' : r_w_list } ) return tf_to_pt_map\n\n\n        Summary:", "input": "", "output": "A map of modules from TF to PyTorch . This time I use a map to keep the PyTorch model as identical to the original PyTorch model as possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load_tf_weights_in_transfo_xl ( model , config , tf_path ) : try : import numpy as np import tensorflow as tf except ImportError : print ( \"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \" \"https://www.tensorflow.org/install/ for installation instructions.\" ) raise # Build TF to PyTorch weights loading map tf_to_pt_map = build_tf_to_pytorch_map ( model , config ) # Load weights from TF model init_vars = tf . train . list_variables ( tf_path ) tf_weights = { } for name , shape in init_vars : print ( \"Loading TF weight {} with shape {}\" . format ( name , shape ) ) array = tf . train . load_variable ( tf_path , name ) tf_weights [ name ] = array for name , pointer in tf_to_pt_map . items ( ) : assert name in tf_weights array = tf_weights [ name ] # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v # which are not required for using pretrained model if 'kernel' in name or 'proj' in name : array = np . transpose ( array ) if ( 'r_r_bias' in name or 'r_w_bias' in name ) and len ( pointer ) > 1 : # Here we will split the TF weigths assert len ( pointer ) == array . shape [ 0 ] for i , p_i in enumerate ( pointer ) : arr_i = array [ i , ... ] try : assert p_i . shape == arr_i . shape except AssertionError as e : e . args += ( p_i . shape , arr_i . shape ) raise print ( \"Initialize PyTorch weight {} for layer {}\" . format ( name , i ) ) p_i . data = torch . from_numpy ( arr_i ) else : try : assert pointer . shape == array . shape except AssertionError as e : e . args += ( pointer . shape , array . shape ) raise print ( \"Initialize PyTorch weight {}\" . format ( name ) ) pointer . data = torch . from_numpy ( array ) tf_weights . pop ( name , None ) tf_weights . pop ( name + '/Adam' , None ) tf_weights . pop ( name + '/Adam_1' , None ) print ( \"Weights not copied to PyTorch model: {}\" . format ( ', ' . join ( tf_weights . keys ( ) ) ) ) return model\n\n\n        Summary:", "input": "", "output": "Load tf checkpoints in a pytorch model"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def init_weights ( self , m ) : classname = m . __class__ . __name__ if classname . find ( 'Linear' ) != - 1 : if hasattr ( m , 'weight' ) and m . weight is not None : self . init_weight ( m . weight ) if hasattr ( m , 'bias' ) and m . bias is not None : self . init_bias ( m . bias ) elif classname . find ( 'AdaptiveEmbedding' ) != - 1 : if hasattr ( m , 'emb_projs' ) : for i in range ( len ( m . emb_projs ) ) : if m . emb_projs [ i ] is not None : nn . init . normal_ ( m . emb_projs [ i ] , 0.0 , self . config . proj_init_std ) elif classname . find ( 'Embedding' ) != - 1 : if hasattr ( m , 'weight' ) : self . init_weight ( m . weight ) elif classname . find ( 'ProjectedAdaptiveLogSoftmax' ) != - 1 : if hasattr ( m , 'cluster_weight' ) and m . cluster_weight is not None : self . init_weight ( m . cluster_weight ) if hasattr ( m , 'cluster_bias' ) and m . cluster_bias is not None : self . init_bias ( m . cluster_bias ) if hasattr ( m , 'out_projs' ) : for i in range ( len ( m . out_projs ) ) : if m . out_projs [ i ] is not None : nn . init . normal_ ( m . out_projs [ i ] , 0.0 , self . config . proj_init_std ) elif classname . find ( 'LayerNorm' ) != - 1 : if hasattr ( m , 'weight' ) : nn . init . normal_ ( m . weight , 1.0 , self . config . init_std ) if hasattr ( m , 'bias' ) and m . bias is not None : self . init_bias ( m . bias ) elif classname . find ( 'TransformerLM' ) != - 1 : if hasattr ( m , 'r_emb' ) : self . init_weight ( m . r_emb ) if hasattr ( m , 'r_w_bias' ) : self . init_weight ( m . r_w_bias ) if hasattr ( m , 'r_r_bias' ) : self . init_weight ( m . r_r_bias ) if hasattr ( m , 'r_bias' ) : self . init_bias ( m . r_bias )\n\n\n        Summary:", "input": "", "output": "Initialize the weights ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_pretrained ( cls , pretrained_model_name_or_path , state_dict = None , cache_dir = None , from_tf = False , * inputs , * * kwargs ) : if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP : archive_file = PRETRAINED_MODEL_ARCHIVE_MAP [ pretrained_model_name_or_path ] config_file = PRETRAINED_CONFIG_ARCHIVE_MAP [ pretrained_model_name_or_path ] else : archive_file = os . path . join ( pretrained_model_name_or_path , WEIGHTS_NAME ) config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) # redirect to the cache, if necessary try : resolved_archive_file = cached_path ( archive_file , cache_dir = cache_dir ) resolved_config_file = cached_path ( config_file , cache_dir = cache_dir ) except EnvironmentError : logger . error ( \"Model name '{}' was not found in model name list ({}). \" \"We assumed '{}' was a path or url but couldn't find files {} and {} \" \"at this path or url.\" . format ( pretrained_model_name_or_path , ', ' . join ( PRETRAINED_MODEL_ARCHIVE_MAP . keys ( ) ) , pretrained_model_name_or_path , archive_file , config_file ) ) return None if resolved_archive_file == archive_file and resolved_config_file == config_file : logger . info ( \"loading weights file {}\" . format ( archive_file ) ) logger . info ( \"loading configuration file {}\" . format ( config_file ) ) else : logger . info ( \"loading weights file {} from cache at {}\" . format ( archive_file , resolved_archive_file ) ) logger . info ( \"loading configuration file {} from cache at {}\" . format ( config_file , resolved_config_file ) ) # Load config config = TransfoXLConfig . from_json_file ( resolved_config_file ) logger . info ( \"Model config {}\" . format ( config ) ) # Instantiate model. model = cls ( config , * inputs , * * kwargs ) if state_dict is None and not from_tf : state_dict = torch . load ( resolved_archive_file , map_location = 'cpu' ) if from_tf : # Directly load from a TensorFlow checkpoint return load_tf_weights_in_transfo_xl ( model , config , pretrained_model_name_or_path ) missing_keys = [ ] unexpected_keys = [ ] error_msgs = [ ] # copy state_dict so _load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy ( ) if metadata is not None : state_dict . _metadata = metadata def load ( module , prefix = '' ) : local_metadata = { } if metadata is None else metadata . get ( prefix [ : - 1 ] , { } ) module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _modules . items ( ) : if child is not None : load ( child , prefix + name + '.' ) start_prefix = '' if not hasattr ( model , 'transformer' ) and any ( s . startswith ( 'transformer.' ) for s in state_dict . keys ( ) ) : start_prefix = 'transformer.' load ( model , prefix = start_prefix ) if len ( missing_keys ) > 0 : logger . info ( \"Weights of {} not initialized from pretrained model: {}\" . format ( model . __class__ . __name__ , missing_keys ) ) if len ( unexpected_keys ) > 0 : logger . info ( \"Weights from pretrained model not used in {}: {}\" . format ( model . __class__ . __name__ , unexpected_keys ) ) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( model . __class__ . __name__ , \"\\n\\t\" . join ( error_msgs ) ) ) # Make sure we are still sharing the input and output embeddings if hasattr ( model , 'tie_weights' ) : model . tie_weights ( ) return model\n\n\n        Summary:", "input": "", "output": "Instantiate a TransfoXLPreTrainedModel from a pre - trained model file or a pytorch state dict . Download and cache the pre - trained model file if needed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def forward ( self , input_ids , mems = None ) : # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library # so we transpose here from shape [bsz, len] to shape [len, bsz] input_ids = input_ids . transpose ( 0 , 1 ) . contiguous ( ) if mems is None : mems = self . init_mems ( input_ids ) last_hidden , new_mems = self . _forward ( input_ids , mems = mems ) # We transpose back here to shape [bsz, len, hidden_dim] last_hidden = last_hidden . transpose ( 0 , 1 ) . contiguous ( ) return ( last_hidden , new_mems )\n\n\n        Summary:", "input": "", "output": "Params : input_ids :: [ bsz len ] mems :: optional mems from previous forwar passes ( or init_mems ) list ( num layers ) of mem states at the entry of each layer shape :: [ self . config . mem_len bsz self . config . d_model ] Note that the first two dimensions are transposed in mems with regards to input_ids and target Returns : tuple ( last_hidden new_mems ) where : new_mems : list ( num layers ) of mem states at the entry of each layer shape :: [ self . config . mem_len bsz self . config . d_model ] last_hidden : output of the last layer : shape :: [ bsz len self . config . d_model ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tie_weights ( self ) : # sampled softmax if self . sample_softmax > 0 : if self . config . tie_weight : self . out_layer . weight = self . transformer . word_emb . weight # adaptive softmax (including standard softmax) else : if self . config . tie_weight : for i in range ( len ( self . crit . out_layers ) ) : self . crit . out_layers [ i ] . weight = self . transformer . word_emb . emb_layers [ i ] . weight if self . config . tie_projs : for i , tie_proj in enumerate ( self . config . tie_projs ) : if tie_proj and self . config . div_val == 1 and self . config . d_model != self . config . d_embed : self . crit . out_projs [ i ] = self . transformer . word_emb . emb_projs [ 0 ] elif tie_proj and self . config . div_val != 1 : self . crit . out_projs [ i ] = self . transformer . word_emb . emb_projs [ i ]\n\n\n        Summary:", "input": "", "output": "Run this to be sure output and input ( adaptive ) softmax weights are tied"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def forward ( self , input_ids , target = None , mems = None ) : bsz = input_ids . size ( 0 ) tgt_len = input_ids . size ( 1 ) last_hidden , new_mems = self . transformer ( input_ids , mems ) pred_hid = last_hidden [ : , - tgt_len : ] if self . sample_softmax > 0 and self . training : assert self . config . tie_weight logit = sample_logits ( self . transformer . word_emb , self . out_layer . bias , target , pred_hid , self . sampler ) softmax_output = - F . log_softmax ( logit , - 1 ) [ : , : , 0 ] else : softmax_output = self . crit ( pred_hid . view ( - 1 , pred_hid . size ( - 1 ) ) , target ) if target is None : softmax_output = softmax_output . view ( bsz , tgt_len , - 1 ) else : softmax_output = softmax_output . view ( bsz , tgt_len ) # We transpose back return ( softmax_output , new_mems )\n\n\n        Summary:", "input": "", "output": "Params : input_ids :: [ bsz len ] target :: [ bsz len ] Returns : tuple ( softmax_output new_mems ) where : new_mems : list ( num layers ) of hidden states at the entry of each layer shape :: [ mem_len bsz self . config . d_model ] :: Warning : shapes are transposed here w . regards to input_ids softmax_output : output of the ( adaptive ) softmax : if target is None : Negative log likelihood of shape :: [ bsz len ] else : log probabilities of tokens shape :: [ bsz len n_tokens ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_offset ( freq ) : if freq is None : return None if isinstance ( freq , DateOffset ) : return freq if isinstance ( freq , tuple ) : name = freq [ 0 ] stride = freq [ 1 ] if isinstance ( stride , str ) : name , stride = stride , name name , _ = libfreqs . _base_and_stride ( name ) delta = get_offset ( name ) * stride elif isinstance ( freq , timedelta ) : delta = None freq = Timedelta ( freq ) try : for name in freq . components . _fields : offset = _name_to_offset_map [ name ] stride = getattr ( freq . components , name ) if stride != 0 : offset = stride * offset if delta is None : delta = offset else : delta = delta + offset except Exception : raise ValueError ( libfreqs . INVALID_FREQ_ERR_MSG . format ( freq ) ) else : delta = None stride_sign = None try : splitted = re . split ( libfreqs . opattern , freq ) if splitted [ - 1 ] != '' and not splitted [ - 1 ] . isspace ( ) : # the last element must be blank raise ValueError ( 'last element must be blank' ) for sep , stride , name in zip ( splitted [ 0 : : 4 ] , splitted [ 1 : : 4 ] , splitted [ 2 : : 4 ] ) : if sep != '' and not sep . isspace ( ) : raise ValueError ( 'separator must be spaces' ) prefix = libfreqs . _lite_rule_alias . get ( name ) or name if stride_sign is None : stride_sign = - 1 if stride . startswith ( '-' ) else 1 if not stride : stride = 1 if prefix in Resolution . _reso_str_bump_map . keys ( ) : stride , name = Resolution . get_stride_from_decimal ( float ( stride ) , prefix ) stride = int ( stride ) offset = get_offset ( name ) offset = offset * int ( np . fabs ( stride ) * stride_sign ) if delta is None : delta = offset else : delta = delta + offset except Exception : raise ValueError ( libfreqs . INVALID_FREQ_ERR_MSG . format ( freq ) ) if delta is None : raise ValueError ( libfreqs . INVALID_FREQ_ERR_MSG . format ( freq ) ) return delta\n\n\n        Summary:", "input": "", "output": "Return DateOffset object from string or tuple representation or datetime . timedelta object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_offset ( name ) : if name not in libfreqs . _dont_uppercase : name = name . upper ( ) name = libfreqs . _lite_rule_alias . get ( name , name ) name = libfreqs . _lite_rule_alias . get ( name . lower ( ) , name ) else : name = libfreqs . _lite_rule_alias . get ( name , name ) if name not in _offset_map : try : split = name . split ( '-' ) klass = prefix_mapping [ split [ 0 ] ] # handles case where there's no suffix (and will TypeError if too # many '-') offset = klass . _from_name ( * split [ 1 : ] ) except ( ValueError , TypeError , KeyError ) : # bad prefix or suffix raise ValueError ( libfreqs . INVALID_FREQ_ERR_MSG . format ( name ) ) # cache _offset_map [ name ] = offset return _offset_map [ name ]\n\n\n        Summary:", "input": "", "output": "Return DateOffset object associated with rule name"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer_freq ( index , warn = True ) : import pandas as pd if isinstance ( index , ABCSeries ) : values = index . _values if not ( is_datetime64_dtype ( values ) or is_timedelta64_dtype ( values ) or values . dtype == object ) : raise TypeError ( \"cannot infer freq from a non-convertible dtype \" \"on a Series of {dtype}\" . format ( dtype = index . dtype ) ) index = values if is_period_arraylike ( index ) : raise TypeError ( \"PeriodIndex given. Check the `freq` attribute \" \"instead of using infer_freq.\" ) elif is_timedelta64_dtype ( index ) : # Allow TimedeltaIndex and TimedeltaArray inferer = _TimedeltaFrequencyInferer ( index , warn = warn ) return inferer . get_freq ( ) if isinstance ( index , pd . Index ) and not isinstance ( index , pd . DatetimeIndex ) : if isinstance ( index , ( pd . Int64Index , pd . Float64Index ) ) : raise TypeError ( \"cannot infer freq from a non-convertible index \" \"type {type}\" . format ( type = type ( index ) ) ) index = index . values if not isinstance ( index , pd . DatetimeIndex ) : try : index = pd . DatetimeIndex ( index ) except AmbiguousTimeError : index = pd . DatetimeIndex ( index . asi8 ) inferer = _FrequencyInferer ( index , warn = warn ) return inferer . get_freq ( )\n\n\n        Summary:", "input": "", "output": "Infer the most likely frequency given the input index . If the frequency is uncertain a warning will be printed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_freq ( self ) : if not self . is_monotonic or not self . index . _is_unique : return None delta = self . deltas [ 0 ] if _is_multiple ( delta , _ONE_DAY ) : return self . _infer_daily_rule ( ) # Business hourly, maybe. 17: one day / 65: one weekend if self . hour_deltas in ( [ 1 , 17 ] , [ 1 , 65 ] , [ 1 , 17 , 65 ] ) : return 'BH' # Possibly intraday frequency. Here we use the # original .asi8 values as the modified values # will not work around DST transitions. See #8772 elif not self . is_unique_asi8 : return None delta = self . deltas_asi8 [ 0 ] if _is_multiple ( delta , _ONE_HOUR ) : # Hours return _maybe_add_count ( 'H' , delta / _ONE_HOUR ) elif _is_multiple ( delta , _ONE_MINUTE ) : # Minutes return _maybe_add_count ( 'T' , delta / _ONE_MINUTE ) elif _is_multiple ( delta , _ONE_SECOND ) : # Seconds return _maybe_add_count ( 'S' , delta / _ONE_SECOND ) elif _is_multiple ( delta , _ONE_MILLI ) : # Milliseconds return _maybe_add_count ( 'L' , delta / _ONE_MILLI ) elif _is_multiple ( delta , _ONE_MICRO ) : # Microseconds return _maybe_add_count ( 'U' , delta / _ONE_MICRO ) else : # Nanoseconds return _maybe_add_count ( 'N' , delta )\n\n\n        Summary:", "input": "", "output": "Find the appropriate frequency string to describe the inferred frequency of self . values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def load ( fh , encoding = None , is_verbose = False ) : try : fh . seek ( 0 ) if encoding is not None : up = Unpickler ( fh , encoding = encoding ) else : up = Unpickler ( fh ) up . is_verbose = is_verbose return up . load ( ) except ( ValueError , TypeError ) : raise\n\n\n        Summary:", "input": "", "output": "load a pickle with a provided encoding"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _new_Index ( cls , d ) : # required for backward compat, because PI can't be instantiated with # ordinals through __new__ GH #13277 if issubclass ( cls , ABCPeriodIndex ) : from pandas . core . indexes . period import _new_PeriodIndex return _new_PeriodIndex ( cls , * * d ) return cls . __new__ ( cls , * * d )\n\n\n        Summary:", "input": "", "output": "This is called upon unpickling rather than the default which doesn t have arguments and breaks __new__ ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ensure_index_from_sequences ( sequences , names = None ) : from . multi import MultiIndex if len ( sequences ) == 1 : if names is not None : names = names [ 0 ] return Index ( sequences [ 0 ] , name = names ) else : return MultiIndex . from_arrays ( sequences , names = names )\n\n\n        Summary:", "input": "", "output": "Construct an index from sequences of data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ensure_index ( index_like , copy = False ) : if isinstance ( index_like , Index ) : if copy : index_like = index_like . copy ( ) return index_like if hasattr ( index_like , 'name' ) : return Index ( index_like , name = index_like . name , copy = copy ) if is_iterator ( index_like ) : index_like = list ( index_like ) # must check for exactly list here because of strict type # check in clean_index_list if isinstance ( index_like , list ) : if type ( index_like ) != list : index_like = list ( index_like ) converted , all_arrays = lib . clean_index_list ( index_like ) if len ( converted ) > 0 and all_arrays : from . multi import MultiIndex return MultiIndex . from_arrays ( converted ) else : index_like = converted else : # clean_index_list does the equivalent of copying # so only need to do this if not list instance if copy : from copy import copy index_like = copy ( index_like ) return Index ( index_like )\n\n\n        Summary:", "input": "", "output": "Ensure that we have an index from some index - like object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _trim_front ( strings ) : trimmed = strings while len ( strings ) > 0 and all ( x [ 0 ] == ' ' for x in trimmed ) : trimmed = [ x [ 1 : ] for x in trimmed ] return trimmed\n\n\n        Summary:", "input": "", "output": "Trims zeros and decimal points ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _simple_new ( cls , values , name = None , dtype = None , * * kwargs ) : if not hasattr ( values , 'dtype' ) : if ( values is None or not len ( values ) ) and dtype is not None : values = np . empty ( 0 , dtype = dtype ) else : values = np . array ( values , copy = False ) if is_object_dtype ( values ) : values = cls ( values , name = name , dtype = dtype , * * kwargs ) . _ndarray_values if isinstance ( values , ( ABCSeries , ABCIndexClass ) ) : # Index._data must always be an ndarray. # This is no-copy for when _values is an ndarray, # which should be always at this point. values = np . asarray ( values . _values ) result = object . __new__ ( cls ) result . _data = values # _index_data is a (temporary?) fix to ensure that the direct data # manipulation we do in `_libs/reduction.pyx` continues to work. # We need access to the actual ndarray, since we're messing with # data buffers and strides. We don't re-use `_ndarray_values`, since # we actually set this value too. result . _index_data = values result . name = name for k , v in kwargs . items ( ) : setattr ( result , k , v ) return result . _reset_identity ( )\n\n\n        Summary:", "input": "", "output": "We require that we have a dtype compat for the values . If we are passed a non - dtype compat then coerce using the constructor ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _shallow_copy_with_infer ( self , values , * * kwargs ) : attributes = self . _get_attributes_dict ( ) attributes . update ( kwargs ) attributes [ 'copy' ] = False if not len ( values ) and 'dtype' not in kwargs : attributes [ 'dtype' ] = self . dtype if self . _infer_as_myclass : try : return self . _constructor ( values , * * attributes ) except ( TypeError , ValueError ) : pass return Index ( values , * * attributes )\n\n\n        Summary:", "input": "", "output": "Create a new Index inferring the class with passed value don t copy the data use the same object attributes with passed in attributes taking precedence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_ ( self , other ) : # use something other than None to be clearer return self . _id is getattr ( other , '_id' , Ellipsis ) and self . _id is not None\n\n\n        Summary:", "input": "", "output": "More flexible faster check like is but that works through views ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _assert_take_fillable ( self , values , indices , allow_fill = True , fill_value = None , na_value = np . nan ) : indices = ensure_platform_int ( indices ) # only fill if we are passing a non-None fill_value if allow_fill and fill_value is not None : if ( indices < - 1 ) . any ( ) : msg = ( 'When allow_fill=True and fill_value is not None, ' 'all indices must be >= -1' ) raise ValueError ( msg ) taken = algos . take ( values , indices , allow_fill = allow_fill , fill_value = na_value ) else : taken = values . take ( indices ) return taken\n\n\n        Summary:", "input": "", "output": "Internal method to handle NA filling of take ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_data ( self , name = None ) : # do we want to justify (only do so for non-objects) is_justify = not ( self . inferred_type in ( 'string' , 'unicode' ) or ( self . inferred_type == 'categorical' and is_object_dtype ( self . categories ) ) ) return format_object_summary ( self , self . _formatter_func , is_justify = is_justify , name = name )\n\n\n        Summary:", "input": "", "output": "Return the formatted data as a unicode string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format ( self , name = False , formatter = None , * * kwargs ) : header = [ ] if name : header . append ( pprint_thing ( self . name , escape_chars = ( '\\t' , '\\r' , '\\n' ) ) if self . name is not None else '' ) if formatter is not None : return header + list ( self . map ( formatter ) ) return self . _format_with_header ( header , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Render a string representation of the Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_native_types ( self , slicer = None , * * kwargs ) : values = self if slicer is not None : values = values [ slicer ] return values . _format_native_types ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "Format specified values of self and return them ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_native_types ( self , na_rep = '' , quoting = None , * * kwargs ) : mask = isna ( self ) if not self . is_object ( ) and not quoting : values = np . asarray ( self ) . astype ( str ) else : values = np . array ( self , dtype = object , copy = True ) values [ mask ] = na_rep return values\n\n\n        Summary:", "input": "", "output": "Actually format specific types of the index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _summary ( self , name = None ) : if len ( self ) > 0 : head = self [ 0 ] if hasattr ( head , 'format' ) and not isinstance ( head , str ) : head = head . format ( ) tail = self [ - 1 ] if hasattr ( tail , 'format' ) and not isinstance ( tail , str ) : tail = tail . format ( ) index_summary = ', %s to %s' % ( pprint_thing ( head ) , pprint_thing ( tail ) ) else : index_summary = '' if name is None : name = type ( self ) . __name__ return '%s: %s entries%s' % ( name , len ( self ) , index_summary )\n\n\n        Summary:", "input": "", "output": "Return a summarized representation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def summary ( self , name = None ) : warnings . warn ( \"'summary' is deprecated and will be removed in a \" \"future version.\" , FutureWarning , stacklevel = 2 ) return self . _summary ( name )\n\n\n        Summary:", "input": "", "output": "Return a summarized representation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_series ( self , index = None , name = None ) : from pandas import Series if index is None : index = self . _shallow_copy ( ) if name is None : name = self . name return Series ( self . values . copy ( ) , index = index , name = name )\n\n\n        Summary:", "input": "", "output": "Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_frame ( self , index = True , name = None ) : from pandas import DataFrame if name is None : name = self . name or 0 result = DataFrame ( { name : self . _values . copy ( ) } ) if index : result . index = self return result\n\n\n        Summary:", "input": "", "output": "Create a DataFrame with a column containing the Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_names ( self , name = None , names = None , deep = False ) : from copy import deepcopy if names is not None and name is not None : raise TypeError ( \"Can only provide one of `names` and `name`\" ) elif names is None and name is None : return deepcopy ( self . names ) if deep else self . names elif names is not None : if not is_list_like ( names ) : raise TypeError ( \"Must pass list-like as `names`.\" ) return names else : if not is_list_like ( name ) : return [ name ] return name\n\n\n        Summary:", "input": "", "output": "Handles the quirks of having a singular name parameter for general Index and plural names parameter for MultiIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_names ( self , values , level = None ) : if not is_list_like ( values ) : raise ValueError ( 'Names must be a list-like' ) if len ( values ) != 1 : raise ValueError ( 'Length of new names must be 1, got %d' % len ( values ) ) # GH 20527 # All items in 'name' need to be hashable: for name in values : if not is_hashable ( name ) : raise TypeError ( '{}.name must be a hashable type' . format ( self . __class__ . __name__ ) ) self . name = values [ 0 ]\n\n\n        Summary:", "input": "", "output": "Set new names on index . Each name has to be a hashable type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_names ( self , names , level = None , inplace = False ) : if level is not None and not isinstance ( self , ABCMultiIndex ) : raise ValueError ( 'Level must be None for non-MultiIndex' ) if level is not None and not is_list_like ( level ) and is_list_like ( names ) : msg = \"Names must be a string when a single level is provided.\" raise TypeError ( msg ) if not is_list_like ( names ) and level is None and self . nlevels > 1 : raise TypeError ( \"Must pass list-like as `names`.\" ) if not is_list_like ( names ) : names = [ names ] if level is not None and not is_list_like ( level ) : level = [ level ] if inplace : idx = self else : idx = self . _shallow_copy ( ) idx . _set_names ( names , level = level ) if not inplace : return idx\n\n\n        Summary:", "input": "", "output": "Set Index or MultiIndex name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename ( self , name , inplace = False ) : return self . set_names ( [ name ] , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Alter Index or MultiIndex name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_index_level ( self , level ) : if isinstance ( level , int ) : if level < 0 and level != - 1 : raise IndexError ( \"Too many levels: Index has only 1 level,\" \" %d is not a valid level number\" % ( level , ) ) elif level > 0 : raise IndexError ( \"Too many levels:\" \" Index has only 1 level, not %d\" % ( level + 1 ) ) elif level != self . name : raise KeyError ( 'Level %s must be same as name (%s)' % ( level , self . name ) )\n\n\n        Summary:", "input": "", "output": "Validate index level ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sortlevel ( self , level = None , ascending = True , sort_remaining = None ) : return self . sort_values ( return_indexer = True , ascending = ascending )\n\n\n        Summary:", "input": "", "output": "For internal compatibility with with the Index API ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def droplevel ( self , level = 0 ) : if not isinstance ( level , ( tuple , list ) ) : level = [ level ] levnums = sorted ( self . _get_level_number ( lev ) for lev in level ) [ : : - 1 ] if len ( level ) == 0 : return self if len ( level ) >= self . nlevels : raise ValueError ( \"Cannot remove {} levels from an index with {} \" \"levels: at least one level must be \" \"left.\" . format ( len ( level ) , self . nlevels ) ) # The two checks above guarantee that here self is a MultiIndex new_levels = list ( self . levels ) new_codes = list ( self . codes ) new_names = list ( self . names ) for i in levnums : new_levels . pop ( i ) new_codes . pop ( i ) new_names . pop ( i ) if len ( new_levels ) == 1 : # set nan if needed mask = new_codes [ 0 ] == - 1 result = new_levels [ 0 ] . take ( new_codes [ 0 ] ) if mask . any ( ) : result = result . putmask ( mask , np . nan ) result . name = new_names [ 0 ] return result else : from . multi import MultiIndex return MultiIndex ( levels = new_levels , codes = new_codes , names = new_names , verify_integrity = False )\n\n\n        Summary:", "input": "", "output": "Return index with requested level ( s ) removed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _isnan ( self ) : if self . _can_hold_na : return isna ( self ) else : # shouldn't reach to this condition by checking hasnans beforehand values = np . empty ( len ( self ) , dtype = np . bool_ ) values . fill ( False ) return values\n\n\n        Summary:", "input": "", "output": "Return if each value is NaN ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_duplicates ( self ) : warnings . warn ( \"'get_duplicates' is deprecated and will be removed in \" \"a future release. You can use \" \"idx[idx.duplicated()].unique() instead\" , FutureWarning , stacklevel = 2 ) return self [ self . duplicated ( ) ] . unique ( )\n\n\n        Summary:", "input": "", "output": "Extract duplicated index elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_unique_index ( self , dropna = False ) : if self . is_unique and not dropna : return self values = self . values if not self . is_unique : values = self . unique ( ) if dropna : try : if self . hasnans : values = values [ ~ isna ( values ) ] except NotImplementedError : pass return self . _shallow_copy ( values )\n\n\n        Summary:", "input": "", "output": "Returns an index containing unique values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_reconciled_name_object ( self , other ) : name = get_op_result_name ( self , other ) if self . name != name : return self . _shallow_copy ( name = name ) return self\n\n\n        Summary:", "input": "", "output": "If the result of a set operation will be self return self unless the name changes in which case make a shallow copy of self ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , other , sort = None ) : self . _validate_sort_keyword ( sort ) self . _assert_can_do_setop ( other ) other = ensure_index ( other ) if len ( other ) == 0 or self . equals ( other ) : return self . _get_reconciled_name_object ( other ) if len ( self ) == 0 : return other . _get_reconciled_name_object ( self ) # TODO: is_dtype_union_equal is a hack around # 1. buggy set ops with duplicates (GH #13432) # 2. CategoricalIndex lacking setops (GH #10186) # Once those are fixed, this workaround can be removed if not is_dtype_union_equal ( self . dtype , other . dtype ) : this = self . astype ( 'O' ) other = other . astype ( 'O' ) return this . union ( other , sort = sort ) # TODO(EA): setops-refactor, clean all this up if is_period_dtype ( self ) or is_datetime64tz_dtype ( self ) : lvals = self . _ndarray_values else : lvals = self . _values if is_period_dtype ( other ) or is_datetime64tz_dtype ( other ) : rvals = other . _ndarray_values else : rvals = other . _values if sort is None and self . is_monotonic and other . is_monotonic : try : result = self . _outer_indexer ( lvals , rvals ) [ 0 ] except TypeError : # incomparable objects result = list ( lvals ) # worth making this faster? a very unusual case value_set = set ( lvals ) result . extend ( [ x for x in rvals if x not in value_set ] ) else : indexer = self . get_indexer ( other ) indexer , = ( indexer == - 1 ) . nonzero ( ) if len ( indexer ) > 0 : other_diff = algos . take_nd ( rvals , indexer , allow_fill = False ) result = _concat . _concat_compat ( ( lvals , other_diff ) ) else : result = lvals if sort is None : try : result = sorting . safe_sort ( result ) except TypeError as e : warnings . warn ( \"{}, sort order is undefined for \" \"incomparable objects\" . format ( e ) , RuntimeWarning , stacklevel = 3 ) # for subclasses return self . _wrap_setop_result ( other , result )\n\n\n        Summary:", "input": "", "output": "Form the union of two Index objects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def intersection ( self , other , sort = False ) : self . _validate_sort_keyword ( sort ) self . _assert_can_do_setop ( other ) other = ensure_index ( other ) if self . equals ( other ) : return self . _get_reconciled_name_object ( other ) if not is_dtype_equal ( self . dtype , other . dtype ) : this = self . astype ( 'O' ) other = other . astype ( 'O' ) return this . intersection ( other , sort = sort ) # TODO(EA): setops-refactor, clean all this up if is_period_dtype ( self ) : lvals = self . _ndarray_values else : lvals = self . _values if is_period_dtype ( other ) : rvals = other . _ndarray_values else : rvals = other . _values if self . is_monotonic and other . is_monotonic : try : result = self . _inner_indexer ( lvals , rvals ) [ 0 ] return self . _wrap_setop_result ( other , result ) except TypeError : pass try : indexer = Index ( rvals ) . get_indexer ( lvals ) indexer = indexer . take ( ( indexer != - 1 ) . nonzero ( ) [ 0 ] ) except Exception : # duplicates indexer = algos . unique1d ( Index ( rvals ) . get_indexer_non_unique ( lvals ) [ 0 ] ) indexer = indexer [ indexer != - 1 ] taken = other . take ( indexer ) if sort is None : taken = sorting . safe_sort ( taken . values ) if self . name != other . name : name = None else : name = self . name return self . _shallow_copy ( taken , name = name ) if self . name != other . name : taken . name = None return taken\n\n\n        Summary:", "input": "", "output": "Form the intersection of two Index objects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def difference ( self , other , sort = None ) : self . _validate_sort_keyword ( sort ) self . _assert_can_do_setop ( other ) if self . equals ( other ) : # pass an empty np.ndarray with the appropriate dtype return self . _shallow_copy ( self . _data [ : 0 ] ) other , result_name = self . _convert_can_do_setop ( other ) this = self . _get_unique_index ( ) indexer = this . get_indexer ( other ) indexer = indexer . take ( ( indexer != - 1 ) . nonzero ( ) [ 0 ] ) label_diff = np . setdiff1d ( np . arange ( this . size ) , indexer , assume_unique = True ) the_diff = this . values . take ( label_diff ) if sort is None : try : the_diff = sorting . safe_sort ( the_diff ) except TypeError : pass return this . _shallow_copy ( the_diff , name = result_name , freq = None )\n\n\n        Summary:", "input": "", "output": "Return a new Index with elements from the index that are not in other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def symmetric_difference ( self , other , result_name = None , sort = None ) : self . _validate_sort_keyword ( sort ) self . _assert_can_do_setop ( other ) other , result_name_update = self . _convert_can_do_setop ( other ) if result_name is None : result_name = result_name_update this = self . _get_unique_index ( ) other = other . _get_unique_index ( ) indexer = this . get_indexer ( other ) # {this} minus {other} common_indexer = indexer . take ( ( indexer != - 1 ) . nonzero ( ) [ 0 ] ) left_indexer = np . setdiff1d ( np . arange ( this . size ) , common_indexer , assume_unique = True ) left_diff = this . values . take ( left_indexer ) # {other} minus {this} right_indexer = ( indexer == - 1 ) . nonzero ( ) [ 0 ] right_diff = other . values . take ( right_indexer ) the_diff = _concat . _concat_compat ( [ left_diff , right_diff ] ) if sort is None : try : the_diff = sorting . safe_sort ( the_diff ) except TypeError : pass attribs = self . _get_attributes_dict ( ) attribs [ 'name' ] = result_name if 'freq' in attribs : attribs [ 'freq' ] = None return self . _shallow_copy_with_infer ( the_diff , * * attribs )\n\n\n        Summary:", "input": "", "output": "Compute the symmetric difference of two Index objects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_fill_indexer_searchsorted ( self , target , method , limit = None ) : if limit is not None : raise ValueError ( 'limit argument for %r method only well-defined ' 'if index and target are monotonic' % method ) side = 'left' if method == 'pad' else 'right' # find exact matches first (this simplifies the algorithm) indexer = self . get_indexer ( target ) nonexact = ( indexer == - 1 ) indexer [ nonexact ] = self . _searchsorted_monotonic ( target [ nonexact ] , side ) if side == 'left' : # searchsorted returns \"indices into a sorted array such that, # if the corresponding elements in v were inserted before the # indices, the order of a would be preserved\". # Thus, we need to subtract 1 to find values to the left. indexer [ nonexact ] -= 1 # This also mapped not found values (values of 0 from # np.searchsorted) to -1, which conveniently is also our # sentinel for missing values else : # Mark indices to the right of the largest value as not found indexer [ indexer == len ( self ) ] = - 1 return indexer\n\n\n        Summary:", "input": "", "output": "Fallback pad / backfill get_indexer that works for monotonic decreasing indexes and non - monotonic targets ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_nearest_indexer ( self , target , limit , tolerance ) : left_indexer = self . get_indexer ( target , 'pad' , limit = limit ) right_indexer = self . get_indexer ( target , 'backfill' , limit = limit ) target = np . asarray ( target ) left_distances = abs ( self . values [ left_indexer ] - target ) right_distances = abs ( self . values [ right_indexer ] - target ) op = operator . lt if self . is_monotonic_increasing else operator . le indexer = np . where ( op ( left_distances , right_distances ) | ( right_indexer == - 1 ) , left_indexer , right_indexer ) if tolerance is not None : indexer = self . _filter_indexer_tolerance ( target , indexer , tolerance ) return indexer\n\n\n        Summary:", "input": "", "output": "Get the indexer for the nearest index labels ; requires an index with values that can be subtracted from each other ( e . g . not strings or tuples ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_listlike_indexer ( self , keyarr , kind = None ) : if isinstance ( keyarr , Index ) : keyarr = self . _convert_index_indexer ( keyarr ) else : keyarr = self . _convert_arr_indexer ( keyarr ) indexer = self . _convert_list_indexer ( keyarr , kind = kind ) return indexer , keyarr\n\n\n        Summary:", "input": "", "output": "Parameters ---------- keyarr : list - like Indexer to convert ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _invalid_indexer ( self , form , key ) : raise TypeError ( \"cannot do {form} indexing on {klass} with these \" \"indexers [{key}] of {kind}\" . format ( form = form , klass = type ( self ) , key = key , kind = type ( key ) ) )\n\n\n        Summary:", "input": "", "output": "Consistent invalid indexer message ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reindex ( self , target , method = None , level = None , limit = None , tolerance = None ) : # GH6552: preserve names when reindexing to non-named target # (i.e. neither Index nor Series). preserve_names = not hasattr ( target , 'name' ) # GH7774: preserve dtype/tz if target is empty and not an Index. target = _ensure_has_len ( target ) # target may be an iterator if not isinstance ( target , Index ) and len ( target ) == 0 : attrs = self . _get_attributes_dict ( ) attrs . pop ( 'freq' , None ) # don't preserve freq values = self . _data [ : 0 ] # appropriately-dtyped empty array target = self . _simple_new ( values , dtype = self . dtype , * * attrs ) else : target = ensure_index ( target ) if level is not None : if method is not None : raise TypeError ( 'Fill method not supported if level passed' ) _ , indexer , _ = self . _join_level ( target , level , how = 'right' , return_indexers = True ) else : if self . equals ( target ) : indexer = None else : if self . is_unique : indexer = self . get_indexer ( target , method = method , limit = limit , tolerance = tolerance ) else : if method is not None or limit is not None : raise ValueError ( \"cannot reindex a non-unique index \" \"with a method or limit\" ) indexer , missing = self . get_indexer_non_unique ( target ) if preserve_names and target . nlevels == 1 and target . name != self . name : target = target . copy ( ) target . name = self . name return target , indexer\n\n\n        Summary:", "input": "", "output": "Create index with target s values ( move / add / delete values as necessary ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reindex_non_unique ( self , target ) : target = ensure_index ( target ) indexer , missing = self . get_indexer_non_unique ( target ) check = indexer != - 1 new_labels = self . take ( indexer [ check ] ) new_indexer = None if len ( missing ) : length = np . arange ( len ( indexer ) ) missing = ensure_platform_int ( missing ) missing_labels = target . take ( missing ) missing_indexer = ensure_int64 ( length [ ~ check ] ) cur_labels = self . take ( indexer [ check ] ) . values cur_indexer = ensure_int64 ( length [ check ] ) new_labels = np . empty ( tuple ( [ len ( indexer ) ] ) , dtype = object ) new_labels [ cur_indexer ] = cur_labels new_labels [ missing_indexer ] = missing_labels # a unique indexer if target . is_unique : # see GH5553, make sure we use the right indexer new_indexer = np . arange ( len ( indexer ) ) new_indexer [ cur_indexer ] = np . arange ( len ( cur_labels ) ) new_indexer [ missing_indexer ] = - 1 # we have a non_unique selector, need to use the original # indexer here else : # need to retake to have the same size as the indexer indexer [ ~ check ] = - 1 # reset the new indexer to account for the new size new_indexer = np . arange ( len ( self . take ( indexer ) ) ) new_indexer [ ~ check ] = - 1 new_index = self . _shallow_copy_with_infer ( new_labels , freq = None ) return new_index , indexer , new_indexer\n\n\n        Summary:", "input": "", "output": "Create a new index with target s values ( move / add / delete values as necessary ) use with non - unique Index and a possibly non - unique target ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _join_level ( self , other , level , how = 'left' , return_indexers = False , keep_order = True ) : from . multi import MultiIndex def _get_leaf_sorter ( labels ) : \"\"\" Returns sorter for the inner most level while preserving the order of higher levels. \"\"\" if labels [ 0 ] . size == 0 : return np . empty ( 0 , dtype = 'int64' ) if len ( labels ) == 1 : lab = ensure_int64 ( labels [ 0 ] ) sorter , _ = libalgos . groupsort_indexer ( lab , 1 + lab . max ( ) ) return sorter # find indexers of beginning of each set of # same-key labels w.r.t all but last level tic = labels [ 0 ] [ : - 1 ] != labels [ 0 ] [ 1 : ] for lab in labels [ 1 : - 1 ] : tic |= lab [ : - 1 ] != lab [ 1 : ] starts = np . hstack ( ( [ True ] , tic , [ True ] ) ) . nonzero ( ) [ 0 ] lab = ensure_int64 ( labels [ - 1 ] ) return lib . get_level_sorter ( lab , ensure_int64 ( starts ) ) if isinstance ( self , MultiIndex ) and isinstance ( other , MultiIndex ) : raise TypeError ( 'Join on level between two MultiIndex objects ' 'is ambiguous' ) left , right = self , other flip_order = not isinstance ( self , MultiIndex ) if flip_order : left , right = right , left how = { 'right' : 'left' , 'left' : 'right' } . get ( how , how ) level = left . _get_level_number ( level ) old_level = left . levels [ level ] if not right . is_unique : raise NotImplementedError ( 'Index._join_level on non-unique index ' 'is not implemented' ) new_level , left_lev_indexer , right_lev_indexer = old_level . join ( right , how = how , return_indexers = True ) if left_lev_indexer is None : if keep_order or len ( left ) == 0 : left_indexer = None join_index = left else : # sort the leaves left_indexer = _get_leaf_sorter ( left . codes [ : level + 1 ] ) join_index = left [ left_indexer ] else : left_lev_indexer = ensure_int64 ( left_lev_indexer ) rev_indexer = lib . get_reverse_indexer ( left_lev_indexer , len ( old_level ) ) new_lev_codes = algos . take_nd ( rev_indexer , left . codes [ level ] , allow_fill = False ) new_codes = list ( left . codes ) new_codes [ level ] = new_lev_codes new_levels = list ( left . levels ) new_levels [ level ] = new_level if keep_order : # just drop missing values. o.w. keep order left_indexer = np . arange ( len ( left ) , dtype = np . intp ) mask = new_lev_codes != - 1 if not mask . all ( ) : new_codes = [ lab [ mask ] for lab in new_codes ] left_indexer = left_indexer [ mask ] else : # tie out the order with other if level == 0 : # outer most level, take the fast route ngroups = 1 + new_lev_codes . max ( ) left_indexer , counts = libalgos . groupsort_indexer ( new_lev_codes , ngroups ) # missing values are placed first; drop them! left_indexer = left_indexer [ counts [ 0 ] : ] new_codes = [ lab [ left_indexer ] for lab in new_codes ] else : # sort the leaves mask = new_lev_codes != - 1 mask_all = mask . all ( ) if not mask_all : new_codes = [ lab [ mask ] for lab in new_codes ] left_indexer = _get_leaf_sorter ( new_codes [ : level + 1 ] ) new_codes = [ lab [ left_indexer ] for lab in new_codes ] # left_indexers are w.r.t masked frame. # reverse to original frame! if not mask_all : left_indexer = mask . nonzero ( ) [ 0 ] [ left_indexer ] join_index = MultiIndex ( levels = new_levels , codes = new_codes , names = left . names , verify_integrity = False ) if right_lev_indexer is not None : right_indexer = algos . take_nd ( right_lev_indexer , join_index . codes [ level ] , allow_fill = False ) else : right_indexer = join_index . codes [ level ] if flip_order : left_indexer , right_indexer = right_indexer , left_indexer if return_indexers : left_indexer = ( None if left_indexer is None else ensure_platform_int ( left_indexer ) ) right_indexer = ( None if right_indexer is None else ensure_platform_int ( right_indexer ) ) return join_index , left_indexer , right_indexer else : return join_index\n\n\n        Summary:", "input": "", "output": "The join method * only * affects the level of the resulting MultiIndex . Otherwise it just exactly aligns the Index data to the labels of the level in the MultiIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_convert_to_int_index ( cls , data , copy , name , dtype ) : from . numeric import Int64Index , UInt64Index if not is_unsigned_integer_dtype ( dtype ) : # skip int64 conversion attempt if uint-like dtype is passed, as # this could return Int64Index when UInt64Index is what's desrired try : res = data . astype ( 'i8' , copy = False ) if ( res == data ) . all ( ) : return Int64Index ( res , copy = copy , name = name ) except ( OverflowError , TypeError , ValueError ) : pass # Conversion to int64 failed (possibly due to overflow) or was skipped, # so let's try now with uint64. try : res = data . astype ( 'u8' , copy = False ) if ( res == data ) . all ( ) : return UInt64Index ( res , copy = copy , name = name ) except ( OverflowError , TypeError , ValueError ) : pass raise ValueError\n\n\n        Summary:", "input": "", "output": "Attempt to convert an array of data into an integer index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _coerce_to_ndarray ( cls , data ) : if not isinstance ( data , ( np . ndarray , Index ) ) : if data is None or is_scalar ( data ) : cls . _scalar_data_error ( data ) # other iterable of some kind if not isinstance ( data , ( ABCSeries , list , tuple ) ) : data = list ( data ) data = np . asarray ( data ) return data\n\n\n        Summary:", "input": "", "output": "Coerces data to ndarray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _coerce_scalar_to_index ( self , item ) : dtype = self . dtype if self . _is_numeric_dtype and isna ( item ) : # We can't coerce to the numeric dtype of \"self\" (unless # it's float) if there are NaN values in our output. dtype = None return Index ( [ item ] , dtype = dtype , * * self . _get_attributes_dict ( ) )\n\n\n        Summary:", "input": "", "output": "We need to coerce a scalar to a compat for our index type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _assert_can_do_op ( self , value ) : if not is_scalar ( value ) : msg = \"'value' must be a scalar, passed: {0}\" raise TypeError ( msg . format ( type ( value ) . __name__ ) )\n\n\n        Summary:", "input": "", "output": "Check value is valid for scalar op ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _can_hold_identifiers_and_holds_name ( self , name ) : if self . is_object ( ) or self . is_categorical ( ) : return name in self return False\n\n\n        Summary:", "input": "", "output": "Faster check for name in self when we know name is a Python identifier ( e . g . in NDFrame . __getattr__ which hits this to support . key lookup ) . For indexes that can t hold identifiers ( everything but object & categorical ) we just return False ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def append ( self , other ) : to_concat = [ self ] if isinstance ( other , ( list , tuple ) ) : to_concat = to_concat + list ( other ) else : to_concat . append ( other ) for obj in to_concat : if not isinstance ( obj , Index ) : raise TypeError ( 'all inputs must be Index' ) names = { obj . name for obj in to_concat } name = None if len ( names ) > 1 else self . name return self . _concat ( to_concat , name )\n\n\n        Summary:", "input": "", "output": "Append a collection of Index options together ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def putmask ( self , mask , value ) : values = self . values . copy ( ) try : np . putmask ( values , mask , self . _convert_for_op ( value ) ) return self . _shallow_copy ( values ) except ( ValueError , TypeError ) as err : if is_object_dtype ( self ) : raise err # coerces to object return self . astype ( object ) . putmask ( mask , value )\n\n\n        Summary:", "input": "", "output": "Return a new Index of the values set with the mask ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def equals ( self , other ) : if self . is_ ( other ) : return True if not isinstance ( other , Index ) : return False if is_object_dtype ( self ) and not is_object_dtype ( other ) : # if other is not object, use other's logic for coercion return other . equals ( self ) try : return array_equivalent ( com . values_from_object ( self ) , com . values_from_object ( other ) ) except Exception : return False\n\n\n        Summary:", "input": "", "output": "Determine if two Index objects contain the same elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def identical ( self , other ) : return ( self . equals ( other ) and all ( ( getattr ( self , c , None ) == getattr ( other , c , None ) for c in self . _comparables ) ) and type ( self ) == type ( other ) )\n\n\n        Summary:", "input": "", "output": "Similar to equals but check that other comparable attributes are also equal ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asof ( self , label ) : try : loc = self . get_loc ( label , method = 'pad' ) except KeyError : return self . _na_value else : if isinstance ( loc , slice ) : loc = loc . indices ( len ( self ) ) [ - 1 ] return self [ loc ]\n\n\n        Summary:", "input": "", "output": "Return the label from the index or if not present the previous one ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asof_locs ( self , where , mask ) : locs = self . values [ mask ] . searchsorted ( where . values , side = 'right' ) locs = np . where ( locs > 0 , locs - 1 , 0 ) result = np . arange ( len ( self ) ) [ mask ] . take ( locs ) first = mask . argmax ( ) result [ ( locs == 0 ) & ( where . values < self . values [ first ] ) ] = - 1 return result\n\n\n        Summary:", "input": "", "output": "Find the locations ( indices ) of the labels from the index for every entry in the where argument ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_values ( self , return_indexer = False , ascending = True ) : _as = self . argsort ( ) if not ascending : _as = _as [ : : - 1 ] sorted_index = self . take ( _as ) if return_indexer : return sorted_index , _as else : return sorted_index\n\n\n        Summary:", "input": "", "output": "Return a sorted copy of the index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argsort ( self , * args , * * kwargs ) : result = self . asi8 if result is None : result = np . array ( self ) return result . argsort ( * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return the integer indices that would sort the index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_value ( self , series , key ) : # if we have something that is Index-like, then # use this, e.g. DatetimeIndex # Things like `Series._get_value` (via .at) pass the EA directly here. s = getattr ( series , '_values' , series ) if isinstance ( s , ( ExtensionArray , Index ) ) and is_scalar ( key ) : # GH 20882, 21257 # Unify Index and ExtensionArray treatment # First try to convert the key to a location # If that fails, raise a KeyError if an integer # index, otherwise, see if key is an integer, and # try that try : iloc = self . get_loc ( key ) return s [ iloc ] except KeyError : if ( len ( self ) > 0 and ( self . holds_integer ( ) or self . is_boolean ( ) ) ) : raise elif is_integer ( key ) : return s [ key ] s = com . values_from_object ( series ) k = com . values_from_object ( key ) k = self . _convert_scalar_indexer ( k , kind = 'getitem' ) try : return self . _engine . get_value ( s , k , tz = getattr ( series . dtype , 'tz' , None ) ) except KeyError as e1 : if len ( self ) > 0 and ( self . holds_integer ( ) or self . is_boolean ( ) ) : raise try : return libindex . get_value_box ( s , key ) except IndexError : raise except TypeError : # generator/iterator-like if is_iterator ( key ) : raise InvalidIndexError ( key ) else : raise e1 except Exception : # pragma: no cover raise e1 except TypeError : # python 3 if is_scalar ( key ) : # pragma: no cover raise IndexError ( key ) raise InvalidIndexError ( key )\n\n\n        Summary:", "input": "", "output": "Fast lookup of value from 1 - dimensional ndarray . Only use this if you know what you re doing ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_value ( self , arr , key , value ) : self . _engine . set_value ( com . values_from_object ( arr ) , com . values_from_object ( key ) , value )\n\n\n        Summary:", "input": "", "output": "Fast lookup of value from 1 - dimensional ndarray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_indexer_for ( self , target , * * kwargs ) : if self . is_unique : return self . get_indexer ( target , * * kwargs ) indexer , _ = self . get_indexer_non_unique ( target , * * kwargs ) return indexer\n\n\n        Summary:", "input": "", "output": "Guaranteed return of an indexer even when non - unique ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupby ( self , values ) : # TODO: if we are a MultiIndex, we can do better # that converting to tuples if isinstance ( values , ABCMultiIndex ) : values = values . values values = ensure_categorical ( values ) result = values . _reverse_indexer ( ) # map to the label result = { k : self . take ( v ) for k , v in result . items ( ) } return result\n\n\n        Summary:", "input": "", "output": "Group the index labels by a given array of values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map ( self , mapper , na_action = None ) : from . multi import MultiIndex new_values = super ( ) . _map_values ( mapper , na_action = na_action ) attributes = self . _get_attributes_dict ( ) # we can return a MultiIndex if new_values . size and isinstance ( new_values [ 0 ] , tuple ) : if isinstance ( self , MultiIndex ) : names = self . names elif attributes . get ( 'name' ) : names = [ attributes . get ( 'name' ) ] * len ( new_values [ 0 ] ) else : names = None return MultiIndex . from_tuples ( new_values , names = names ) attributes [ 'copy' ] = False if not new_values . size : # empty attributes [ 'dtype' ] = self . dtype return Index ( new_values , * * attributes )\n\n\n        Summary:", "input": "", "output": "Map values using input correspondence ( a dict Series or function ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isin ( self , values , level = None ) : if level is not None : self . _validate_index_level ( level ) return algos . isin ( self , values )\n\n\n        Summary:", "input": "", "output": "Return a boolean array where the index values are in values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def slice_indexer ( self , start = None , end = None , step = None , kind = None ) : start_slice , end_slice = self . slice_locs ( start , end , step = step , kind = kind ) # return a slice if not is_scalar ( start_slice ) : raise AssertionError ( \"Start slice bound is non-scalar\" ) if not is_scalar ( end_slice ) : raise AssertionError ( \"End slice bound is non-scalar\" ) return slice ( start_slice , end_slice , step )\n\n\n        Summary:", "input": "", "output": "For an ordered or unique index compute the slice indexer for input labels and step ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_cast_indexer ( self , key ) : if is_float ( key ) and not self . is_floating ( ) : try : ckey = int ( key ) if ckey == key : key = ckey except ( OverflowError , ValueError , TypeError ) : pass return key\n\n\n        Summary:", "input": "", "output": "If we have a float key and are not a floating index then try to cast to an int if equivalent ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_indexer ( self , form , key , kind ) : assert kind in [ 'ix' , 'loc' , 'getitem' , 'iloc' ] if key is None : pass elif is_integer ( key ) : pass elif kind in [ 'iloc' , 'getitem' ] : self . _invalid_indexer ( form , key ) return key\n\n\n        Summary:", "input": "", "output": "If we are positional indexer validate that we have appropriate typed bounds must be an integer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_slice_bound ( self , label , side , kind ) : assert kind in [ 'ix' , 'loc' , 'getitem' , None ] if side not in ( 'left' , 'right' ) : raise ValueError ( \"Invalid value for side kwarg,\" \" must be either 'left' or 'right': %s\" % ( side , ) ) original_label = label # For datetime indices label may be a string that has to be converted # to datetime boundary according to its resolution. label = self . _maybe_cast_slice_bound ( label , side , kind ) # we need to look up the label try : slc = self . _get_loc_only_exact_matches ( label ) except KeyError as err : try : return self . _searchsorted_monotonic ( label , side ) except ValueError : # raise the original KeyError raise err if isinstance ( slc , np . ndarray ) : # get_loc may return a boolean array or an array of indices, which # is OK as long as they are representable by a slice. if is_bool_dtype ( slc ) : slc = lib . maybe_booleans_to_slice ( slc . view ( 'u1' ) ) else : slc = lib . maybe_indices_to_slice ( slc . astype ( 'i8' ) , len ( self ) ) if isinstance ( slc , np . ndarray ) : raise KeyError ( \"Cannot get %s slice bound for non-unique \" \"label: %r\" % ( side , original_label ) ) if isinstance ( slc , slice ) : if side == 'left' : return slc . start else : return slc . stop else : if side == 'right' : return slc + 1 else : return slc\n\n\n        Summary:", "input": "", "output": "Calculate slice bound that corresponds to given label ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def slice_locs ( self , start = None , end = None , step = None , kind = None ) : inc = ( step is None or step >= 0 ) if not inc : # If it's a reverse slice, temporarily swap bounds. start , end = end , start # GH 16785: If start and end happen to be date strings with UTC offsets # attempt to parse and check that the offsets are the same if ( isinstance ( start , ( str , datetime ) ) and isinstance ( end , ( str , datetime ) ) ) : try : ts_start = Timestamp ( start ) ts_end = Timestamp ( end ) except ( ValueError , TypeError ) : pass else : if not tz_compare ( ts_start . tzinfo , ts_end . tzinfo ) : raise ValueError ( \"Both dates must have the \" \"same UTC offset\" ) start_slice = None if start is not None : start_slice = self . get_slice_bound ( start , 'left' , kind ) if start_slice is None : start_slice = 0 end_slice = None if end is not None : end_slice = self . get_slice_bound ( end , 'right' , kind ) if end_slice is None : end_slice = len ( self ) if not inc : # Bounds at this moment are swapped, swap them back and shift by 1. # # slice_locs('B', 'A', step=-1): s='B', e='A' # # s='A' e='B' # AFTER SWAP: | | # v ------------------> V # ----------------------------------- # | | |A|A|A|A| | | | | |B|B| | | | | # ----------------------------------- # ^ <------------------ ^ # SHOULD BE: | | # end=s-1 start=e-1 # end_slice , start_slice = start_slice - 1 , end_slice - 1 # i == -1 triggers ``len(self) + i`` selection that points to the # last element, not before-the-first one, subtracting len(self) # compensates that. if end_slice == - 1 : end_slice -= len ( self ) if start_slice == - 1 : start_slice -= len ( self ) return start_slice , end_slice\n\n\n        Summary:", "input": "", "output": "Compute slice locations for input labels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def delete ( self , loc ) : return self . _shallow_copy ( np . delete ( self . _data , loc ) )\n\n\n        Summary:", "input": "", "output": "Make new Index with passed location ( - s ) deleted ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def insert ( self , loc , item ) : _self = np . asarray ( self ) item = self . _coerce_scalar_to_index ( item ) . _ndarray_values idx = np . concatenate ( ( _self [ : loc ] , item , _self [ loc : ] ) ) return self . _shallow_copy_with_infer ( idx )\n\n\n        Summary:", "input": "", "output": "Make new Index inserting new item at location ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def drop ( self , labels , errors = 'raise' ) : arr_dtype = 'object' if self . dtype == 'object' else None labels = com . index_labels_to_array ( labels , dtype = arr_dtype ) indexer = self . get_indexer ( labels ) mask = indexer == - 1 if mask . any ( ) : if errors != 'ignore' : raise KeyError ( '{} not found in axis' . format ( labels [ mask ] ) ) indexer = indexer [ ~ mask ] return self . delete ( indexer )\n\n\n        Summary:", "input": "", "output": "Make new Index with passed list of labels deleted ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_comparison_methods ( cls ) : cls . __eq__ = _make_comparison_op ( operator . eq , cls ) cls . __ne__ = _make_comparison_op ( operator . ne , cls ) cls . __lt__ = _make_comparison_op ( operator . lt , cls ) cls . __gt__ = _make_comparison_op ( operator . gt , cls ) cls . __le__ = _make_comparison_op ( operator . le , cls ) cls . __ge__ = _make_comparison_op ( operator . ge , cls )\n\n\n        Summary:", "input": "", "output": "Add in comparison methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_methods_add_sub_disabled ( cls ) : cls . __add__ = make_invalid_op ( '__add__' ) cls . __radd__ = make_invalid_op ( '__radd__' ) cls . __iadd__ = make_invalid_op ( '__iadd__' ) cls . __sub__ = make_invalid_op ( '__sub__' ) cls . __rsub__ = make_invalid_op ( '__rsub__' ) cls . __isub__ = make_invalid_op ( '__isub__' )\n\n\n        Summary:", "input": "", "output": "Add in the numeric add / sub methods to disable ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_methods_disabled ( cls ) : cls . __pow__ = make_invalid_op ( '__pow__' ) cls . __rpow__ = make_invalid_op ( '__rpow__' ) cls . __mul__ = make_invalid_op ( '__mul__' ) cls . __rmul__ = make_invalid_op ( '__rmul__' ) cls . __floordiv__ = make_invalid_op ( '__floordiv__' ) cls . __rfloordiv__ = make_invalid_op ( '__rfloordiv__' ) cls . __truediv__ = make_invalid_op ( '__truediv__' ) cls . __rtruediv__ = make_invalid_op ( '__rtruediv__' ) cls . __mod__ = make_invalid_op ( '__mod__' ) cls . __divmod__ = make_invalid_op ( '__divmod__' ) cls . __neg__ = make_invalid_op ( '__neg__' ) cls . __pos__ = make_invalid_op ( '__pos__' ) cls . __abs__ = make_invalid_op ( '__abs__' ) cls . __inv__ = make_invalid_op ( '__inv__' )\n\n\n        Summary:", "input": "", "output": "Add in numeric methods to disable other than add / sub ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_for_numeric_unaryop ( self , op , opstr ) : if not self . _is_numeric_dtype : raise TypeError ( \"cannot evaluate a numeric op \" \"{opstr} for type: {typ}\" . format ( opstr = opstr , typ = type ( self ) . __name__ ) )\n\n\n        Summary:", "input": "", "output": "Validate if we can perform a numeric unary operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_for_numeric_binop ( self , other , op ) : opstr = '__{opname}__' . format ( opname = op . __name__ ) # if we are an inheritor of numeric, # but not actually numeric (e.g. DatetimeIndex/PeriodIndex) if not self . _is_numeric_dtype : raise TypeError ( \"cannot evaluate a numeric op {opstr} \" \"for type: {typ}\" . format ( opstr = opstr , typ = type ( self ) . __name__ ) ) if isinstance ( other , Index ) : if not other . _is_numeric_dtype : raise TypeError ( \"cannot evaluate a numeric op \" \"{opstr} with type: {typ}\" . format ( opstr = opstr , typ = type ( other ) ) ) elif isinstance ( other , np . ndarray ) and not other . ndim : other = other . item ( ) if isinstance ( other , ( Index , ABCSeries , np . ndarray ) ) : if len ( self ) != len ( other ) : raise ValueError ( \"cannot evaluate a numeric op with \" \"unequal lengths\" ) other = com . values_from_object ( other ) if other . dtype . kind not in [ 'f' , 'i' , 'u' ] : raise TypeError ( \"cannot evaluate a numeric op \" \"with a non-numeric dtype\" ) elif isinstance ( other , ( ABCDateOffset , np . timedelta64 , timedelta ) ) : # higher up to handle pass elif isinstance ( other , ( datetime , np . datetime64 ) ) : # higher up to handle pass else : if not ( is_float ( other ) or is_integer ( other ) ) : raise TypeError ( \"can only perform ops with scalar values\" ) return other\n\n\n        Summary:", "input": "", "output": "Return valid other ; evaluate or raise TypeError if we are not of the appropriate type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_methods_binary ( cls ) : cls . __add__ = _make_arithmetic_op ( operator . add , cls ) cls . __radd__ = _make_arithmetic_op ( ops . radd , cls ) cls . __sub__ = _make_arithmetic_op ( operator . sub , cls ) cls . __rsub__ = _make_arithmetic_op ( ops . rsub , cls ) cls . __rpow__ = _make_arithmetic_op ( ops . rpow , cls ) cls . __pow__ = _make_arithmetic_op ( operator . pow , cls ) cls . __truediv__ = _make_arithmetic_op ( operator . truediv , cls ) cls . __rtruediv__ = _make_arithmetic_op ( ops . rtruediv , cls ) # TODO: rmod? rdivmod? cls . __mod__ = _make_arithmetic_op ( operator . mod , cls ) cls . __floordiv__ = _make_arithmetic_op ( operator . floordiv , cls ) cls . __rfloordiv__ = _make_arithmetic_op ( ops . rfloordiv , cls ) cls . __divmod__ = _make_arithmetic_op ( divmod , cls ) cls . __mul__ = _make_arithmetic_op ( operator . mul , cls ) cls . __rmul__ = _make_arithmetic_op ( ops . rmul , cls )\n\n\n        Summary:", "input": "", "output": "Add in numeric methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_methods_unary ( cls ) : def _make_evaluate_unary ( op , opstr ) : def _evaluate_numeric_unary ( self ) : self . _validate_for_numeric_unaryop ( op , opstr ) attrs = self . _get_attributes_dict ( ) attrs = self . _maybe_update_attributes ( attrs ) return Index ( op ( self . values ) , * * attrs ) _evaluate_numeric_unary . __name__ = opstr return _evaluate_numeric_unary cls . __neg__ = _make_evaluate_unary ( operator . neg , '__neg__' ) cls . __pos__ = _make_evaluate_unary ( operator . pos , '__pos__' ) cls . __abs__ = _make_evaluate_unary ( np . abs , '__abs__' ) cls . __inv__ = _make_evaluate_unary ( lambda x : - x , '__inv__' )\n\n\n        Summary:", "input": "", "output": "Add in numeric unary methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_logical_methods ( cls ) : _doc = \"\"\" %(desc)s Parameters ---------- *args These parameters will be passed to numpy.%(outname)s. **kwargs These parameters will be passed to numpy.%(outname)s. Returns ------- %(outname)s : bool or array_like (if axis is specified) A single element array_like may be converted to bool.\"\"\" _index_shared_docs [ 'index_all' ] = dedent ( \"\"\" See Also -------- Index.any : Return whether any element in an Index is True. Series.any : Return whether any element in a Series is True. Series.all : Return whether all elements in a Series are True. Notes ----- Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero. Examples -------- **all** True, because nonzero integers are considered True. >>> pd.Index([1, 2, 3]).all() True False, because ``0`` is considered False. >>> pd.Index([0, 1, 2]).all() False **any** True, because ``1`` is considered True. >>> pd.Index([0, 0, 1]).any() True False, because ``0`` is considered False. >>> pd.Index([0, 0, 0]).any() False \"\"\" ) _index_shared_docs [ 'index_any' ] = dedent ( \"\"\" See Also -------- Index.all : Return whether all elements are True. Series.all : Return whether all elements are True. Notes ----- Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero. Examples -------- >>> index = pd.Index([0, 1, 2]) >>> index.any() True >>> index = pd.Index([0, 0, 0]) >>> index.any() False \"\"\" ) def _make_logical_function ( name , desc , f ) : @ Substitution ( outname = name , desc = desc ) @ Appender ( _index_shared_docs [ 'index_' + name ] ) @ Appender ( _doc ) def logical_func ( self , * args , * * kwargs ) : result = f ( self . values ) if ( isinstance ( result , ( np . ndarray , ABCSeries , Index ) ) and result . ndim == 0 ) : # return NumPy type return result . dtype . type ( result . item ( ) ) else : # pragma: no cover return result logical_func . __name__ = name return logical_func cls . all = _make_logical_function ( 'all' , 'Return whether all elements ' 'are True.' , np . all ) cls . any = _make_logical_function ( 'any' , 'Return whether any element is True.' , np . any )\n\n\n        Summary:", "input": "", "output": "Add in logical methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_grouper ( obj , key = None , axis = 0 , level = None , sort = True , observed = False , mutated = False , validate = True ) : group_axis = obj . _get_axis ( axis ) # validate that the passed single level is compatible with the passed # axis of the object if level is not None : # TODO: These if-block and else-block are almost same. # MultiIndex instance check is removable, but it seems that there are # some processes only for non-MultiIndex in else-block, # eg. `obj.index.name != level`. We have to consider carefully whether # these are applicable for MultiIndex. Even if these are applicable, # we need to check if it makes no side effect to subsequent processes # on the outside of this condition. # (GH 17621) if isinstance ( group_axis , MultiIndex ) : if is_list_like ( level ) and len ( level ) == 1 : level = level [ 0 ] if key is None and is_scalar ( level ) : # Get the level values from group_axis key = group_axis . get_level_values ( level ) level = None else : # allow level to be a length-one list-like object # (e.g., level=[0]) # GH 13901 if is_list_like ( level ) : nlevels = len ( level ) if nlevels == 1 : level = level [ 0 ] elif nlevels == 0 : raise ValueError ( 'No group keys passed!' ) else : raise ValueError ( 'multiple levels only valid with ' 'MultiIndex' ) if isinstance ( level , str ) : if obj . index . name != level : raise ValueError ( 'level name {} is not the name of the ' 'index' . format ( level ) ) elif level > 0 or level < - 1 : raise ValueError ( 'level > 0 or level < -1 only valid with MultiIndex' ) # NOTE: `group_axis` and `group_axis.get_level_values(level)` # are same in this section. level = None key = group_axis # a passed-in Grouper, directly convert if isinstance ( key , Grouper ) : binner , grouper , obj = key . _get_grouper ( obj , validate = False ) if key . key is None : return grouper , [ ] , obj else : return grouper , { key . key } , obj # already have a BaseGrouper, just return it elif isinstance ( key , BaseGrouper ) : return key , [ ] , obj # In the future, a tuple key will always mean an actual key, # not an iterable of keys. In the meantime, we attempt to provide # a warning. We can assume that the user wanted a list of keys when # the key is not in the index. We just have to be careful with # unhashble elements of `key`. Any unhashable elements implies that # they wanted a list of keys. # https://github.com/pandas-dev/pandas/issues/18314 is_tuple = isinstance ( key , tuple ) all_hashable = is_tuple and is_hashable ( key ) if is_tuple : if ( ( all_hashable and key not in obj and set ( key ) . issubset ( obj ) ) or not all_hashable ) : # column names ('a', 'b') -> ['a', 'b'] # arrays like (a, b) -> [a, b] msg = ( \"Interpreting tuple 'by' as a list of keys, rather than \" \"a single key. Use 'by=[...]' instead of 'by=(...)'. In \" \"the future, a tuple will always mean a single key.\" ) warnings . warn ( msg , FutureWarning , stacklevel = 5 ) key = list ( key ) if not isinstance ( key , list ) : keys = [ key ] match_axis_length = False else : keys = key match_axis_length = len ( keys ) == len ( group_axis ) # what are we after, exactly? any_callable = any ( callable ( g ) or isinstance ( g , dict ) for g in keys ) any_groupers = any ( isinstance ( g , Grouper ) for g in keys ) any_arraylike = any ( isinstance ( g , ( list , tuple , Series , Index , np . ndarray ) ) for g in keys ) # is this an index replacement? if ( not any_callable and not any_arraylike and not any_groupers and match_axis_length and level is None ) : if isinstance ( obj , DataFrame ) : all_in_columns_index = all ( g in obj . columns or g in obj . index . names for g in keys ) elif isinstance ( obj , Series ) : all_in_columns_index = all ( g in obj . index . names for g in keys ) if not all_in_columns_index : keys = [ com . asarray_tuplesafe ( keys ) ] if isinstance ( level , ( tuple , list ) ) : if key is None : keys = [ None ] * len ( level ) levels = level else : levels = [ level ] * len ( keys ) groupings = [ ] exclusions = [ ] # if the actual grouper should be obj[key] def is_in_axis ( key ) : if not _is_label_like ( key ) : try : obj . _data . items . get_loc ( key ) except Exception : return False return True # if the grouper is obj[name] def is_in_obj ( gpr ) : try : return id ( gpr ) == id ( obj [ gpr . name ] ) except Exception : return False for i , ( gpr , level ) in enumerate ( zip ( keys , levels ) ) : if is_in_obj ( gpr ) : # df.groupby(df['name']) in_axis , name = True , gpr . name exclusions . append ( name ) elif is_in_axis ( gpr ) : # df.groupby('name') if gpr in obj : if validate : obj . _check_label_or_level_ambiguity ( gpr ) in_axis , name , gpr = True , gpr , obj [ gpr ] exclusions . append ( name ) elif obj . _is_level_reference ( gpr ) : in_axis , name , level , gpr = False , None , gpr , None else : raise KeyError ( gpr ) elif isinstance ( gpr , Grouper ) and gpr . key is not None : # Add key to exclusions exclusions . append ( gpr . key ) in_axis , name = False , None else : in_axis , name = False , None if is_categorical_dtype ( gpr ) and len ( gpr ) != obj . shape [ axis ] : raise ValueError ( ( \"Length of grouper ({len_gpr}) and axis ({len_axis})\" \" must be same length\" . format ( len_gpr = len ( gpr ) , len_axis = obj . shape [ axis ] ) ) ) # create the Grouping # allow us to passing the actual Grouping as the gpr ping = ( Grouping ( group_axis , gpr , obj = obj , name = name , level = level , sort = sort , observed = observed , in_axis = in_axis ) if not isinstance ( gpr , Grouping ) else gpr ) groupings . append ( ping ) if len ( groupings ) == 0 : raise ValueError ( 'No group keys passed!' ) # create the internals grouper grouper = BaseGrouper ( group_axis , groupings , sort = sort , mutated = mutated ) return grouper , exclusions , obj\n\n\n        Summary:", "input": "", "output": "create and return a BaseGrouper which is an internal mapping of how to create the grouper indexers . This may be composed of multiple Grouping objects indicating multiple groupers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_grouper ( self , obj , validate = True ) : self . _set_grouper ( obj ) self . grouper , exclusions , self . obj = _get_grouper ( self . obj , [ self . key ] , axis = self . axis , level = self . level , sort = self . sort , validate = validate ) return self . binner , self . grouper , self . obj\n\n\n        Summary:", "input": "", "output": "Parameters ---------- obj : the subject object validate : boolean default True if True validate the grouper"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_grouper ( self , obj , sort = False ) : if self . key is not None and self . level is not None : raise ValueError ( \"The Grouper cannot specify both a key and a level!\" ) # Keep self.grouper value before overriding if self . _grouper is None : self . _grouper = self . grouper # the key must be a valid info item if self . key is not None : key = self . key # The 'on' is already defined if ( getattr ( self . grouper , 'name' , None ) == key and isinstance ( obj , ABCSeries ) ) : ax = self . _grouper . take ( obj . index ) else : if key not in obj . _info_axis : raise KeyError ( \"The grouper name {0} is not found\" . format ( key ) ) ax = Index ( obj [ key ] , name = key ) else : ax = obj . _get_axis ( self . axis ) if self . level is not None : level = self . level # if a level is given it must be a mi level or # equivalent to the axis name if isinstance ( ax , MultiIndex ) : level = ax . _get_level_number ( level ) ax = Index ( ax . _get_level_values ( level ) , name = ax . names [ level ] ) else : if level not in ( 0 , ax . name ) : raise ValueError ( \"The level {0} is not valid\" . format ( level ) ) # possibly sort if ( self . sort or sort ) and not ax . is_monotonic : # use stable sort to support first, last, nth indexer = self . indexer = ax . argsort ( kind = 'mergesort' ) ax = ax . take ( indexer ) obj = obj . _take ( indexer , axis = self . axis , is_copy = False ) self . obj = obj self . grouper = ax return self . grouper\n\n\n        Summary:", "input": "", "output": "given an object and the specifications setup the internal grouper for this particular specification"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_pickle ( obj , path , compression = 'infer' , protocol = pickle . HIGHEST_PROTOCOL ) : path = _stringify_path ( path ) f , fh = _get_handle ( path , 'wb' , compression = compression , is_text = False ) if protocol < 0 : protocol = pickle . HIGHEST_PROTOCOL try : f . write ( pickle . dumps ( obj , protocol = protocol ) ) finally : f . close ( ) for _f in fh : _f . close ( )\n\n\n        Summary:", "input": "", "output": "Pickle ( serialize ) object to file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_pickle ( path , compression = 'infer' ) : path = _stringify_path ( path ) f , fh = _get_handle ( path , 'rb' , compression = compression , is_text = False ) # 1) try standard libary Pickle # 2) try pickle_compat (older pandas version) to handle subclass changes # 3) try pickle_compat with latin1 encoding try : with warnings . catch_warnings ( record = True ) : # We want to silence any warnings about, e.g. moved modules. warnings . simplefilter ( \"ignore\" , Warning ) return pickle . load ( f ) except Exception : # noqa: E722 try : return pc . load ( f , encoding = None ) except Exception : # noqa: E722 return pc . load ( f , encoding = 'latin1' ) finally : f . close ( ) for _f in fh : _f . close ( )\n\n\n        Summary:", "input": "", "output": "Load pickled pandas object ( or any object ) from file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mask_missing ( arr , values_to_mask ) : dtype , values_to_mask = infer_dtype_from_array ( values_to_mask ) try : values_to_mask = np . array ( values_to_mask , dtype = dtype ) except Exception : values_to_mask = np . array ( values_to_mask , dtype = object ) na_mask = isna ( values_to_mask ) nonna = values_to_mask [ ~ na_mask ] mask = None for x in nonna : if mask is None : # numpy elementwise comparison warning if is_numeric_v_string_like ( arr , x ) : mask = False else : mask = arr == x # if x is a string and arr is not, then we get False and we must # expand the mask to size arr.shape if is_scalar ( mask ) : mask = np . zeros ( arr . shape , dtype = bool ) else : # numpy elementwise comparison warning if is_numeric_v_string_like ( arr , x ) : mask |= False else : mask |= arr == x if na_mask . any ( ) : if mask is None : mask = isna ( arr ) else : mask |= isna ( arr ) # GH 21977 if mask is None : mask = np . zeros ( arr . shape , dtype = bool ) return mask\n\n\n        Summary:", "input": "", "output": "Return a masking array of same size / shape as arr with entries equaling any member of values_to_mask set to True"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def interpolate_1d ( xvalues , yvalues , method = 'linear' , limit = None , limit_direction = 'forward' , limit_area = None , fill_value = None , bounds_error = False , order = None , * * kwargs ) : # Treat the original, non-scipy methods first. invalid = isna ( yvalues ) valid = ~ invalid if not valid . any ( ) : # have to call np.asarray(xvalues) since xvalues could be an Index # which can't be mutated result = np . empty_like ( np . asarray ( xvalues ) , dtype = np . float64 ) result . fill ( np . nan ) return result if valid . all ( ) : return yvalues if method == 'time' : if not getattr ( xvalues , 'is_all_dates' , None ) : # if not issubclass(xvalues.dtype.type, np.datetime64): raise ValueError ( 'time-weighted interpolation only works ' 'on Series or DataFrames with a ' 'DatetimeIndex' ) method = 'values' valid_limit_directions = [ 'forward' , 'backward' , 'both' ] limit_direction = limit_direction . lower ( ) if limit_direction not in valid_limit_directions : msg = ( 'Invalid limit_direction: expecting one of {valid!r}, ' 'got {invalid!r}.' ) raise ValueError ( msg . format ( valid = valid_limit_directions , invalid = limit_direction ) ) if limit_area is not None : valid_limit_areas = [ 'inside' , 'outside' ] limit_area = limit_area . lower ( ) if limit_area not in valid_limit_areas : raise ValueError ( 'Invalid limit_area: expecting one of {}, got ' '{}.' . format ( valid_limit_areas , limit_area ) ) # default limit is unlimited GH #16282 if limit is None : # limit = len(xvalues) pass elif not is_integer ( limit ) : raise ValueError ( 'Limit must be an integer' ) elif limit < 1 : raise ValueError ( 'Limit must be greater than 0' ) from pandas import Series ys = Series ( yvalues ) # These are sets of index pointers to invalid values... i.e. {0, 1, etc... all_nans = set ( np . flatnonzero ( invalid ) ) start_nans = set ( range ( ys . first_valid_index ( ) ) ) end_nans = set ( range ( 1 + ys . last_valid_index ( ) , len ( valid ) ) ) mid_nans = all_nans - start_nans - end_nans # Like the sets above, preserve_nans contains indices of invalid values, # but in this case, it is the final set of indices that need to be # preserved as NaN after the interpolation. # For example if limit_direction='forward' then preserve_nans will # contain indices of NaNs at the beginning of the series, and NaNs that # are more than'limit' away from the prior non-NaN. # set preserve_nans based on direction using _interp_limit if limit_direction == 'forward' : preserve_nans = start_nans | set ( _interp_limit ( invalid , limit , 0 ) ) elif limit_direction == 'backward' : preserve_nans = end_nans | set ( _interp_limit ( invalid , 0 , limit ) ) else : # both directions... just use _interp_limit preserve_nans = set ( _interp_limit ( invalid , limit , limit ) ) # if limit_area is set, add either mid or outside indices # to preserve_nans GH #16284 if limit_area == 'inside' : # preserve NaNs on the outside preserve_nans |= start_nans | end_nans elif limit_area == 'outside' : # preserve NaNs on the inside preserve_nans |= mid_nans # sort preserve_nans and covert to list preserve_nans = sorted ( preserve_nans ) xvalues = getattr ( xvalues , 'values' , xvalues ) yvalues = getattr ( yvalues , 'values' , yvalues ) result = yvalues . copy ( ) if method in [ 'linear' , 'time' , 'index' , 'values' ] : if method in ( 'values' , 'index' ) : inds = np . asarray ( xvalues ) # hack for DatetimeIndex, #1646 if needs_i8_conversion ( inds . dtype . type ) : inds = inds . view ( np . int64 ) if inds . dtype == np . object_ : inds = lib . maybe_convert_objects ( inds ) else : inds = xvalues result [ invalid ] = np . interp ( inds [ invalid ] , inds [ valid ] , yvalues [ valid ] ) result [ preserve_nans ] = np . nan return result sp_methods = [ 'nearest' , 'zero' , 'slinear' , 'quadratic' , 'cubic' , 'barycentric' , 'krogh' , 'spline' , 'polynomial' , 'from_derivatives' , 'piecewise_polynomial' , 'pchip' , 'akima' ] if method in sp_methods : inds = np . asarray ( xvalues ) # hack for DatetimeIndex, #1646 if issubclass ( inds . dtype . type , np . datetime64 ) : inds = inds . view ( np . int64 ) result [ invalid ] = _interpolate_scipy_wrapper ( inds [ valid ] , yvalues [ valid ] , inds [ invalid ] , method = method , fill_value = fill_value , bounds_error = bounds_error , order = order , * * kwargs ) result [ preserve_nans ] = np . nan return result\n\n\n        Summary:", "input": "", "output": "Logic for the 1 - d interpolation . The result should be 1 - d inputs xvalues and yvalues will each be 1 - d arrays of the same length ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _interpolate_scipy_wrapper ( x , y , new_x , method , fill_value = None , bounds_error = False , order = None , * * kwargs ) : try : from scipy import interpolate # TODO: Why is DatetimeIndex being imported here? from pandas import DatetimeIndex # noqa except ImportError : raise ImportError ( '{method} interpolation requires SciPy' . format ( method = method ) ) new_x = np . asarray ( new_x ) # ignores some kwargs that could be passed along. alt_methods = { 'barycentric' : interpolate . barycentric_interpolate , 'krogh' : interpolate . krogh_interpolate , 'from_derivatives' : _from_derivatives , 'piecewise_polynomial' : _from_derivatives , } if getattr ( x , 'is_all_dates' , False ) : # GH 5975, scipy.interp1d can't hande datetime64s x , new_x = x . _values . astype ( 'i8' ) , new_x . astype ( 'i8' ) if method == 'pchip' : try : alt_methods [ 'pchip' ] = interpolate . pchip_interpolate except AttributeError : raise ImportError ( \"Your version of Scipy does not support \" \"PCHIP interpolation.\" ) elif method == 'akima' : try : from scipy . interpolate import Akima1DInterpolator # noqa alt_methods [ 'akima' ] = _akima_interpolate except ImportError : raise ImportError ( \"Your version of Scipy does not support \" \"Akima interpolation.\" ) interp1d_methods = [ 'nearest' , 'zero' , 'slinear' , 'quadratic' , 'cubic' , 'polynomial' ] if method in interp1d_methods : if method == 'polynomial' : method = order terp = interpolate . interp1d ( x , y , kind = method , fill_value = fill_value , bounds_error = bounds_error ) new_y = terp ( new_x ) elif method == 'spline' : # GH #10633, #24014 if isna ( order ) or ( order <= 0 ) : raise ValueError ( \"order needs to be specified and greater than 0; \" \"got order: {}\" . format ( order ) ) terp = interpolate . UnivariateSpline ( x , y , k = order , * * kwargs ) new_y = terp ( new_x ) else : # GH 7295: need to be able to write for some reason # in some circumstances: check all three if not x . flags . writeable : x = x . copy ( ) if not y . flags . writeable : y = y . copy ( ) if not new_x . flags . writeable : new_x = new_x . copy ( ) method = alt_methods [ method ] new_y = method ( x , y , new_x , * * kwargs ) return new_y\n\n\n        Summary:", "input": "", "output": "Passed off to scipy . interpolate . interp1d . method is scipy s kind . Returns an array interpolated at new_x . Add any new methods to the list in _clean_interp_method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_derivatives ( xi , yi , x , order = None , der = 0 , extrapolate = False ) : from scipy import interpolate # return the method for compat with scipy version & backwards compat method = interpolate . BPoly . from_derivatives m = method ( xi , yi . reshape ( - 1 , 1 ) , orders = order , extrapolate = extrapolate ) return m ( x )\n\n\n        Summary:", "input": "", "output": "Convenience function for interpolate . BPoly . from_derivatives ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _akima_interpolate ( xi , yi , x , der = 0 , axis = 0 ) : from scipy import interpolate try : P = interpolate . Akima1DInterpolator ( xi , yi , axis = axis ) except TypeError : # Scipy earlier than 0.17.0 missing axis P = interpolate . Akima1DInterpolator ( xi , yi ) if der == 0 : return P ( x ) elif interpolate . _isscalar ( der ) : return P ( x , der = der ) else : return [ P ( x , nu ) for nu in der ]\n\n\n        Summary:", "input": "", "output": "Convenience function for akima interpolation . xi and yi are arrays of values used to approximate some function f with yi = f ( xi ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def interpolate_2d ( values , method = 'pad' , axis = 0 , limit = None , fill_value = None , dtype = None ) : transf = ( lambda x : x ) if axis == 0 else ( lambda x : x . T ) # reshape a 1 dim if needed ndim = values . ndim if values . ndim == 1 : if axis != 0 : # pragma: no cover raise AssertionError ( \"cannot interpolate on a ndim == 1 with \" \"axis != 0\" ) values = values . reshape ( tuple ( ( 1 , ) + values . shape ) ) if fill_value is None : mask = None else : # todo create faster fill func without masking mask = mask_missing ( transf ( values ) , fill_value ) method = clean_fill_method ( method ) if method == 'pad' : values = transf ( pad_2d ( transf ( values ) , limit = limit , mask = mask , dtype = dtype ) ) else : values = transf ( backfill_2d ( transf ( values ) , limit = limit , mask = mask , dtype = dtype ) ) # reshape back if ndim == 1 : values = values [ 0 ] return values\n\n\n        Summary:", "input": "", "output": "Perform an actual interpolation of values values will be make 2 - d if needed fills inplace returns the result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cast_values_for_fillna ( values , dtype ) : # TODO: for int-dtypes we make a copy, but for everything else this # alters the values in-place. Is this intentional? if ( is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) or is_timedelta64_dtype ( dtype ) ) : values = values . view ( np . int64 ) elif is_integer_dtype ( values ) : # NB: this check needs to come after the datetime64 check above values = ensure_float64 ( values ) return values\n\n\n        Summary:", "input": "", "output": "Cast values to a dtype that algos . pad and algos . backfill can handle ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fill_zeros ( result , x , y , name , fill ) : if fill is None or is_float_dtype ( result ) : return result if name . startswith ( ( 'r' , '__r' ) ) : x , y = y , x is_variable_type = ( hasattr ( y , 'dtype' ) or hasattr ( y , 'type' ) ) is_scalar_type = is_scalar ( y ) if not is_variable_type and not is_scalar_type : return result if is_scalar_type : y = np . array ( y ) if is_integer_dtype ( y ) : if ( y == 0 ) . any ( ) : # GH 7325, mask and nans must be broadcastable (also: PR 9308) # Raveling and then reshaping makes np.putmask faster mask = ( ( y == 0 ) & ~ np . isnan ( result ) ) . ravel ( ) shape = result . shape result = result . astype ( 'float64' , copy = False ) . ravel ( ) np . putmask ( result , mask , fill ) # if we have a fill of inf, then sign it correctly # (GH 6178 and PR 9308) if np . isinf ( fill ) : signs = y if name . startswith ( ( 'r' , '__r' ) ) else x signs = np . sign ( signs . astype ( 'float' , copy = False ) ) negative_inf_mask = ( signs . ravel ( ) < 0 ) & mask np . putmask ( result , negative_inf_mask , - fill ) if \"floordiv\" in name : # (PR 9308) nan_mask = ( ( y == 0 ) & ( x == 0 ) ) . ravel ( ) np . putmask ( result , nan_mask , np . nan ) result = result . reshape ( shape ) return result\n\n\n        Summary:", "input": "", "output": "If this is a reversed op then flip x y"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mask_zero_div_zero ( x , y , result , copy = False ) : if is_scalar ( y ) : y = np . array ( y ) zmask = y == 0 if zmask . any ( ) : shape = result . shape nan_mask = ( zmask & ( x == 0 ) ) . ravel ( ) neginf_mask = ( zmask & ( x < 0 ) ) . ravel ( ) posinf_mask = ( zmask & ( x > 0 ) ) . ravel ( ) if nan_mask . any ( ) or neginf_mask . any ( ) or posinf_mask . any ( ) : # Fill negative/0 with -inf, positive/0 with +inf, 0/0 with NaN result = result . astype ( 'float64' , copy = copy ) . ravel ( ) np . putmask ( result , nan_mask , np . nan ) np . putmask ( result , posinf_mask , np . inf ) np . putmask ( result , neginf_mask , - np . inf ) result = result . reshape ( shape ) return result\n\n\n        Summary:", "input": "", "output": "Set results of 0 / 0 or 0 // 0 to np . nan regardless of the dtypes of the numerator or the denominator ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dispatch_missing ( op , left , right , result ) : opstr = '__{opname}__' . format ( opname = op . __name__ ) . replace ( '____' , '__' ) if op in [ operator . truediv , operator . floordiv , getattr ( operator , 'div' , None ) ] : result = mask_zero_div_zero ( left , right , result ) elif op is operator . mod : result = fill_zeros ( result , left , right , opstr , np . nan ) elif op is divmod : res0 = mask_zero_div_zero ( left , right , result [ 0 ] ) res1 = fill_zeros ( result [ 1 ] , left , right , opstr , np . nan ) result = ( res0 , res1 ) return result\n\n\n        Summary:", "input": "", "output": "Fill nulls caused by division by zero casting to a diffferent dtype if necessary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _interp_limit ( invalid , fw_limit , bw_limit ) : # handle forward first; the backward direction is the same except # 1. operate on the reversed array # 2. subtract the returned indices from N - 1 N = len ( invalid ) f_idx = set ( ) b_idx = set ( ) def inner ( invalid , limit ) : limit = min ( limit , N ) windowed = _rolling_window ( invalid , limit + 1 ) . all ( 1 ) idx = ( set ( np . where ( windowed ) [ 0 ] + limit ) | set ( np . where ( ( ~ invalid [ : limit + 1 ] ) . cumsum ( ) == 0 ) [ 0 ] ) ) return idx if fw_limit is not None : if fw_limit == 0 : f_idx = set ( np . where ( invalid ) [ 0 ] ) else : f_idx = inner ( invalid , fw_limit ) if bw_limit is not None : if bw_limit == 0 : # then we don't even need to care about backwards # just use forwards return f_idx else : b_idx = list ( inner ( invalid [ : : - 1 ] , bw_limit ) ) b_idx = set ( N - 1 - np . asarray ( b_idx ) ) if fw_limit == 0 : return b_idx return f_idx & b_idx\n\n\n        Summary:", "input": "", "output": "Get indexers of values that won t be filled because they exceed the limits ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _rolling_window ( a , window ) : # https://stackoverflow.com/a/6811241 shape = a . shape [ : - 1 ] + ( a . shape [ - 1 ] - window + 1 , window ) strides = a . strides + ( a . strides [ - 1 ] , ) return np . lib . stride_tricks . as_strided ( a , shape = shape , strides = strides )\n\n\n        Summary:", "input": "", "output": "[ True True False True False ] 2 - >"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_console_size ( ) : from pandas import get_option display_width = get_option ( 'display.width' ) # deprecated. display_height = get_option ( 'display.max_rows' ) # Consider # interactive shell terminal, can detect term size # interactive non-shell terminal (ipnb/ipqtconsole), cannot detect term # size non-interactive script, should disregard term size # in addition # width,height have default values, but setting to 'None' signals # should use Auto-Detection, But only in interactive shell-terminal. # Simple. yeah. if in_interactive_session ( ) : if in_ipython_frontend ( ) : # sane defaults for interactive non-shell terminal # match default for width,height in config_init from pandas . _config . config import get_default_val terminal_width = get_default_val ( 'display.width' ) terminal_height = get_default_val ( 'display.max_rows' ) else : # pure terminal terminal_width , terminal_height = get_terminal_size ( ) else : terminal_width , terminal_height = None , None # Note if the User sets width/Height to None (auto-detection) # and we're in a script (non-inter), this will return (None,None) # caller needs to deal. return ( display_width or terminal_width , display_height or terminal_height )\n\n\n        Summary:", "input": "", "output": "Return console size as tuple = ( width height ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def in_interactive_session ( ) : from pandas import get_option def check_main ( ) : try : import __main__ as main except ModuleNotFoundError : return get_option ( 'mode.sim_interactive' ) return ( not hasattr ( main , '__file__' ) or get_option ( 'mode.sim_interactive' ) ) try : return __IPYTHON__ or check_main ( ) # noqa except NameError : return check_main ( )\n\n\n        Summary:", "input": "", "output": "check if we re running in an interactive shell"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def recode_for_groupby ( c , sort , observed ) : # we only care about observed values if observed : unique_codes = unique1d ( c . codes ) take_codes = unique_codes [ unique_codes != - 1 ] if c . ordered : take_codes = np . sort ( take_codes ) # we recode according to the uniques categories = c . categories . take ( take_codes ) codes = _recode_for_categories ( c . codes , c . categories , categories ) # return a new categorical that maps our new codes # and categories dtype = CategoricalDtype ( categories , ordered = c . ordered ) return Categorical ( codes , dtype = dtype , fastpath = True ) , c # Already sorted according to c.categories; all is fine if sort : return c , None # sort=False should order groups in as-encountered order (GH-8868) cat = c . unique ( ) # But for groupby to work, all categories should be present, # including those missing from the data (GH-13179), which .unique() # above dropped cat = cat . add_categories ( c . categories [ ~ c . categories . isin ( cat . categories ) ] ) return c . reorder_categories ( cat . categories ) , None\n\n\n        Summary:", "input": "", "output": "Code the categories to ensure we can groupby for categoricals ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def recode_from_groupby ( c , sort , ci ) : # we re-order to the original category orderings if sort : return ci . set_categories ( c . categories ) # we are not sorting, so add unobserved to the end return ci . add_categories ( c . categories [ ~ c . categories . isin ( ci . categories ) ] )\n\n\n        Summary:", "input": "", "output": "Reverse the codes_to_groupby to account for sort / observed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_engine ( engine ) : if engine == 'auto' : engine = get_option ( 'io.parquet.engine' ) if engine == 'auto' : # try engines in this order try : return PyArrowImpl ( ) except ImportError : pass try : return FastParquetImpl ( ) except ImportError : pass raise ImportError ( \"Unable to find a usable engine; \" \"tried using: 'pyarrow', 'fastparquet'.\\n\" \"pyarrow or fastparquet is required for parquet \" \"support\" ) if engine not in [ 'pyarrow' , 'fastparquet' ] : raise ValueError ( \"engine must be one of 'pyarrow', 'fastparquet'\" ) if engine == 'pyarrow' : return PyArrowImpl ( ) elif engine == 'fastparquet' : return FastParquetImpl ( )\n\n\n        Summary:", "input": "", "output": "return our implementation"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_parquet ( df , path , engine = 'auto' , compression = 'snappy' , index = None , partition_cols = None , * * kwargs ) : impl = get_engine ( engine ) return impl . write ( df , path , compression = compression , index = index , partition_cols = partition_cols , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Write a DataFrame to the parquet format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_parquet ( path , engine = 'auto' , columns = None , * * kwargs ) : impl = get_engine ( engine ) return impl . read ( path , columns = columns , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Load a parquet object from the file path returning a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate_bins_generic ( values , binner , closed ) : lenidx = len ( values ) lenbin = len ( binner ) if lenidx <= 0 or lenbin <= 0 : raise ValueError ( \"Invalid length for values or for binner\" ) # check binner fits data if values [ 0 ] < binner [ 0 ] : raise ValueError ( \"Values falls before first bin\" ) if values [ lenidx - 1 ] > binner [ lenbin - 1 ] : raise ValueError ( \"Values falls after last bin\" ) bins = np . empty ( lenbin - 1 , dtype = np . int64 ) j = 0 # index into values bc = 0 # bin count # linear scan, presume nothing about values/binner except that it fits ok for i in range ( 0 , lenbin - 1 ) : r_bin = binner [ i + 1 ] # count values in current bin, advance to next bin while j < lenidx and ( values [ j ] < r_bin or ( closed == 'right' and values [ j ] == r_bin ) ) : j += 1 bins [ bc ] = j bc += 1 return bins\n\n\n        Summary:", "input": "", "output": "Generate bin edge offsets and bin labels for one array using another array which has bin edge values . Both arrays must be sorted ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_iterator ( self , data , axis = 0 ) : splitter = self . _get_splitter ( data , axis = axis ) keys = self . _get_group_keys ( ) for key , ( i , group ) in zip ( keys , splitter ) : yield key , group\n\n\n        Summary:", "input": "", "output": "Groupby iterator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def indices ( self ) : if len ( self . groupings ) == 1 : return self . groupings [ 0 ] . indices else : label_list = [ ping . labels for ping in self . groupings ] keys = [ com . values_from_object ( ping . group_index ) for ping in self . groupings ] return get_indexer_dict ( label_list , keys )\n\n\n        Summary:", "input": "", "output": "dict { group name - > group indices }"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def size ( self ) : ids , _ , ngroup = self . group_info ids = ensure_platform_int ( ids ) if ngroup : out = np . bincount ( ids [ ids != - 1 ] , minlength = ngroup ) else : out = [ ] return Series ( out , index = self . result_index , dtype = 'int64' )\n\n\n        Summary:", "input": "", "output": "Compute group sizes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groups ( self ) : if len ( self . groupings ) == 1 : return self . groupings [ 0 ] . groups else : to_groupby = lzip ( * ( ping . grouper for ping in self . groupings ) ) to_groupby = Index ( to_groupby ) return self . axis . groupby ( to_groupby )\n\n\n        Summary:", "input": "", "output": "dict { group name - > group labels }"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groups ( self ) : # this is mainly for compat # GH 3881 result = { key : value for key , value in zip ( self . binlabels , self . bins ) if key is not NaT } return result\n\n\n        Summary:", "input": "", "output": "dict { group name - > group labels }"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_iterator ( self , data , axis = 0 ) : if isinstance ( data , NDFrame ) : slicer = lambda start , edge : data . _slice ( slice ( start , edge ) , axis = axis ) length = len ( data . axes [ axis ] ) else : slicer = lambda start , edge : data [ slice ( start , edge ) ] length = len ( data ) start = 0 for edge , label in zip ( self . bins , self . binlabels ) : if label is not NaT : yield label , slicer ( start , edge ) start = edge if start < length : yield self . binlabels [ - 1 ] , slicer ( start , None )\n\n\n        Summary:", "input": "", "output": "Groupby iterator"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def json_normalize ( data , record_path = None , meta = None , meta_prefix = None , record_prefix = None , errors = 'raise' , sep = '.' ) : def _pull_field ( js , spec ) : result = js if isinstance ( spec , list ) : for field in spec : result = result [ field ] else : result = result [ spec ] return result if isinstance ( data , list ) and not data : return DataFrame ( ) # A bit of a hackjob if isinstance ( data , dict ) : data = [ data ] if record_path is None : if any ( [ isinstance ( x , dict ) for x in y . values ( ) ] for y in data ) : # naive normalization, this is idempotent for flat records # and potentially will inflate the data considerably for # deeply nested structures: # {VeryLong: { b: 1,c:2}} -> {VeryLong.b:1 ,VeryLong.c:@} # # TODO: handle record value which are lists, at least error # reasonably data = nested_to_record ( data , sep = sep ) return DataFrame ( data ) elif not isinstance ( record_path , list ) : record_path = [ record_path ] if meta is None : meta = [ ] elif not isinstance ( meta , list ) : meta = [ meta ] meta = [ m if isinstance ( m , list ) else [ m ] for m in meta ] # Disastrously inefficient for now records = [ ] lengths = [ ] meta_vals = defaultdict ( list ) if not isinstance ( sep , str ) : sep = str ( sep ) meta_keys = [ sep . join ( val ) for val in meta ] def _recursive_extract ( data , path , seen_meta , level = 0 ) : if isinstance ( data , dict ) : data = [ data ] if len ( path ) > 1 : for obj in data : for val , key in zip ( meta , meta_keys ) : if level + 1 == len ( val ) : seen_meta [ key ] = _pull_field ( obj , val [ - 1 ] ) _recursive_extract ( obj [ path [ 0 ] ] , path [ 1 : ] , seen_meta , level = level + 1 ) else : for obj in data : recs = _pull_field ( obj , path [ 0 ] ) # For repeating the metadata later lengths . append ( len ( recs ) ) for val , key in zip ( meta , meta_keys ) : if level + 1 > len ( val ) : meta_val = seen_meta [ key ] else : try : meta_val = _pull_field ( obj , val [ level : ] ) except KeyError as e : if errors == 'ignore' : meta_val = np . nan else : raise KeyError ( \"Try running with \" \"errors='ignore' as key \" \"{err} is not always present\" . format ( err = e ) ) meta_vals [ key ] . append ( meta_val ) records . extend ( recs ) _recursive_extract ( data , record_path , { } , level = 0 ) result = DataFrame ( records ) if record_prefix is not None : result = result . rename ( columns = lambda x : \"{p}{c}\" . format ( p = record_prefix , c = x ) ) # Data types, a problem for k , v in meta_vals . items ( ) : if meta_prefix is not None : k = meta_prefix + k if k in result : raise ValueError ( 'Conflicting metadata name {name}, ' 'need distinguishing prefix ' . format ( name = k ) ) # forcing dtype to object to avoid the metadata being casted to string result [ k ] = np . array ( v , dtype = object ) . repeat ( lengths ) return result\n\n\n        Summary:", "input": "", "output": "Normalize semi - structured JSON data into a flat table ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lreshape ( data , groups , dropna = True , label = None ) : if isinstance ( groups , dict ) : keys = list ( groups . keys ( ) ) values = list ( groups . values ( ) ) else : keys , values = zip ( * groups ) all_cols = list ( set . union ( * [ set ( x ) for x in values ] ) ) id_cols = list ( data . columns . difference ( all_cols ) ) K = len ( values [ 0 ] ) for seq in values : if len ( seq ) != K : raise ValueError ( 'All column lists must be same length' ) mdata = { } pivot_cols = [ ] for target , names in zip ( keys , values ) : to_concat = [ data [ col ] . values for col in names ] import pandas . core . dtypes . concat as _concat mdata [ target ] = _concat . _concat_compat ( to_concat ) pivot_cols . append ( target ) for col in id_cols : mdata [ col ] = np . tile ( data [ col ] . values , K ) if dropna : mask = np . ones ( len ( mdata [ pivot_cols [ 0 ] ] ) , dtype = bool ) for c in pivot_cols : mask &= notna ( mdata [ c ] ) if not mask . all ( ) : mdata = { k : v [ mask ] for k , v in mdata . items ( ) } return data . _constructor ( mdata , columns = id_cols + pivot_cols )\n\n\n        Summary:", "input": "", "output": "Reshape long - format data to wide . Generalized inverse of DataFrame . pivot"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def wide_to_long ( df , stubnames , i , j , sep = \"\" , suffix = r'\\d+' ) : def get_var_names ( df , stub , sep , suffix ) : regex = r'^{stub}{sep}{suffix}$' . format ( stub = re . escape ( stub ) , sep = re . escape ( sep ) , suffix = suffix ) pattern = re . compile ( regex ) return [ col for col in df . columns if pattern . match ( col ) ] def melt_stub ( df , stub , i , j , value_vars , sep ) : newdf = melt ( df , id_vars = i , value_vars = value_vars , value_name = stub . rstrip ( sep ) , var_name = j ) newdf [ j ] = Categorical ( newdf [ j ] ) newdf [ j ] = newdf [ j ] . str . replace ( re . escape ( stub + sep ) , \"\" ) # GH17627 Cast numerics suffixes to int/float newdf [ j ] = to_numeric ( newdf [ j ] , errors = 'ignore' ) return newdf . set_index ( i + [ j ] ) if not is_list_like ( stubnames ) : stubnames = [ stubnames ] else : stubnames = list ( stubnames ) if any ( col in stubnames for col in df . columns ) : raise ValueError ( \"stubname can't be identical to a column name\" ) if not is_list_like ( i ) : i = [ i ] else : i = list ( i ) if df [ i ] . duplicated ( ) . any ( ) : raise ValueError ( \"the id variables need to uniquely identify each row\" ) value_vars = [ get_var_names ( df , stub , sep , suffix ) for stub in stubnames ] value_vars_flattened = [ e for sublist in value_vars for e in sublist ] id_vars = list ( set ( df . columns . tolist ( ) ) . difference ( value_vars_flattened ) ) melted = [ melt_stub ( df , s , i , j , v , sep ) for s , v in zip ( stubnames , value_vars ) ] melted = melted [ 0 ] . join ( melted [ 1 : ] , how = 'outer' ) if len ( i ) == 1 : new = df [ id_vars ] . set_index ( i ) . join ( melted ) return new new = df [ id_vars ] . merge ( melted . reset_index ( ) , on = i ) . set_index ( i + [ j ] ) return new\n\n\n        Summary:", "input": "", "output": "r Wide panel to long format . Less flexible but more user - friendly than melt ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_indices ( self , names ) : def get_converter ( s ) : # possibly convert to the actual key types # in the indices, could be a Timestamp or a np.datetime64 if isinstance ( s , ( Timestamp , datetime . datetime ) ) : return lambda key : Timestamp ( key ) elif isinstance ( s , np . datetime64 ) : return lambda key : Timestamp ( key ) . asm8 else : return lambda key : key if len ( names ) == 0 : return [ ] if len ( self . indices ) > 0 : index_sample = next ( iter ( self . indices ) ) else : index_sample = None # Dummy sample name_sample = names [ 0 ] if isinstance ( index_sample , tuple ) : if not isinstance ( name_sample , tuple ) : msg = ( \"must supply a tuple to get_group with multiple\" \" grouping keys\" ) raise ValueError ( msg ) if not len ( name_sample ) == len ( index_sample ) : try : # If the original grouper was a tuple return [ self . indices [ name ] for name in names ] except KeyError : # turns out it wasn't a tuple msg = ( \"must supply a same-length tuple to get_group\" \" with multiple grouping keys\" ) raise ValueError ( msg ) converters = [ get_converter ( s ) for s in index_sample ] names = ( tuple ( f ( n ) for f , n in zip ( converters , name ) ) for name in names ) else : converter = get_converter ( index_sample ) names = ( converter ( name ) for name in names ) return [ self . indices . get ( name , [ ] ) for name in names ]\n\n\n        Summary:", "input": "", "output": "Safe get multiple indices translate keys for datelike to underlying repr ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_group_selection ( self ) : grp = self . grouper if not ( self . as_index and getattr ( grp , 'groupings' , None ) is not None and self . obj . ndim > 1 and self . _group_selection is None ) : return ax = self . obj . _info_axis groupers = [ g . name for g in grp . groupings if g . level is None and g . in_axis ] if len ( groupers ) : # GH12839 clear selected obj cache when group selection changes self . _group_selection = ax . difference ( Index ( groupers ) , sort = False ) . tolist ( ) self . _reset_cache ( '_selected_obj' )\n\n\n        Summary:", "input": "", "output": "Create group based selection ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_group ( self , name , obj = None ) : if obj is None : obj = self . _selected_obj inds = self . _get_index ( name ) if not len ( inds ) : raise KeyError ( name ) return obj . _take ( inds , axis = self . axis )\n\n\n        Summary:", "input": "", "output": "Construct NDFrame from group with provided name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cumcount_array ( self , ascending = True ) : ids , _ , ngroups = self . grouper . group_info sorter = get_group_index_sorter ( ids , ngroups ) ids , count = ids [ sorter ] , len ( ids ) if count == 0 : return np . empty ( 0 , dtype = np . int64 ) run = np . r_ [ True , ids [ : - 1 ] != ids [ 1 : ] ] rep = np . diff ( np . r_ [ np . nonzero ( run ) [ 0 ] , count ] ) out = ( ~ run ) . cumsum ( ) if ascending : out -= np . repeat ( out [ run ] , rep ) else : out = np . repeat ( out [ np . r_ [ run [ 1 : ] , True ] ] , rep ) - out rev = np . empty ( count , dtype = np . intp ) rev [ sorter ] = np . arange ( count , dtype = np . intp ) return out [ rev ] . astype ( np . int64 , copy = False )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- ascending : bool default True If False number in reverse from length of group - 1 to 0 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_cast ( self , result , obj , numeric_only = False ) : if obj . ndim > 1 : dtype = obj . _values . dtype else : dtype = obj . dtype if not is_scalar ( result ) : if is_datetime64tz_dtype ( dtype ) : # GH 23683 # Prior results _may_ have been generated in UTC. # Ensure we localize to UTC first before converting # to the target timezone try : result = obj . _values . _from_sequence ( result , dtype = 'datetime64[ns, UTC]' ) result = result . astype ( dtype ) except TypeError : # _try_cast was called at a point where the result # was already tz-aware pass elif is_extension_array_dtype ( dtype ) : # The function can return something of any type, so check # if the type is compatible with the calling EA. try : result = obj . _values . _from_sequence ( result , dtype = dtype ) except Exception : # https://github.com/pandas-dev/pandas/issues/22850 # pandas has no control over what 3rd-party ExtensionArrays # do in _values_from_sequence. We still want ops to work # though, so we catch any regular Exception. pass elif numeric_only and is_numeric_dtype ( dtype ) or not numeric_only : result = maybe_downcast_to_dtype ( result , dtype ) return result\n\n\n        Summary:", "input": "", "output": "Try to cast the result to our obj original type we may have roundtripped through object in the mean - time ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _transform_should_cast ( self , func_nm ) : return ( self . size ( ) . fillna ( 0 ) > 0 ) . any ( ) and ( func_nm not in base . cython_cast_blacklist )\n\n\n        Summary:", "input": "", "output": "Parameters : ----------- func_nm : str The name of the aggregation function being performed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _bool_agg ( self , val_test , skipna ) : def objs_to_bool ( vals : np . ndarray ) -> Tuple [ np . ndarray , Type ] : if is_object_dtype ( vals ) : vals = np . array ( [ bool ( x ) for x in vals ] ) else : vals = vals . astype ( np . bool ) return vals . view ( np . uint8 ) , np . bool def result_to_bool ( result : np . ndarray , inference : Type ) -> np . ndarray : return result . astype ( inference , copy = False ) return self . _get_cythonized_result ( 'group_any_all' , self . grouper , aggregate = True , cython_dtype = np . uint8 , needs_values = True , needs_mask = True , pre_processing = objs_to_bool , post_processing = result_to_bool , val_test = val_test , skipna = skipna )\n\n\n        Summary:", "input": "", "output": "Shared func to call any / all Cython GroupBy implementations ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mean ( self , * args , * * kwargs ) : nv . validate_groupby_func ( 'mean' , args , kwargs , [ 'numeric_only' ] ) try : return self . _cython_agg_general ( 'mean' , * * kwargs ) except GroupByError : raise except Exception : # pragma: no cover with _group_selection_context ( self ) : f = lambda x : x . mean ( axis = self . axis , * * kwargs ) return self . _python_agg_general ( f )\n\n\n        Summary:", "input": "", "output": "Compute mean of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def median ( self , * * kwargs ) : try : return self . _cython_agg_general ( 'median' , * * kwargs ) except GroupByError : raise except Exception : # pragma: no cover def f ( x ) : if isinstance ( x , np . ndarray ) : x = Series ( x ) return x . median ( axis = self . axis , * * kwargs ) with _group_selection_context ( self ) : return self . _python_agg_general ( f )\n\n\n        Summary:", "input": "", "output": "Compute median of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def std ( self , ddof = 1 , * args , * * kwargs ) : # TODO: implement at Cython level? nv . validate_groupby_func ( 'std' , args , kwargs ) return np . sqrt ( self . var ( ddof = ddof , * * kwargs ) )\n\n\n        Summary:", "input": "", "output": "Compute standard deviation of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def var ( self , ddof = 1 , * args , * * kwargs ) : nv . validate_groupby_func ( 'var' , args , kwargs ) if ddof == 1 : try : return self . _cython_agg_general ( 'var' , * * kwargs ) except Exception : f = lambda x : x . var ( ddof = ddof , * * kwargs ) with _group_selection_context ( self ) : return self . _python_agg_general ( f ) else : f = lambda x : x . var ( ddof = ddof , * * kwargs ) with _group_selection_context ( self ) : return self . _python_agg_general ( f )\n\n\n        Summary:", "input": "", "output": "Compute variance of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sem ( self , ddof = 1 ) : return self . std ( ddof = ddof ) / np . sqrt ( self . count ( ) )\n\n\n        Summary:", "input": "", "output": "Compute standard error of the mean of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def size ( self ) : result = self . grouper . size ( ) if isinstance ( self . obj , Series ) : result . name = getattr ( self . obj , 'name' , None ) return result\n\n\n        Summary:", "input": "", "output": "Compute group sizes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_operations ( cls ) : def groupby_function ( name , alias , npfunc , numeric_only = True , _convert = False , min_count = - 1 ) : _local_template = \"Compute %(f)s of group values\" @ Substitution ( name = 'groupby' , f = name ) @ Appender ( _common_see_also ) @ Appender ( _local_template ) def f ( self , * * kwargs ) : if 'numeric_only' not in kwargs : kwargs [ 'numeric_only' ] = numeric_only if 'min_count' not in kwargs : kwargs [ 'min_count' ] = min_count self . _set_group_selection ( ) try : return self . _cython_agg_general ( alias , alt = npfunc , * * kwargs ) except AssertionError as e : raise SpecificationError ( str ( e ) ) except Exception : result = self . aggregate ( lambda x : npfunc ( x , axis = self . axis ) ) if _convert : result = result . _convert ( datetime = True ) return result set_function_name ( f , name , cls ) return f def first_compat ( x , axis = 0 ) : def first ( x ) : x = x . to_numpy ( ) x = x [ notna ( x ) ] if len ( x ) == 0 : return np . nan return x [ 0 ] if isinstance ( x , DataFrame ) : return x . apply ( first , axis = axis ) else : return first ( x ) def last_compat ( x , axis = 0 ) : def last ( x ) : x = x . to_numpy ( ) x = x [ notna ( x ) ] if len ( x ) == 0 : return np . nan return x [ - 1 ] if isinstance ( x , DataFrame ) : return x . apply ( last , axis = axis ) else : return last ( x ) cls . sum = groupby_function ( 'sum' , 'add' , np . sum , min_count = 0 ) cls . prod = groupby_function ( 'prod' , 'prod' , np . prod , min_count = 0 ) cls . min = groupby_function ( 'min' , 'min' , np . min , numeric_only = False ) cls . max = groupby_function ( 'max' , 'max' , np . max , numeric_only = False ) cls . first = groupby_function ( 'first' , 'first' , first_compat , numeric_only = False ) cls . last = groupby_function ( 'last' , 'last' , last_compat , numeric_only = False )\n\n\n        Summary:", "input": "", "output": "Add numeric operations to the GroupBy generically ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def resample ( self , rule , * args , * * kwargs ) : from pandas . core . resample import get_resampler_for_grouping return get_resampler_for_grouping ( self , rule , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Provide resampling when using a TimeGrouper ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rolling ( self , * args , * * kwargs ) : from pandas . core . window import RollingGroupby return RollingGroupby ( self , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return a rolling grouper providing rolling functionality per group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def expanding ( self , * args , * * kwargs ) : from pandas . core . window import ExpandingGroupby return ExpandingGroupby ( self , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return an expanding grouper providing expanding functionality per group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _fill ( self , direction , limit = None ) : # Need int value for Cython if limit is None : limit = - 1 return self . _get_cythonized_result ( 'group_fillna_indexer' , self . grouper , needs_mask = True , cython_dtype = np . int64 , result_is_index = True , direction = direction , limit = limit )\n\n\n        Summary:", "input": "", "output": "Shared function for pad and backfill to call Cython method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nth ( self , n , dropna = None ) : if isinstance ( n , int ) : nth_values = [ n ] elif isinstance ( n , ( set , list , tuple ) ) : nth_values = list ( set ( n ) ) if dropna is not None : raise ValueError ( \"dropna option with a list of nth values is not supported\" ) else : raise TypeError ( \"n needs to be an int or a list/set/tuple of ints\" ) nth_values = np . array ( nth_values , dtype = np . intp ) self . _set_group_selection ( ) if not dropna : mask_left = np . in1d ( self . _cumcount_array ( ) , nth_values ) mask_right = np . in1d ( self . _cumcount_array ( ascending = False ) + 1 , - nth_values ) mask = mask_left | mask_right out = self . _selected_obj [ mask ] if not self . as_index : return out ids , _ , _ = self . grouper . group_info out . index = self . grouper . result_index [ ids [ mask ] ] return out . sort_index ( ) if self . sort else out if dropna not in [ 'any' , 'all' ] : if isinstance ( self . _selected_obj , Series ) and dropna is True : warnings . warn ( \"the dropna={dropna} keyword is deprecated,\" \"use dropna='all' instead. \" \"For a Series groupby, dropna must be \" \"either None, 'any' or 'all'.\" . format ( dropna = dropna ) , FutureWarning , stacklevel = 2 ) dropna = 'all' else : # Note: when agg-ing picker doesn't raise this, # just returns NaN raise ValueError ( \"For a DataFrame groupby, dropna must be \" \"either None, 'any' or 'all', \" \"(was passed {dropna}).\" . format ( dropna = dropna ) ) # old behaviour, but with all and any support for DataFrames. # modified in GH 7559 to have better perf max_len = n if n >= 0 else - 1 - n dropped = self . obj . dropna ( how = dropna , axis = self . axis ) # get a new grouper for our dropped obj if self . keys is None and self . level is None : # we don't have the grouper info available # (e.g. we have selected out # a column that is not in the current object) axis = self . grouper . axis grouper = axis [ axis . isin ( dropped . index ) ] else : # create a grouper with the original parameters, but on the dropped # object from pandas . core . groupby . grouper import _get_grouper grouper , _ , _ = _get_grouper ( dropped , key = self . keys , axis = self . axis , level = self . level , sort = self . sort , mutated = self . mutated ) grb = dropped . groupby ( grouper , as_index = self . as_index , sort = self . sort ) sizes , result = grb . size ( ) , grb . nth ( n ) mask = ( sizes < max_len ) . values # set the results which don't meet the criteria if len ( result ) and mask . any ( ) : result . loc [ mask ] = np . nan # reset/reindex to the original groups if ( len ( self . obj ) == len ( dropped ) or len ( result ) == len ( self . grouper . result_index ) ) : result . index = self . grouper . result_index else : result = result . reindex ( self . grouper . result_index ) return result\n\n\n        Summary:", "input": "", "output": "Take the nth row from each group if n is an int or a subset of rows if n is a list of ints ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def quantile ( self , q = 0.5 , interpolation = 'linear' ) : def pre_processor ( vals : np . ndarray ) -> Tuple [ np . ndarray , Optional [ Type ] ] : if is_object_dtype ( vals ) : raise TypeError ( \"'quantile' cannot be performed against \" \"'object' dtypes!\" ) inference = None if is_integer_dtype ( vals ) : inference = np . int64 elif is_datetime64_dtype ( vals ) : inference = 'datetime64[ns]' vals = vals . astype ( np . float ) return vals , inference def post_processor ( vals : np . ndarray , inference : Optional [ Type ] ) -> np . ndarray : if inference : # Check for edge case if not ( is_integer_dtype ( inference ) and interpolation in { 'linear' , 'midpoint' } ) : vals = vals . astype ( inference ) return vals return self . _get_cythonized_result ( 'group_quantile' , self . grouper , aggregate = True , needs_values = True , needs_mask = True , cython_dtype = np . float64 , pre_processing = pre_processor , post_processing = post_processor , q = q , interpolation = interpolation )\n\n\n        Summary:", "input": "", "output": "Return group values at the given quantile a la numpy . percentile ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ngroup ( self , ascending = True ) : with _group_selection_context ( self ) : index = self . _selected_obj . index result = Series ( self . grouper . group_info [ 0 ] , index ) if not ascending : result = self . ngroups - 1 - result return result\n\n\n        Summary:", "input": "", "output": "Number each group from 0 to the number of groups - 1 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cumcount ( self , ascending = True ) : with _group_selection_context ( self ) : index = self . _selected_obj . index cumcounts = self . _cumcount_array ( ascending = ascending ) return Series ( cumcounts , index )\n\n\n        Summary:", "input": "", "output": "Number each item in each group from 0 to the length of that group - 1 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rank ( self , method = 'average' , ascending = True , na_option = 'keep' , pct = False , axis = 0 ) : if na_option not in { 'keep' , 'top' , 'bottom' } : msg = \"na_option must be one of 'keep', 'top', or 'bottom'\" raise ValueError ( msg ) return self . _cython_transform ( 'rank' , numeric_only = False , ties_method = method , ascending = ascending , na_option = na_option , pct = pct , axis = axis )\n\n\n        Summary:", "input": "", "output": "Provide the rank of values within each group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cumprod ( self , axis = 0 , * args , * * kwargs ) : nv . validate_groupby_func ( 'cumprod' , args , kwargs , [ 'numeric_only' , 'skipna' ] ) if axis != 0 : return self . apply ( lambda x : x . cumprod ( axis = axis , * * kwargs ) ) return self . _cython_transform ( 'cumprod' , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Cumulative product for each group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cummin ( self , axis = 0 , * * kwargs ) : if axis != 0 : return self . apply ( lambda x : np . minimum . accumulate ( x , axis ) ) return self . _cython_transform ( 'cummin' , numeric_only = False )\n\n\n        Summary:", "input": "", "output": "Cumulative min for each group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cummax ( self , axis = 0 , * * kwargs ) : if axis != 0 : return self . apply ( lambda x : np . maximum . accumulate ( x , axis ) ) return self . _cython_transform ( 'cummax' , numeric_only = False )\n\n\n        Summary:", "input": "", "output": "Cumulative max for each group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_cythonized_result ( self , how , grouper , aggregate = False , cython_dtype = None , needs_values = False , needs_mask = False , needs_ngroups = False , result_is_index = False , pre_processing = None , post_processing = None , * * kwargs ) : if result_is_index and aggregate : raise ValueError ( \"'result_is_index' and 'aggregate' cannot both \" \"be True!\" ) if post_processing : if not callable ( pre_processing ) : raise ValueError ( \"'post_processing' must be a callable!\" ) if pre_processing : if not callable ( pre_processing ) : raise ValueError ( \"'pre_processing' must be a callable!\" ) if not needs_values : raise ValueError ( \"Cannot use 'pre_processing' without \" \"specifying 'needs_values'!\" ) labels , _ , ngroups = grouper . group_info output = collections . OrderedDict ( ) base_func = getattr ( libgroupby , how ) for name , obj in self . _iterate_slices ( ) : if aggregate : result_sz = ngroups else : result_sz = len ( obj . values ) if not cython_dtype : cython_dtype = obj . values . dtype result = np . zeros ( result_sz , dtype = cython_dtype ) func = partial ( base_func , result , labels ) inferences = None if needs_values : vals = obj . values if pre_processing : vals , inferences = pre_processing ( vals ) func = partial ( func , vals ) if needs_mask : mask = isna ( obj . values ) . view ( np . uint8 ) func = partial ( func , mask ) if needs_ngroups : func = partial ( func , ngroups ) func ( * * kwargs ) # Call func to modify indexer values in place if result_is_index : result = algorithms . take_nd ( obj . values , result ) if post_processing : result = post_processing ( result , inferences ) output [ name ] = result if aggregate : return self . _wrap_aggregated_output ( output ) else : return self . _wrap_transformed_output ( output )\n\n\n        Summary:", "input": "", "output": "Get result for Cythonized functions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods = 1 , freq = None , axis = 0 , fill_value = None ) : if freq is not None or axis != 0 or not isna ( fill_value ) : return self . apply ( lambda x : x . shift ( periods , freq , axis , fill_value ) ) return self . _get_cythonized_result ( 'group_shift_indexer' , self . grouper , cython_dtype = np . int64 , needs_ngroups = True , result_is_index = True , periods = periods )\n\n\n        Summary:", "input": "", "output": "Shift each group by periods observations ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def head ( self , n = 5 ) : self . _reset_group_selection ( ) mask = self . _cumcount_array ( ) < n return self . _selected_obj [ mask ]\n\n\n        Summary:", "input": "", "output": "Return first n rows of each group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tail ( self , n = 5 ) : self . _reset_group_selection ( ) mask = self . _cumcount_array ( ascending = False ) < n return self . _selected_obj [ mask ]\n\n\n        Summary:", "input": "", "output": "Return last n rows of each group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def next_monday ( dt ) : if dt . weekday ( ) == 5 : return dt + timedelta ( 2 ) elif dt . weekday ( ) == 6 : return dt + timedelta ( 1 ) return dt\n\n\n        Summary:", "input": "", "output": "If holiday falls on Saturday use following Monday instead ; if holiday falls on Sunday use Monday instead"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def next_monday_or_tuesday ( dt ) : dow = dt . weekday ( ) if dow == 5 or dow == 6 : return dt + timedelta ( 2 ) elif dow == 0 : return dt + timedelta ( 1 ) return dt\n\n\n        Summary:", "input": "", "output": "For second holiday of two adjacent ones! If holiday falls on Saturday use following Monday instead ; if holiday falls on Sunday or Monday use following Tuesday instead ( because Monday is already taken by adjacent holiday on the day before )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def previous_friday ( dt ) : if dt . weekday ( ) == 5 : return dt - timedelta ( 1 ) elif dt . weekday ( ) == 6 : return dt - timedelta ( 2 ) return dt\n\n\n        Summary:", "input": "", "output": "If holiday falls on Saturday or Sunday use previous Friday instead ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def weekend_to_monday ( dt ) : if dt . weekday ( ) == 6 : return dt + timedelta ( 1 ) elif dt . weekday ( ) == 5 : return dt + timedelta ( 2 ) return dt\n\n\n        Summary:", "input": "", "output": "If holiday falls on Sunday or Saturday use day thereafter ( Monday ) instead . Needed for holidays such as Christmas observation in Europe"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nearest_workday ( dt ) : if dt . weekday ( ) == 5 : return dt - timedelta ( 1 ) elif dt . weekday ( ) == 6 : return dt + timedelta ( 1 ) return dt\n\n\n        Summary:", "input": "", "output": "If holiday falls on Saturday use day before ( Friday ) instead ; if holiday falls on Sunday use day thereafter ( Monday ) instead ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def next_workday ( dt ) : dt += timedelta ( days = 1 ) while dt . weekday ( ) > 4 : # Mon-Fri are 0-4 dt += timedelta ( days = 1 ) return dt\n\n\n        Summary:", "input": "", "output": "returns next weekday used for observances"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def previous_workday ( dt ) : dt -= timedelta ( days = 1 ) while dt . weekday ( ) > 4 : # Mon-Fri are 0-4 dt -= timedelta ( days = 1 ) return dt\n\n\n        Summary:", "input": "", "output": "returns previous weekday used for observances"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dates ( self , start_date , end_date , return_name = False ) : start_date = Timestamp ( start_date ) end_date = Timestamp ( end_date ) filter_start_date = start_date filter_end_date = end_date if self . year is not None : dt = Timestamp ( datetime ( self . year , self . month , self . day ) ) if return_name : return Series ( self . name , index = [ dt ] ) else : return [ dt ] dates = self . _reference_dates ( start_date , end_date ) holiday_dates = self . _apply_rule ( dates ) if self . days_of_week is not None : holiday_dates = holiday_dates [ np . in1d ( holiday_dates . dayofweek , self . days_of_week ) ] if self . start_date is not None : filter_start_date = max ( self . start_date . tz_localize ( filter_start_date . tz ) , filter_start_date ) if self . end_date is not None : filter_end_date = min ( self . end_date . tz_localize ( filter_end_date . tz ) , filter_end_date ) holiday_dates = holiday_dates [ ( holiday_dates >= filter_start_date ) & ( holiday_dates <= filter_end_date ) ] if return_name : return Series ( self . name , index = holiday_dates ) return holiday_dates\n\n\n        Summary:", "input": "", "output": "Calculate holidays observed between start date and end date"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reference_dates ( self , start_date , end_date ) : if self . start_date is not None : start_date = self . start_date . tz_localize ( start_date . tz ) if self . end_date is not None : end_date = self . end_date . tz_localize ( start_date . tz ) year_offset = DateOffset ( years = 1 ) reference_start_date = Timestamp ( datetime ( start_date . year - 1 , self . month , self . day ) ) reference_end_date = Timestamp ( datetime ( end_date . year + 1 , self . month , self . day ) ) # Don't process unnecessary holidays dates = date_range ( start = reference_start_date , end = reference_end_date , freq = year_offset , tz = start_date . tz ) return dates\n\n\n        Summary:", "input": "", "output": "Get reference dates for the holiday ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_rule ( self , dates ) : if self . observance is not None : return dates . map ( lambda d : self . observance ( d ) ) if self . offset is not None : if not isinstance ( self . offset , list ) : offsets = [ self . offset ] else : offsets = self . offset for offset in offsets : # if we are adding a non-vectorized value # ignore the PerformanceWarnings: with warnings . catch_warnings ( ) : warnings . simplefilter ( \"ignore\" , PerformanceWarning ) dates += offset return dates\n\n\n        Summary:", "input": "", "output": "Apply the given offset / observance to a DatetimeIndex of dates ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def holidays ( self , start = None , end = None , return_name = False ) : if self . rules is None : raise Exception ( 'Holiday Calendar {name} does not have any ' 'rules specified' . format ( name = self . name ) ) if start is None : start = AbstractHolidayCalendar . start_date if end is None : end = AbstractHolidayCalendar . end_date start = Timestamp ( start ) end = Timestamp ( end ) holidays = None # If we don't have a cache or the dates are outside the prior cache, we # get them again if ( self . _cache is None or start < self . _cache [ 0 ] or end > self . _cache [ 1 ] ) : for rule in self . rules : rule_holidays = rule . dates ( start , end , return_name = True ) if holidays is None : holidays = rule_holidays else : holidays = holidays . append ( rule_holidays ) self . _cache = ( start , end , holidays . sort_index ( ) ) holidays = self . _cache [ 2 ] holidays = holidays [ start : end ] if return_name : return holidays else : return holidays . index\n\n\n        Summary:", "input": "", "output": "Returns a curve with holidays between start_date and end_date"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def merge_class ( base , other ) : try : other = other . rules except AttributeError : pass if not isinstance ( other , list ) : other = [ other ] other_holidays = { holiday . name : holiday for holiday in other } try : base = base . rules except AttributeError : pass if not isinstance ( base , list ) : base = [ base ] base_holidays = { holiday . name : holiday for holiday in base } other_holidays . update ( base_holidays ) return list ( other_holidays . values ( ) )\n\n\n        Summary:", "input": "", "output": "Merge holiday calendars together . The base calendar will take precedence to other . The merge will be done based on each holiday s name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def merge ( self , other , inplace = False ) : holidays = self . merge_class ( self , other ) if inplace : self . rules = holidays else : return holidays\n\n\n        Summary:", "input": "", "output": "Merge holiday calendars together . The caller s class rules take precedence . The merge will be done based on each holiday s name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def register_option ( key , defval , doc = '' , validator = None , cb = None ) : import tokenize import keyword key = key . lower ( ) if key in _registered_options : msg = \"Option '{key}' has already been registered\" raise OptionError ( msg . format ( key = key ) ) if key in _reserved_keys : msg = \"Option '{key}' is a reserved key\" raise OptionError ( msg . format ( key = key ) ) # the default value should be legal if validator : validator ( defval ) # walk the nested dict, creating dicts as needed along the path path = key . split ( '.' ) for k in path : if not bool ( re . match ( '^' + tokenize . Name + '$' , k ) ) : raise ValueError ( \"{k} is not a valid identifier\" . format ( k = k ) ) if keyword . iskeyword ( k ) : raise ValueError ( \"{k} is a python keyword\" . format ( k = k ) ) cursor = _global_config msg = \"Path prefix to option '{option}' is already an option\" for i , p in enumerate ( path [ : - 1 ] ) : if not isinstance ( cursor , dict ) : raise OptionError ( msg . format ( option = '.' . join ( path [ : i ] ) ) ) if p not in cursor : cursor [ p ] = { } cursor = cursor [ p ] if not isinstance ( cursor , dict ) : raise OptionError ( msg . format ( option = '.' . join ( path [ : - 1 ] ) ) ) cursor [ path [ - 1 ] ] = defval # initialize # save the option metadata _registered_options [ key ] = RegisteredOption ( key = key , defval = defval , doc = doc , validator = validator , cb = cb )\n\n\n        Summary:", "input": "", "output": "Register an option in the package - wide pandas config object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def deprecate_option ( key , msg = None , rkey = None , removal_ver = None ) : key = key . lower ( ) if key in _deprecated_options : msg = \"Option '{key}' has already been defined as deprecated.\" raise OptionError ( msg . format ( key = key ) ) _deprecated_options [ key ] = DeprecatedOption ( key , msg , rkey , removal_ver )\n\n\n        Summary:", "input": "", "output": "Mark option key as deprecated if code attempts to access this option a warning will be produced using msg if given or a default message if not . if rkey is given any access to the key will be re - routed to rkey ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _select_options ( pat ) : # short-circuit for exact key if pat in _registered_options : return [ pat ] # else look through all of them keys = sorted ( _registered_options . keys ( ) ) if pat == 'all' : # reserved key return keys return [ k for k in keys if re . search ( pat , k , re . I ) ]\n\n\n        Summary:", "input": "", "output": "returns a list of keys matching pat"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _translate_key ( key ) : d = _get_deprecated_option ( key ) if d : return d . rkey or key else : return key\n\n\n        Summary:", "input": "", "output": "if key id deprecated and a replacement key defined will return the replacement key otherwise returns key as - is"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _build_option_description ( k ) : o = _get_registered_option ( k ) d = _get_deprecated_option ( k ) s = '{k} ' . format ( k = k ) if o . doc : s += '\\n' . join ( o . doc . strip ( ) . split ( '\\n' ) ) else : s += 'No description available.' if o : s += ( '\\n [default: {default}] [currently: {current}]' . format ( default = o . defval , current = _get_option ( k , True ) ) ) if d : s += '\\n (Deprecated' s += ( ', use `{rkey}` instead.' . format ( rkey = d . rkey if d . rkey else '' ) ) s += ')' return s\n\n\n        Summary:", "input": "", "output": "Builds a formatted description of a registered option and prints it"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def config_prefix ( prefix ) : # Note: reset_option relies on set_option, and on key directly # it does not fit in to this monkey-patching scheme global register_option , get_option , set_option , reset_option def wrap ( func ) : def inner ( key , * args , * * kwds ) : pkey = '{prefix}.{key}' . format ( prefix = prefix , key = key ) return func ( pkey , * args , * * kwds ) return inner _register_option = register_option _get_option = get_option _set_option = set_option set_option = wrap ( set_option ) get_option = wrap ( get_option ) register_option = wrap ( register_option ) yield None set_option = _set_option get_option = _get_option register_option = _register_option\n\n\n        Summary:", "input": "", "output": "contextmanager for multiple invocations of API with a common prefix"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parse ( self , declarations_str ) : for decl in declarations_str . split ( ';' ) : if not decl . strip ( ) : continue prop , sep , val = decl . partition ( ':' ) prop = prop . strip ( ) . lower ( ) # TODO: don't lowercase case sensitive parts of values (strings) val = val . strip ( ) . lower ( ) if sep : yield prop , val else : warnings . warn ( 'Ill-formatted attribute: expected a colon ' 'in {decl!r}' . format ( decl = decl ) , CSSWarning )\n\n\n        Summary:", "input": "", "output": "Generates ( prop value ) pairs from declarations"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array ( data : Sequence [ object ] , dtype : Optional [ Union [ str , np . dtype , ExtensionDtype ] ] = None , copy : bool = True , ) -> ABCExtensionArray : from pandas . core . arrays import ( period_array , ExtensionArray , IntervalArray , PandasArray , DatetimeArray , TimedeltaArray , ) from pandas . core . internals . arrays import extract_array if lib . is_scalar ( data ) : msg = ( \"Cannot pass scalar '{}' to 'pandas.array'.\" ) raise ValueError ( msg . format ( data ) ) data = extract_array ( data , extract_numpy = True ) if dtype is None and isinstance ( data , ExtensionArray ) : dtype = data . dtype # this returns None for not-found dtypes. if isinstance ( dtype , str ) : dtype = registry . find ( dtype ) or dtype if is_extension_array_dtype ( dtype ) : cls = dtype . construct_array_type ( ) return cls . _from_sequence ( data , dtype = dtype , copy = copy ) if dtype is None : inferred_dtype = lib . infer_dtype ( data , skipna = False ) if inferred_dtype == 'period' : try : return period_array ( data , copy = copy ) except tslibs . IncompatibleFrequency : # We may have a mixture of frequencies. # We choose to return an ndarray, rather than raising. pass elif inferred_dtype == 'interval' : try : return IntervalArray ( data , copy = copy ) except ValueError : # We may have a mixture of `closed` here. # We choose to return an ndarray, rather than raising. pass elif inferred_dtype . startswith ( 'datetime' ) : # datetime, datetime64 try : return DatetimeArray . _from_sequence ( data , copy = copy ) except ValueError : # Mixture of timezones, fall back to PandasArray pass elif inferred_dtype . startswith ( 'timedelta' ) : # timedelta, timedelta64 return TimedeltaArray . _from_sequence ( data , copy = copy ) # TODO(BooleanArray): handle this type # Pandas overrides NumPy for # 1. datetime64[ns] # 2. timedelta64[ns] # so that a DatetimeArray is returned. if is_datetime64_ns_dtype ( dtype ) : return DatetimeArray . _from_sequence ( data , dtype = dtype , copy = copy ) elif is_timedelta64_ns_dtype ( dtype ) : return TimedeltaArray . _from_sequence ( data , dtype = dtype , copy = copy ) result = PandasArray . _from_sequence ( data , dtype = dtype , copy = copy ) return result\n\n\n        Summary:", "input": "", "output": "Create an array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_convert_platform_interval ( values ) : if isinstance ( values , ( list , tuple ) ) and len ( values ) == 0 : # GH 19016 # empty lists/tuples get object dtype by default, but this is not # prohibited for IntervalArray, so coerce to integer instead return np . array ( [ ] , dtype = np . int64 ) elif is_categorical_dtype ( values ) : values = np . asarray ( values ) return maybe_convert_platform ( values )\n\n\n        Summary:", "input": "", "output": "Try to do platform conversion with special casing for IntervalArray . Wrapper around maybe_convert_platform that alters the default return dtype in certain cases to be compatible with IntervalArray . For example empty lists return with integer dtype instead of object dtype which is prohibited for IntervalArray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_file_like ( obj ) : if not ( hasattr ( obj , 'read' ) or hasattr ( obj , 'write' ) ) : return False if not hasattr ( obj , \"__iter__\" ) : return False return True\n\n\n        Summary:", "input": "", "output": "Check if the object is a file - like object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_list_like ( obj , allow_sets = True ) : return ( isinstance ( obj , abc . Iterable ) and # we do not count strings/unicode/bytes as list-like not isinstance ( obj , ( str , bytes ) ) and # exclude zero-dimensional numpy arrays, effectively scalars not ( isinstance ( obj , np . ndarray ) and obj . ndim == 0 ) and # exclude sets if allow_sets is False not ( allow_sets is False and isinstance ( obj , abc . Set ) ) )\n\n\n        Summary:", "input": "", "output": "Check if the object is list - like ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_nested_list_like ( obj ) : return ( is_list_like ( obj ) and hasattr ( obj , '__len__' ) and len ( obj ) > 0 and all ( is_list_like ( item ) for item in obj ) )\n\n\n        Summary:", "input": "", "output": "Check if the object is list - like and that all of its elements are also list - like ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_dict_like ( obj ) : dict_like_attrs = ( \"__getitem__\" , \"keys\" , \"__contains__\" ) return ( all ( hasattr ( obj , attr ) for attr in dict_like_attrs ) # [GH 25196] exclude classes and not isinstance ( obj , type ) )\n\n\n        Summary:", "input": "", "output": "Check if the object is dict - like ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_sequence ( obj ) : try : iter ( obj ) # Can iterate over it. len ( obj ) # Has a length associated with it. return not isinstance ( obj , ( str , bytes ) ) except ( TypeError , AttributeError ) : return False\n\n\n        Summary:", "input": "", "output": "Check if the object is a sequence of objects . String types are not included as sequences here ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _new_DatetimeIndex ( cls , d ) : if \"data\" in d and not isinstance ( d [ \"data\" ] , DatetimeIndex ) : # Avoid need to verify integrity by calling simple_new directly data = d . pop ( \"data\" ) result = cls . _simple_new ( data , * * d ) else : with warnings . catch_warnings ( ) : # we ignore warnings from passing verify_integrity=False # TODO: If we knew what was going in to **d, we might be able to # go through _simple_new instead warnings . simplefilter ( \"ignore\" ) result = cls . __new__ ( cls , verify_integrity = False , * * d ) return result\n\n\n        Summary:", "input": "", "output": "This is called upon unpickling rather than the default which doesn t have arguments and breaks __new__"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def date_range ( start = None , end = None , periods = None , freq = None , tz = None , normalize = False , name = None , closed = None , * * kwargs ) : if freq is None and com . _any_none ( periods , start , end ) : freq = 'D' dtarr = DatetimeArray . _generate_range ( start = start , end = end , periods = periods , freq = freq , tz = tz , normalize = normalize , closed = closed , * * kwargs ) return DatetimeIndex . _simple_new ( dtarr , tz = dtarr . tz , freq = dtarr . freq , name = name )\n\n\n        Summary:", "input": "", "output": "Return a fixed frequency DatetimeIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def bdate_range ( start = None , end = None , periods = None , freq = 'B' , tz = None , normalize = True , name = None , weekmask = None , holidays = None , closed = None , * * kwargs ) : if freq is None : msg = 'freq must be specified for bdate_range; use date_range instead' raise TypeError ( msg ) if is_string_like ( freq ) and freq . startswith ( 'C' ) : try : weekmask = weekmask or 'Mon Tue Wed Thu Fri' freq = prefix_mapping [ freq ] ( holidays = holidays , weekmask = weekmask ) except ( KeyError , TypeError ) : msg = 'invalid custom frequency string: {freq}' . format ( freq = freq ) raise ValueError ( msg ) elif holidays or weekmask : msg = ( 'a custom frequency string is required when holidays or ' 'weekmask are passed, got frequency {freq}' ) . format ( freq = freq ) raise ValueError ( msg ) return date_range ( start = start , end = end , periods = periods , freq = freq , tz = tz , normalize = normalize , name = name , closed = closed , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return a fixed frequency DatetimeIndex with business day as the default frequency"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cdate_range ( start = None , end = None , periods = None , freq = 'C' , tz = None , normalize = True , name = None , closed = None , * * kwargs ) : warnings . warn ( \"cdate_range is deprecated and will be removed in a future \" \"version, instead use pd.bdate_range(..., freq='{freq}')\" . format ( freq = freq ) , FutureWarning , stacklevel = 2 ) if freq == 'C' : holidays = kwargs . pop ( 'holidays' , [ ] ) weekmask = kwargs . pop ( 'weekmask' , 'Mon Tue Wed Thu Fri' ) freq = CDay ( holidays = holidays , weekmask = weekmask ) return date_range ( start = start , end = end , periods = periods , freq = freq , tz = tz , normalize = normalize , name = name , closed = closed , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return a fixed frequency DatetimeIndex with CustomBusinessDay as the default frequency"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_blocks ( self ) : obj , index = self . _convert_freq ( ) if index is not None : index = self . _on # filter out the on from the object if self . on is not None : if obj . ndim == 2 : obj = obj . reindex ( columns = obj . columns . difference ( [ self . on ] ) , copy = False ) blocks = obj . _to_dict_of_blocks ( copy = False ) . values ( ) return blocks , obj , index\n\n\n        Summary:", "input": "", "output": "Split data into blocks & return conformed data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gotitem ( self , key , ndim , subset = None ) : # create a new object to prevent aliasing if subset is None : subset = self . obj self = self . _shallow_copy ( subset ) self . _reset_cache ( ) if subset . ndim == 2 : if is_scalar ( key ) and key in subset or is_list_like ( key ) : self . _selection = key return self\n\n\n        Summary:", "input": "", "output": "Sub - classes to define . Return a sliced object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_index ( self , index = None ) : if self . is_freq_type : if index is None : index = self . _on return index , index . asi8 return index , index\n\n\n        Summary:", "input": "", "output": "Return index as ndarrays ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_result ( self , result , block = None , obj = None ) : if obj is None : obj = self . _selected_obj index = obj . index if isinstance ( result , np . ndarray ) : # coerce if necessary if block is not None : if is_timedelta64_dtype ( block . values . dtype ) : from pandas import to_timedelta result = to_timedelta ( result . ravel ( ) , unit = 'ns' ) . values . reshape ( result . shape ) if result . ndim == 1 : from pandas import Series return Series ( result , index , name = obj . name ) return type ( obj ) ( result , index = index , columns = block . columns ) return result\n\n\n        Summary:", "input": "", "output": "Wrap a single result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_results ( self , results , blocks , obj ) : from pandas import Series , concat from pandas . core . index import ensure_index final = [ ] for result , block in zip ( results , blocks ) : result = self . _wrap_result ( result , block = block , obj = obj ) if result . ndim == 1 : return result final . append ( result ) # if we have an 'on' column # we want to put it back into the results # in the same location columns = self . _selected_obj . columns if self . on is not None and not self . _on . equals ( obj . index ) : name = self . _on . name final . append ( Series ( self . _on , index = obj . index , name = name ) ) if self . _selection is not None : selection = ensure_index ( self . _selection ) # need to reorder to include original location of # the on column (if its not already there) if name not in selection : columns = self . obj . columns indexer = columns . get_indexer ( selection . tolist ( ) + [ name ] ) columns = columns . take ( sorted ( indexer ) ) if not len ( final ) : return obj . astype ( 'float64' ) return concat ( final , axis = 1 ) . reindex ( columns = columns , copy = False )\n\n\n        Summary:", "input": "", "output": "Wrap the results ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _center_window ( self , result , window ) : if self . axis > result . ndim - 1 : raise ValueError ( \"Requested axis is larger then no. of argument \" \"dimensions\" ) offset = _offset ( window , True ) if offset > 0 : if isinstance ( result , ( ABCSeries , ABCDataFrame ) ) : result = result . slice_shift ( - offset , axis = self . axis ) else : lead_indexer = [ slice ( None ) ] * result . ndim lead_indexer [ self . axis ] = slice ( offset , None ) result = np . copy ( result [ tuple ( lead_indexer ) ] ) return result\n\n\n        Summary:", "input": "", "output": "Center the result in the window ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _prep_window ( self , * * kwargs ) : window = self . _get_window ( ) if isinstance ( window , ( list , tuple , np . ndarray ) ) : return com . asarray_tuplesafe ( window ) . astype ( float ) elif is_integer ( window ) : import scipy . signal as sig # the below may pop from kwargs def _validate_win_type ( win_type , kwargs ) : arg_map = { 'kaiser' : [ 'beta' ] , 'gaussian' : [ 'std' ] , 'general_gaussian' : [ 'power' , 'width' ] , 'slepian' : [ 'width' ] } if win_type in arg_map : return tuple ( [ win_type ] + _pop_args ( win_type , arg_map [ win_type ] , kwargs ) ) return win_type def _pop_args ( win_type , arg_names , kwargs ) : msg = '%s window requires %%s' % win_type all_args = [ ] for n in arg_names : if n not in kwargs : raise ValueError ( msg % n ) all_args . append ( kwargs . pop ( n ) ) return all_args win_type = _validate_win_type ( self . win_type , kwargs ) # GH #15662. `False` makes symmetric window, rather than periodic. return sig . get_window ( win_type , window , False ) . astype ( float )\n\n\n        Summary:", "input": "", "output": "Provide validation for our window type return the window we have already been validated ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_window ( self , mean = True , * * kwargs ) : window = self . _prep_window ( * * kwargs ) center = self . center blocks , obj , index = self . _create_blocks ( ) results = [ ] for b in blocks : try : values = self . _prep_values ( b . values ) except TypeError : results . append ( b . values . copy ( ) ) continue if values . size == 0 : results . append ( values . copy ( ) ) continue offset = _offset ( window , center ) additional_nans = np . array ( [ np . NaN ] * offset ) def f ( arg , * args , * * kwargs ) : minp = _use_window ( self . min_periods , len ( window ) ) return libwindow . roll_window ( np . concatenate ( ( arg , additional_nans ) ) if center else arg , window , minp , avg = mean ) result = np . apply_along_axis ( f , self . axis , values ) if center : result = self . _center_window ( result , window ) results . append ( result ) return self . _wrap_results ( results , blocks , obj )\n\n\n        Summary:", "input": "", "output": "Applies a moving window of type window_type on the data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply ( self , func , name , window = None , center = None , check_minp = None , * * kwargs ) : def f ( x , name = name , * args ) : x = self . _shallow_copy ( x ) if isinstance ( name , str ) : return getattr ( x , name ) ( * args , * * kwargs ) return x . apply ( name , * args , * * kwargs ) return self . _groupby . apply ( f )\n\n\n        Summary:", "input": "", "output": "Dispatch to apply ; we are stripping all of the _apply kwargs and performing the original function call on the grouped object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply ( self , func , name = None , window = None , center = None , check_minp = None , * * kwargs ) : if center is None : center = self . center if window is None : window = self . _get_window ( ) if check_minp is None : check_minp = _use_window blocks , obj , index = self . _create_blocks ( ) index , indexi = self . _get_index ( index = index ) results = [ ] for b in blocks : values = self . _prep_values ( b . values ) if values . size == 0 : results . append ( values . copy ( ) ) continue # if we have a string function name, wrap it if isinstance ( func , str ) : cfunc = getattr ( libwindow , func , None ) if cfunc is None : raise ValueError ( \"we do not support this function \" \"in libwindow.{func}\" . format ( func = func ) ) def func ( arg , window , min_periods = None , closed = None ) : minp = check_minp ( min_periods , window ) # ensure we are only rolling on floats arg = ensure_float64 ( arg ) return cfunc ( arg , window , minp , indexi , closed , * * kwargs ) # calculation function if center : offset = _offset ( window , center ) additional_nans = np . array ( [ np . NaN ] * offset ) def calc ( x ) : return func ( np . concatenate ( ( x , additional_nans ) ) , window , min_periods = self . min_periods , closed = self . closed ) else : def calc ( x ) : return func ( x , window , min_periods = self . min_periods , closed = self . closed ) with np . errstate ( all = 'ignore' ) : if values . ndim > 1 : result = np . apply_along_axis ( calc , self . axis , values ) else : result = calc ( values ) if center : result = self . _center_window ( result , window ) results . append ( result ) return self . _wrap_results ( results , blocks , obj )\n\n\n        Summary:", "input": "", "output": "Rolling statistical measure using supplied function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_monotonic ( self ) : if not self . _on . is_monotonic : formatted = self . on or 'index' raise ValueError ( \"{0} must be \" \"monotonic\" . format ( formatted ) )\n\n\n        Summary:", "input": "", "output": "Validate on is_monotonic ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_freq ( self ) : from pandas . tseries . frequencies import to_offset try : return to_offset ( self . window ) except ( TypeError , ValueError ) : raise ValueError ( \"passed window {0} is not \" \"compatible with a datetimelike \" \"index\" . format ( self . window ) )\n\n\n        Summary:", "input": "", "output": "Validate & return window frequency ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_window ( self , other = None ) : axis = self . obj . _get_axis ( self . axis ) length = len ( axis ) + ( other is not None ) * len ( axis ) other = self . min_periods or - 1 return max ( length , other )\n\n\n        Summary:", "input": "", "output": "Get the window length over which to perform some operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply ( self , func , * * kwargs ) : blocks , obj , index = self . _create_blocks ( ) results = [ ] for b in blocks : try : values = self . _prep_values ( b . values ) except TypeError : results . append ( b . values . copy ( ) ) continue if values . size == 0 : results . append ( values . copy ( ) ) continue # if we have a string function name, wrap it if isinstance ( func , str ) : cfunc = getattr ( libwindow , func , None ) if cfunc is None : raise ValueError ( \"we do not support this function \" \"in libwindow.{func}\" . format ( func = func ) ) def func ( arg ) : return cfunc ( arg , self . com , int ( self . adjust ) , int ( self . ignore_na ) , int ( self . min_periods ) ) results . append ( np . apply_along_axis ( func , self . axis , values ) ) return self . _wrap_results ( results , blocks , obj )\n\n\n        Summary:", "input": "", "output": "Rolling statistical measure using supplied function . Designed to be used with passed - in Cython array - based functions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mean ( self , * args , * * kwargs ) : nv . validate_window_func ( 'mean' , args , kwargs ) return self . _apply ( 'ewma' , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Exponential weighted moving average ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def std ( self , bias = False , * args , * * kwargs ) : nv . validate_window_func ( 'std' , args , kwargs ) return _zsqrt ( self . var ( bias = bias , * * kwargs ) )\n\n\n        Summary:", "input": "", "output": "Exponential weighted moving stddev ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def var ( self , bias = False , * args , * * kwargs ) : nv . validate_window_func ( 'var' , args , kwargs ) def f ( arg ) : return libwindow . ewmcov ( arg , arg , self . com , int ( self . adjust ) , int ( self . ignore_na ) , int ( self . min_periods ) , int ( bias ) ) return self . _apply ( f , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Exponential weighted moving variance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cov ( self , other = None , pairwise = None , bias = False , * * kwargs ) : if other is None : other = self . _selected_obj # only default unset pairwise = True if pairwise is None else pairwise other = self . _shallow_copy ( other ) def _get_cov ( X , Y ) : X = self . _shallow_copy ( X ) Y = self . _shallow_copy ( Y ) cov = libwindow . ewmcov ( X . _prep_values ( ) , Y . _prep_values ( ) , self . com , int ( self . adjust ) , int ( self . ignore_na ) , int ( self . min_periods ) , int ( bias ) ) return X . _wrap_result ( cov ) return _flex_binary_moment ( self . _selected_obj , other . _selected_obj , _get_cov , pairwise = bool ( pairwise ) )\n\n\n        Summary:", "input": "", "output": "Exponential weighted sample covariance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corr ( self , other = None , pairwise = None , * * kwargs ) : if other is None : other = self . _selected_obj # only default unset pairwise = True if pairwise is None else pairwise other = self . _shallow_copy ( other ) def _get_corr ( X , Y ) : X = self . _shallow_copy ( X ) Y = self . _shallow_copy ( Y ) def _cov ( x , y ) : return libwindow . ewmcov ( x , y , self . com , int ( self . adjust ) , int ( self . ignore_na ) , int ( self . min_periods ) , 1 ) x_values = X . _prep_values ( ) y_values = Y . _prep_values ( ) with np . errstate ( all = 'ignore' ) : cov = _cov ( x_values , y_values ) x_var = _cov ( x_values , x_values ) y_var = _cov ( y_values , y_values ) corr = cov / _zsqrt ( x_var * y_var ) return X . _wrap_result ( corr ) return _flex_binary_moment ( self . _selected_obj , other . _selected_obj , _get_corr , pairwise = bool ( pairwise ) )\n\n\n        Summary:", "input": "", "output": "Exponential weighted sample correlation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_like_indices ( time , panels ) : n_time = len ( time ) n_panel = len ( panels ) u_panels = np . unique ( panels ) # this sorts! u_time = np . unique ( time ) if len ( u_time ) == n_time : time = np . tile ( u_time , len ( u_panels ) ) if len ( u_panels ) == n_panel : panels = np . repeat ( u_panels , len ( u_time ) ) return time , panels\n\n\n        Summary:", "input": "", "output": "Makes sure that time and panels are conformable ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def panel_index ( time , panels , names = None ) : if names is None : names = [ 'time' , 'panel' ] time , panels = _ensure_like_indices ( time , panels ) return MultiIndex . from_arrays ( [ time , panels ] , sortorder = None , names = names )\n\n\n        Summary:", "input": "", "output": "Returns a multi - index suitable for a panel - like DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _init_data ( self , data , copy , dtype , * * kwargs ) : if data is None : data = { } if dtype is not None : dtype = self . _validate_dtype ( dtype ) passed_axes = [ kwargs . pop ( a , None ) for a in self . _AXIS_ORDERS ] if kwargs : raise TypeError ( '_init_data() got an unexpected keyword ' 'argument \"{0}\"' . format ( list ( kwargs . keys ( ) ) [ 0 ] ) ) axes = None if isinstance ( data , BlockManager ) : if com . _any_not_none ( * passed_axes ) : axes = [ x if x is not None else y for x , y in zip ( passed_axes , data . axes ) ] mgr = data elif isinstance ( data , dict ) : mgr = self . _init_dict ( data , passed_axes , dtype = dtype ) copy = False dtype = None elif isinstance ( data , ( np . ndarray , list ) ) : mgr = self . _init_matrix ( data , passed_axes , dtype = dtype , copy = copy ) copy = False dtype = None elif is_scalar ( data ) and com . _all_not_none ( * passed_axes ) : values = cast_scalar_to_array ( [ len ( x ) for x in passed_axes ] , data , dtype = dtype ) mgr = self . _init_matrix ( values , passed_axes , dtype = values . dtype , copy = False ) copy = False else : # pragma: no cover raise ValueError ( 'Panel constructor not properly called!' ) NDFrame . __init__ ( self , mgr , axes = axes , copy = copy , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Generate ND initialization ; axes are passed as required objects to __init__ ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_dict ( cls , data , intersect = False , orient = 'items' , dtype = None ) : from collections import defaultdict orient = orient . lower ( ) if orient == 'minor' : new_data = defaultdict ( OrderedDict ) for col , df in data . items ( ) : for item , s in df . items ( ) : new_data [ item ] [ col ] = s data = new_data elif orient != 'items' : # pragma: no cover raise ValueError ( 'Orientation must be one of {items, minor}.' ) d = cls . _homogenize_dict ( cls , data , intersect = intersect , dtype = dtype ) ks = list ( d [ 'data' ] . keys ( ) ) if not isinstance ( d [ 'data' ] , OrderedDict ) : ks = list ( sorted ( ks ) ) d [ cls . _info_axis_name ] = Index ( ks ) return cls ( * * d )\n\n\n        Summary:", "input": "", "output": "Construct Panel from dict of DataFrame objects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_plane_axes_index ( self , axis ) : axis_name = self . _get_axis_name ( axis ) if axis_name == 'major_axis' : index = 'minor_axis' columns = 'items' if axis_name == 'minor_axis' : index = 'major_axis' columns = 'items' elif axis_name == 'items' : index = 'major_axis' columns = 'minor_axis' return index , columns\n\n\n        Summary:", "input": "", "output": "Get my plane axes indexes : these are already ( as compared with higher level planes ) as we are returning a DataFrame axes indexes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_plane_axes ( self , axis ) : return [ self . _get_axis ( axi ) for axi in self . _get_plane_axes_index ( axis ) ]\n\n\n        Summary:", "input": "", "output": "Get my plane axes indexes : these are already ( as compared with higher level planes ) as we are returning a DataFrame axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_excel ( self , path , na_rep = '' , engine = None , * * kwargs ) : from pandas . io . excel import ExcelWriter if isinstance ( path , str ) : writer = ExcelWriter ( path , engine = engine ) else : writer = path kwargs [ 'na_rep' ] = na_rep for item , df in self . iteritems ( ) : name = str ( item ) df . to_excel ( writer , name , * * kwargs ) writer . save ( )\n\n\n        Summary:", "input": "", "output": "Write each DataFrame in Panel to a separate excel sheet ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_value ( self , * args , * * kwargs ) : warnings . warn ( \"get_value is deprecated and will be removed \" \"in a future release. Please use \" \".at[] or .iat[] accessors instead\" , FutureWarning , stacklevel = 2 ) return self . _get_value ( * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Quickly retrieve single value at ( item major minor ) location ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_value ( self , * args , * * kwargs ) : warnings . warn ( \"set_value is deprecated and will be removed \" \"in a future release. Please use \" \".at[] or .iat[] accessors instead\" , FutureWarning , stacklevel = 2 ) return self . _set_value ( * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Quickly set single value at ( item major minor ) location ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unpickle_panel_compat ( self , state ) : # pragma: no cover from pandas . io . pickle import _unpickle_array _unpickle = _unpickle_array vals , items , major , minor = state items = _unpickle ( items ) major = _unpickle ( major ) minor = _unpickle ( minor ) values = _unpickle ( vals ) wp = Panel ( values , items , major , minor ) self . _data = wp . _data\n\n\n        Summary:", "input": "", "output": "Unpickle the panel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def conform ( self , frame , axis = 'items' ) : axes = self . _get_plane_axes ( axis ) return frame . reindex ( * * self . _extract_axes_for_slice ( self , axes ) )\n\n\n        Summary:", "input": "", "output": "Conform input DataFrame to align with chosen axis pair ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def round ( self , decimals = 0 , * args , * * kwargs ) : nv . validate_round ( args , kwargs ) if is_integer ( decimals ) : result = np . apply_along_axis ( np . round , 0 , self . values ) return self . _wrap_result ( result , axis = 0 ) raise TypeError ( \"decimals must be an integer\" )\n\n\n        Summary:", "input": "", "output": "Round each value in Panel to a specified number of decimal places ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dropna ( self , axis = 0 , how = 'any' , inplace = False ) : axis = self . _get_axis_number ( axis ) values = self . values mask = notna ( values ) for ax in reversed ( sorted ( set ( range ( self . _AXIS_LEN ) ) - { axis } ) ) : mask = mask . sum ( ax ) per_slice = np . prod ( values . shape [ : axis ] + values . shape [ axis + 1 : ] ) if how == 'all' : cond = mask > 0 else : cond = mask == per_slice new_ax = self . _get_axis ( axis ) [ cond ] result = self . reindex_axis ( new_ax , axis = axis ) if inplace : self . _update_inplace ( result ) else : return result\n\n\n        Summary:", "input": "", "output": "Drop 2D from panel holding passed axis constant ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def xs ( self , key , axis = 1 ) : axis = self . _get_axis_number ( axis ) if axis == 0 : return self [ key ] self . _consolidate_inplace ( ) axis_number = self . _get_axis_number ( axis ) new_data = self . _data . xs ( key , axis = axis_number , copy = False ) result = self . _construct_return_type ( new_data ) copy = new_data . is_mixed_type result . _set_is_copy ( self , copy = copy ) return result\n\n\n        Summary:", "input": "", "output": "Return slice of panel along selected axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ixs ( self , i , axis = 0 ) : ax = self . _get_axis ( axis ) key = ax [ i ] # xs cannot handle a non-scalar key, so just reindex here # if we have a multi-index and a single tuple, then its a reduction # (GH 7516) if not ( isinstance ( ax , MultiIndex ) and isinstance ( key , tuple ) ) : if is_list_like ( key ) : indexer = { self . _get_axis_name ( axis ) : key } return self . reindex ( * * indexer ) # a reduction if axis == 0 : values = self . _data . iget ( i ) return self . _box_item_values ( key , values ) # xs by position self . _consolidate_inplace ( ) new_data = self . _data . xs ( i , axis = axis , copy = True , takeable = True ) return self . _construct_return_type ( new_data )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- i : int slice or sequence of integers axis : int"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_frame ( self , filter_observations = True ) : _ , N , K = self . shape if filter_observations : # shaped like the return DataFrame mask = notna ( self . values ) . all ( axis = 0 ) # size = mask.sum() selector = mask . ravel ( ) else : # size = N * K selector = slice ( None , None ) data = { item : self [ item ] . values . ravel ( ) [ selector ] for item in self . items } def construct_multi_parts ( idx , n_repeat , n_shuffle = 1 ) : # Replicates and shuffles MultiIndex, returns individual attributes codes = [ np . repeat ( x , n_repeat ) for x in idx . codes ] # Assumes that each label is divisible by n_shuffle codes = [ x . reshape ( n_shuffle , - 1 ) . ravel ( order = 'F' ) for x in codes ] codes = [ x [ selector ] for x in codes ] levels = idx . levels names = idx . names return codes , levels , names def construct_index_parts ( idx , major = True ) : levels = [ idx ] if major : codes = [ np . arange ( N ) . repeat ( K ) [ selector ] ] names = idx . name or 'major' else : codes = np . arange ( K ) . reshape ( 1 , K ) [ np . zeros ( N , dtype = int ) ] codes = [ codes . ravel ( ) [ selector ] ] names = idx . name or 'minor' names = [ names ] return codes , levels , names if isinstance ( self . major_axis , MultiIndex ) : major_codes , major_levels , major_names = construct_multi_parts ( self . major_axis , n_repeat = K ) else : major_codes , major_levels , major_names = construct_index_parts ( self . major_axis ) if isinstance ( self . minor_axis , MultiIndex ) : minor_codes , minor_levels , minor_names = construct_multi_parts ( self . minor_axis , n_repeat = N , n_shuffle = K ) else : minor_codes , minor_levels , minor_names = construct_index_parts ( self . minor_axis , major = False ) levels = major_levels + minor_levels codes = major_codes + minor_codes names = major_names + minor_names index = MultiIndex ( levels = levels , codes = codes , names = names , verify_integrity = False ) return DataFrame ( data , index = index , columns = self . items )\n\n\n        Summary:", "input": "", "output": "Transform wide format into long ( stacked ) format as DataFrame whose columns are the Panel s items and whose index is a MultiIndex formed of the Panel s major and minor axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , func , axis = 'major' , * * kwargs ) : if kwargs and not isinstance ( func , np . ufunc ) : f = lambda x : func ( x , * * kwargs ) else : f = func # 2d-slabs if isinstance ( axis , ( tuple , list ) ) and len ( axis ) == 2 : return self . _apply_2d ( f , axis = axis ) axis = self . _get_axis_number ( axis ) # try ufunc like if isinstance ( f , np . ufunc ) : try : with np . errstate ( all = 'ignore' ) : result = np . apply_along_axis ( func , axis , self . values ) return self . _wrap_result ( result , axis = axis ) except ( AttributeError ) : pass # 1d return self . _apply_1d ( f , axis = axis )\n\n\n        Summary:", "input": "", "output": "Apply function along axis ( or axes ) of the Panel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_2d ( self , func , axis ) : ndim = self . ndim axis = [ self . _get_axis_number ( a ) for a in axis ] # construct slabs, in 2-d this is a DataFrame result indexer_axis = list ( range ( ndim ) ) for a in axis : indexer_axis . remove ( a ) indexer_axis = indexer_axis [ 0 ] slicer = [ slice ( None , None ) ] * ndim ax = self . _get_axis ( indexer_axis ) results = [ ] for i , e in enumerate ( ax ) : slicer [ indexer_axis ] = i sliced = self . iloc [ tuple ( slicer ) ] obj = func ( sliced ) results . append ( ( e , obj ) ) return self . _construct_return_type ( dict ( results ) )\n\n\n        Summary:", "input": "", "output": "Handle 2 - d slices equiv to iterating over the other axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_return_type ( self , result , axes = None ) : ndim = getattr ( result , 'ndim' , None ) # need to assume they are the same if ndim is None : if isinstance ( result , dict ) : ndim = getattr ( list ( result . values ( ) ) [ 0 ] , 'ndim' , 0 ) # have a dict, so top-level is +1 dim if ndim != 0 : ndim += 1 # scalar if ndim == 0 : return Series ( result ) # same as self elif self . ndim == ndim : # return the construction dictionary for these axes if axes is None : return self . _constructor ( result ) return self . _constructor ( result , * * self . _construct_axes_dict ( ) ) # sliced elif self . ndim == ndim + 1 : if axes is None : return self . _constructor_sliced ( result ) return self . _constructor_sliced ( result , * * self . _extract_axes_for_slice ( self , axes ) ) raise ValueError ( 'invalid _construct_return_type [self->{self}] ' '[result->{result}]' . format ( self = self , result = result ) )\n\n\n        Summary:", "input": "", "output": "Return the type for the ndim of the result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def count ( self , axis = 'major' ) : i = self . _get_axis_number ( axis ) values = self . values mask = np . isfinite ( values ) result = mask . sum ( axis = i , dtype = 'int64' ) return self . _wrap_result ( result , axis )\n\n\n        Summary:", "input": "", "output": "Return number of observations over requested axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods = 1 , freq = None , axis = 'major' ) : if freq : return self . tshift ( periods , freq , axis = axis ) return super ( ) . slice_shift ( periods , axis = axis )\n\n\n        Summary:", "input": "", "output": "Shift index by desired number of periods with an optional time freq ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def join ( self , other , how = 'left' , lsuffix = '' , rsuffix = '' ) : from pandas . core . reshape . concat import concat if isinstance ( other , Panel ) : join_major , join_minor = self . _get_join_index ( other , how ) this = self . reindex ( major = join_major , minor = join_minor ) other = other . reindex ( major = join_major , minor = join_minor ) merged_data = this . _data . merge ( other . _data , lsuffix , rsuffix ) return self . _constructor ( merged_data ) else : if lsuffix or rsuffix : raise ValueError ( 'Suffixes not supported when passing ' 'multiple panels' ) if how == 'left' : how = 'outer' join_axes = [ self . major_axis , self . minor_axis ] elif how == 'right' : raise ValueError ( 'Right join not supported with multiple ' 'panels' ) else : join_axes = None return concat ( [ self ] + list ( other ) , axis = 0 , join = how , join_axes = join_axes , verify_integrity = True )\n\n\n        Summary:", "input": "", "output": "Join items with other Panel either on major and minor axes column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update ( self , other , join = 'left' , overwrite = True , filter_func = None , errors = 'ignore' ) : if not isinstance ( other , self . _constructor ) : other = self . _constructor ( other ) axis_name = self . _info_axis_name axis_values = self . _info_axis other = other . reindex ( * * { axis_name : axis_values } ) for frame in axis_values : self [ frame ] . update ( other [ frame ] , join = join , overwrite = overwrite , filter_func = filter_func , errors = errors )\n\n\n        Summary:", "input": "", "output": "Modify Panel in place using non - NA values from other Panel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _extract_axes ( self , data , axes , * * kwargs ) : return [ self . _extract_axis ( self , data , axis = i , * * kwargs ) for i , a in enumerate ( axes ) ]\n\n\n        Summary:", "input": "", "output": "Return a list of the axis indices ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _extract_axes_for_slice ( self , axes ) : return { self . _AXIS_SLICEMAP [ i ] : a for i , a in zip ( self . _AXIS_ORDERS [ self . _AXIS_LEN - len ( axes ) : ] , axes ) }\n\n\n        Summary:", "input": "", "output": "Return the slice dictionary for these axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _homogenize_dict ( self , frames , intersect = True , dtype = None ) : result = dict ( ) # caller differs dict/ODict, preserved type if isinstance ( frames , OrderedDict ) : result = OrderedDict ( ) adj_frames = OrderedDict ( ) for k , v in frames . items ( ) : if isinstance ( v , dict ) : adj_frames [ k ] = self . _constructor_sliced ( v ) else : adj_frames [ k ] = v axes = self . _AXIS_ORDERS [ 1 : ] axes_dict = { a : ax for a , ax in zip ( axes , self . _extract_axes ( self , adj_frames , axes , intersect = intersect ) ) } reindex_dict = { self . _AXIS_SLICEMAP [ a ] : axes_dict [ a ] for a in axes } reindex_dict [ 'copy' ] = False for key , frame in adj_frames . items ( ) : if frame is not None : result [ key ] = frame . reindex ( * * reindex_dict ) else : result [ key ] = None axes_dict [ 'data' ] = result axes_dict [ 'dtype' ] = dtype return axes_dict\n\n\n        Summary:", "input": "", "output": "Conform set of _constructor_sliced - like objects to either an intersection of indices / columns or a union ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_group_index ( labels , shape , sort , xnull ) : def _int64_cut_off ( shape ) : acc = 1 for i , mul in enumerate ( shape ) : acc *= int ( mul ) if not acc < _INT64_MAX : return i return len ( shape ) def maybe_lift ( lab , size ) : # promote nan values (assigned -1 label in lab array) # so that all output values are non-negative return ( lab + 1 , size + 1 ) if ( lab == - 1 ) . any ( ) else ( lab , size ) labels = map ( ensure_int64 , labels ) if not xnull : labels , shape = map ( list , zip ( * map ( maybe_lift , labels , shape ) ) ) labels = list ( labels ) shape = list ( shape ) # Iteratively process all the labels in chunks sized so less # than _INT64_MAX unique int ids will be required for each chunk while True : # how many levels can be done without overflow: nlev = _int64_cut_off ( shape ) # compute flat ids for the first `nlev` levels stride = np . prod ( shape [ 1 : nlev ] , dtype = 'i8' ) out = stride * labels [ 0 ] . astype ( 'i8' , subok = False , copy = False ) for i in range ( 1 , nlev ) : if shape [ i ] == 0 : stride = 0 else : stride //= shape [ i ] out += labels [ i ] * stride if xnull : # exclude nulls mask = labels [ 0 ] == - 1 for lab in labels [ 1 : nlev ] : mask |= lab == - 1 out [ mask ] = - 1 if nlev == len ( shape ) : # all levels done! break # compress what has been done so far in order to avoid overflow # to retain lexical ranks, obs_ids should be sorted comp_ids , obs_ids = compress_group_index ( out , sort = sort ) labels = [ comp_ids ] + labels [ nlev : ] shape = [ len ( obs_ids ) ] + shape [ nlev : ] return out\n\n\n        Summary:", "input": "", "output": "For the particular label_list gets the offsets into the hypothetical list representing the totally ordered cartesian product of all possible label combinations * as long as * this space fits within int64 bounds ; otherwise though group indices identify unique combinations of labels they cannot be deconstructed . - If sort rank of returned ids preserve lexical ranks of labels . i . e . returned id s can be used to do lexical sort on labels ; - If xnull nulls ( - 1 labels ) are passed through ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def decons_obs_group_ids ( comp_ids , obs_ids , shape , labels , xnull ) : if not xnull : lift = np . fromiter ( ( ( a == - 1 ) . any ( ) for a in labels ) , dtype = 'i8' ) shape = np . asarray ( shape , dtype = 'i8' ) + lift if not is_int64_overflow_possible ( shape ) : # obs ids are deconstructable! take the fast route! out = decons_group_index ( obs_ids , shape ) return out if xnull or not lift . any ( ) else [ x - y for x , y in zip ( out , lift ) ] i = unique_label_indices ( comp_ids ) i8copy = lambda a : a . astype ( 'i8' , subok = False , copy = True ) return [ i8copy ( lab [ i ] ) for lab in labels ]\n\n\n        Summary:", "input": "", "output": "reconstruct labels from observed group ids"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nargsort ( items , kind = 'quicksort' , ascending = True , na_position = 'last' ) : # specially handle Categorical if is_categorical_dtype ( items ) : if na_position not in { 'first' , 'last' } : raise ValueError ( 'invalid na_position: {!r}' . format ( na_position ) ) mask = isna ( items ) cnt_null = mask . sum ( ) sorted_idx = items . argsort ( ascending = ascending , kind = kind ) if ascending and na_position == 'last' : # NaN is coded as -1 and is listed in front after sorting sorted_idx = np . roll ( sorted_idx , - cnt_null ) elif not ascending and na_position == 'first' : # NaN is coded as -1 and is listed in the end after sorting sorted_idx = np . roll ( sorted_idx , cnt_null ) return sorted_idx with warnings . catch_warnings ( ) : # https://github.com/pandas-dev/pandas/issues/25439 # can be removed once ExtensionArrays are properly handled by nargsort warnings . filterwarnings ( \"ignore\" , category = FutureWarning , message = \"Converting timezone-aware DatetimeArray to\" ) items = np . asanyarray ( items ) idx = np . arange ( len ( items ) ) mask = isna ( items ) non_nans = items [ ~ mask ] non_nan_idx = idx [ ~ mask ] nan_idx = np . nonzero ( mask ) [ 0 ] if not ascending : non_nans = non_nans [ : : - 1 ] non_nan_idx = non_nan_idx [ : : - 1 ] indexer = non_nan_idx [ non_nans . argsort ( kind = kind ) ] if not ascending : indexer = indexer [ : : - 1 ] # Finally, place the NaNs at the end or the beginning according to # na_position if na_position == 'last' : indexer = np . concatenate ( [ indexer , nan_idx ] ) elif na_position == 'first' : indexer = np . concatenate ( [ nan_idx , indexer ] ) else : raise ValueError ( 'invalid na_position: {!r}' . format ( na_position ) ) return indexer\n\n\n        Summary:", "input": "", "output": "This is intended to be a drop - in replacement for np . argsort which handles NaNs . It adds ascending and na_position parameters . GH #6399 #5231"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_indexer_dict ( label_list , keys ) : shape = list ( map ( len , keys ) ) group_index = get_group_index ( label_list , shape , sort = True , xnull = True ) ngroups = ( ( group_index . size and group_index . max ( ) ) + 1 ) if is_int64_overflow_possible ( shape ) else np . prod ( shape , dtype = 'i8' ) sorter = get_group_index_sorter ( group_index , ngroups ) sorted_labels = [ lab . take ( sorter ) for lab in label_list ] group_index = group_index . take ( sorter ) return lib . indices_fast ( sorter , group_index , keys , sorted_labels )\n\n\n        Summary:", "input": "", "output": "return a diction of { labels } - > { indexers }"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_group_index_sorter ( group_index , ngroups ) : count = len ( group_index ) alpha = 0.0 # taking complexities literally; there may be beta = 1.0 # some room for fine-tuning these parameters do_groupsort = ( count > 0 and ( ( alpha + beta * ngroups ) < ( count * np . log ( count ) ) ) ) if do_groupsort : sorter , _ = algos . groupsort_indexer ( ensure_int64 ( group_index ) , ngroups ) return ensure_platform_int ( sorter ) else : return group_index . argsort ( kind = 'mergesort' )\n\n\n        Summary:", "input": "", "output": "algos . groupsort_indexer implements counting sort and it is at least O ( ngroups ) where ngroups = prod ( shape ) shape = map ( len keys ) that is linear in the number of combinations ( cartesian product ) of unique values of groupby keys . This can be huge when doing multi - key groupby . np . argsort ( kind = mergesort ) is O ( count x log ( count )) where count is the length of the data - frame ; Both algorithms are stable sort and that is necessary for correctness of groupby operations . e . g . consider : df . groupby ( key ) [ col ] . transform ( first )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def compress_group_index ( group_index , sort = True ) : size_hint = min ( len ( group_index ) , hashtable . _SIZE_HINT_LIMIT ) table = hashtable . Int64HashTable ( size_hint ) group_index = ensure_int64 ( group_index ) # note, group labels come out ascending (ie, 1,2,3 etc) comp_ids , obs_group_ids = table . get_labels_groupby ( group_index ) if sort and len ( obs_group_ids ) > 0 : obs_group_ids , comp_ids = _reorder_by_uniques ( obs_group_ids , comp_ids ) return comp_ids , obs_group_ids\n\n\n        Summary:", "input": "", "output": "Group_index is offsets into cartesian product of all possible labels . This space can be huge so this function compresses it by computing offsets ( comp_ids ) into the list of unique labels ( obs_group_ids ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def safe_sort ( values , labels = None , na_sentinel = - 1 , assume_unique = False ) : if not is_list_like ( values ) : raise TypeError ( \"Only list-like objects are allowed to be passed to\" \"safe_sort as values\" ) if not isinstance ( values , np . ndarray ) : # don't convert to string types dtype , _ = infer_dtype_from_array ( values ) values = np . asarray ( values , dtype = dtype ) def sort_mixed ( values ) : # order ints before strings, safe in py3 str_pos = np . array ( [ isinstance ( x , str ) for x in values ] , dtype = bool ) nums = np . sort ( values [ ~ str_pos ] ) strs = np . sort ( values [ str_pos ] ) return np . concatenate ( [ nums , np . asarray ( strs , dtype = object ) ] ) sorter = None if lib . infer_dtype ( values , skipna = False ) == 'mixed-integer' : # unorderable in py3 if mixed str/int ordered = sort_mixed ( values ) else : try : sorter = values . argsort ( ) ordered = values . take ( sorter ) except TypeError : # try this anyway ordered = sort_mixed ( values ) # labels: if labels is None : return ordered if not is_list_like ( labels ) : raise TypeError ( \"Only list-like objects or None are allowed to be\" \"passed to safe_sort as labels\" ) labels = ensure_platform_int ( np . asarray ( labels ) ) from pandas import Index if not assume_unique and not Index ( values ) . is_unique : raise ValueError ( \"values should be unique if labels is not None\" ) if sorter is None : # mixed types ( hash_klass , _ ) , values = algorithms . _get_data_algo ( values , algorithms . _hashtables ) t = hash_klass ( len ( values ) ) t . map_locations ( values ) sorter = ensure_platform_int ( t . lookup ( ordered ) ) reverse_indexer = np . empty ( len ( sorter ) , dtype = np . int_ ) reverse_indexer . put ( sorter , np . arange ( len ( sorter ) ) ) mask = ( labels < - len ( values ) ) | ( labels >= len ( values ) ) | ( labels == na_sentinel ) # (Out of bound indices will be masked with `na_sentinel` next, so we may # deal with them here without performance loss using `mode='wrap'`.) new_labels = reverse_indexer . take ( labels , mode = 'wrap' ) np . putmask ( new_labels , mask , na_sentinel ) return ordered , ensure_platform_int ( new_labels )\n\n\n        Summary:", "input": "", "output": "Sort values and reorder corresponding labels . values should be unique if labels is not None . Safe for use with mixed types ( int str ) orders ints before strs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_ne_builtin_clash ( expr ) : names = expr . names overlap = names & _ne_builtins if overlap : s = ', ' . join ( map ( repr , overlap ) ) raise NumExprClobberingError ( 'Variables in expression \"{expr}\" ' 'overlap with builtins: ({s})' . format ( expr = expr , s = s ) )\n\n\n        Summary:", "input": "", "output": "Attempt to prevent foot - shooting in a helpful way ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def evaluate ( self ) : if not self . _is_aligned : self . result_type , self . aligned_axes = _align ( self . expr . terms ) # make sure no names in resolvers and locals/globals clash res = self . _evaluate ( ) return _reconstruct_object ( self . result_type , res , self . aligned_axes , self . expr . terms . return_type )\n\n\n        Summary:", "input": "", "output": "Run the engine on the expression"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_block_type ( values , dtype = None ) : dtype = dtype or values . dtype vtype = dtype . type if is_sparse ( dtype ) : # Need this first(ish) so that Sparse[datetime] is sparse cls = ExtensionBlock elif is_categorical ( values ) : cls = CategoricalBlock elif issubclass ( vtype , np . datetime64 ) : assert not is_datetime64tz_dtype ( values ) cls = DatetimeBlock elif is_datetime64tz_dtype ( values ) : cls = DatetimeTZBlock elif is_interval_dtype ( dtype ) or is_period_dtype ( dtype ) : cls = ObjectValuesExtensionBlock elif is_extension_array_dtype ( values ) : cls = ExtensionBlock elif issubclass ( vtype , np . floating ) : cls = FloatBlock elif issubclass ( vtype , np . timedelta64 ) : assert issubclass ( vtype , np . integer ) cls = TimeDeltaBlock elif issubclass ( vtype , np . complexfloating ) : cls = ComplexBlock elif issubclass ( vtype , np . integer ) : cls = IntBlock elif dtype == np . bool_ : cls = BoolBlock else : cls = ObjectBlock return cls\n\n\n        Summary:", "input": "", "output": "Find the appropriate Block subclass to use for the given values and dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _extend_blocks ( result , blocks = None ) : from pandas . core . internals import BlockManager if blocks is None : blocks = [ ] if isinstance ( result , list ) : for r in result : if isinstance ( r , list ) : blocks . extend ( r ) else : blocks . append ( r ) elif isinstance ( result , BlockManager ) : blocks . extend ( result . blocks ) else : blocks . append ( result ) return blocks\n\n\n        Summary:", "input": "", "output": "return a new extended blocks givin the result"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _block_shape ( values , ndim = 1 , shape = None ) : if values . ndim < ndim : if shape is None : shape = values . shape if not is_extension_array_dtype ( values ) : # TODO: https://github.com/pandas-dev/pandas/issues/23023 # block.shape is incorrect for \"2D\" ExtensionArrays # We can't, and don't need to, reshape. values = values . reshape ( tuple ( ( 1 , ) + shape ) ) return values\n\n\n        Summary:", "input": "", "output": "guarantee the shape of the values to be at least 1 d"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _safe_reshape ( arr , new_shape ) : if isinstance ( arr , ABCSeries ) : arr = arr . _values if not isinstance ( arr , ABCExtensionArray ) : arr = arr . reshape ( new_shape ) return arr\n\n\n        Summary:", "input": "", "output": "If possible reshape arr to have shape new_shape with a couple of exceptions ( see gh - 13012 ) :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _putmask_smart ( v , m , n ) : # we cannot use np.asarray() here as we cannot have conversions # that numpy does when numeric are mixed with strings # n should be the length of the mask or a scalar here if not is_list_like ( n ) : n = np . repeat ( n , len ( m ) ) elif isinstance ( n , np . ndarray ) and n . ndim == 0 : # numpy scalar n = np . repeat ( np . array ( n , ndmin = 1 ) , len ( m ) ) # see if we are only masking values that if putted # will work in the current dtype try : nn = n [ m ] # make sure that we have a nullable type # if we have nulls if not _isna_compat ( v , nn [ 0 ] ) : raise ValueError # we ignore ComplexWarning here with warnings . catch_warnings ( record = True ) : warnings . simplefilter ( \"ignore\" , np . ComplexWarning ) nn_at = nn . astype ( v . dtype ) # avoid invalid dtype comparisons # between numbers & strings # only compare integers/floats # don't compare integers to datetimelikes if ( not is_numeric_v_string_like ( nn , nn_at ) and ( is_float_dtype ( nn . dtype ) or is_integer_dtype ( nn . dtype ) and is_float_dtype ( nn_at . dtype ) or is_integer_dtype ( nn_at . dtype ) ) ) : comp = ( nn == nn_at ) if is_list_like ( comp ) and comp . all ( ) : nv = v . copy ( ) nv [ m ] = nn_at return nv except ( ValueError , IndexError , TypeError , OverflowError ) : pass n = np . asarray ( n ) def _putmask_preserve ( nv , n ) : try : nv [ m ] = n [ m ] except ( IndexError , ValueError ) : nv [ m ] = n return nv # preserves dtype if possible if v . dtype . kind == n . dtype . kind : return _putmask_preserve ( v , n ) # change the dtype if needed dtype , _ = maybe_promote ( n . dtype ) if is_extension_type ( v . dtype ) and is_object_dtype ( dtype ) : v = v . get_values ( dtype ) else : v = v . astype ( dtype ) return _putmask_preserve ( v , n )\n\n\n        Summary:", "input": "", "output": "Return a new ndarray try to preserve dtype if possible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_ndim ( self , values , ndim ) : if ndim is None : ndim = values . ndim if self . _validate_ndim and values . ndim != ndim : msg = ( \"Wrong number of dimensions. values.ndim != ndim \" \"[{} != {}]\" ) raise ValueError ( msg . format ( values . ndim , ndim ) ) return ndim\n\n\n        Summary:", "input": "", "output": "ndim inference and validation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_categorical_astype ( self , dtype ) : if dtype is Categorical or dtype is CategoricalDtype : # this is a pd.Categorical, but is not # a valid type for astypeing raise TypeError ( \"invalid type {0} for astype\" . format ( dtype ) ) elif is_categorical_dtype ( dtype ) : return True return False\n\n\n        Summary:", "input": "", "output": "validate that we have a astypeable to categorical returns a boolean if we are a categorical"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_values ( self , dtype = None ) : if is_object_dtype ( dtype ) : return self . values . astype ( object ) return self . values\n\n\n        Summary:", "input": "", "output": "return an internal format currently just the ndarray this is often overridden to handle to_dense like operations"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_block ( self , values , placement = None , ndim = None ) : if placement is None : placement = self . mgr_locs if ndim is None : ndim = self . ndim return make_block ( values , placement = placement , ndim = ndim )\n\n\n        Summary:", "input": "", "output": "Create a new block with type inference propagate any values that are not specified"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_block_same_class ( self , values , placement = None , ndim = None , dtype = None ) : if dtype is not None : # issue 19431 fastparquet is passing this warnings . warn ( \"dtype argument is deprecated, will be removed \" \"in a future release.\" , DeprecationWarning ) if placement is None : placement = self . mgr_locs return make_block ( values , placement = placement , ndim = ndim , klass = self . __class__ , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Wrap given values in a block of same type as self ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def getitem_block ( self , slicer , new_mgr_locs = None ) : if new_mgr_locs is None : if isinstance ( slicer , tuple ) : axis0_slicer = slicer [ 0 ] else : axis0_slicer = slicer new_mgr_locs = self . mgr_locs [ axis0_slicer ] new_values = self . _slice ( slicer ) if self . _validate_ndim and new_values . ndim != self . ndim : raise ValueError ( \"Only same dim slicing is allowed\" ) return self . make_block_same_class ( new_values , new_mgr_locs )\n\n\n        Summary:", "input": "", "output": "Perform __getitem__ - like return result as block ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concat_same_type ( self , to_concat , placement = None ) : values = self . _concatenator ( [ blk . values for blk in to_concat ] , axis = self . ndim - 1 ) return self . make_block_same_class ( values , placement = placement or slice ( 0 , len ( values ) , 1 ) )\n\n\n        Summary:", "input": "", "output": "Concatenate list of single blocks of the same type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def delete ( self , loc ) : self . values = np . delete ( self . values , loc , 0 ) self . mgr_locs = self . mgr_locs . delete ( loc )\n\n\n        Summary:", "input": "", "output": "Delete given loc ( - s ) from block in - place ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , func , * * kwargs ) : with np . errstate ( all = 'ignore' ) : result = func ( self . values , * * kwargs ) if not isinstance ( result , Block ) : result = self . make_block ( values = _block_shape ( result , ndim = self . ndim ) ) return result\n\n\n        Summary:", "input": "", "output": "apply the function to my values ; return a block if we are not one"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fillna ( self , value , limit = None , inplace = False , downcast = None ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if not self . _can_hold_na : if inplace : return self else : return self . copy ( ) mask = isna ( self . values ) if limit is not None : if not is_integer ( limit ) : raise ValueError ( 'Limit must be an integer' ) if limit < 1 : raise ValueError ( 'Limit must be greater than 0' ) if self . ndim > 2 : raise NotImplementedError ( \"number of dimensions for 'fillna' \" \"is currently limited to 2\" ) mask [ mask . cumsum ( self . ndim - 1 ) > limit ] = False # fillna, but if we cannot coerce, then try again as an ObjectBlock try : values , _ = self . _try_coerce_args ( self . values , value ) blocks = self . putmask ( mask , value , inplace = inplace ) blocks = [ b . make_block ( values = self . _try_coerce_result ( b . values ) ) for b in blocks ] return self . _maybe_downcast ( blocks , downcast ) except ( TypeError , ValueError ) : # we can't process the value, but nothing to do if not mask . any ( ) : return self if inplace else self . copy ( ) # operate column-by-column def f ( m , v , i ) : block = self . coerce_to_target_dtype ( value ) # slice out our block if i is not None : block = block . getitem_block ( slice ( i , i + 1 ) ) return block . fillna ( value , limit = limit , inplace = inplace , downcast = None ) return self . split_and_operate ( mask , f , inplace )\n\n\n        Summary:", "input": "", "output": "fillna on the block with the value . If we fail then convert to ObjectBlock and try again"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def split_and_operate ( self , mask , f , inplace ) : if mask is None : mask = np . ones ( self . shape , dtype = bool ) new_values = self . values def make_a_block ( nv , ref_loc ) : if isinstance ( nv , Block ) : block = nv elif isinstance ( nv , list ) : block = nv [ 0 ] else : # Put back the dimension that was taken from it and make # a block out of the result. try : nv = _block_shape ( nv , ndim = self . ndim ) except ( AttributeError , NotImplementedError ) : pass block = self . make_block ( values = nv , placement = ref_loc ) return block # ndim == 1 if self . ndim == 1 : if mask . any ( ) : nv = f ( mask , new_values , None ) else : nv = new_values if inplace else new_values . copy ( ) block = make_a_block ( nv , self . mgr_locs ) return [ block ] # ndim > 1 new_blocks = [ ] for i , ref_loc in enumerate ( self . mgr_locs ) : m = mask [ i ] v = new_values [ i ] # need a new block if m . any ( ) : nv = f ( m , v , i ) else : nv = v if inplace else v . copy ( ) block = make_a_block ( nv , [ ref_loc ] ) new_blocks . append ( block ) return new_blocks\n\n\n        Summary:", "input": "", "output": "split the block per - column and apply the callable f per - column return a new block for each . Handle masking which will not change a block unless needed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def downcast ( self , dtypes = None ) : # turn it off completely if dtypes is False : return self values = self . values # single block handling if self . _is_single_block : # try to cast all non-floats here if dtypes is None : dtypes = 'infer' nv = maybe_downcast_to_dtype ( values , dtypes ) return self . make_block ( nv ) # ndim > 1 if dtypes is None : return self if not ( dtypes == 'infer' or isinstance ( dtypes , dict ) ) : raise ValueError ( \"downcast must have a dictionary or 'infer' as \" \"its argument\" ) # operate column-by-column # this is expensive as it splits the blocks items-by-item def f ( m , v , i ) : if dtypes == 'infer' : dtype = 'infer' else : raise AssertionError ( \"dtypes as dict is not supported yet\" ) if dtype is not None : v = maybe_downcast_to_dtype ( v , dtype ) return v return self . split_and_operate ( None , f , False )\n\n\n        Summary:", "input": "", "output": "try to downcast each item to the dict of dtypes if present"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _astype ( self , dtype , copy = False , errors = 'raise' , values = None , * * kwargs ) : errors_legal_values = ( 'raise' , 'ignore' ) if errors not in errors_legal_values : invalid_arg = ( \"Expected value of kwarg 'errors' to be one of {}. \" \"Supplied value is '{}'\" . format ( list ( errors_legal_values ) , errors ) ) raise ValueError ( invalid_arg ) if ( inspect . isclass ( dtype ) and issubclass ( dtype , ( PandasExtensionDtype , ExtensionDtype ) ) ) : msg = ( \"Expected an instance of {}, but got the class instead. \" \"Try instantiating 'dtype'.\" . format ( dtype . __name__ ) ) raise TypeError ( msg ) # may need to convert to categorical if self . is_categorical_astype ( dtype ) : # deprecated 17636 if ( 'categories' in kwargs or 'ordered' in kwargs ) : if isinstance ( dtype , CategoricalDtype ) : raise TypeError ( \"Cannot specify a CategoricalDtype and also \" \"`categories` or `ordered`. Use \" \"`dtype=CategoricalDtype(categories, ordered)`\" \" instead.\" ) warnings . warn ( \"specifying 'categories' or 'ordered' in \" \".astype() is deprecated; pass a \" \"CategoricalDtype instead\" , FutureWarning , stacklevel = 7 ) categories = kwargs . get ( 'categories' , None ) ordered = kwargs . get ( 'ordered' , None ) if com . _any_not_none ( categories , ordered ) : dtype = CategoricalDtype ( categories , ordered ) if is_categorical_dtype ( self . values ) : # GH 10696/18593: update an existing categorical efficiently return self . make_block ( self . values . astype ( dtype , copy = copy ) ) return self . make_block ( Categorical ( self . values , dtype = dtype ) ) dtype = pandas_dtype ( dtype ) # astype processing if is_dtype_equal ( self . dtype , dtype ) : if copy : return self . copy ( ) return self try : # force the copy here if values is None : if self . is_extension : values = self . values . astype ( dtype ) else : if issubclass ( dtype . type , str ) : # use native type formatting for datetime/tz/timedelta if self . is_datelike : values = self . to_native_types ( ) # astype formatting else : values = self . get_values ( ) else : values = self . get_values ( dtype = dtype ) # _astype_nansafe works fine with 1-d only values = astype_nansafe ( values . ravel ( ) , dtype , copy = True ) # TODO(extension) # should we make this attribute? try : values = values . reshape ( self . shape ) except AttributeError : pass newb = make_block ( values , placement = self . mgr_locs , ndim = self . ndim ) except Exception : # noqa: E722 if errors == 'raise' : raise newb = self . copy ( ) if copy else self if newb . is_numeric and self . is_numeric : if newb . shape != self . shape : raise TypeError ( \"cannot set astype for copy = [{copy}] for dtype \" \"({dtype} [{shape}]) to different shape \" \"({newb_dtype} [{newb_shape}])\" . format ( copy = copy , dtype = self . dtype . name , shape = self . shape , newb_dtype = newb . dtype . name , newb_shape = newb . shape ) ) return newb\n\n\n        Summary:", "input": "", "output": "Coerce to the new type"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _can_hold_element ( self , element ) : dtype = self . values . dtype . type tipo = maybe_infer_dtype_type ( element ) if tipo is not None : return issubclass ( tipo . type , dtype ) return isinstance ( element , dtype )\n\n\n        Summary:", "input": "", "output": "require the same dtype as ourselves"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_cast_result ( self , result , dtype = None ) : if dtype is None : dtype = self . dtype if self . is_integer or self . is_bool or self . is_datetime : pass elif self . is_float and result . dtype == self . dtype : # protect against a bool/object showing up here if isinstance ( dtype , str ) and dtype == 'infer' : return result if not isinstance ( dtype , type ) : dtype = dtype . type if issubclass ( dtype , ( np . bool_ , np . object_ ) ) : if issubclass ( dtype , np . bool_ ) : if isna ( result ) . all ( ) : return result . astype ( np . bool_ ) else : result = result . astype ( np . object_ ) result [ result == 1 ] = True result [ result == 0 ] = False return result else : return result . astype ( np . object_ ) return result # may need to change the dtype here return maybe_downcast_to_dtype ( result , dtype )\n\n\n        Summary:", "input": "", "output": "try to cast the result to our original type we may have roundtripped thru object in the mean - time"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_args ( self , values , other ) : if np . any ( notna ( other ) ) and not self . _can_hold_element ( other ) : # coercion issues # let higher levels handle raise TypeError ( \"cannot convert {} to an {}\" . format ( type ( other ) . __name__ , type ( self ) . __name__ . lower ( ) . replace ( 'Block' , '' ) ) ) return values , other\n\n\n        Summary:", "input": "", "output": "provide coercion to our input arguments"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_native_types ( self , slicer = None , na_rep = 'nan' , quoting = None , * * kwargs ) : values = self . get_values ( ) if slicer is not None : values = values [ : , slicer ] mask = isna ( values ) if not self . is_object and not quoting : values = values . astype ( str ) else : values = np . array ( values , dtype = 'object' ) values [ mask ] = na_rep return values\n\n\n        Summary:", "input": "", "output": "convert to our native types format slicing if desired"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , deep = True ) : values = self . values if deep : values = values . copy ( ) return self . make_block_same_class ( values , ndim = self . ndim )\n\n\n        Summary:", "input": "", "output": "copy constructor"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def replace ( self , to_replace , value , inplace = False , filter = None , regex = False , convert = True ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) original_to_replace = to_replace # try to replace, if we raise an error, convert to ObjectBlock and # retry try : values , to_replace = self . _try_coerce_args ( self . values , to_replace ) mask = missing . mask_missing ( values , to_replace ) if filter is not None : filtered_out = ~ self . mgr_locs . isin ( filter ) mask [ filtered_out . nonzero ( ) [ 0 ] ] = False blocks = self . putmask ( mask , value , inplace = inplace ) if convert : blocks = [ b . convert ( by_item = True , numeric = False , copy = not inplace ) for b in blocks ] return blocks except ( TypeError , ValueError ) : # GH 22083, TypeError or ValueError occurred within error handling # causes infinite loop. Cast and retry only if not objectblock. if is_object_dtype ( self ) : raise # try again with a compatible block block = self . astype ( object ) return block . replace ( to_replace = original_to_replace , value = value , inplace = inplace , filter = filter , regex = regex , convert = convert )\n\n\n        Summary:", "input": "", "output": "replace the to_replace value with value possible to create new blocks here this is just a call to putmask . regex is not used here . It is used in ObjectBlocks . It is here for API compatibility ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setitem ( self , indexer , value ) : # coerce None values, if appropriate if value is None : if self . is_numeric : value = np . nan # coerce if block dtype can store value values = self . values try : values , value = self . _try_coerce_args ( values , value ) # can keep its own dtype if hasattr ( value , 'dtype' ) and is_dtype_equal ( values . dtype , value . dtype ) : dtype = self . dtype else : dtype = 'infer' except ( TypeError , ValueError ) : # current dtype cannot store value, coerce to common dtype find_dtype = False if hasattr ( value , 'dtype' ) : dtype = value . dtype find_dtype = True elif lib . is_scalar ( value ) : if isna ( value ) : # NaN promotion is handled in latter path dtype = False else : dtype , _ = infer_dtype_from_scalar ( value , pandas_dtype = True ) find_dtype = True else : dtype = 'infer' if find_dtype : dtype = find_common_type ( [ values . dtype , dtype ] ) if not is_dtype_equal ( self . dtype , dtype ) : b = self . astype ( dtype ) return b . setitem ( indexer , value ) # value must be storeable at this moment arr_value = np . array ( value ) # cast the values to a type that can hold nan (if necessary) if not self . _can_hold_element ( value ) : dtype , _ = maybe_promote ( arr_value . dtype ) values = values . astype ( dtype ) transf = ( lambda x : x . T ) if self . ndim == 2 else ( lambda x : x ) values = transf ( values ) # length checking check_setitem_lengths ( indexer , value , values ) def _is_scalar_indexer ( indexer ) : # return True if we are all scalar indexers if arr_value . ndim == 1 : if not isinstance ( indexer , tuple ) : indexer = tuple ( [ indexer ] ) return any ( isinstance ( idx , np . ndarray ) and len ( idx ) == 0 for idx in indexer ) return False def _is_empty_indexer ( indexer ) : # return a boolean if we have an empty indexer if is_list_like ( indexer ) and not len ( indexer ) : return True if arr_value . ndim == 1 : if not isinstance ( indexer , tuple ) : indexer = tuple ( [ indexer ] ) return any ( isinstance ( idx , np . ndarray ) and len ( idx ) == 0 for idx in indexer ) return False # empty indexers # 8669 (empty) if _is_empty_indexer ( indexer ) : pass # setting a single element for each dim and with a rhs that could # be say a list # GH 6043 elif _is_scalar_indexer ( indexer ) : values [ indexer ] = value # if we are an exact match (ex-broadcasting), # then use the resultant dtype elif ( len ( arr_value . shape ) and arr_value . shape [ 0 ] == values . shape [ 0 ] and np . prod ( arr_value . shape ) == np . prod ( values . shape ) ) : values [ indexer ] = value try : values = values . astype ( arr_value . dtype ) except ValueError : pass # set else : values [ indexer ] = value # coerce and try to infer the dtypes of the result values = self . _try_coerce_and_cast_result ( values , dtype ) block = self . make_block ( transf ( values ) ) return block\n\n\n        Summary:", "input": "", "output": "Set the value inplace returning a a maybe different typed block ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def putmask ( self , mask , new , align = True , inplace = False , axis = 0 , transpose = False ) : new_values = self . values if inplace else self . values . copy ( ) new = getattr ( new , 'values' , new ) mask = getattr ( mask , 'values' , mask ) # if we are passed a scalar None, convert it here if not is_list_like ( new ) and isna ( new ) and not self . is_object : new = self . fill_value if self . _can_hold_element ( new ) : _ , new = self . _try_coerce_args ( new_values , new ) if transpose : new_values = new_values . T # If the default repeat behavior in np.putmask would go in the # wrong direction, then explicitly repeat and reshape new instead if getattr ( new , 'ndim' , 0 ) >= 1 : if self . ndim - 1 == new . ndim and axis == 1 : new = np . repeat ( new , new_values . shape [ - 1 ] ) . reshape ( self . shape ) new = new . astype ( new_values . dtype ) # we require exact matches between the len of the # values we are setting (or is compat). np.putmask # doesn't check this and will simply truncate / pad # the output, but we want sane error messages # # TODO: this prob needs some better checking # for 2D cases if ( ( is_list_like ( new ) and np . any ( mask [ mask ] ) and getattr ( new , 'ndim' , 1 ) == 1 ) ) : if not ( mask . shape [ - 1 ] == len ( new ) or mask [ mask ] . shape [ - 1 ] == len ( new ) or len ( new ) == 1 ) : raise ValueError ( \"cannot assign mismatch \" \"length to masked array\" ) np . putmask ( new_values , mask , new ) # maybe upcast me elif mask . any ( ) : if transpose : mask = mask . T if isinstance ( new , np . ndarray ) : new = new . T axis = new_values . ndim - axis - 1 # Pseudo-broadcast if getattr ( new , 'ndim' , 0 ) >= 1 : if self . ndim - 1 == new . ndim : new_shape = list ( new . shape ) new_shape . insert ( axis , 1 ) new = new . reshape ( tuple ( new_shape ) ) # operate column-by-column def f ( m , v , i ) : if i is None : # ndim==1 case. n = new else : if isinstance ( new , np . ndarray ) : n = np . squeeze ( new [ i % new . shape [ 0 ] ] ) else : n = np . array ( new ) # type of the new block dtype , _ = maybe_promote ( n . dtype ) # we need to explicitly astype here to make a copy n = n . astype ( dtype ) nv = _putmask_smart ( v , m , n ) return nv new_blocks = self . split_and_operate ( mask , f , inplace ) return new_blocks if inplace : return [ self ] if transpose : new_values = new_values . T return [ self . make_block ( new_values ) ]\n\n\n        Summary:", "input": "", "output": "putmask the data to the block ; it is possible that we may create a new dtype of block"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def coerce_to_target_dtype ( self , other ) : # if we cannot then coerce to object dtype , _ = infer_dtype_from ( other , pandas_dtype = True ) if is_dtype_equal ( self . dtype , dtype ) : return self if self . is_bool or is_object_dtype ( dtype ) or is_bool_dtype ( dtype ) : # we don't upcast to bool return self . astype ( object ) elif ( ( self . is_float or self . is_complex ) and ( is_integer_dtype ( dtype ) or is_float_dtype ( dtype ) ) ) : # don't coerce float/complex to int return self elif ( self . is_datetime or is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) ) : # not a datetime if not ( ( is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) ) and self . is_datetime ) : return self . astype ( object ) # don't upcast timezone with different timezone or no timezone mytz = getattr ( self . dtype , 'tz' , None ) othertz = getattr ( dtype , 'tz' , None ) if str ( mytz ) != str ( othertz ) : return self . astype ( object ) raise AssertionError ( \"possible recursion in \" \"coerce_to_target_dtype: {} {}\" . format ( self , other ) ) elif ( self . is_timedelta or is_timedelta64_dtype ( dtype ) ) : # not a timedelta if not ( is_timedelta64_dtype ( dtype ) and self . is_timedelta ) : return self . astype ( object ) raise AssertionError ( \"possible recursion in \" \"coerce_to_target_dtype: {} {}\" . format ( self , other ) ) try : return self . astype ( dtype ) except ( ValueError , TypeError , OverflowError ) : pass return self . astype ( object )\n\n\n        Summary:", "input": "", "output": "coerce the current block to a dtype compat for other we will return a block possibly object and not raise"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _interpolate_with_fill ( self , method = 'pad' , axis = 0 , inplace = False , limit = None , fill_value = None , coerce = False , downcast = None ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) # if we are coercing, then don't force the conversion # if the block can't hold the type if coerce : if not self . _can_hold_na : if inplace : return [ self ] else : return [ self . copy ( ) ] values = self . values if inplace else self . values . copy ( ) values , fill_value = self . _try_coerce_args ( values , fill_value ) values = missing . interpolate_2d ( values , method = method , axis = axis , limit = limit , fill_value = fill_value , dtype = self . dtype ) values = self . _try_coerce_result ( values ) blocks = [ self . make_block_same_class ( values , ndim = self . ndim ) ] return self . _maybe_downcast ( blocks , downcast )\n\n\n        Summary:", "input": "", "output": "fillna but using the interpolate machinery"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _interpolate ( self , method = None , index = None , values = None , fill_value = None , axis = 0 , limit = None , limit_direction = 'forward' , limit_area = None , inplace = False , downcast = None , * * kwargs ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) data = self . values if inplace else self . values . copy ( ) # only deal with floats if not self . is_float : if not self . is_integer : return self data = data . astype ( np . float64 ) if fill_value is None : fill_value = self . fill_value if method in ( 'krogh' , 'piecewise_polynomial' , 'pchip' ) : if not index . is_monotonic : raise ValueError ( \"{0} interpolation requires that the \" \"index be monotonic.\" . format ( method ) ) # process 1-d slices in the axis direction def func ( x ) : # process a 1-d slice, returning it # should the axis argument be handled below in apply_along_axis? # i.e. not an arg to missing.interpolate_1d return missing . interpolate_1d ( index , x , method = method , limit = limit , limit_direction = limit_direction , limit_area = limit_area , fill_value = fill_value , bounds_error = False , * * kwargs ) # interp each column independently interp_values = np . apply_along_axis ( func , axis , data ) blocks = [ self . make_block_same_class ( interp_values ) ] return self . _maybe_downcast ( blocks , downcast )\n\n\n        Summary:", "input": "", "output": "interpolate using scipy wrappers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take_nd ( self , indexer , axis , new_mgr_locs = None , fill_tuple = None ) : # algos.take_nd dispatches for DatetimeTZBlock, CategoricalBlock # so need to preserve types # sparse is treated like an ndarray, but needs .get_values() shaping values = self . values if self . is_sparse : values = self . get_values ( ) if fill_tuple is None : fill_value = self . fill_value new_values = algos . take_nd ( values , indexer , axis = axis , allow_fill = False , fill_value = fill_value ) else : fill_value = fill_tuple [ 0 ] new_values = algos . take_nd ( values , indexer , axis = axis , allow_fill = True , fill_value = fill_value ) if new_mgr_locs is None : if axis == 0 : slc = libinternals . indexer_as_slice ( indexer ) if slc is not None : new_mgr_locs = self . mgr_locs [ slc ] else : new_mgr_locs = self . mgr_locs [ indexer ] else : new_mgr_locs = self . mgr_locs if not is_dtype_equal ( new_values . dtype , self . dtype ) : return self . make_block ( new_values , new_mgr_locs ) else : return self . make_block_same_class ( new_values , new_mgr_locs )\n\n\n        Summary:", "input": "", "output": "Take values according to indexer and return them as a block . bb"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def diff ( self , n , axis = 1 ) : new_values = algos . diff ( self . values , n , axis = axis ) return [ self . make_block ( values = new_values ) ]\n\n\n        Summary:", "input": "", "output": "return block for the diff of the values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods , axis = 0 , fill_value = None ) : # convert integer to float if necessary. need to do a lot more than # that, handle boolean etc also new_values , fill_value = maybe_upcast ( self . values , fill_value ) # make sure array sent to np.roll is c_contiguous f_ordered = new_values . flags . f_contiguous if f_ordered : new_values = new_values . T axis = new_values . ndim - axis - 1 if np . prod ( new_values . shape ) : new_values = np . roll ( new_values , ensure_platform_int ( periods ) , axis = axis ) axis_indexer = [ slice ( None ) ] * self . ndim if periods > 0 : axis_indexer [ axis ] = slice ( None , periods ) else : axis_indexer [ axis ] = slice ( periods , None ) new_values [ tuple ( axis_indexer ) ] = fill_value # restore original order if f_ordered : new_values = new_values . T return [ self . make_block ( new_values ) ]\n\n\n        Summary:", "input": "", "output": "shift the block by periods possibly upcast"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def where ( self , other , cond , align = True , errors = 'raise' , try_cast = False , axis = 0 , transpose = False ) : import pandas . core . computation . expressions as expressions assert errors in [ 'raise' , 'ignore' ] values = self . values orig_other = other if transpose : values = values . T other = getattr ( other , '_values' , getattr ( other , 'values' , other ) ) cond = getattr ( cond , 'values' , cond ) # If the default broadcasting would go in the wrong direction, then # explicitly reshape other instead if getattr ( other , 'ndim' , 0 ) >= 1 : if values . ndim - 1 == other . ndim and axis == 1 : other = other . reshape ( tuple ( other . shape + ( 1 , ) ) ) elif transpose and values . ndim == self . ndim - 1 : cond = cond . T if not hasattr ( cond , 'shape' ) : raise ValueError ( \"where must have a condition that is ndarray \" \"like\" ) # our where function def func ( cond , values , other ) : if cond . ravel ( ) . all ( ) : return values values , other = self . _try_coerce_args ( values , other ) try : return self . _try_coerce_result ( expressions . where ( cond , values , other ) ) except Exception as detail : if errors == 'raise' : raise TypeError ( 'Could not operate [{other!r}] with block values ' '[{detail!s}]' . format ( other = other , detail = detail ) ) else : # return the values result = np . empty ( values . shape , dtype = 'float64' ) result . fill ( np . nan ) return result # see if we can operate on the entire block, or need item-by-item # or if we are a single block (ndim == 1) try : result = func ( cond , values , other ) except TypeError : # we cannot coerce, return a compat dtype # we are explicitly ignoring errors block = self . coerce_to_target_dtype ( other ) blocks = block . where ( orig_other , cond , align = align , errors = errors , try_cast = try_cast , axis = axis , transpose = transpose ) return self . _maybe_downcast ( blocks , 'infer' ) if self . _can_hold_na or self . ndim == 1 : if transpose : result = result . T # try to cast if requested if try_cast : result = self . _try_cast_result ( result ) return self . make_block ( result ) # might need to separate out blocks axis = cond . ndim - 1 cond = cond . swapaxes ( axis , 0 ) mask = np . array ( [ cond [ i ] . all ( ) for i in range ( cond . shape [ 0 ] ) ] , dtype = bool ) result_blocks = [ ] for m in [ mask , ~ mask ] : if m . any ( ) : r = self . _try_cast_result ( result . take ( m . nonzero ( ) [ 0 ] , axis = axis ) ) result_blocks . append ( self . make_block ( r . T , placement = self . mgr_locs [ m ] ) ) return result_blocks\n\n\n        Summary:", "input": "", "output": "evaluate the block ; return result block ( s ) from the result"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unstack ( self , unstacker_func , new_columns , n_rows , fill_value ) : unstacker = unstacker_func ( self . values . T ) new_items = unstacker . get_new_columns ( ) new_placement = new_columns . get_indexer ( new_items ) new_values , mask = unstacker . get_new_values ( ) mask = mask . any ( 0 ) new_values = new_values . T [ mask ] new_placement = new_placement [ mask ] blocks = [ make_block ( new_values , placement = new_placement ) ] return blocks , mask\n\n\n        Summary:", "input": "", "output": "Return a list of unstacked blocks of self"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def quantile ( self , qs , interpolation = 'linear' , axis = 0 ) : if self . is_datetimetz : # TODO: cleanup this special case. # We need to operate on i8 values for datetimetz # but `Block.get_values()` returns an ndarray of objects # right now. We need an API for \"values to do numeric-like ops on\" values = self . values . asi8 # TODO: NonConsolidatableMixin shape # Usual shape inconsistencies for ExtensionBlocks if self . ndim > 1 : values = values [ None , : ] else : values = self . get_values ( ) values , _ = self . _try_coerce_args ( values , values ) is_empty = values . shape [ axis ] == 0 orig_scalar = not is_list_like ( qs ) if orig_scalar : # make list-like, unpack later qs = [ qs ] if is_empty : if self . ndim == 1 : result = self . _na_value else : # create the array of na_values # 2d len(values) * len(qs) result = np . repeat ( np . array ( [ self . fill_value ] * len ( qs ) ) , len ( values ) ) . reshape ( len ( values ) , len ( qs ) ) else : # asarray needed for Sparse, see GH#24600 # TODO: Why self.values and not values? mask = np . asarray ( isna ( self . values ) ) result = nanpercentile ( values , np . array ( qs ) * 100 , axis = axis , na_value = self . fill_value , mask = mask , ndim = self . ndim , interpolation = interpolation ) result = np . array ( result , copy = False ) if self . ndim > 1 : result = result . T if orig_scalar and not lib . is_scalar ( result ) : # result could be scalar in case with is_empty and self.ndim == 1 assert result . shape [ - 1 ] == 1 , result . shape result = result [ ... , 0 ] result = lib . item_from_zerodim ( result ) ndim = getattr ( result , 'ndim' , None ) or 0 result = self . _try_coerce_result ( result ) return make_block ( result , placement = np . arange ( len ( result ) ) , ndim = ndim )\n\n\n        Summary:", "input": "", "output": "compute the quantiles of the"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replace_coerce ( self , to_replace , value , inplace = True , regex = False , convert = False , mask = None ) : if mask . any ( ) : if not regex : self = self . coerce_to_target_dtype ( value ) return self . putmask ( mask , value , inplace = inplace ) else : return self . _replace_single ( to_replace , value , inplace = inplace , regex = regex , convert = convert , mask = mask ) return self\n\n\n        Summary:", "input": "", "output": "Replace value corresponding to the given boolean array with another value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def putmask ( self , mask , new , align = True , inplace = False , axis = 0 , transpose = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) # use block's copy logic. # .values may be an Index which does shallow copy by default new_values = self . values if inplace else self . copy ( ) . values new_values , new = self . _try_coerce_args ( new_values , new ) if isinstance ( new , np . ndarray ) and len ( new ) == len ( mask ) : new = new [ mask ] mask = _safe_reshape ( mask , new_values . shape ) new_values [ mask ] = new new_values = self . _try_coerce_result ( new_values ) return [ self . make_block ( values = new_values ) ]\n\n\n        Summary:", "input": "", "output": "putmask the data to the block ; we must be a single block and not generate other blocks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_unstack_items ( self , unstacker , new_columns ) : # shared with ExtensionBlock new_items = unstacker . get_new_columns ( ) new_placement = new_columns . get_indexer ( new_items ) new_values , mask = unstacker . get_new_values ( ) mask = mask . any ( 0 ) return new_placement , new_values , mask\n\n\n        Summary:", "input": "", "output": "Get the placement values and mask for a Block unstack ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_coerce_values ( self , values ) : if isinstance ( values , ( ABCIndexClass , ABCSeries ) ) : values = values . _values return values\n\n\n        Summary:", "input": "", "output": "Unbox to an extension array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def setitem ( self , indexer , value ) : if isinstance ( indexer , tuple ) : # we are always 1-D indexer = indexer [ 0 ] check_setitem_lengths ( indexer , value , self . values ) self . values [ indexer ] = value return self\n\n\n        Summary:", "input": "", "output": "Set the value inplace returning a same - typed block ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take_nd ( self , indexer , axis = 0 , new_mgr_locs = None , fill_tuple = None ) : if fill_tuple is None : fill_value = None else : fill_value = fill_tuple [ 0 ] # axis doesn't matter; we are really a single-dim object # but are passed the axis depending on the calling routing # if its REALLY axis 0, then this will be a reindex and not a take new_values = self . values . take ( indexer , fill_value = fill_value , allow_fill = True ) if self . ndim == 1 and new_mgr_locs is None : new_mgr_locs = [ 0 ] else : if new_mgr_locs is None : new_mgr_locs = self . mgr_locs return self . make_block_same_class ( new_values , new_mgr_locs )\n\n\n        Summary:", "input": "", "output": "Take values according to indexer and return them as a block ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _slice ( self , slicer ) : # slice the category # return same dims as we currently have if isinstance ( slicer , tuple ) and len ( slicer ) == 2 : if not com . is_null_slice ( slicer [ 0 ] ) : raise AssertionError ( \"invalid slicing for a 1-ndim \" \"categorical\" ) slicer = slicer [ 1 ] return self . values [ slicer ]\n\n\n        Summary:", "input": "", "output": "return a slice of my values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concat_same_type ( self , to_concat , placement = None ) : values = self . _holder . _concat_same_type ( [ blk . values for blk in to_concat ] ) placement = placement or slice ( 0 , len ( values ) , 1 ) return self . make_block_same_class ( values , ndim = self . ndim , placement = placement )\n\n\n        Summary:", "input": "", "output": "Concatenate list of single blocks of the same type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods : int , axis : libinternals . BlockPlacement = 0 , fill_value : Any = None ) -> List [ 'ExtensionBlock' ] : return [ self . make_block_same_class ( self . values . shift ( periods = periods , fill_value = fill_value ) , placement = self . mgr_locs , ndim = self . ndim ) ]\n\n\n        Summary:", "input": "", "output": "Shift the block by periods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_native_types ( self , slicer = None , na_rep = '' , float_format = None , decimal = '.' , quoting = None , * * kwargs ) : values = self . values if slicer is not None : values = values [ : , slicer ] # see gh-13418: no special formatting is desired at the # output (important for appropriate 'quoting' behaviour), # so do not pass it through the FloatArrayFormatter if float_format is None and decimal == '.' : mask = isna ( values ) if not quoting : values = values . astype ( str ) else : values = np . array ( values , dtype = 'object' ) values [ mask ] = na_rep return values from pandas . io . formats . format import FloatArrayFormatter formatter = FloatArrayFormatter ( values , na_rep = na_rep , float_format = float_format , decimal = decimal , quoting = quoting , fixed_width = False ) return formatter . get_result_as_array ( )\n\n\n        Summary:", "input": "", "output": "convert to our native types format slicing if desired"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_values ( self , dtype = None ) : if is_object_dtype ( dtype ) : values = self . values . ravel ( ) result = self . _holder ( values ) . astype ( object ) return result . reshape ( self . values . shape ) return self . values\n\n\n        Summary:", "input": "", "output": "return object dtype as boxed values such as Timestamps / Timedelta"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_coerce_values ( self , values ) : if values . dtype != _NS_DTYPE : values = conversion . ensure_datetime64ns ( values ) if isinstance ( values , DatetimeArray ) : values = values . _data assert isinstance ( values , np . ndarray ) , type ( values ) return values\n\n\n        Summary:", "input": "", "output": "Input validation for values passed to __init__ . Ensure that we have datetime64ns coercing if necessary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _astype ( self , dtype , * * kwargs ) : dtype = pandas_dtype ( dtype ) # if we are passed a datetime64[ns, tz] if is_datetime64tz_dtype ( dtype ) : values = self . values if getattr ( values , 'tz' , None ) is None : values = DatetimeIndex ( values ) . tz_localize ( 'UTC' ) values = values . tz_convert ( dtype . tz ) return self . make_block ( values ) # delegate return super ( ) . _astype ( dtype = dtype , * * kwargs )\n\n\n        Summary:", "input": "", "output": "these automatically copy so copy = True has no effect raise on an except if raise == True"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_args ( self , values , other ) : values = values . view ( 'i8' ) if isinstance ( other , bool ) : raise TypeError elif is_null_datetimelike ( other ) : other = tslibs . iNaT elif isinstance ( other , ( datetime , np . datetime64 , date ) ) : other = self . _box_func ( other ) if getattr ( other , 'tz' ) is not None : raise TypeError ( \"cannot coerce a Timestamp with a tz on a \" \"naive Block\" ) other = other . asm8 . view ( 'i8' ) elif hasattr ( other , 'dtype' ) and is_datetime64_dtype ( other ) : other = other . astype ( 'i8' , copy = False ) . view ( 'i8' ) else : # coercion issues # let higher levels handle raise TypeError ( other ) return values , other\n\n\n        Summary:", "input": "", "output": "Coerce values and other to dtype i8 . NaN and NaT convert to the smallest i8 and will correctly round - trip to NaT if converted back in _try_coerce_result . values is always ndarray - like other may not be"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_result ( self , result ) : if isinstance ( result , np . ndarray ) : if result . dtype . kind in [ 'i' , 'f' ] : result = result . astype ( 'M8[ns]' ) elif isinstance ( result , ( np . integer , np . float , np . datetime64 ) ) : result = self . _box_func ( result ) return result\n\n\n        Summary:", "input": "", "output": "reverse of try_coerce_args"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_native_types ( self , slicer = None , na_rep = None , date_format = None , quoting = None , * * kwargs ) : values = self . values i8values = self . values . view ( 'i8' ) if slicer is not None : values = values [ ... , slicer ] i8values = i8values [ ... , slicer ] from pandas . io . formats . format import _get_format_datetime64_from_values fmt = _get_format_datetime64_from_values ( values , date_format ) result = tslib . format_array_from_datetime ( i8values . ravel ( ) , tz = getattr ( self . values , 'tz' , None ) , format = fmt , na_rep = na_rep ) . reshape ( i8values . shape ) return np . atleast_2d ( result )\n\n\n        Summary:", "input": "", "output": "convert to our native types format slicing if desired"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set ( self , locs , values ) : values = conversion . ensure_datetime64ns ( values , copy = False ) self . values [ locs ] = values\n\n\n        Summary:", "input": "", "output": "Modify Block in - place with new item value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_coerce_values ( self , values ) : if not isinstance ( values , self . _holder ) : values = self . _holder ( values ) if values . tz is None : raise ValueError ( \"cannot create a DatetimeTZBlock without a tz\" ) return values\n\n\n        Summary:", "input": "", "output": "Input validation for values passed to __init__ . Ensure that we have datetime64TZ coercing if necessary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_values ( self , dtype = None ) : values = self . values if is_object_dtype ( dtype ) : values = values . _box_values ( values . _data ) values = np . asarray ( values ) if self . ndim == 2 : # Ensure that our shape is correct for DataFrame. # ExtensionArrays are always 1-D, even in a DataFrame when # the analogous NumPy-backed column would be a 2-D ndarray. values = values . reshape ( 1 , - 1 ) return values\n\n\n        Summary:", "input": "", "output": "Returns an ndarray of values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _slice ( self , slicer ) : if isinstance ( slicer , tuple ) : col , loc = slicer if not com . is_null_slice ( col ) and col != 0 : raise IndexError ( \"{0} only contains one item\" . format ( self ) ) return self . values [ loc ] return self . values [ slicer ]\n\n\n        Summary:", "input": "", "output": "return a slice of my values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_args ( self , values , other ) : # asi8 is a view, needs copy values = _block_shape ( values . view ( \"i8\" ) , ndim = self . ndim ) if isinstance ( other , ABCSeries ) : other = self . _holder ( other ) if isinstance ( other , bool ) : raise TypeError elif is_datetime64_dtype ( other ) : # add the tz back other = self . _holder ( other , dtype = self . dtype ) elif is_null_datetimelike ( other ) : other = tslibs . iNaT elif isinstance ( other , self . _holder ) : if other . tz != self . values . tz : raise ValueError ( \"incompatible or non tz-aware value\" ) other = _block_shape ( other . asi8 , ndim = self . ndim ) elif isinstance ( other , ( np . datetime64 , datetime , date ) ) : other = tslibs . Timestamp ( other ) tz = getattr ( other , 'tz' , None ) # test we can have an equal time zone if tz is None or str ( tz ) != str ( self . values . tz ) : raise ValueError ( \"incompatible or non tz-aware value\" ) other = other . value else : raise TypeError ( other ) return values , other\n\n\n        Summary:", "input": "", "output": "localize and return i8 for the values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_result ( self , result ) : if isinstance ( result , np . ndarray ) : if result . dtype . kind in [ 'i' , 'f' ] : result = result . astype ( 'M8[ns]' ) elif isinstance ( result , ( np . integer , np . float , np . datetime64 ) ) : result = self . _box_func ( result ) if isinstance ( result , np . ndarray ) : # allow passing of > 1dim if its trivial if result . ndim > 1 : result = result . reshape ( np . prod ( result . shape ) ) # GH#24096 new values invalidates a frequency result = self . _holder . _simple_new ( result , freq = None , dtype = self . values . dtype ) return result\n\n\n        Summary:", "input": "", "output": "reverse of try_coerce_args"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def diff ( self , n , axis = 0 ) : if axis == 0 : # Cannot currently calculate diff across multiple blocks since this # function is invoked via apply raise NotImplementedError new_values = ( self . values - self . shift ( n , axis = axis ) [ 0 ] . values ) . asi8 # Reshape the new_values like how algos.diff does for timedelta data new_values = new_values . reshape ( 1 , len ( new_values ) ) new_values = new_values . astype ( 'timedelta64[ns]' ) return [ TimeDeltaBlock ( new_values , placement = self . mgr_locs . indexer ) ]\n\n\n        Summary:", "input": "", "output": "1st discrete difference"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_args ( self , values , other ) : values = values . view ( 'i8' ) if isinstance ( other , bool ) : raise TypeError elif is_null_datetimelike ( other ) : other = tslibs . iNaT elif isinstance ( other , ( timedelta , np . timedelta64 ) ) : other = Timedelta ( other ) . value elif hasattr ( other , 'dtype' ) and is_timedelta64_dtype ( other ) : other = other . astype ( 'i8' , copy = False ) . view ( 'i8' ) else : # coercion issues # let higher levels handle raise TypeError ( other ) return values , other\n\n\n        Summary:", "input": "", "output": "Coerce values and other to int64 with null values converted to iNaT . values is always ndarray - like other may not be"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_result ( self , result ) : if isinstance ( result , np . ndarray ) : mask = isna ( result ) if result . dtype . kind in [ 'i' , 'f' ] : result = result . astype ( 'm8[ns]' ) result [ mask ] = tslibs . iNaT elif isinstance ( result , ( np . integer , np . float ) ) : result = self . _box_func ( result ) return result\n\n\n        Summary:", "input": "", "output": "reverse of try_coerce_args / try_operate"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_native_types ( self , slicer = None , na_rep = None , quoting = None , * * kwargs ) : values = self . values if slicer is not None : values = values [ : , slicer ] mask = isna ( values ) rvalues = np . empty ( values . shape , dtype = object ) if na_rep is None : na_rep = 'NaT' rvalues [ mask ] = na_rep imask = ( ~ mask ) . ravel ( ) # FIXME: # should use the formats.format.Timedelta64Formatter here # to figure what format to pass to the Timedelta # e.g. to not show the decimals say rvalues . flat [ imask ] = np . array ( [ Timedelta ( val ) . _repr_base ( format = 'all' ) for val in values . ravel ( ) [ imask ] ] , dtype = object ) return rvalues\n\n\n        Summary:", "input": "", "output": "convert to our native types format slicing if desired"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert ( self , * args , * * kwargs ) : if args : raise NotImplementedError by_item = kwargs . get ( 'by_item' , True ) new_inputs = [ 'coerce' , 'datetime' , 'numeric' , 'timedelta' ] new_style = False for kw in new_inputs : new_style |= kw in kwargs if new_style : fn = soft_convert_objects fn_inputs = new_inputs else : fn = maybe_convert_objects fn_inputs = [ 'convert_dates' , 'convert_numeric' , 'convert_timedeltas' ] fn_inputs += [ 'copy' ] fn_kwargs = { key : kwargs [ key ] for key in fn_inputs if key in kwargs } # operate column-by-column def f ( m , v , i ) : shape = v . shape values = fn ( v . ravel ( ) , * * fn_kwargs ) try : values = values . reshape ( shape ) values = _block_shape ( values , ndim = self . ndim ) except ( AttributeError , NotImplementedError ) : pass return values if by_item and not self . _is_single_block : blocks = self . split_and_operate ( None , f , False ) else : values = f ( None , self . values . ravel ( ) , None ) blocks = [ make_block ( values , ndim = self . ndim , placement = self . mgr_locs ) ] return blocks\n\n\n        Summary:", "input": "", "output": "attempt to coerce any object types to better types return a copy of the block ( if copy = True ) by definition we ARE an ObjectBlock!!!!!"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set ( self , locs , values ) : try : self . values [ locs ] = values except ( ValueError ) : # broadcasting error # see GH6171 new_shape = list ( values . shape ) new_shape [ 0 ] = len ( self . items ) self . values = np . empty ( tuple ( new_shape ) , dtype = self . dtype ) self . values . fill ( np . nan ) self . values [ locs ] = values\n\n\n        Summary:", "input": "", "output": "Modify Block in - place with new item value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_args ( self , values , other ) : if isinstance ( other , ABCDatetimeIndex ) : # May get a DatetimeIndex here. Unbox it. other = other . array if isinstance ( other , DatetimeArray ) : # hit in pandas/tests/indexing/test_coercion.py # ::TestWhereCoercion::test_where_series_datetime64[datetime64tz] # when falling back to ObjectBlock.where other = other . astype ( object ) return values , other\n\n\n        Summary:", "input": "", "output": "provide coercion to our input arguments"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replace_single ( self , to_replace , value , inplace = False , filter = None , regex = False , convert = True , mask = None ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) # to_replace is regex compilable to_rep_re = regex and is_re_compilable ( to_replace ) # regex is regex compilable regex_re = is_re_compilable ( regex ) # only one will survive if to_rep_re and regex_re : raise AssertionError ( 'only one of to_replace and regex can be ' 'regex compilable' ) # if regex was passed as something that can be a regex (rather than a # boolean) if regex_re : to_replace = regex regex = regex_re or to_rep_re # try to get the pattern attribute (compiled re) or it's a string try : pattern = to_replace . pattern except AttributeError : pattern = to_replace # if the pattern is not empty and to_replace is either a string or a # regex if regex and pattern : rx = re . compile ( to_replace ) else : # if the thing to replace is not a string or compiled regex call # the superclass method -> to_replace is some kind of object return super ( ) . replace ( to_replace , value , inplace = inplace , filter = filter , regex = regex ) new_values = self . values if inplace else self . values . copy ( ) # deal with replacing values with objects (strings) that match but # whose replacement is not a string (numeric, nan, object) if isna ( value ) or not isinstance ( value , str ) : def re_replacer ( s ) : try : return value if rx . search ( s ) is not None else s except TypeError : return s else : # value is guaranteed to be a string here, s can be either a string # or null if it's null it gets returned def re_replacer ( s ) : try : return rx . sub ( value , s ) except TypeError : return s f = np . vectorize ( re_replacer , otypes = [ self . dtype ] ) if filter is None : filt = slice ( None ) else : filt = self . mgr_locs . isin ( filter ) . nonzero ( ) [ 0 ] if mask is None : new_values [ filt ] = f ( new_values [ filt ] ) else : new_values [ filt ] [ mask ] = f ( new_values [ filt ] [ mask ] ) # convert block = self . make_block ( new_values ) if convert : block = block . convert ( by_item = True , numeric = False ) return block\n\n\n        Summary:", "input": "", "output": "Replace elements by the given value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replace_coerce ( self , to_replace , value , inplace = True , regex = False , convert = False , mask = None ) : if mask . any ( ) : block = super ( ) . _replace_coerce ( to_replace = to_replace , value = value , inplace = inplace , regex = regex , convert = convert , mask = mask ) if convert : block = [ b . convert ( by_item = True , numeric = False , copy = True ) for b in block ] return block return self\n\n\n        Summary:", "input": "", "output": "Replace value corresponding to the given boolean array with another value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_coerce_result ( self , result ) : # GH12564: CategoricalBlock is 1-dim only # while returned results could be any dim if ( ( not is_categorical_dtype ( result ) ) and isinstance ( result , np . ndarray ) ) : result = _block_shape ( result , ndim = self . ndim ) return result\n\n\n        Summary:", "input": "", "output": "reverse of try_coerce_args"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_native_types ( self , slicer = None , na_rep = '' , quoting = None , * * kwargs ) : values = self . values if slicer is not None : # Categorical is always one dimension values = values [ slicer ] mask = isna ( values ) values = np . array ( values , dtype = 'object' ) values [ mask ] = na_rep # we are expected to return a 2-d ndarray return values . reshape ( 1 , len ( values ) )\n\n\n        Summary:", "input": "", "output": "convert to our native types format slicing if desired"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _style_to_xlwt ( cls , item , firstlevel = True , field_sep = ',' , line_sep = ';' ) : if hasattr ( item , 'items' ) : if firstlevel : it = [ \"{key}: {val}\" . format ( key = key , val = cls . _style_to_xlwt ( value , False ) ) for key , value in item . items ( ) ] out = \"{sep} \" . format ( sep = ( line_sep ) . join ( it ) ) return out else : it = [ \"{key} {val}\" . format ( key = key , val = cls . _style_to_xlwt ( value , False ) ) for key , value in item . items ( ) ] out = \"{sep} \" . format ( sep = ( field_sep ) . join ( it ) ) return out else : item = \"{item}\" . format ( item = item ) item = item . replace ( \"True\" , \"on\" ) item = item . replace ( \"False\" , \"off\" ) return item\n\n\n        Summary:", "input": "", "output": "helper which recursively generate an xlwt easy style string for example :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_to_style ( cls , style_dict , num_format_str = None ) : import xlwt if style_dict : xlwt_stylestr = cls . _style_to_xlwt ( style_dict ) style = xlwt . easyxf ( xlwt_stylestr , field_sep = ',' , line_sep = ';' ) else : style = xlwt . XFStyle ( ) if num_format_str is not None : style . num_format_str = num_format_str return style\n\n\n        Summary:", "input": "", "output": "converts a style_dict to an xlwt style object Parameters ---------- style_dict : style dictionary to convert num_format_str : optional number format string"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert ( cls , style_dict , num_format_str = None ) : # Create a XlsxWriter format object. props = { } if num_format_str is not None : props [ 'num_format' ] = num_format_str if style_dict is None : return props if 'borders' in style_dict : style_dict = style_dict . copy ( ) style_dict [ 'border' ] = style_dict . pop ( 'borders' ) for style_group_key , style_group in style_dict . items ( ) : for src , dst in cls . STYLE_MAPPING . get ( style_group_key , [ ] ) : # src is a sequence of keys into a nested dict # dst is a flat key if dst in props : continue v = style_group for k in src : try : v = v [ k ] except ( KeyError , TypeError ) : break else : props [ dst ] = v if isinstance ( props . get ( 'pattern' ) , str ) : # TODO: support other fill patterns props [ 'pattern' ] = 0 if props [ 'pattern' ] == 'none' else 1 for k in [ 'border' , 'top' , 'right' , 'bottom' , 'left' ] : if isinstance ( props . get ( k ) , str ) : try : props [ k ] = [ 'none' , 'thin' , 'medium' , 'dashed' , 'dotted' , 'thick' , 'double' , 'hair' , 'mediumDashed' , 'dashDot' , 'mediumDashDot' , 'dashDotDot' , 'mediumDashDotDot' , 'slantDashDot' ] . index ( props [ k ] ) except ValueError : props [ k ] = 2 if isinstance ( props . get ( 'font_script' ) , str ) : props [ 'font_script' ] = [ 'baseline' , 'superscript' , 'subscript' ] . index ( props [ 'font_script' ] ) if isinstance ( props . get ( 'underline' ) , str ) : props [ 'underline' ] = { 'none' : 0 , 'single' : 1 , 'double' : 2 , 'singleAccounting' : 33 , 'doubleAccounting' : 34 } [ props [ 'underline' ] ] return props\n\n\n        Summary:", "input": "", "output": "converts a style_dict to an xlsxwriter format dict"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unstack_extension_series ( series , level , fill_value ) : # Implementation note: the basic idea is to # 1. Do a regular unstack on a dummy array of integers # 2. Followup with a columnwise take. # We use the dummy take to discover newly-created missing values # introduced by the reshape. from pandas . core . reshape . concat import concat dummy_arr = np . arange ( len ( series ) ) # fill_value=-1, since we will do a series.values.take later result = _Unstacker ( dummy_arr , series . index , level = level , fill_value = - 1 ) . get_result ( ) out = [ ] values = extract_array ( series , extract_numpy = False ) for col , indices in result . iteritems ( ) : out . append ( Series ( values . take ( indices . values , allow_fill = True , fill_value = fill_value ) , name = col , index = result . index ) ) return concat ( out , axis = 'columns' , copy = False , keys = result . columns )\n\n\n        Summary:", "input": "", "output": "Unstack an ExtensionArray - backed Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stack ( frame , level = - 1 , dropna = True ) : def factorize ( index ) : if index . is_unique : return index , np . arange ( len ( index ) ) codes , categories = _factorize_from_iterable ( index ) return categories , codes N , K = frame . shape # Will also convert negative level numbers and check if out of bounds. level_num = frame . columns . _get_level_number ( level ) if isinstance ( frame . columns , MultiIndex ) : return _stack_multi_columns ( frame , level_num = level_num , dropna = dropna ) elif isinstance ( frame . index , MultiIndex ) : new_levels = list ( frame . index . levels ) new_codes = [ lab . repeat ( K ) for lab in frame . index . codes ] clev , clab = factorize ( frame . columns ) new_levels . append ( clev ) new_codes . append ( np . tile ( clab , N ) . ravel ( ) ) new_names = list ( frame . index . names ) new_names . append ( frame . columns . name ) new_index = MultiIndex ( levels = new_levels , codes = new_codes , names = new_names , verify_integrity = False ) else : levels , ( ilab , clab ) = zip ( * map ( factorize , ( frame . index , frame . columns ) ) ) codes = ilab . repeat ( K ) , np . tile ( clab , N ) . ravel ( ) new_index = MultiIndex ( levels = levels , codes = codes , names = [ frame . index . name , frame . columns . name ] , verify_integrity = False ) if frame . _is_homogeneous_type : # For homogeneous EAs, frame.values will coerce to object. So # we concatenate instead. dtypes = list ( frame . dtypes . values ) dtype = dtypes [ 0 ] if is_extension_array_dtype ( dtype ) : arr = dtype . construct_array_type ( ) new_values = arr . _concat_same_type ( [ col . _values for _ , col in frame . iteritems ( ) ] ) new_values = _reorder_for_extension_array_stack ( new_values , N , K ) else : # homogeneous, non-EA new_values = frame . values . ravel ( ) else : # non-homogeneous new_values = frame . values . ravel ( ) if dropna : mask = notna ( new_values ) new_values = new_values [ mask ] new_index = new_index [ mask ] return frame . _constructor_sliced ( new_values , index = new_index )\n\n\n        Summary:", "input": "", "output": "Convert DataFrame to Series with multi - level Index . Columns become the second level of the resulting hierarchical index"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_dummies ( data , prefix = None , prefix_sep = '_' , dummy_na = False , columns = None , sparse = False , drop_first = False , dtype = None ) : from pandas . core . reshape . concat import concat from itertools import cycle dtypes_to_encode = [ 'object' , 'category' ] if isinstance ( data , DataFrame ) : # determine columns being encoded if columns is None : data_to_encode = data . select_dtypes ( include = dtypes_to_encode ) else : data_to_encode = data [ columns ] # validate prefixes and separator to avoid silently dropping cols def check_len ( item , name ) : len_msg = ( \"Length of '{name}' ({len_item}) did not match the \" \"length of the columns being encoded ({len_enc}).\" ) if is_list_like ( item ) : if not len ( item ) == data_to_encode . shape [ 1 ] : len_msg = len_msg . format ( name = name , len_item = len ( item ) , len_enc = data_to_encode . shape [ 1 ] ) raise ValueError ( len_msg ) check_len ( prefix , 'prefix' ) check_len ( prefix_sep , 'prefix_sep' ) if isinstance ( prefix , str ) : prefix = cycle ( [ prefix ] ) if isinstance ( prefix , dict ) : prefix = [ prefix [ col ] for col in data_to_encode . columns ] if prefix is None : prefix = data_to_encode . columns # validate separators if isinstance ( prefix_sep , str ) : prefix_sep = cycle ( [ prefix_sep ] ) elif isinstance ( prefix_sep , dict ) : prefix_sep = [ prefix_sep [ col ] for col in data_to_encode . columns ] if data_to_encode . shape == data . shape : # Encoding the entire df, do not prepend any dropped columns with_dummies = [ ] elif columns is not None : # Encoding only cols specified in columns. Get all cols not in # columns to prepend to result. with_dummies = [ data . drop ( columns , axis = 1 ) ] else : # Encoding only object and category dtype columns. Get remaining # columns to prepend to result. with_dummies = [ data . select_dtypes ( exclude = dtypes_to_encode ) ] for ( col , pre , sep ) in zip ( data_to_encode . iteritems ( ) , prefix , prefix_sep ) : # col is (column_name, column), use just column data here dummy = _get_dummies_1d ( col [ 1 ] , prefix = pre , prefix_sep = sep , dummy_na = dummy_na , sparse = sparse , drop_first = drop_first , dtype = dtype ) with_dummies . append ( dummy ) result = concat ( with_dummies , axis = 1 ) else : result = _get_dummies_1d ( data , prefix , prefix_sep , dummy_na , sparse = sparse , drop_first = drop_first , dtype = dtype ) return result\n\n\n        Summary:", "input": "", "output": "Convert categorical variable into dummy / indicator variables ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_axis_dummies ( frame , axis = 'minor' , transform = None ) : numbers = { 'major' : 0 , 'minor' : 1 } num = numbers . get ( axis , axis ) items = frame . index . levels [ num ] codes = frame . index . codes [ num ] if transform is not None : mapped_items = items . map ( transform ) codes , items = _factorize_from_iterable ( mapped_items . take ( codes ) ) values = np . eye ( len ( items ) , dtype = float ) values = values . take ( codes , axis = 0 ) return DataFrame ( values , columns = items , index = frame . index )\n\n\n        Summary:", "input": "", "output": "Construct 1 - 0 dummy variables corresponding to designated axis labels"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reorder_for_extension_array_stack ( arr , n_rows , n_columns ) : # final take to get the order correct. # idx is an indexer like # [c0r0, c1r0, c2r0, ..., # c0r1, c1r1, c2r1, ...] idx = np . arange ( n_rows * n_columns ) . reshape ( n_columns , n_rows ) . T . ravel ( ) return arr . take ( idx )\n\n\n        Summary:", "input": "", "output": "Re - orders the values when stacking multiple extension - arrays ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _split_line ( s , parts ) : out = { } start = 0 for name , length in parts : out [ name ] = s [ start : start + length ] . strip ( ) start += length del out [ '_' ] return out\n\n\n        Summary:", "input": "", "output": "Parameters ---------- s : string Fixed - length string to split parts : list of ( name length ) pairs Used to break up string name _ will be filtered from output ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_float_vec ( vec ) : dtype = np . dtype ( '>u4,>u4' ) vec1 = vec . view ( dtype = dtype ) xport1 = vec1 [ 'f0' ] xport2 = vec1 [ 'f1' ] # Start by setting first half of ieee number to first half of IBM # number sans exponent ieee1 = xport1 & 0x00ffffff # The fraction bit to the left of the binary point in the ieee # format was set and the number was shifted 0, 1, 2, or 3 # places. This will tell us how to adjust the ibm exponent to be a # power of 2 ieee exponent and how to shift the fraction bits to # restore the correct magnitude. shift = np . zeros ( len ( vec ) , dtype = np . uint8 ) shift [ np . where ( xport1 & 0x00200000 ) ] = 1 shift [ np . where ( xport1 & 0x00400000 ) ] = 2 shift [ np . where ( xport1 & 0x00800000 ) ] = 3 # shift the ieee number down the correct number of places then # set the second half of the ieee number to be the second half # of the ibm number shifted appropriately, ored with the bits # from the first half that would have been shifted in if we # could shift a double. All we are worried about are the low # order 3 bits of the first half since we're only shifting by # 1, 2, or 3. ieee1 >>= shift ieee2 = ( xport2 >> shift ) | ( ( xport1 & 0x00000007 ) << ( 29 + ( 3 - shift ) ) ) # clear the 1 bit to the left of the binary point ieee1 &= 0xffefffff # set the exponent of the ieee number to be the actual exponent # plus the shift count + 1023. Or this into the first half of the # ieee number. The ibm exponent is excess 64 but is adjusted by 65 # since during conversion to ibm format the exponent is # incremented by 1 and the fraction bits left 4 positions to the # right of the radix point. (had to add >> 24 because C treats & # 0x7f as 0x7f000000 and Python doesn't) ieee1 |= ( ( ( ( ( ( xport1 >> 24 ) & 0x7f ) - 65 ) << 2 ) + shift + 1023 ) << 20 ) | ( xport1 & 0x80000000 ) ieee = np . empty ( ( len ( ieee1 ) , ) , dtype = '>u4,>u4' ) ieee [ 'f0' ] = ieee1 ieee [ 'f1' ] = ieee2 ieee = ieee . view ( dtype = '>f8' ) ieee = ieee . astype ( 'f8' ) return ieee\n\n\n        Summary:", "input": "", "output": "Parse a vector of float values representing IBM 8 byte floats into native 8 byte floats ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _record_count ( self ) : self . filepath_or_buffer . seek ( 0 , 2 ) total_records_length = ( self . filepath_or_buffer . tell ( ) - self . record_start ) if total_records_length % 80 != 0 : warnings . warn ( \"xport file may be corrupted\" ) if self . record_length > 80 : self . filepath_or_buffer . seek ( self . record_start ) return total_records_length // self . record_length self . filepath_or_buffer . seek ( - 80 , 2 ) last_card = self . filepath_or_buffer . read ( 80 ) last_card = np . frombuffer ( last_card , dtype = np . uint64 ) # 8 byte blank ix = np . flatnonzero ( last_card == 2314885530818453536 ) if len ( ix ) == 0 : tail_pad = 0 else : tail_pad = 8 * len ( ix ) self . filepath_or_buffer . seek ( self . record_start ) return ( total_records_length - tail_pad ) // self . record_length\n\n\n        Summary:", "input": "", "output": "Get number of records in file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_chunk ( self , size = None ) : if size is None : size = self . _chunksize return self . read ( nrows = size )\n\n\n        Summary:", "input": "", "output": "Reads lines from Xport file and returns as dataframe"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def construction_error ( tot_items , block_shape , axes , e = None ) : passed = tuple ( map ( int , [ tot_items ] + list ( block_shape ) ) ) # Correcting the user facing error message during dataframe construction if len ( passed ) <= 2 : passed = passed [ : : - 1 ] implied = tuple ( len ( ax ) for ax in axes ) # Correcting the user facing error message during dataframe construction if len ( implied ) <= 2 : implied = implied [ : : - 1 ] if passed == implied and e is not None : raise e if block_shape [ 0 ] == 0 : raise ValueError ( \"Empty data passed with indices specified.\" ) raise ValueError ( \"Shape of passed values is {0}, indices imply {1}\" . format ( passed , implied ) )\n\n\n        Summary:", "input": "", "output": "raise a helpful message about our construction"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _simple_blockify ( tuples , dtype ) : values , placement = _stack_arrays ( tuples , dtype ) # CHECK DTYPE? if dtype is not None and values . dtype != dtype : # pragma: no cover values = values . astype ( dtype ) block = make_block ( values , placement = placement ) return [ block ]\n\n\n        Summary:", "input": "", "output": "return a single array of a block that has a single dtype ; if dtype is not None coerce to this dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _multi_blockify ( tuples , dtype = None ) : # group by dtype grouper = itertools . groupby ( tuples , lambda x : x [ 2 ] . dtype ) new_blocks = [ ] for dtype , tup_block in grouper : values , placement = _stack_arrays ( list ( tup_block ) , dtype ) block = make_block ( values , placement = placement ) new_blocks . append ( block ) return new_blocks\n\n\n        Summary:", "input": "", "output": "return an array of blocks that potentially have different dtypes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sparse_blockify ( tuples , dtype = None ) : new_blocks = [ ] for i , names , array in tuples : array = _maybe_to_sparse ( array ) block = make_block ( array , placement = [ i ] ) new_blocks . append ( block ) return new_blocks\n\n\n        Summary:", "input": "", "output": "return an array of blocks that potentially have different dtypes ( and are sparse )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _interleaved_dtype ( blocks : List [ Block ] ) -> Optional [ Union [ np . dtype , ExtensionDtype ] ] : if not len ( blocks ) : return None return find_common_type ( [ b . dtype for b in blocks ] )\n\n\n        Summary:", "input": "", "output": "Find the common dtype for blocks ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _consolidate ( blocks ) : # sort by _can_consolidate, dtype gkey = lambda x : x . _consolidate_key grouper = itertools . groupby ( sorted ( blocks , key = gkey ) , gkey ) new_blocks = [ ] for ( _can_consolidate , dtype ) , group_blocks in grouper : merged_blocks = _merge_blocks ( list ( group_blocks ) , dtype = dtype , _can_consolidate = _can_consolidate ) new_blocks = _extend_blocks ( merged_blocks , new_blocks ) return new_blocks\n\n\n        Summary:", "input": "", "output": "Merge blocks having same dtype exclude non - consolidating blocks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _compare_or_regex_search ( a , b , regex = False ) : if not regex : op = lambda x : operator . eq ( x , b ) else : op = np . vectorize ( lambda x : bool ( re . search ( b , x ) ) if isinstance ( x , str ) else False ) is_a_array = isinstance ( a , np . ndarray ) is_b_array = isinstance ( b , np . ndarray ) # numpy deprecation warning to have i8 vs integer comparisons if is_datetimelike_v_numeric ( a , b ) : result = False # numpy deprecation warning if comparing numeric vs string-like elif is_numeric_v_string_like ( a , b ) : result = False else : result = op ( a ) if is_scalar ( result ) and ( is_a_array or is_b_array ) : type_names = [ type ( a ) . __name__ , type ( b ) . __name__ ] if is_a_array : type_names [ 0 ] = 'ndarray(dtype={dtype})' . format ( dtype = a . dtype ) if is_b_array : type_names [ 1 ] = 'ndarray(dtype={dtype})' . format ( dtype = b . dtype ) raise TypeError ( \"Cannot compare types {a!r} and {b!r}\" . format ( a = type_names [ 0 ] , b = type_names [ 1 ] ) ) return result\n\n\n        Summary:", "input": "", "output": "Compare two array_like inputs of the same shape or two scalar values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def items_overlap_with_suffix ( left , lsuffix , right , rsuffix ) : to_rename = left . intersection ( right ) if len ( to_rename ) == 0 : return left , right else : if not lsuffix and not rsuffix : raise ValueError ( 'columns overlap but no suffix specified: ' '{rename}' . format ( rename = to_rename ) ) def renamer ( x , suffix ) : \"\"\"Rename the left and right indices. If there is overlap, and suffix is not None, add suffix, otherwise, leave it as-is. Parameters ---------- x : original column name suffix : str or None Returns ------- x : renamed column name \"\"\" if x in to_rename and suffix is not None : return '{x}{suffix}' . format ( x = x , suffix = suffix ) return x lrenamer = partial ( renamer , suffix = lsuffix ) rrenamer = partial ( renamer , suffix = rsuffix ) return ( _transform_index ( left , lrenamer ) , _transform_index ( right , rrenamer ) )\n\n\n        Summary:", "input": "", "output": "If two indices overlap add suffixes to overlapping entries ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _transform_index ( index , func , level = None ) : if isinstance ( index , MultiIndex ) : if level is not None : items = [ tuple ( func ( y ) if i == level else y for i , y in enumerate ( x ) ) for x in index ] else : items = [ tuple ( func ( y ) for y in x ) for x in index ] return MultiIndex . from_tuples ( items , names = index . names ) else : items = [ func ( x ) for x in index ] return Index ( items , name = index . name , tupleize_cols = False )\n\n\n        Summary:", "input": "", "output": "Apply function to all values found in index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _fast_count_smallints ( arr ) : counts = np . bincount ( arr . astype ( np . int_ ) ) nz = counts . nonzero ( ) [ 0 ] return np . c_ [ nz , counts [ nz ] ]\n\n\n        Summary:", "input": "", "output": "Faster version of set ( arr ) for sequences of small numbers ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concatenate_block_managers ( mgrs_indexers , axes , concat_axis , copy ) : concat_plans = [ get_mgr_concatenation_plan ( mgr , indexers ) for mgr , indexers in mgrs_indexers ] concat_plan = combine_concat_plans ( concat_plans , concat_axis ) blocks = [ ] for placement , join_units in concat_plan : if len ( join_units ) == 1 and not join_units [ 0 ] . indexers : b = join_units [ 0 ] . block values = b . values if copy : values = values . copy ( ) elif not copy : values = values . view ( ) b = b . make_block_same_class ( values , placement = placement ) elif is_uniform_join_units ( join_units ) : b = join_units [ 0 ] . block . concat_same_type ( [ ju . block for ju in join_units ] , placement = placement ) else : b = make_block ( concatenate_join_units ( join_units , concat_axis , copy = copy ) , placement = placement ) blocks . append ( b ) return BlockManager ( blocks , axes )\n\n\n        Summary:", "input": "", "output": "Concatenate block managers into one ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_empty ( self , axes = None ) : if axes is None : axes = [ ensure_index ( [ ] ) ] + [ ensure_index ( a ) for a in self . axes [ 1 : ] ] # preserve dtype if possible if self . ndim == 1 : blocks = np . array ( [ ] , dtype = self . array_dtype ) else : blocks = [ ] return self . __class__ ( blocks , axes )\n\n\n        Summary:", "input": "", "output": "return an empty BlockManager with the items axis of len 0"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename_axis ( self , mapper , axis , copy = True , level = None ) : obj = self . copy ( deep = copy ) obj . set_axis ( axis , _transform_index ( self . axes [ axis ] , mapper , level ) ) return obj\n\n\n        Summary:", "input": "", "output": "Rename one of axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _rebuild_blknos_and_blklocs ( self ) : new_blknos = np . empty ( self . shape [ 0 ] , dtype = np . int64 ) new_blklocs = np . empty ( self . shape [ 0 ] , dtype = np . int64 ) new_blknos . fill ( - 1 ) new_blklocs . fill ( - 1 ) for blkno , blk in enumerate ( self . blocks ) : rl = blk . mgr_locs new_blknos [ rl . indexer ] = blkno new_blklocs [ rl . indexer ] = np . arange ( len ( rl ) ) if ( new_blknos == - 1 ) . any ( ) : raise AssertionError ( \"Gaps in blk ref_locs\" ) self . _blknos = new_blknos self . _blklocs = new_blklocs\n\n\n        Summary:", "input": "", "output": "Update mgr . _blknos / mgr . _blklocs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_counts ( self , f ) : self . _consolidate_inplace ( ) counts = dict ( ) for b in self . blocks : v = f ( b ) counts [ v ] = counts . get ( v , 0 ) + b . shape [ 0 ] return counts\n\n\n        Summary:", "input": "", "output": "return a dict of the counts of the function in BlockManager"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , f , axes = None , filter = None , do_integrity_check = False , consolidate = True , * * kwargs ) : result_blocks = [ ] # filter kwarg is used in replace-* family of methods if filter is not None : filter_locs = set ( self . items . get_indexer_for ( filter ) ) if len ( filter_locs ) == len ( self . items ) : # All items are included, as if there were no filtering filter = None else : kwargs [ 'filter' ] = filter_locs if consolidate : self . _consolidate_inplace ( ) if f == 'where' : align_copy = True if kwargs . get ( 'align' , True ) : align_keys = [ 'other' , 'cond' ] else : align_keys = [ 'cond' ] elif f == 'putmask' : align_copy = False if kwargs . get ( 'align' , True ) : align_keys = [ 'new' , 'mask' ] else : align_keys = [ 'mask' ] elif f == 'fillna' : # fillna internally does putmask, maybe it's better to do this # at mgr, not block level? align_copy = False align_keys = [ 'value' ] else : align_keys = [ ] # TODO(EA): may interfere with ExtensionBlock.setitem for blocks # with a .values attribute. aligned_args = { k : kwargs [ k ] for k in align_keys if hasattr ( kwargs [ k ] , 'values' ) and not isinstance ( kwargs [ k ] , ABCExtensionArray ) } for b in self . blocks : if filter is not None : if not b . mgr_locs . isin ( filter_locs ) . any ( ) : result_blocks . append ( b ) continue if aligned_args : b_items = self . items [ b . mgr_locs . indexer ] for k , obj in aligned_args . items ( ) : axis = getattr ( obj , '_info_axis_number' , 0 ) kwargs [ k ] = obj . reindex ( b_items , axis = axis , copy = align_copy ) applied = getattr ( b , f ) ( * * kwargs ) result_blocks = _extend_blocks ( applied , result_blocks ) if len ( result_blocks ) == 0 : return self . make_empty ( axes or self . axes ) bm = self . __class__ ( result_blocks , axes or self . axes , do_integrity_check = do_integrity_check ) bm . _consolidate_inplace ( ) return bm\n\n\n        Summary:", "input": "", "output": "iterate over the blocks collect and create a new block manager"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def quantile ( self , axis = 0 , consolidate = True , transposed = False , interpolation = 'linear' , qs = None , numeric_only = None ) : # Series dispatches to DataFrame for quantile, which allows us to # simplify some of the code here and in the blocks assert self . ndim >= 2 if consolidate : self . _consolidate_inplace ( ) def get_axe ( block , qs , axes ) : from pandas import Float64Index if is_list_like ( qs ) : ax = Float64Index ( qs ) elif block . ndim == 1 : ax = Float64Index ( [ qs ] ) else : ax = axes [ 0 ] return ax axes , blocks = [ ] , [ ] for b in self . blocks : block = b . quantile ( axis = axis , qs = qs , interpolation = interpolation ) axe = get_axe ( b , qs , axes = self . axes ) axes . append ( axe ) blocks . append ( block ) # note that some DatetimeTZ, Categorical are always ndim==1 ndim = { b . ndim for b in blocks } assert 0 not in ndim , ndim if 2 in ndim : new_axes = list ( self . axes ) # multiple blocks that are reduced if len ( blocks ) > 1 : new_axes [ 1 ] = axes [ 0 ] # reset the placement to the original for b , sb in zip ( blocks , self . blocks ) : b . mgr_locs = sb . mgr_locs else : new_axes [ axis ] = Index ( np . concatenate ( [ ax . values for ax in axes ] ) ) if transposed : new_axes = new_axes [ : : - 1 ] blocks = [ b . make_block ( b . values . T , placement = np . arange ( b . shape [ 1 ] ) ) for b in blocks ] return self . __class__ ( blocks , new_axes ) # single block, i.e. ndim == {1} values = _concat . _concat_compat ( [ b . values for b in blocks ] ) # compute the orderings of our original data if len ( self . blocks ) > 1 : indexer = np . empty ( len ( self . axes [ 0 ] ) , dtype = np . intp ) i = 0 for b in self . blocks : for j in b . mgr_locs : indexer [ j ] = i i = i + 1 values = values . take ( indexer ) return SingleBlockManager ( [ make_block ( values , ndim = 1 , placement = np . arange ( len ( values ) ) ) ] , axes [ 0 ] )\n\n\n        Summary:", "input": "", "output": "Iterate over blocks applying quantile reduction . This routine is intended for reduction type operations and will do inference on the generated blocks ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def replace_list ( self , src_list , dest_list , inplace = False , regex = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) # figure out our mask a-priori to avoid repeated replacements values = self . as_array ( ) def comp ( s , regex = False ) : \"\"\" Generate a bool array by perform an equality check, or perform an element-wise regular expression matching \"\"\" if isna ( s ) : return isna ( values ) if hasattr ( s , 'asm8' ) : return _compare_or_regex_search ( maybe_convert_objects ( values ) , getattr ( s , 'asm8' ) , regex ) return _compare_or_regex_search ( values , s , regex ) masks = [ comp ( s , regex ) for i , s in enumerate ( src_list ) ] result_blocks = [ ] src_len = len ( src_list ) - 1 for blk in self . blocks : # its possible to get multiple result blocks here # replace ALWAYS will return a list rb = [ blk if inplace else blk . copy ( ) ] for i , ( s , d ) in enumerate ( zip ( src_list , dest_list ) ) : new_rb = [ ] for b in rb : m = masks [ i ] [ b . mgr_locs . indexer ] convert = i == src_len result = b . _replace_coerce ( mask = m , to_replace = s , value = d , inplace = inplace , convert = convert , regex = regex ) if m . any ( ) : new_rb = _extend_blocks ( result , new_rb ) else : new_rb . append ( b ) rb = new_rb result_blocks . extend ( rb ) bm = self . __class__ ( result_blocks , self . axes ) bm . _consolidate_inplace ( ) return bm\n\n\n        Summary:", "input": "", "output": "do a list replace"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_bool_data ( self , copy = False ) : self . _consolidate_inplace ( ) return self . combine ( [ b for b in self . blocks if b . is_bool ] , copy )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- copy : boolean default False Whether to copy the blocks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_numeric_data ( self , copy = False ) : self . _consolidate_inplace ( ) return self . combine ( [ b for b in self . blocks if b . is_numeric ] , copy )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- copy : boolean default False Whether to copy the blocks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine ( self , blocks , copy = True ) : if len ( blocks ) == 0 : return self . make_empty ( ) # FIXME: optimization potential indexer = np . sort ( np . concatenate ( [ b . mgr_locs . as_array for b in blocks ] ) ) inv_indexer = lib . get_reverse_indexer ( indexer , self . shape [ 0 ] ) new_blocks = [ ] for b in blocks : b = b . copy ( deep = copy ) b . mgr_locs = algos . take_1d ( inv_indexer , b . mgr_locs . as_array , axis = 0 , allow_fill = False ) new_blocks . append ( b ) axes = list ( self . axes ) axes [ 0 ] = self . items . take ( indexer ) return self . __class__ ( new_blocks , axes , do_integrity_check = False )\n\n\n        Summary:", "input": "", "output": "return a new manager with the blocks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , deep = True ) : # this preserves the notion of view copying of axes if deep : if deep == 'all' : copy = lambda ax : ax . copy ( deep = True ) else : copy = lambda ax : ax . view ( ) new_axes = [ copy ( ax ) for ax in self . axes ] else : new_axes = list ( self . axes ) return self . apply ( 'copy' , axes = new_axes , deep = deep , do_integrity_check = False )\n\n\n        Summary:", "input": "", "output": "Make deep or shallow copy of BlockManager"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def as_array ( self , transpose = False , items = None ) : if len ( self . blocks ) == 0 : arr = np . empty ( self . shape , dtype = float ) return arr . transpose ( ) if transpose else arr if items is not None : mgr = self . reindex_axis ( items , axis = 0 ) else : mgr = self if self . _is_single_block and mgr . blocks [ 0 ] . is_datetimetz : # TODO(Block.get_values): Make DatetimeTZBlock.get_values # always be object dtype. Some callers seem to want the # DatetimeArray (previously DTI) arr = mgr . blocks [ 0 ] . get_values ( dtype = object ) elif self . _is_single_block or not self . is_mixed_type : arr = np . asarray ( mgr . blocks [ 0 ] . get_values ( ) ) else : arr = mgr . _interleave ( ) return arr . transpose ( ) if transpose else arr\n\n\n        Summary:", "input": "", "output": "Convert the blockmanager data into an numpy array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _interleave ( self ) : from pandas . core . dtypes . common import is_sparse dtype = _interleaved_dtype ( self . blocks ) # TODO: https://github.com/pandas-dev/pandas/issues/22791 # Give EAs some input on what happens here. Sparse needs this. if is_sparse ( dtype ) : dtype = dtype . subtype elif is_extension_array_dtype ( dtype ) : dtype = 'object' result = np . empty ( self . shape , dtype = dtype ) itemmask = np . zeros ( self . shape [ 0 ] ) for blk in self . blocks : rl = blk . mgr_locs result [ rl . indexer ] = blk . get_values ( dtype ) itemmask [ rl . indexer ] = 1 if not itemmask . all ( ) : raise AssertionError ( 'Some items were not contained in blocks' ) return result\n\n\n        Summary:", "input": "", "output": "Return ndarray from blocks with specified item order Items must be contained in the blocks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_dict ( self , copy = True ) : self . _consolidate_inplace ( ) bd = { } for b in self . blocks : bd . setdefault ( str ( b . dtype ) , [ ] ) . append ( b ) return { dtype : self . combine ( blocks , copy = copy ) for dtype , blocks in bd . items ( ) }\n\n\n        Summary:", "input": "", "output": "Return a dict of str ( dtype ) - > BlockManager"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fast_xs ( self , loc ) : if len ( self . blocks ) == 1 : return self . blocks [ 0 ] . iget ( ( slice ( None ) , loc ) ) items = self . items # non-unique (GH4726) if not items . is_unique : result = self . _interleave ( ) if self . ndim == 2 : result = result . T return result [ loc ] # unique dtype = _interleaved_dtype ( self . blocks ) n = len ( items ) if is_extension_array_dtype ( dtype ) : # we'll eventually construct an ExtensionArray. result = np . empty ( n , dtype = object ) else : result = np . empty ( n , dtype = dtype ) for blk in self . blocks : # Such assignment may incorrectly coerce NaT to None # result[blk.mgr_locs] = blk._slice((slice(None), loc)) for i , rl in enumerate ( blk . mgr_locs ) : result [ rl ] = blk . _try_coerce_result ( blk . iget ( ( i , loc ) ) ) if is_extension_array_dtype ( dtype ) : result = dtype . construct_array_type ( ) . _from_sequence ( result , dtype = dtype ) return result\n\n\n        Summary:", "input": "", "output": "get a cross sectional for a given location in the items ; handle dups"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def consolidate ( self ) : if self . is_consolidated ( ) : return self bm = self . __class__ ( self . blocks , self . axes ) bm . _is_consolidated = False bm . _consolidate_inplace ( ) return bm\n\n\n        Summary:", "input": "", "output": "Join together blocks having same dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( self , item , fastpath = True ) : if self . items . is_unique : if not isna ( item ) : loc = self . items . get_loc ( item ) else : indexer = np . arange ( len ( self . items ) ) [ isna ( self . items ) ] # allow a single nan location indexer if not is_scalar ( indexer ) : if len ( indexer ) == 1 : loc = indexer . item ( ) else : raise ValueError ( \"cannot label index with a null key\" ) return self . iget ( loc , fastpath = fastpath ) else : if isna ( item ) : raise TypeError ( \"cannot label index with a null key\" ) indexer = self . items . get_indexer_for ( [ item ] ) return self . reindex_indexer ( new_axis = self . items [ indexer ] , indexer = indexer , axis = 0 , allow_dups = True )\n\n\n        Summary:", "input": "", "output": "Return values for selected item ( ndarray or BlockManager ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def iget ( self , i , fastpath = True ) : block = self . blocks [ self . _blknos [ i ] ] values = block . iget ( self . _blklocs [ i ] ) if not fastpath or not block . _box_to_block_values or values . ndim != 1 : return values # fastpath shortcut for select a single-dim from a 2-dim BM return SingleBlockManager ( [ block . make_block_same_class ( values , placement = slice ( 0 , len ( values ) ) , ndim = 1 ) ] , self . axes [ 1 ] )\n\n\n        Summary:", "input": "", "output": "Return the data as a SingleBlockManager if fastpath = True and possible"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def delete ( self , item ) : indexer = self . items . get_loc ( item ) is_deleted = np . zeros ( self . shape [ 0 ] , dtype = np . bool_ ) is_deleted [ indexer ] = True ref_loc_offset = - is_deleted . cumsum ( ) is_blk_deleted = [ False ] * len ( self . blocks ) if isinstance ( indexer , int ) : affected_start = indexer else : affected_start = is_deleted . nonzero ( ) [ 0 ] [ 0 ] for blkno , _ in _fast_count_smallints ( self . _blknos [ affected_start : ] ) : blk = self . blocks [ blkno ] bml = blk . mgr_locs blk_del = is_deleted [ bml . indexer ] . nonzero ( ) [ 0 ] if len ( blk_del ) == len ( bml ) : is_blk_deleted [ blkno ] = True continue elif len ( blk_del ) != 0 : blk . delete ( blk_del ) bml = blk . mgr_locs blk . mgr_locs = bml . add ( ref_loc_offset [ bml . indexer ] ) # FIXME: use Index.delete as soon as it uses fastpath=True self . axes [ 0 ] = self . items [ ~ is_deleted ] self . blocks = tuple ( b for blkno , b in enumerate ( self . blocks ) if not is_blk_deleted [ blkno ] ) self . _shape = None self . _rebuild_blknos_and_blklocs ( )\n\n\n        Summary:", "input": "", "output": "Delete selected item ( items if non - unique ) in - place ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set ( self , item , value ) : # FIXME: refactor, clearly separate broadcasting & zip-like assignment # can prob also fix the various if tests for sparse/categorical # TODO(EA): Remove an is_extension_ when all extension types satisfy # the interface value_is_extension_type = ( is_extension_type ( value ) or is_extension_array_dtype ( value ) ) # categorical/spares/datetimetz if value_is_extension_type : def value_getitem ( placement ) : return value else : if value . ndim == self . ndim - 1 : value = _safe_reshape ( value , ( 1 , ) + value . shape ) def value_getitem ( placement ) : return value else : def value_getitem ( placement ) : return value [ placement . indexer ] if value . shape [ 1 : ] != self . shape [ 1 : ] : raise AssertionError ( 'Shape of new values must be compatible ' 'with manager shape' ) try : loc = self . items . get_loc ( item ) except KeyError : # This item wasn't present, just insert at end self . insert ( len ( self . items ) , item , value ) return if isinstance ( loc , int ) : loc = [ loc ] blknos = self . _blknos [ loc ] blklocs = self . _blklocs [ loc ] . copy ( ) unfit_mgr_locs = [ ] unfit_val_locs = [ ] removed_blknos = [ ] for blkno , val_locs in libinternals . get_blkno_placements ( blknos , self . nblocks , group = True ) : blk = self . blocks [ blkno ] blk_locs = blklocs [ val_locs . indexer ] if blk . should_store ( value ) : blk . set ( blk_locs , value_getitem ( val_locs ) ) else : unfit_mgr_locs . append ( blk . mgr_locs . as_array [ blk_locs ] ) unfit_val_locs . append ( val_locs ) # If all block items are unfit, schedule the block for removal. if len ( val_locs ) == len ( blk . mgr_locs ) : removed_blknos . append ( blkno ) else : self . _blklocs [ blk . mgr_locs . indexer ] = - 1 blk . delete ( blk_locs ) self . _blklocs [ blk . mgr_locs . indexer ] = np . arange ( len ( blk ) ) if len ( removed_blknos ) : # Remove blocks & update blknos accordingly is_deleted = np . zeros ( self . nblocks , dtype = np . bool_ ) is_deleted [ removed_blknos ] = True new_blknos = np . empty ( self . nblocks , dtype = np . int64 ) new_blknos . fill ( - 1 ) new_blknos [ ~ is_deleted ] = np . arange ( self . nblocks - len ( removed_blknos ) ) self . _blknos = algos . take_1d ( new_blknos , self . _blknos , axis = 0 , allow_fill = False ) self . blocks = tuple ( blk for i , blk in enumerate ( self . blocks ) if i not in set ( removed_blknos ) ) if unfit_val_locs : unfit_mgr_locs = np . concatenate ( unfit_mgr_locs ) unfit_count = len ( unfit_mgr_locs ) new_blocks = [ ] if value_is_extension_type : # This code (ab-)uses the fact that sparse blocks contain only # one item. new_blocks . extend ( make_block ( values = value . copy ( ) , ndim = self . ndim , placement = slice ( mgr_loc , mgr_loc + 1 ) ) for mgr_loc in unfit_mgr_locs ) self . _blknos [ unfit_mgr_locs ] = ( np . arange ( unfit_count ) + len ( self . blocks ) ) self . _blklocs [ unfit_mgr_locs ] = 0 else : # unfit_val_locs contains BlockPlacement objects unfit_val_items = unfit_val_locs [ 0 ] . append ( unfit_val_locs [ 1 : ] ) new_blocks . append ( make_block ( values = value_getitem ( unfit_val_items ) , ndim = self . ndim , placement = unfit_mgr_locs ) ) self . _blknos [ unfit_mgr_locs ] = len ( self . blocks ) self . _blklocs [ unfit_mgr_locs ] = np . arange ( unfit_count ) self . blocks += tuple ( new_blocks ) # Newly created block's dtype may already be present. self . _known_consolidated = False\n\n\n        Summary:", "input": "", "output": "Set new item in - place . Does not consolidate . Adds new Block if not contained in the current set of items"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def insert ( self , loc , item , value , allow_duplicates = False ) : if not allow_duplicates and item in self . items : # Should this be a different kind of error?? raise ValueError ( 'cannot insert {}, already exists' . format ( item ) ) if not isinstance ( loc , int ) : raise TypeError ( \"loc must be int\" ) # insert to the axis; this could possibly raise a TypeError new_axis = self . items . insert ( loc , item ) block = make_block ( values = value , ndim = self . ndim , placement = slice ( loc , loc + 1 ) ) for blkno , count in _fast_count_smallints ( self . _blknos [ loc : ] ) : blk = self . blocks [ blkno ] if count == len ( blk . mgr_locs ) : blk . mgr_locs = blk . mgr_locs . add ( 1 ) else : new_mgr_locs = blk . mgr_locs . as_array . copy ( ) new_mgr_locs [ new_mgr_locs >= loc ] += 1 blk . mgr_locs = new_mgr_locs if loc == self . _blklocs . shape [ 0 ] : # np.append is a lot faster, let's use it if we can. self . _blklocs = np . append ( self . _blklocs , 0 ) self . _blknos = np . append ( self . _blknos , len ( self . blocks ) ) else : self . _blklocs = np . insert ( self . _blklocs , loc , 0 ) self . _blknos = np . insert ( self . _blknos , loc , len ( self . blocks ) ) self . axes [ 0 ] = new_axis self . blocks += ( block , ) self . _shape = None self . _known_consolidated = False if len ( self . blocks ) > 100 : self . _consolidate_inplace ( )\n\n\n        Summary:", "input": "", "output": "Insert item at selected position ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reindex_axis ( self , new_index , axis , method = None , limit = None , fill_value = None , copy = True ) : new_index = ensure_index ( new_index ) new_index , indexer = self . axes [ axis ] . reindex ( new_index , method = method , limit = limit ) return self . reindex_indexer ( new_index , indexer , axis = axis , fill_value = fill_value , copy = copy )\n\n\n        Summary:", "input": "", "output": "Conform block manager to new index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reindex_indexer ( self , new_axis , indexer , axis , fill_value = None , allow_dups = False , copy = True ) : if indexer is None : if new_axis is self . axes [ axis ] and not copy : return self result = self . copy ( deep = copy ) result . axes = list ( self . axes ) result . axes [ axis ] = new_axis return result self . _consolidate_inplace ( ) # some axes don't allow reindexing with dups if not allow_dups : self . axes [ axis ] . _can_reindex ( indexer ) if axis >= self . ndim : raise IndexError ( \"Requested axis not found in manager\" ) if axis == 0 : new_blocks = self . _slice_take_blocks_ax0 ( indexer , fill_tuple = ( fill_value , ) ) else : new_blocks = [ blk . take_nd ( indexer , axis = axis , fill_tuple = ( fill_value if fill_value is not None else blk . fill_value , ) ) for blk in self . blocks ] new_axes = list ( self . axes ) new_axes [ axis ] = new_axis return self . __class__ ( new_blocks , new_axes )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- new_axis : Index indexer : ndarray of int64 or None axis : int fill_value : object allow_dups : bool"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _slice_take_blocks_ax0 ( self , slice_or_indexer , fill_tuple = None ) : allow_fill = fill_tuple is not None sl_type , slobj , sllen = _preprocess_slice_or_indexer ( slice_or_indexer , self . shape [ 0 ] , allow_fill = allow_fill ) if self . _is_single_block : blk = self . blocks [ 0 ] if sl_type in ( 'slice' , 'mask' ) : return [ blk . getitem_block ( slobj , new_mgr_locs = slice ( 0 , sllen ) ) ] elif not allow_fill or self . ndim == 1 : if allow_fill and fill_tuple [ 0 ] is None : _ , fill_value = maybe_promote ( blk . dtype ) fill_tuple = ( fill_value , ) return [ blk . take_nd ( slobj , axis = 0 , new_mgr_locs = slice ( 0 , sllen ) , fill_tuple = fill_tuple ) ] if sl_type in ( 'slice' , 'mask' ) : blknos = self . _blknos [ slobj ] blklocs = self . _blklocs [ slobj ] else : blknos = algos . take_1d ( self . _blknos , slobj , fill_value = - 1 , allow_fill = allow_fill ) blklocs = algos . take_1d ( self . _blklocs , slobj , fill_value = - 1 , allow_fill = allow_fill ) # When filling blknos, make sure blknos is updated before appending to # blocks list, that way new blkno is exactly len(blocks). # # FIXME: mgr_groupby_blknos must return mgr_locs in ascending order, # pytables serialization will break otherwise. blocks = [ ] for blkno , mgr_locs in libinternals . get_blkno_placements ( blknos , self . nblocks , group = True ) : if blkno == - 1 : # If we've got here, fill_tuple was not None. fill_value = fill_tuple [ 0 ] blocks . append ( self . _make_na_block ( placement = mgr_locs , fill_value = fill_value ) ) else : blk = self . blocks [ blkno ] # Otherwise, slicing along items axis is necessary. if not blk . _can_consolidate : # A non-consolidatable block, it's easy, because there's # only one item and each mgr loc is a copy of that single # item. for mgr_loc in mgr_locs : newblk = blk . copy ( deep = True ) newblk . mgr_locs = slice ( mgr_loc , mgr_loc + 1 ) blocks . append ( newblk ) else : blocks . append ( blk . take_nd ( blklocs [ mgr_locs . indexer ] , axis = 0 , new_mgr_locs = mgr_locs , fill_tuple = None ) ) return blocks\n\n\n        Summary:", "input": "", "output": "Slice / take blocks along axis = 0 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take ( self , indexer , axis = 1 , verify = True , convert = True ) : self . _consolidate_inplace ( ) indexer = ( np . arange ( indexer . start , indexer . stop , indexer . step , dtype = 'int64' ) if isinstance ( indexer , slice ) else np . asanyarray ( indexer , dtype = 'int64' ) ) n = self . shape [ axis ] if convert : indexer = maybe_convert_indices ( indexer , n ) if verify : if ( ( indexer == - 1 ) | ( indexer >= n ) ) . any ( ) : raise Exception ( 'Indices must be nonzero and less than ' 'the axis length' ) new_labels = self . axes [ axis ] . take ( indexer ) return self . reindex_indexer ( new_axis = new_labels , indexer = indexer , axis = axis , allow_dups = True )\n\n\n        Summary:", "input": "", "output": "Take items along any axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unstack ( self , unstacker_func , fill_value ) : n_rows = self . shape [ - 1 ] dummy = unstacker_func ( np . empty ( ( 0 , 0 ) ) , value_columns = self . items ) new_columns = dummy . get_new_columns ( ) new_index = dummy . get_new_index ( ) new_blocks = [ ] columns_mask = [ ] for blk in self . blocks : blocks , mask = blk . _unstack ( partial ( unstacker_func , value_columns = self . items [ blk . mgr_locs . indexer ] ) , new_columns , n_rows , fill_value ) new_blocks . extend ( blocks ) columns_mask . extend ( mask ) new_columns = new_columns [ columns_mask ] bm = BlockManager ( new_blocks , [ new_columns , new_index ] ) return bm\n\n\n        Summary:", "input": "", "output": "Return a blockmanager with all blocks unstacked ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def delete ( self , item ) : loc = self . items . get_loc ( item ) self . _block . delete ( loc ) self . axes [ 0 ] = self . axes [ 0 ] . delete ( loc )\n\n\n        Summary:", "input": "", "output": "Delete single item from SingleBlockManager ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concat ( self , to_concat , new_axis ) : non_empties = [ x for x in to_concat if len ( x ) > 0 ] # check if all series are of the same block type: if len ( non_empties ) > 0 : blocks = [ obj . blocks [ 0 ] for obj in non_empties ] if len ( { b . dtype for b in blocks } ) == 1 : new_block = blocks [ 0 ] . concat_same_type ( blocks ) else : values = [ x . values for x in blocks ] values = _concat . _concat_compat ( values ) new_block = make_block ( values , placement = slice ( 0 , len ( values ) , 1 ) ) else : values = [ x . _block . values for x in to_concat ] values = _concat . _concat_compat ( values ) new_block = make_block ( values , placement = slice ( 0 , len ( values ) , 1 ) ) mgr = SingleBlockManager ( new_block , new_axis ) return mgr\n\n\n        Summary:", "input": "", "output": "Concatenate a list of SingleBlockManagers into a single SingleBlockManager ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_array ( cls , arr , index = None , name = None , copy = False , fill_value = None , fastpath = False ) : warnings . warn ( \"'from_array' is deprecated and will be removed in a \" \"future version. Please use the pd.SparseSeries(..) \" \"constructor instead.\" , FutureWarning , stacklevel = 2 ) return cls ( arr , index = index , name = name , copy = copy , fill_value = fill_value , fastpath = fastpath )\n\n\n        Summary:", "input": "", "output": "Construct SparseSeries from array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def as_sparse_array ( self , kind = None , fill_value = None , copy = False ) : if fill_value is None : fill_value = self . fill_value if kind is None : kind = self . kind return SparseArray ( self . values , sparse_index = self . sp_index , fill_value = fill_value , kind = kind , copy = copy )\n\n\n        Summary:", "input": "", "output": "return my self as a sparse array do not copy by default"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reduce ( self , op , name , axis = 0 , skipna = True , numeric_only = None , filter_type = None , * * kwds ) : return op ( self . get_values ( ) , skipna = skipna , * * kwds )\n\n\n        Summary:", "input": "", "output": "perform a reduction operation"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ixs ( self , i , axis = 0 ) : label = self . index [ i ] if isinstance ( label , Index ) : return self . take ( i , axis = axis ) else : return self . _get_val_at ( i )\n\n\n        Summary:", "input": "", "output": "Return the i - th value or values in the SparseSeries by location"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def abs ( self ) : return self . _constructor ( np . abs ( self . values ) , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Return an object with absolute value taken . Only applicable to objects that are all numeric"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( self , label , default = None ) : if label in self . index : loc = self . index . get_loc ( label ) return self . _get_val_at ( loc ) else : return default\n\n\n        Summary:", "input": "", "output": "Returns value occupying requested label default to specified missing value if not present . Analogous to dict . get"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_value ( self , label , takeable = False ) : warnings . warn ( \"get_value is deprecated and will be removed \" \"in a future release. Please use \" \".at[] or .iat[] accessors instead\" , FutureWarning , stacklevel = 2 ) return self . _get_value ( label , takeable = takeable )\n\n\n        Summary:", "input": "", "output": "Retrieve single value at passed index label"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_value ( self , label , value , takeable = False ) : warnings . warn ( \"set_value is deprecated and will be removed \" \"in a future release. Please use \" \".at[] or .iat[] accessors instead\" , FutureWarning , stacklevel = 2 ) return self . _set_value ( label , value , takeable = takeable )\n\n\n        Summary:", "input": "", "output": "Quickly set single value at passed label . If label is not contained a new object is created with the label placed at the end of the result index"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_dense ( self ) : return Series ( self . values . to_dense ( ) , index = self . index , name = self . name )\n\n\n        Summary:", "input": "", "output": "Convert SparseSeries to a Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , deep = True ) : # TODO: https://github.com/pandas-dev/pandas/issues/22314 # We skip the block manager till that is resolved. new_data = self . values . copy ( deep = deep ) return self . _constructor ( new_data , sparse_index = self . sp_index , fill_value = self . fill_value , index = self . index . copy ( ) , name = self . name ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Make a copy of the SparseSeries . Only the actual sparse values need to be copied"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sparse_reindex ( self , new_index ) : if not isinstance ( new_index , splib . SparseIndex ) : raise TypeError ( \"new index must be a SparseIndex\" ) values = self . values values = values . sp_index . to_int_index ( ) . reindex ( values . sp_values . astype ( 'float64' ) , values . fill_value , new_index ) values = SparseArray ( values , sparse_index = new_index , fill_value = self . values . fill_value ) return self . _constructor ( values , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Conform sparse values to new SparseIndex"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cumsum ( self , axis = 0 , * args , * * kwargs ) : nv . validate_cumsum ( args , kwargs ) # Validate axis if axis is not None : self . _get_axis_number ( axis ) new_array = self . values . cumsum ( ) return self . _constructor ( new_array , index = self . index , sparse_index = new_array . sp_index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Cumulative sum of non - NA / null values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dropna ( self , axis = 0 , inplace = False , * * kwargs ) : # TODO: make more efficient # Validate axis self . _get_axis_number ( axis or 0 ) dense_valid = self . to_dense ( ) . dropna ( ) if inplace : raise NotImplementedError ( \"Cannot perform inplace dropna\" \" operations on a SparseSeries\" ) if isna ( self . fill_value ) : return dense_valid else : dense_valid = dense_valid [ dense_valid != self . fill_value ] return dense_valid . to_sparse ( fill_value = self . fill_value )\n\n\n        Summary:", "input": "", "output": "Analogous to Series . dropna . If fill_value = NaN returns a dense Series"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine_first ( self , other ) : if isinstance ( other , SparseSeries ) : other = other . to_dense ( ) dense_combined = self . to_dense ( ) . combine_first ( other ) return dense_combined . to_sparse ( fill_value = self . fill_value )\n\n\n        Summary:", "input": "", "output": "Combine Series values choosing the calling Series s values first . Result index will be the union of the two indexes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_cache ( arg , format , cache , convert_listlike ) : from pandas import Series cache_array = Series ( ) if cache : # Perform a quicker unique check from pandas import Index unique_dates = Index ( arg ) . unique ( ) if len ( unique_dates ) < len ( arg ) : cache_dates = convert_listlike ( unique_dates . to_numpy ( ) , True , format ) cache_array = Series ( cache_dates , index = unique_dates ) return cache_array\n\n\n        Summary:", "input": "", "output": "Create a cache of unique dates from an array of dates"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_and_box_cache ( arg , cache_array , box , errors , name = None ) : from pandas import Series , DatetimeIndex , Index result = Series ( arg ) . map ( cache_array ) if box : if errors == 'ignore' : return Index ( result , name = name ) else : return DatetimeIndex ( result , name = name ) return result . values\n\n\n        Summary:", "input": "", "output": "Convert array of dates with a cache and box the result"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _return_parsed_timezone_results ( result , timezones , box , tz , name ) : if tz is not None : raise ValueError ( \"Cannot pass a tz argument when \" \"parsing strings with timezone \" \"information.\" ) tz_results = np . array ( [ Timestamp ( res ) . tz_localize ( zone ) for res , zone in zip ( result , timezones ) ] ) if box : from pandas import Index return Index ( tz_results , name = name ) return tz_results\n\n\n        Summary:", "input": "", "output": "Return results from array_strptime if a %z or %Z directive was passed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_listlike_datetimes ( arg , box , format , name = None , tz = None , unit = None , errors = None , infer_datetime_format = None , dayfirst = None , yearfirst = None , exact = None ) : from pandas import DatetimeIndex from pandas . core . arrays import DatetimeArray from pandas . core . arrays . datetimes import ( maybe_convert_dtype , objects_to_datetime64ns ) if isinstance ( arg , ( list , tuple ) ) : arg = np . array ( arg , dtype = 'O' ) # these are shortcutable if is_datetime64tz_dtype ( arg ) : if not isinstance ( arg , ( DatetimeArray , DatetimeIndex ) ) : return DatetimeIndex ( arg , tz = tz , name = name ) if tz == 'utc' : arg = arg . tz_convert ( None ) . tz_localize ( tz ) return arg elif is_datetime64_ns_dtype ( arg ) : if box and not isinstance ( arg , ( DatetimeArray , DatetimeIndex ) ) : try : return DatetimeIndex ( arg , tz = tz , name = name ) except ValueError : pass return arg elif unit is not None : if format is not None : raise ValueError ( \"cannot specify both format and unit\" ) arg = getattr ( arg , 'values' , arg ) result = tslib . array_with_unit_to_datetime ( arg , unit , errors = errors ) if box : if errors == 'ignore' : from pandas import Index result = Index ( result , name = name ) # GH 23758: We may still need to localize the result with tz try : return result . tz_localize ( tz ) except AttributeError : return result return DatetimeIndex ( result , tz = tz , name = name ) return result elif getattr ( arg , 'ndim' , 1 ) > 1 : raise TypeError ( 'arg must be a string, datetime, list, tuple, ' '1-d array, or Series' ) # warn if passing timedelta64, raise for PeriodDtype # NB: this must come after unit transformation orig_arg = arg arg , _ = maybe_convert_dtype ( arg , copy = False ) arg = ensure_object ( arg ) require_iso8601 = False if infer_datetime_format and format is None : format = _guess_datetime_format_for_array ( arg , dayfirst = dayfirst ) if format is not None : # There is a special fast-path for iso8601 formatted # datetime strings, so in those cases don't use the inferred # format because this path makes process slower in this # special case format_is_iso8601 = _format_is_iso ( format ) if format_is_iso8601 : require_iso8601 = not infer_datetime_format format = None tz_parsed = None result = None if format is not None : try : # shortcut formatting here if format == '%Y%m%d' : try : # pass orig_arg as float-dtype may have been converted to # datetime64[ns] orig_arg = ensure_object ( orig_arg ) result = _attempt_YYYYMMDD ( orig_arg , errors = errors ) except ( ValueError , TypeError , tslibs . OutOfBoundsDatetime ) : raise ValueError ( \"cannot convert the input to \" \"'%Y%m%d' date format\" ) # fallback if result is None : try : result , timezones = array_strptime ( arg , format , exact = exact , errors = errors ) if '%Z' in format or '%z' in format : return _return_parsed_timezone_results ( result , timezones , box , tz , name ) except tslibs . OutOfBoundsDatetime : if errors == 'raise' : raise elif errors == 'coerce' : result = np . empty ( arg . shape , dtype = 'M8[ns]' ) iresult = result . view ( 'i8' ) iresult . fill ( tslibs . iNaT ) else : result = arg except ValueError : # if format was inferred, try falling back # to array_to_datetime - terminate here # for specified formats if not infer_datetime_format : if errors == 'raise' : raise elif errors == 'coerce' : result = np . empty ( arg . shape , dtype = 'M8[ns]' ) iresult = result . view ( 'i8' ) iresult . fill ( tslibs . iNaT ) else : result = arg except ValueError as e : # Fallback to try to convert datetime objects if timezone-aware # datetime objects are found without passing `utc=True` try : values , tz = conversion . datetime_to_datetime64 ( arg ) return DatetimeIndex . _simple_new ( values , name = name , tz = tz ) except ( ValueError , TypeError ) : raise e if result is None : assert format is None or infer_datetime_format utc = tz == 'utc' result , tz_parsed = objects_to_datetime64ns ( arg , dayfirst = dayfirst , yearfirst = yearfirst , utc = utc , errors = errors , require_iso8601 = require_iso8601 , allow_object = True ) if tz_parsed is not None : if box : # We can take a shortcut since the datetime64 numpy array # is in UTC return DatetimeIndex . _simple_new ( result , name = name , tz = tz_parsed ) else : # Convert the datetime64 numpy array to an numpy array # of datetime objects result = [ Timestamp ( ts , tz = tz_parsed ) . to_pydatetime ( ) for ts in result ] return np . array ( result , dtype = object ) if box : # Ensure we return an Index in all cases where box=True if is_datetime64_dtype ( result ) : return DatetimeIndex ( result , tz = tz , name = name ) elif is_object_dtype ( result ) : # e.g. an Index of datetime objects from pandas import Index return Index ( result , name = name ) return result\n\n\n        Summary:", "input": "", "output": "Helper function for to_datetime . Performs the conversions of 1D listlike of dates"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _adjust_to_origin ( arg , origin , unit ) : if origin == 'julian' : original = arg j0 = Timestamp ( 0 ) . to_julian_date ( ) if unit != 'D' : raise ValueError ( \"unit must be 'D' for origin='julian'\" ) try : arg = arg - j0 except TypeError : raise ValueError ( \"incompatible 'arg' type for given \" \"'origin'='julian'\" ) # premptively check this for a nice range j_max = Timestamp . max . to_julian_date ( ) - j0 j_min = Timestamp . min . to_julian_date ( ) - j0 if np . any ( arg > j_max ) or np . any ( arg < j_min ) : raise tslibs . OutOfBoundsDatetime ( \"{original} is Out of Bounds for \" \"origin='julian'\" . format ( original = original ) ) else : # arg must be numeric if not ( ( is_scalar ( arg ) and ( is_integer ( arg ) or is_float ( arg ) ) ) or is_numeric_dtype ( np . asarray ( arg ) ) ) : raise ValueError ( \"'{arg}' is not compatible with origin='{origin}'; \" \"it must be numeric with a unit specified \" . format ( arg = arg , origin = origin ) ) # we are going to offset back to unix / epoch time try : offset = Timestamp ( origin ) except tslibs . OutOfBoundsDatetime : raise tslibs . OutOfBoundsDatetime ( \"origin {origin} is Out of Bounds\" . format ( origin = origin ) ) except ValueError : raise ValueError ( \"origin {origin} cannot be converted \" \"to a Timestamp\" . format ( origin = origin ) ) if offset . tz is not None : raise ValueError ( \"origin offset {} must be tz-naive\" . format ( offset ) ) offset -= Timestamp ( 0 ) # convert the offset to the unit of the arg # this should be lossless in terms of precision offset = offset // tslibs . Timedelta ( 1 , unit = unit ) # scalars & ndarray-like can handle the addition if is_list_like ( arg ) and not isinstance ( arg , ( ABCSeries , ABCIndexClass , np . ndarray ) ) : arg = np . asarray ( arg ) arg = arg + offset return arg\n\n\n        Summary:", "input": "", "output": "Helper function for to_datetime . Adjust input argument to the specified origin"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_datetime ( arg , errors = 'raise' , dayfirst = False , yearfirst = False , utc = None , box = True , format = None , exact = True , unit = None , infer_datetime_format = False , origin = 'unix' , cache = False ) : if arg is None : return None if origin != 'unix' : arg = _adjust_to_origin ( arg , origin , unit ) tz = 'utc' if utc else None convert_listlike = partial ( _convert_listlike_datetimes , tz = tz , unit = unit , dayfirst = dayfirst , yearfirst = yearfirst , errors = errors , exact = exact , infer_datetime_format = infer_datetime_format ) if isinstance ( arg , Timestamp ) : result = arg if tz is not None : if arg . tz is not None : result = result . tz_convert ( tz ) else : result = result . tz_localize ( tz ) elif isinstance ( arg , ABCSeries ) : cache_array = _maybe_cache ( arg , format , cache , convert_listlike ) if not cache_array . empty : result = arg . map ( cache_array ) else : values = convert_listlike ( arg . _values , True , format ) result = arg . _constructor ( values , index = arg . index , name = arg . name ) elif isinstance ( arg , ( ABCDataFrame , abc . MutableMapping ) ) : result = _assemble_from_unit_mappings ( arg , errors , box , tz ) elif isinstance ( arg , ABCIndexClass ) : cache_array = _maybe_cache ( arg , format , cache , convert_listlike ) if not cache_array . empty : result = _convert_and_box_cache ( arg , cache_array , box , errors , name = arg . name ) else : convert_listlike = partial ( convert_listlike , name = arg . name ) result = convert_listlike ( arg , box , format ) elif is_list_like ( arg ) : cache_array = _maybe_cache ( arg , format , cache , convert_listlike ) if not cache_array . empty : result = _convert_and_box_cache ( arg , cache_array , box , errors ) else : result = convert_listlike ( arg , box , format ) else : result = convert_listlike ( np . array ( [ arg ] ) , box , format ) [ 0 ] return result\n\n\n        Summary:", "input": "", "output": "Convert argument to datetime ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _assemble_from_unit_mappings ( arg , errors , box , tz ) : from pandas import to_timedelta , to_numeric , DataFrame arg = DataFrame ( arg ) if not arg . columns . is_unique : raise ValueError ( \"cannot assemble with duplicate keys\" ) # replace passed unit with _unit_map def f ( value ) : if value in _unit_map : return _unit_map [ value ] # m is case significant if value . lower ( ) in _unit_map : return _unit_map [ value . lower ( ) ] return value unit = { k : f ( k ) for k in arg . keys ( ) } unit_rev = { v : k for k , v in unit . items ( ) } # we require at least Ymd required = [ 'year' , 'month' , 'day' ] req = sorted ( list ( set ( required ) - set ( unit_rev . keys ( ) ) ) ) if len ( req ) : raise ValueError ( \"to assemble mappings requires at least that \" \"[year, month, day] be specified: [{required}] \" \"is missing\" . format ( required = ',' . join ( req ) ) ) # keys we don't recognize excess = sorted ( list ( set ( unit_rev . keys ( ) ) - set ( _unit_map . values ( ) ) ) ) if len ( excess ) : raise ValueError ( \"extra keys have been passed \" \"to the datetime assemblage: \" \"[{excess}]\" . format ( excess = ',' . join ( excess ) ) ) def coerce ( values ) : # we allow coercion to if errors allows values = to_numeric ( values , errors = errors ) # prevent overflow in case of int8 or int16 if is_integer_dtype ( values ) : values = values . astype ( 'int64' , copy = False ) return values values = ( coerce ( arg [ unit_rev [ 'year' ] ] ) * 10000 + coerce ( arg [ unit_rev [ 'month' ] ] ) * 100 + coerce ( arg [ unit_rev [ 'day' ] ] ) ) try : values = to_datetime ( values , format = '%Y%m%d' , errors = errors , utc = tz ) except ( TypeError , ValueError ) as e : raise ValueError ( \"cannot assemble the \" \"datetimes: {error}\" . format ( error = e ) ) for u in [ 'h' , 'm' , 's' , 'ms' , 'us' , 'ns' ] : value = unit_rev . get ( u ) if value is not None and value in arg : try : values += to_timedelta ( coerce ( arg [ value ] ) , unit = u , errors = errors ) except ( TypeError , ValueError ) as e : raise ValueError ( \"cannot assemble the datetimes [{value}]: \" \"{error}\" . format ( value = value , error = e ) ) if not box : return values . values return values\n\n\n        Summary:", "input": "", "output": "assemble the unit specified fields from the arg ( DataFrame ) Return a Series for actual parsing"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _attempt_YYYYMMDD ( arg , errors ) : def calc ( carg ) : # calculate the actual result carg = carg . astype ( object ) parsed = parsing . try_parse_year_month_day ( carg / 10000 , carg / 100 % 100 , carg % 100 ) return tslib . array_to_datetime ( parsed , errors = errors ) [ 0 ] def calc_with_mask ( carg , mask ) : result = np . empty ( carg . shape , dtype = 'M8[ns]' ) iresult = result . view ( 'i8' ) iresult [ ~ mask ] = tslibs . iNaT masked_result = calc ( carg [ mask ] . astype ( np . float64 ) . astype ( np . int64 ) ) result [ mask ] = masked_result . astype ( 'M8[ns]' ) return result # try intlike / strings that are ints try : return calc ( arg . astype ( np . int64 ) ) except ValueError : pass # a float with actual np.nan try : carg = arg . astype ( np . float64 ) return calc_with_mask ( carg , notna ( carg ) ) except ValueError : pass # string with NaN-like try : mask = ~ algorithms . isin ( arg , list ( tslib . nat_strings ) ) return calc_with_mask ( arg , mask ) except ValueError : pass return None\n\n\n        Summary:", "input": "", "output": "try to parse the YYYYMMDD / %Y%m%d format try to deal with NaT - like arg is a passed in as an object dtype but could really be ints / strings with nan - like / or floats ( e . g . with nan )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_time ( arg , format = None , infer_time_format = False , errors = 'raise' ) : def _convert_listlike ( arg , format ) : if isinstance ( arg , ( list , tuple ) ) : arg = np . array ( arg , dtype = 'O' ) elif getattr ( arg , 'ndim' , 1 ) > 1 : raise TypeError ( 'arg must be a string, datetime, list, tuple, ' '1-d array, or Series' ) arg = ensure_object ( arg ) if infer_time_format and format is None : format = _guess_time_format_for_array ( arg ) times = [ ] if format is not None : for element in arg : try : times . append ( datetime . strptime ( element , format ) . time ( ) ) except ( ValueError , TypeError ) : if errors == 'raise' : msg = ( \"Cannot convert {element} to a time with given \" \"format {format}\" ) . format ( element = element , format = format ) raise ValueError ( msg ) elif errors == 'ignore' : return arg else : times . append ( None ) else : formats = _time_formats [ : ] format_found = False for element in arg : time_object = None for time_format in formats : try : time_object = datetime . strptime ( element , time_format ) . time ( ) if not format_found : # Put the found format in front fmt = formats . pop ( formats . index ( time_format ) ) formats . insert ( 0 , fmt ) format_found = True break except ( ValueError , TypeError ) : continue if time_object is not None : times . append ( time_object ) elif errors == 'raise' : raise ValueError ( \"Cannot convert arg {arg} to \" \"a time\" . format ( arg = arg ) ) elif errors == 'ignore' : return arg else : times . append ( None ) return times if arg is None : return arg elif isinstance ( arg , time ) : return arg elif isinstance ( arg , ABCSeries ) : values = _convert_listlike ( arg . _values , format ) return arg . _constructor ( values , index = arg . index , name = arg . name ) elif isinstance ( arg , ABCIndexClass ) : return _convert_listlike ( arg , format ) elif is_list_like ( arg ) : return _convert_listlike ( arg , format ) return _convert_listlike ( np . array ( [ arg ] ) , format ) [ 0 ]\n\n\n        Summary:", "input": "", "output": "Parse time strings to time objects using fixed strptime formats ( %H : %M %H%M %I : %M%p %I%M%p %H : %M : %S %H%M%S %I : %M : %S%p %I%M%S%p )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def deprecate ( name , alternative , version , alt_name = None , klass = None , stacklevel = 2 , msg = None ) : alt_name = alt_name or alternative . __name__ klass = klass or FutureWarning warning_msg = msg or '{} is deprecated, use {} instead' . format ( name , alt_name ) @ wraps ( alternative ) def wrapper ( * args , * * kwargs ) : warnings . warn ( warning_msg , klass , stacklevel = stacklevel ) return alternative ( * args , * * kwargs ) # adding deprecated directive to the docstring msg = msg or 'Use `{alt_name}` instead.' . format ( alt_name = alt_name ) doc_error_msg = ( 'deprecate needs a correctly formatted docstring in ' 'the target function (should have a one liner short ' 'summary, and opening quotes should be in their own ' 'line). Found:\\n{}' . format ( alternative . __doc__ ) ) # when python is running in optimized mode (i.e. `-OO`), docstrings are # removed, so we check that a docstring with correct formatting is used # but we allow empty docstrings if alternative . __doc__ : if alternative . __doc__ . count ( '\\n' ) < 3 : raise AssertionError ( doc_error_msg ) empty1 , summary , empty2 , doc = alternative . __doc__ . split ( '\\n' , 3 ) if empty1 or empty2 and not summary : raise AssertionError ( doc_error_msg ) wrapper . __doc__ = dedent ( \"\"\" {summary} .. deprecated:: {depr_version} {depr_msg} {rest_of_docstring}\"\"\" ) . format ( summary = summary . strip ( ) , depr_version = version , depr_msg = msg , rest_of_docstring = dedent ( doc ) ) return wrapper\n\n\n        Summary:", "input": "", "output": "Return a new function that emits a deprecation warning on use ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def deprecate_kwarg ( old_arg_name , new_arg_name , mapping = None , stacklevel = 2 ) : if mapping is not None and not hasattr ( mapping , 'get' ) and not callable ( mapping ) : raise TypeError ( \"mapping from old to new argument values \" \"must be dict or callable!\" ) def _deprecate_kwarg ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : old_arg_value = kwargs . pop ( old_arg_name , None ) if new_arg_name is None and old_arg_value is not None : msg = ( \"the '{old_name}' keyword is deprecated and will be \" \"removed in a future version. \" \"Please take steps to stop the use of '{old_name}'\" ) . format ( old_name = old_arg_name ) warnings . warn ( msg , FutureWarning , stacklevel = stacklevel ) kwargs [ old_arg_name ] = old_arg_value return func ( * args , * * kwargs ) if old_arg_value is not None : if mapping is not None : if hasattr ( mapping , 'get' ) : new_arg_value = mapping . get ( old_arg_value , old_arg_value ) else : new_arg_value = mapping ( old_arg_value ) msg = ( \"the {old_name}={old_val!r} keyword is deprecated, \" \"use {new_name}={new_val!r} instead\" ) . format ( old_name = old_arg_name , old_val = old_arg_value , new_name = new_arg_name , new_val = new_arg_value ) else : new_arg_value = old_arg_value msg = ( \"the '{old_name}' keyword is deprecated, \" \"use '{new_name}' instead\" ) . format ( old_name = old_arg_name , new_name = new_arg_name ) warnings . warn ( msg , FutureWarning , stacklevel = stacklevel ) if kwargs . get ( new_arg_name , None ) is not None : msg = ( \"Can only specify '{old_name}' or '{new_name}', \" \"not both\" ) . format ( old_name = old_arg_name , new_name = new_arg_name ) raise TypeError ( msg ) else : kwargs [ new_arg_name ] = new_arg_value return func ( * args , * * kwargs ) return wrapper return _deprecate_kwarg\n\n\n        Summary:", "input": "", "output": "Decorator to deprecate a keyword argument of a function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_signature ( func ) : spec = inspect . getfullargspec ( func ) if spec . defaults is None : n_wo_defaults = len ( spec . args ) defaults = ( '' , ) * n_wo_defaults else : n_wo_defaults = len ( spec . args ) - len ( spec . defaults ) defaults = ( '' , ) * n_wo_defaults + tuple ( spec . defaults ) args = [ ] for var , default in zip ( spec . args , defaults ) : args . append ( var if default == '' else var + '=' + repr ( default ) ) if spec . varargs : args . append ( '*' + spec . varargs ) if spec . varkw : args . append ( '**' + spec . varkw ) return args , spec . args\n\n\n        Summary:", "input": "", "output": "Returns a tuple containing the paramenter list with defaults and parameter list ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def period_range ( start = None , end = None , periods = None , freq = None , name = None ) : if com . count_not_none ( start , end , periods ) != 2 : raise ValueError ( 'Of the three parameters: start, end, and periods, ' 'exactly two must be specified' ) if freq is None and ( not isinstance ( start , Period ) and not isinstance ( end , Period ) ) : freq = 'D' data , freq = PeriodArray . _generate_range ( start , end , periods , freq , fields = { } ) data = PeriodArray ( data , freq = freq ) return PeriodIndex ( data , name = name )\n\n\n        Summary:", "input": "", "output": "Return a fixed frequency PeriodIndex with day ( calendar ) as the default frequency"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_range ( cls , data , name = None , dtype = None , * * kwargs ) : if not isinstance ( data , range ) : raise TypeError ( '{0}(...) must be called with object coercible to a ' 'range, {1} was passed' . format ( cls . __name__ , repr ( data ) ) ) start , stop , step = data . start , data . stop , data . step return RangeIndex ( start , stop , step , dtype = dtype , name = name , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Create RangeIndex from a range object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_attrs ( self ) : attrs = self . _get_data_as_items ( ) if self . name is not None : attrs . append ( ( 'name' , ibase . default_pprint ( self . name ) ) ) return attrs\n\n\n        Summary:", "input": "", "output": "Return a list of tuples of the ( attr formatted_value )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def min ( self , axis = None , skipna = True ) : nv . validate_minmax_axis ( axis ) return self . _minmax ( 'min' )\n\n\n        Summary:", "input": "", "output": "The minimum value of the RangeIndex"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def max ( self , axis = None , skipna = True ) : nv . validate_minmax_axis ( axis ) return self . _minmax ( 'max' )\n\n\n        Summary:", "input": "", "output": "The maximum value of the RangeIndex"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argsort ( self , * args , * * kwargs ) : nv . validate_argsort ( args , kwargs ) if self . _step > 0 : return np . arange ( len ( self ) ) else : return np . arange ( len ( self ) - 1 , - 1 , - 1 )\n\n\n        Summary:", "input": "", "output": "Returns the indices that would sort the index and its underlying data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def equals ( self , other ) : if isinstance ( other , RangeIndex ) : ls = len ( self ) lo = len ( other ) return ( ls == lo == 0 or ls == lo == 1 and self . _start == other . _start or ls == lo and self . _start == other . _start and self . _step == other . _step ) return super ( ) . equals ( other )\n\n\n        Summary:", "input": "", "output": "Determines if two Index objects contain the same elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def intersection ( self , other , sort = False ) : self . _validate_sort_keyword ( sort ) if self . equals ( other ) : return self . _get_reconciled_name_object ( other ) if not isinstance ( other , RangeIndex ) : return super ( ) . intersection ( other , sort = sort ) if not len ( self ) or not len ( other ) : return RangeIndex . _simple_new ( None ) first = self [ : : - 1 ] if self . _step < 0 else self second = other [ : : - 1 ] if other . _step < 0 else other # check whether intervals intersect # deals with in- and decreasing ranges int_low = max ( first . _start , second . _start ) int_high = min ( first . _stop , second . _stop ) if int_high <= int_low : return RangeIndex . _simple_new ( None ) # Method hint: linear Diophantine equation # solve intersection problem # performance hint: for identical step sizes, could use # cheaper alternative gcd , s , t = first . _extended_gcd ( first . _step , second . _step ) # check whether element sets intersect if ( first . _start - second . _start ) % gcd : return RangeIndex . _simple_new ( None ) # calculate parameters for the RangeIndex describing the # intersection disregarding the lower bounds tmp_start = first . _start + ( second . _start - first . _start ) * first . _step // gcd * s new_step = first . _step * second . _step // gcd new_index = RangeIndex . _simple_new ( tmp_start , int_high , new_step ) # adjust index to limiting interval new_index . _start = new_index . _min_fitting_element ( int_low ) if ( self . _step < 0 and other . _step < 0 ) is not ( new_index . _step < 0 ) : new_index = new_index [ : : - 1 ] if sort is None : new_index = new_index . sort_values ( ) return new_index\n\n\n        Summary:", "input": "", "output": "Form the intersection of two Index objects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _min_fitting_element ( self , lower_limit ) : no_steps = - ( - ( lower_limit - self . _start ) // abs ( self . _step ) ) return self . _start + abs ( self . _step ) * no_steps\n\n\n        Summary:", "input": "", "output": "Returns the smallest element greater than or equal to the limit"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _max_fitting_element ( self , upper_limit ) : no_steps = ( upper_limit - self . _start ) // abs ( self . _step ) return self . _start + abs ( self . _step ) * no_steps\n\n\n        Summary:", "input": "", "output": "Returns the largest element smaller than or equal to the limit"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _extended_gcd ( self , a , b ) : s , old_s = 0 , 1 t , old_t = 1 , 0 r , old_r = b , a while r : quotient = old_r // r old_r , r = r , old_r - quotient * r old_s , s = s , old_s - quotient * s old_t , t = t , old_t - quotient * t return old_r , old_s , old_t\n\n\n        Summary:", "input": "", "output": "Extended Euclidean algorithms to solve Bezout s identity : a * x + b * y = gcd ( x y ) Finds one particular solution for x y : s t Returns : gcd s t"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , other , sort = None ) : self . _assert_can_do_setop ( other ) if len ( other ) == 0 or self . equals ( other ) or len ( self ) == 0 : return super ( ) . union ( other , sort = sort ) if isinstance ( other , RangeIndex ) and sort is None : start_s , step_s = self . _start , self . _step end_s = self . _start + self . _step * ( len ( self ) - 1 ) start_o , step_o = other . _start , other . _step end_o = other . _start + other . _step * ( len ( other ) - 1 ) if self . _step < 0 : start_s , step_s , end_s = end_s , - step_s , start_s if other . _step < 0 : start_o , step_o , end_o = end_o , - step_o , start_o if len ( self ) == 1 and len ( other ) == 1 : step_s = step_o = abs ( self . _start - other . _start ) elif len ( self ) == 1 : step_s = step_o elif len ( other ) == 1 : step_o = step_s start_r = min ( start_s , start_o ) end_r = max ( end_s , end_o ) if step_o == step_s : if ( ( start_s - start_o ) % step_s == 0 and ( start_s - end_o ) <= step_s and ( start_o - end_s ) <= step_s ) : return RangeIndex ( start_r , end_r + step_s , step_s ) if ( ( step_s % 2 == 0 ) and ( abs ( start_s - start_o ) <= step_s / 2 ) and ( abs ( end_s - end_o ) <= step_s / 2 ) ) : return RangeIndex ( start_r , end_r + step_s / 2 , step_s / 2 ) elif step_o % step_s == 0 : if ( ( start_o - start_s ) % step_s == 0 and ( start_o + step_s >= start_s ) and ( end_o - step_s <= end_s ) ) : return RangeIndex ( start_r , end_r + step_s , step_s ) elif step_s % step_o == 0 : if ( ( start_s - start_o ) % step_o == 0 and ( start_s + step_o >= start_o ) and ( end_s - step_o <= end_o ) ) : return RangeIndex ( start_r , end_r + step_o , step_o ) return self . _int64index . union ( other , sort = sort )\n\n\n        Summary:", "input": "", "output": "Form the union of two Index objects and sorts if possible"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_methods_binary ( cls ) : def _make_evaluate_binop ( op , step = False ) : \"\"\" Parameters ---------- op : callable that accepts 2 parms perform the binary op step : callable, optional, default to False op to apply to the step parm if not None if False, use the existing step \"\"\" def _evaluate_numeric_binop ( self , other ) : if isinstance ( other , ( ABCSeries , ABCDataFrame ) ) : return NotImplemented elif isinstance ( other , ABCTimedeltaIndex ) : # Defer to TimedeltaIndex implementation return NotImplemented elif isinstance ( other , ( timedelta , np . timedelta64 ) ) : # GH#19333 is_integer evaluated True on timedelta64, # so we need to catch these explicitly return op ( self . _int64index , other ) elif is_timedelta64_dtype ( other ) : # Must be an np.ndarray; GH#22390 return op ( self . _int64index , other ) other = self . _validate_for_numeric_binop ( other , op ) attrs = self . _get_attributes_dict ( ) attrs = self . _maybe_update_attributes ( attrs ) left , right = self , other try : # apply if we have an override if step : with np . errstate ( all = 'ignore' ) : rstep = step ( left . _step , right ) # we don't have a representable op # so return a base index if not is_integer ( rstep ) or not rstep : raise ValueError else : rstep = left . _step with np . errstate ( all = 'ignore' ) : rstart = op ( left . _start , right ) rstop = op ( left . _stop , right ) result = RangeIndex ( rstart , rstop , rstep , * * attrs ) # for compat with numpy / Int64Index # even if we can represent as a RangeIndex, return # as a Float64Index if we have float-like descriptors if not all ( is_integer ( x ) for x in [ rstart , rstop , rstep ] ) : result = result . astype ( 'float64' ) return result except ( ValueError , TypeError , ZeroDivisionError ) : # Defer to Int64Index implementation return op ( self . _int64index , other ) # TODO: Do attrs get handled reliably? name = '__{name}__' . format ( name = op . __name__ ) return compat . set_function_name ( _evaluate_numeric_binop , name , cls ) cls . __add__ = _make_evaluate_binop ( operator . add ) cls . __radd__ = _make_evaluate_binop ( ops . radd ) cls . __sub__ = _make_evaluate_binop ( operator . sub ) cls . __rsub__ = _make_evaluate_binop ( ops . rsub ) cls . __mul__ = _make_evaluate_binop ( operator . mul , step = operator . mul ) cls . __rmul__ = _make_evaluate_binop ( ops . rmul , step = ops . rmul ) cls . __truediv__ = _make_evaluate_binop ( operator . truediv , step = operator . truediv ) cls . __rtruediv__ = _make_evaluate_binop ( ops . rtruediv , step = ops . rtruediv )\n\n\n        Summary:", "input": "", "output": "add in numeric methods specialized to RangeIndex"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_numpy ( self , dtype = None , copy = False ) : result = np . asarray ( self . _ndarray , dtype = dtype ) if copy and result is self . _ndarray : result = result . copy ( ) return result\n\n\n        Summary:", "input": "", "output": "Convert the PandasArray to a : class : numpy . ndarray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def adjoin ( space , * lists , * * kwargs ) : strlen = kwargs . pop ( 'strlen' , len ) justfunc = kwargs . pop ( 'justfunc' , justify ) out_lines = [ ] newLists = [ ] lengths = [ max ( map ( strlen , x ) ) + space for x in lists [ : - 1 ] ] # not the last one lengths . append ( max ( map ( len , lists [ - 1 ] ) ) ) maxLen = max ( map ( len , lists ) ) for i , lst in enumerate ( lists ) : nl = justfunc ( lst , lengths [ i ] , mode = 'left' ) nl . extend ( [ ' ' * lengths [ i ] ] * ( maxLen - len ( lst ) ) ) newLists . append ( nl ) toJoin = zip ( * newLists ) for lines in toJoin : out_lines . append ( _join_unicode ( lines ) ) return _join_unicode ( out_lines , sep = '\\n' )\n\n\n        Summary:", "input": "", "output": "Glues together two sets of strings using the amount of space requested . The idea is to prettify ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def justify ( texts , max_len , mode = 'right' ) : if mode == 'left' : return [ x . ljust ( max_len ) for x in texts ] elif mode == 'center' : return [ x . center ( max_len ) for x in texts ] else : return [ x . rjust ( max_len ) for x in texts ]\n\n\n        Summary:", "input": "", "output": "Perform ljust center rjust against string or list - like"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _pprint_seq ( seq , _nest_lvl = 0 , max_seq_items = None , * * kwds ) : if isinstance ( seq , set ) : fmt = \"{{{body}}}\" else : fmt = \"[{body}]\" if hasattr ( seq , '__setitem__' ) else \"({body})\" if max_seq_items is False : nitems = len ( seq ) else : nitems = max_seq_items or get_option ( \"max_seq_items\" ) or len ( seq ) s = iter ( seq ) # handle sets, no slicing r = [ pprint_thing ( next ( s ) , _nest_lvl + 1 , max_seq_items = max_seq_items , * * kwds ) for i in range ( min ( nitems , len ( seq ) ) ) ] body = \", \" . join ( r ) if nitems < len ( seq ) : body += \", ...\" elif isinstance ( seq , tuple ) and len ( seq ) == 1 : body += ',' return fmt . format ( body = body )\n\n\n        Summary:", "input": "", "output": "internal . pprinter for iterables . you should probably use pprint_thing () rather then calling this directly ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _pprint_dict ( seq , _nest_lvl = 0 , max_seq_items = None , * * kwds ) : fmt = \"{{{things}}}\" pairs = [ ] pfmt = \"{key}: {val}\" if max_seq_items is False : nitems = len ( seq ) else : nitems = max_seq_items or get_option ( \"max_seq_items\" ) or len ( seq ) for k , v in list ( seq . items ( ) ) [ : nitems ] : pairs . append ( pfmt . format ( key = pprint_thing ( k , _nest_lvl + 1 , max_seq_items = max_seq_items , * * kwds ) , val = pprint_thing ( v , _nest_lvl + 1 , max_seq_items = max_seq_items , * * kwds ) ) ) if nitems < len ( seq ) : return fmt . format ( things = \", \" . join ( pairs ) + \", ...\" ) else : return fmt . format ( things = \", \" . join ( pairs ) )\n\n\n        Summary:", "input": "", "output": "internal . pprinter for iterables . you should probably use pprint_thing () rather then calling this directly ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pprint_thing ( thing , _nest_lvl = 0 , escape_chars = None , default_escapes = False , quote_strings = False , max_seq_items = None ) : def as_escaped_unicode ( thing , escape_chars = escape_chars ) : # Unicode is fine, else we try to decode using utf-8 and 'replace' # if that's not it either, we have no way of knowing and the user # should deal with it himself. try : result = str ( thing ) # we should try this first except UnicodeDecodeError : # either utf-8 or we replace errors result = str ( thing ) . decode ( 'utf-8' , \"replace\" ) translate = { '\\t' : r'\\t' , '\\n' : r'\\n' , '\\r' : r'\\r' , } if isinstance ( escape_chars , dict ) : if default_escapes : translate . update ( escape_chars ) else : translate = escape_chars escape_chars = list ( escape_chars . keys ( ) ) else : escape_chars = escape_chars or tuple ( ) for c in escape_chars : result = result . replace ( c , translate [ c ] ) return str ( result ) if hasattr ( thing , '__next__' ) : return str ( thing ) elif ( isinstance ( thing , dict ) and _nest_lvl < get_option ( \"display.pprint_nest_depth\" ) ) : result = _pprint_dict ( thing , _nest_lvl , quote_strings = True , max_seq_items = max_seq_items ) elif ( is_sequence ( thing ) and _nest_lvl < get_option ( \"display.pprint_nest_depth\" ) ) : result = _pprint_seq ( thing , _nest_lvl , escape_chars = escape_chars , quote_strings = quote_strings , max_seq_items = max_seq_items ) elif isinstance ( thing , str ) and quote_strings : result = \"'{thing}'\" . format ( thing = as_escaped_unicode ( thing ) ) else : result = as_escaped_unicode ( thing ) return str ( result )\n\n\n        Summary:", "input": "", "output": "This function is the sanctioned way of converting objects to a unicode representation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format_object_summary ( obj , formatter , is_justify = True , name = None , indent_for_name = True ) : from pandas . io . formats . console import get_console_size from pandas . io . formats . format import _get_adjustment display_width , _ = get_console_size ( ) if display_width is None : display_width = get_option ( 'display.width' ) or 80 if name is None : name = obj . __class__ . __name__ if indent_for_name : name_len = len ( name ) space1 = \"\\n%s\" % ( ' ' * ( name_len + 1 ) ) space2 = \"\\n%s\" % ( ' ' * ( name_len + 2 ) ) else : space1 = \"\\n\" space2 = \"\\n \" # space for the opening '[' n = len ( obj ) sep = ',' max_seq_items = get_option ( 'display.max_seq_items' ) or n # are we a truncated display is_truncated = n > max_seq_items # adj can optionally handle unicode eastern asian width adj = _get_adjustment ( ) def _extend_line ( s , line , value , display_width , next_line_prefix ) : if ( adj . len ( line . rstrip ( ) ) + adj . len ( value . rstrip ( ) ) >= display_width ) : s += line . rstrip ( ) line = next_line_prefix line += value return s , line def best_len ( values ) : if values : return max ( adj . len ( x ) for x in values ) else : return 0 close = ', ' if n == 0 : summary = '[]{}' . format ( close ) elif n == 1 : first = formatter ( obj [ 0 ] ) summary = '[{}]{}' . format ( first , close ) elif n == 2 : first = formatter ( obj [ 0 ] ) last = formatter ( obj [ - 1 ] ) summary = '[{}, {}]{}' . format ( first , last , close ) else : if n > max_seq_items : n = min ( max_seq_items // 2 , 10 ) head = [ formatter ( x ) for x in obj [ : n ] ] tail = [ formatter ( x ) for x in obj [ - n : ] ] else : head = [ ] tail = [ formatter ( x ) for x in obj ] # adjust all values to max length if needed if is_justify : # however, if we are not truncated and we are only a single # line, then don't justify if ( is_truncated or not ( len ( ', ' . join ( head ) ) < display_width and len ( ', ' . join ( tail ) ) < display_width ) ) : max_len = max ( best_len ( head ) , best_len ( tail ) ) head = [ x . rjust ( max_len ) for x in head ] tail = [ x . rjust ( max_len ) for x in tail ] summary = \"\" line = space2 for i in range ( len ( head ) ) : word = head [ i ] + sep + ' ' summary , line = _extend_line ( summary , line , word , display_width , space2 ) if is_truncated : # remove trailing space of last line summary += line . rstrip ( ) + space2 + '...' line = space2 for i in range ( len ( tail ) - 1 ) : word = tail [ i ] + sep + ' ' summary , line = _extend_line ( summary , line , word , display_width , space2 ) # last value: no sep added + 1 space of width used for trailing ',' summary , line = _extend_line ( summary , line , tail [ - 1 ] , display_width - 2 , space2 ) summary += line # right now close is either '' or ', ' # Now we want to include the ']', but not the maybe space. close = ']' + close . rstrip ( ' ' ) summary += close if len ( summary ) > ( display_width ) : summary += space1 else : # one row summary += ' ' # remove initial space summary = '[' + summary [ len ( space2 ) : ] return summary\n\n\n        Summary:", "input": "", "output": "Return the formatted obj as a unicode string"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format_object_attrs ( obj ) : attrs = [ ] if hasattr ( obj , 'dtype' ) : attrs . append ( ( 'dtype' , \"'{}'\" . format ( obj . dtype ) ) ) if getattr ( obj , 'name' , None ) is not None : attrs . append ( ( 'name' , default_pprint ( obj . name ) ) ) max_seq_items = get_option ( 'display.max_seq_items' ) or len ( obj ) if len ( obj ) > max_seq_items : attrs . append ( ( 'length' , len ( obj ) ) ) return attrs\n\n\n        Summary:", "input": "", "output": "Return a list of tuples of the ( attr formatted_value ) for common attrs including dtype name length"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_gbq ( query , project_id = None , index_col = None , col_order = None , reauth = False , auth_local_webserver = False , dialect = None , location = None , configuration = None , credentials = None , use_bqstorage_api = None , private_key = None , verbose = None ) : pandas_gbq = _try_import ( ) kwargs = { } # START: new kwargs. Don't populate unless explicitly set. if use_bqstorage_api is not None : kwargs [ \"use_bqstorage_api\" ] = use_bqstorage_api # END: new kwargs # START: deprecated kwargs. Don't populate unless explicitly set. if verbose is not None : kwargs [ \"verbose\" ] = verbose if private_key is not None : kwargs [ \"private_key\" ] = private_key # END: deprecated kwargs return pandas_gbq . read_gbq ( query , project_id = project_id , index_col = index_col , col_order = col_order , reauth = reauth , auth_local_webserver = auth_local_webserver , dialect = dialect , location = location , configuration = configuration , credentials = credentials , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Load data from Google BigQuery ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def scatter_matrix ( frame , alpha = 0.5 , figsize = None , ax = None , grid = False , diagonal = 'hist' , marker = '.' , density_kwds = None , hist_kwds = None , range_padding = 0.05 , * * kwds ) : df = frame . _get_numeric_data ( ) n = df . columns . size naxes = n * n fig , axes = _subplots ( naxes = naxes , figsize = figsize , ax = ax , squeeze = False ) # no gaps between subplots fig . subplots_adjust ( wspace = 0 , hspace = 0 ) mask = notna ( df ) marker = _get_marker_compat ( marker ) hist_kwds = hist_kwds or { } density_kwds = density_kwds or { } # GH 14855 kwds . setdefault ( 'edgecolors' , 'none' ) boundaries_list = [ ] for a in df . columns : values = df [ a ] . values [ mask [ a ] . values ] rmin_ , rmax_ = np . min ( values ) , np . max ( values ) rdelta_ext = ( rmax_ - rmin_ ) * range_padding / 2. boundaries_list . append ( ( rmin_ - rdelta_ext , rmax_ + rdelta_ext ) ) for i , a in zip ( lrange ( n ) , df . columns ) : for j , b in zip ( lrange ( n ) , df . columns ) : ax = axes [ i , j ] if i == j : values = df [ a ] . values [ mask [ a ] . values ] # Deal with the diagonal by drawing a histogram there. if diagonal == 'hist' : ax . hist ( values , * * hist_kwds ) elif diagonal in ( 'kde' , 'density' ) : from scipy . stats import gaussian_kde y = values gkde = gaussian_kde ( y ) ind = np . linspace ( y . min ( ) , y . max ( ) , 1000 ) ax . plot ( ind , gkde . evaluate ( ind ) , * * density_kwds ) ax . set_xlim ( boundaries_list [ i ] ) else : common = ( mask [ a ] & mask [ b ] ) . values ax . scatter ( df [ b ] [ common ] , df [ a ] [ common ] , marker = marker , alpha = alpha , * * kwds ) ax . set_xlim ( boundaries_list [ j ] ) ax . set_ylim ( boundaries_list [ i ] ) ax . set_xlabel ( b ) ax . set_ylabel ( a ) if j != 0 : ax . yaxis . set_visible ( False ) if i != n - 1 : ax . xaxis . set_visible ( False ) if len ( df . columns ) > 1 : lim1 = boundaries_list [ 0 ] locs = axes [ 0 ] [ 1 ] . yaxis . get_majorticklocs ( ) locs = locs [ ( lim1 [ 0 ] <= locs ) & ( locs <= lim1 [ 1 ] ) ] adj = ( locs - lim1 [ 0 ] ) / ( lim1 [ 1 ] - lim1 [ 0 ] ) lim0 = axes [ 0 ] [ 0 ] . get_ylim ( ) adj = adj * ( lim0 [ 1 ] - lim0 [ 0 ] ) + lim0 [ 0 ] axes [ 0 ] [ 0 ] . yaxis . set_ticks ( adj ) if np . all ( locs == locs . astype ( int ) ) : # if all ticks are int locs = locs . astype ( int ) axes [ 0 ] [ 0 ] . yaxis . set_ticklabels ( locs ) _set_ticks_props ( axes , xlabelsize = 8 , xrot = 90 , ylabelsize = 8 , yrot = 0 ) return axes\n\n\n        Summary:", "input": "", "output": "Draw a matrix of scatter plots ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def radviz ( frame , class_column , ax = None , color = None , colormap = None , * * kwds ) : import matplotlib . pyplot as plt import matplotlib . patches as patches def normalize ( series ) : a = min ( series ) b = max ( series ) return ( series - a ) / ( b - a ) n = len ( frame ) classes = frame [ class_column ] . drop_duplicates ( ) class_col = frame [ class_column ] df = frame . drop ( class_column , axis = 1 ) . apply ( normalize ) if ax is None : ax = plt . gca ( xlim = [ - 1 , 1 ] , ylim = [ - 1 , 1 ] ) to_plot = { } colors = _get_standard_colors ( num_colors = len ( classes ) , colormap = colormap , color_type = 'random' , color = color ) for kls in classes : to_plot [ kls ] = [ [ ] , [ ] ] m = len ( frame . columns ) - 1 s = np . array ( [ ( np . cos ( t ) , np . sin ( t ) ) for t in [ 2.0 * np . pi * ( i / float ( m ) ) for i in range ( m ) ] ] ) for i in range ( n ) : row = df . iloc [ i ] . values row_ = np . repeat ( np . expand_dims ( row , axis = 1 ) , 2 , axis = 1 ) y = ( s * row_ ) . sum ( axis = 0 ) / row . sum ( ) kls = class_col . iat [ i ] to_plot [ kls ] [ 0 ] . append ( y [ 0 ] ) to_plot [ kls ] [ 1 ] . append ( y [ 1 ] ) for i , kls in enumerate ( classes ) : ax . scatter ( to_plot [ kls ] [ 0 ] , to_plot [ kls ] [ 1 ] , color = colors [ i ] , label = pprint_thing ( kls ) , * * kwds ) ax . legend ( ) ax . add_patch ( patches . Circle ( ( 0.0 , 0.0 ) , radius = 1.0 , facecolor = 'none' ) ) for xy , name in zip ( s , df . columns ) : ax . add_patch ( patches . Circle ( xy , radius = 0.025 , facecolor = 'gray' ) ) if xy [ 0 ] < 0.0 and xy [ 1 ] < 0.0 : ax . text ( xy [ 0 ] - 0.025 , xy [ 1 ] - 0.025 , name , ha = 'right' , va = 'top' , size = 'small' ) elif xy [ 0 ] < 0.0 and xy [ 1 ] >= 0.0 : ax . text ( xy [ 0 ] - 0.025 , xy [ 1 ] + 0.025 , name , ha = 'right' , va = 'bottom' , size = 'small' ) elif xy [ 0 ] >= 0.0 and xy [ 1 ] < 0.0 : ax . text ( xy [ 0 ] + 0.025 , xy [ 1 ] - 0.025 , name , ha = 'left' , va = 'top' , size = 'small' ) elif xy [ 0 ] >= 0.0 and xy [ 1 ] >= 0.0 : ax . text ( xy [ 0 ] + 0.025 , xy [ 1 ] + 0.025 , name , ha = 'left' , va = 'bottom' , size = 'small' ) ax . axis ( 'equal' ) return ax\n\n\n        Summary:", "input": "", "output": "Plot a multidimensional dataset in 2D ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def andrews_curves ( frame , class_column , ax = None , samples = 200 , color = None , colormap = None , * * kwds ) : from math import sqrt , pi import matplotlib . pyplot as plt def function ( amplitudes ) : def f ( t ) : x1 = amplitudes [ 0 ] result = x1 / sqrt ( 2.0 ) # Take the rest of the coefficients and resize them # appropriately. Take a copy of amplitudes as otherwise numpy # deletes the element from amplitudes itself. coeffs = np . delete ( np . copy ( amplitudes ) , 0 ) coeffs . resize ( int ( ( coeffs . size + 1 ) / 2 ) , 2 ) # Generate the harmonics and arguments for the sin and cos # functions. harmonics = np . arange ( 0 , coeffs . shape [ 0 ] ) + 1 trig_args = np . outer ( harmonics , t ) result += np . sum ( coeffs [ : , 0 , np . newaxis ] * np . sin ( trig_args ) + coeffs [ : , 1 , np . newaxis ] * np . cos ( trig_args ) , axis = 0 ) return result return f n = len ( frame ) class_col = frame [ class_column ] classes = frame [ class_column ] . drop_duplicates ( ) df = frame . drop ( class_column , axis = 1 ) t = np . linspace ( - pi , pi , samples ) used_legends = set ( ) color_values = _get_standard_colors ( num_colors = len ( classes ) , colormap = colormap , color_type = 'random' , color = color ) colors = dict ( zip ( classes , color_values ) ) if ax is None : ax = plt . gca ( xlim = ( - pi , pi ) ) for i in range ( n ) : row = df . iloc [ i ] . values f = function ( row ) y = f ( t ) kls = class_col . iat [ i ] label = pprint_thing ( kls ) if label not in used_legends : used_legends . add ( label ) ax . plot ( t , y , color = colors [ kls ] , label = label , * * kwds ) else : ax . plot ( t , y , color = colors [ kls ] , * * kwds ) ax . legend ( loc = 'upper right' ) ax . grid ( ) return ax\n\n\n        Summary:", "input": "", "output": "Generate a matplotlib plot of Andrews curves for visualising clusters of multivariate data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def bootstrap_plot ( series , fig = None , size = 50 , samples = 500 , * * kwds ) : import random import matplotlib . pyplot as plt # random.sample(ndarray, int) fails on python 3.3, sigh data = list ( series . values ) samplings = [ random . sample ( data , size ) for _ in range ( samples ) ] means = np . array ( [ np . mean ( sampling ) for sampling in samplings ] ) medians = np . array ( [ np . median ( sampling ) for sampling in samplings ] ) midranges = np . array ( [ ( min ( sampling ) + max ( sampling ) ) * 0.5 for sampling in samplings ] ) if fig is None : fig = plt . figure ( ) x = lrange ( samples ) axes = [ ] ax1 = fig . add_subplot ( 2 , 3 , 1 ) ax1 . set_xlabel ( \"Sample\" ) axes . append ( ax1 ) ax1 . plot ( x , means , * * kwds ) ax2 = fig . add_subplot ( 2 , 3 , 2 ) ax2 . set_xlabel ( \"Sample\" ) axes . append ( ax2 ) ax2 . plot ( x , medians , * * kwds ) ax3 = fig . add_subplot ( 2 , 3 , 3 ) ax3 . set_xlabel ( \"Sample\" ) axes . append ( ax3 ) ax3 . plot ( x , midranges , * * kwds ) ax4 = fig . add_subplot ( 2 , 3 , 4 ) ax4 . set_xlabel ( \"Mean\" ) axes . append ( ax4 ) ax4 . hist ( means , * * kwds ) ax5 = fig . add_subplot ( 2 , 3 , 5 ) ax5 . set_xlabel ( \"Median\" ) axes . append ( ax5 ) ax5 . hist ( medians , * * kwds ) ax6 = fig . add_subplot ( 2 , 3 , 6 ) ax6 . set_xlabel ( \"Midrange\" ) axes . append ( ax6 ) ax6 . hist ( midranges , * * kwds ) for axis in axes : plt . setp ( axis . get_xticklabels ( ) , fontsize = 8 ) plt . setp ( axis . get_yticklabels ( ) , fontsize = 8 ) return fig\n\n\n        Summary:", "input": "", "output": "Bootstrap plot on mean median and mid - range statistics ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parallel_coordinates ( frame , class_column , cols = None , ax = None , color = None , use_columns = False , xticks = None , colormap = None , axvlines = True , axvlines_kwds = None , sort_labels = False , * * kwds ) : if axvlines_kwds is None : axvlines_kwds = { 'linewidth' : 1 , 'color' : 'black' } import matplotlib . pyplot as plt n = len ( frame ) classes = frame [ class_column ] . drop_duplicates ( ) class_col = frame [ class_column ] if cols is None : df = frame . drop ( class_column , axis = 1 ) else : df = frame [ cols ] used_legends = set ( ) ncols = len ( df . columns ) # determine values to use for xticks if use_columns is True : if not np . all ( np . isreal ( list ( df . columns ) ) ) : raise ValueError ( 'Columns must be numeric to be used as xticks' ) x = df . columns elif xticks is not None : if not np . all ( np . isreal ( xticks ) ) : raise ValueError ( 'xticks specified must be numeric' ) elif len ( xticks ) != ncols : raise ValueError ( 'Length of xticks must match number of columns' ) x = xticks else : x = lrange ( ncols ) if ax is None : ax = plt . gca ( ) color_values = _get_standard_colors ( num_colors = len ( classes ) , colormap = colormap , color_type = 'random' , color = color ) if sort_labels : classes = sorted ( classes ) color_values = sorted ( color_values ) colors = dict ( zip ( classes , color_values ) ) for i in range ( n ) : y = df . iloc [ i ] . values kls = class_col . iat [ i ] label = pprint_thing ( kls ) if label not in used_legends : used_legends . add ( label ) ax . plot ( x , y , color = colors [ kls ] , label = label , * * kwds ) else : ax . plot ( x , y , color = colors [ kls ] , * * kwds ) if axvlines : for i in x : ax . axvline ( i , * * axvlines_kwds ) ax . set_xticks ( x ) ax . set_xticklabels ( df . columns ) ax . set_xlim ( x [ 0 ] , x [ - 1 ] ) ax . legend ( loc = 'upper right' ) ax . grid ( ) return ax\n\n\n        Summary:", "input": "", "output": "Parallel coordinates plotting ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lag_plot ( series , lag = 1 , ax = None , * * kwds ) : import matplotlib . pyplot as plt # workaround because `c='b'` is hardcoded in matplotlibs scatter method kwds . setdefault ( 'c' , plt . rcParams [ 'patch.facecolor' ] ) data = series . values y1 = data [ : - lag ] y2 = data [ lag : ] if ax is None : ax = plt . gca ( ) ax . set_xlabel ( \"y(t)\" ) ax . set_ylabel ( \"y(t + {lag})\" . format ( lag = lag ) ) ax . scatter ( y1 , y2 , * * kwds ) return ax\n\n\n        Summary:", "input": "", "output": "Lag plot for time series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def autocorrelation_plot ( series , ax = None , * * kwds ) : import matplotlib . pyplot as plt n = len ( series ) data = np . asarray ( series ) if ax is None : ax = plt . gca ( xlim = ( 1 , n ) , ylim = ( - 1.0 , 1.0 ) ) mean = np . mean ( data ) c0 = np . sum ( ( data - mean ) ** 2 ) / float ( n ) def r ( h ) : return ( ( data [ : n - h ] - mean ) * ( data [ h : ] - mean ) ) . sum ( ) / float ( n ) / c0 x = np . arange ( n ) + 1 y = lmap ( r , x ) z95 = 1.959963984540054 z99 = 2.5758293035489004 ax . axhline ( y = z99 / np . sqrt ( n ) , linestyle = '--' , color = 'grey' ) ax . axhline ( y = z95 / np . sqrt ( n ) , color = 'grey' ) ax . axhline ( y = 0.0 , color = 'black' ) ax . axhline ( y = - z95 / np . sqrt ( n ) , color = 'grey' ) ax . axhline ( y = - z99 / np . sqrt ( n ) , linestyle = '--' , color = 'grey' ) ax . set_xlabel ( \"Lag\" ) ax . set_ylabel ( \"Autocorrelation\" ) ax . plot ( x , y , * * kwds ) if 'label' in kwds : ax . legend ( ) ax . grid ( ) return ax\n\n\n        Summary:", "input": "", "output": "Autocorrelation plot for time series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _any_pandas_objects ( terms ) : return any ( isinstance ( term . value , pd . core . generic . PandasObject ) for term in terms )\n\n\n        Summary:", "input": "", "output": "Check a sequence of terms for instances of PandasObject ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _align ( terms ) : try : # flatten the parse tree (a nested list, really) terms = list ( com . flatten ( terms ) ) except TypeError : # can't iterate so it must just be a constant or single variable if isinstance ( terms . value , pd . core . generic . NDFrame ) : typ = type ( terms . value ) return typ , _zip_axes_from_type ( typ , terms . value . axes ) return np . result_type ( terms . type ) , None # if all resolved variables are numeric scalars if all ( term . is_scalar for term in terms ) : return _result_type_many ( * ( term . value for term in terms ) ) . type , None # perform the main alignment typ , axes = _align_core ( terms ) return typ , axes\n\n\n        Summary:", "input": "", "output": "Align a set of terms"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reconstruct_object ( typ , obj , axes , dtype ) : try : typ = typ . type except AttributeError : pass res_t = np . result_type ( obj . dtype , dtype ) if ( not isinstance ( typ , partial ) and issubclass ( typ , pd . core . generic . PandasObject ) ) : return typ ( obj , dtype = res_t , * * axes ) # special case for pathological things like ~True/~False if hasattr ( res_t , 'type' ) and typ == np . bool_ and res_t != np . bool_ : ret_value = res_t . type ( obj ) else : ret_value = typ ( obj ) . astype ( res_t ) # The condition is to distinguish 0-dim array (returned in case of # scalar) and 1 element array # e.g. np.array(0) and np.array([0]) if len ( obj . shape ) == 1 and len ( obj ) == 1 : if not isinstance ( ret_value , np . ndarray ) : ret_value = np . array ( [ ret_value ] ) . astype ( res_t ) return ret_value\n\n\n        Summary:", "input": "", "output": "Reconstruct an object given its type raw value and possibly empty ( None ) axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tsplot ( series , plotf , ax = None , * * kwargs ) : import warnings warnings . warn ( \"'tsplot' is deprecated and will be removed in a \" \"future version. Please use Series.plot() instead.\" , FutureWarning , stacklevel = 2 ) # Used inferred freq is possible, need a test case for inferred if ax is None : import matplotlib . pyplot as plt ax = plt . gca ( ) freq , series = _maybe_resample ( series , ax , kwargs ) # Set ax with freq info _decorate_axes ( ax , freq , kwargs ) ax . _plot_data . append ( ( series , plotf , kwargs ) ) lines = plotf ( ax , series . index . _mpl_repr ( ) , series . values , * * kwargs ) # set date formatter, locators and rescale limits format_dateaxis ( ax , ax . freq , series . index ) return lines\n\n\n        Summary:", "input": "", "output": "Plots a Series on the given Matplotlib axes or the current axes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _decorate_axes ( ax , freq , kwargs ) : if not hasattr ( ax , '_plot_data' ) : ax . _plot_data = [ ] ax . freq = freq xaxis = ax . get_xaxis ( ) xaxis . freq = freq if not hasattr ( ax , 'legendlabels' ) : ax . legendlabels = [ kwargs . get ( 'label' , None ) ] else : ax . legendlabels . append ( kwargs . get ( 'label' , None ) ) ax . view_interval = None ax . date_axis_info = None\n\n\n        Summary:", "input": "", "output": "Initialize axes for time - series plotting"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_ax_freq ( ax ) : ax_freq = getattr ( ax , 'freq' , None ) if ax_freq is None : # check for left/right ax in case of secondary yaxis if hasattr ( ax , 'left_ax' ) : ax_freq = getattr ( ax . left_ax , 'freq' , None ) elif hasattr ( ax , 'right_ax' ) : ax_freq = getattr ( ax . right_ax , 'freq' , None ) if ax_freq is None : # check if a shared ax (sharex/twinx) has already freq set shared_axes = ax . get_shared_x_axes ( ) . get_siblings ( ax ) if len ( shared_axes ) > 1 : for shared_ax in shared_axes : ax_freq = getattr ( shared_ax , 'freq' , None ) if ax_freq is not None : break return ax_freq\n\n\n        Summary:", "input": "", "output": "Get the freq attribute of the ax object if set . Also checks shared axes ( eg when using secondary yaxis sharex = True or twinx )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format_timedelta_ticks ( x , pos , n_decimals ) : s , ns = divmod ( x , 1e9 ) m , s = divmod ( s , 60 ) h , m = divmod ( m , 60 ) d , h = divmod ( h , 24 ) decimals = int ( ns * 10 ** ( n_decimals - 9 ) ) s = r'{:02d}:{:02d}:{:02d}' . format ( int ( h ) , int ( m ) , int ( s ) ) if n_decimals > 0 : s += '.{{:0{:0d}d}}' . format ( n_decimals ) . format ( decimals ) if d != 0 : s = '{:d} days ' . format ( int ( d ) ) + s return s\n\n\n        Summary:", "input": "", "output": "Convert seconds to D days HH : MM : SS . F"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format_dateaxis ( subplot , freq , index ) : # handle index specific formatting # Note: DatetimeIndex does not use this # interface. DatetimeIndex uses matplotlib.date directly if isinstance ( index , ABCPeriodIndex ) : majlocator = TimeSeries_DateLocator ( freq , dynamic_mode = True , minor_locator = False , plot_obj = subplot ) minlocator = TimeSeries_DateLocator ( freq , dynamic_mode = True , minor_locator = True , plot_obj = subplot ) subplot . xaxis . set_major_locator ( majlocator ) subplot . xaxis . set_minor_locator ( minlocator ) majformatter = TimeSeries_DateFormatter ( freq , dynamic_mode = True , minor_locator = False , plot_obj = subplot ) minformatter = TimeSeries_DateFormatter ( freq , dynamic_mode = True , minor_locator = True , plot_obj = subplot ) subplot . xaxis . set_major_formatter ( majformatter ) subplot . xaxis . set_minor_formatter ( minformatter ) # x and y coord info subplot . format_coord = functools . partial ( _format_coord , freq ) elif isinstance ( index , ABCTimedeltaIndex ) : subplot . xaxis . set_major_formatter ( TimeSeries_TimedeltaFormatter ( ) ) else : raise TypeError ( 'index type not supported' ) pylab . draw_if_interactive ( )\n\n\n        Summary:", "input": "", "output": "Pretty - formats the date axis ( x - axis ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_homogeneous_type ( self ) : if self . _data . any_extension_types : return len ( { block . dtype for block in self . _data . blocks } ) == 1 else : return not self . _data . is_mixed_type\n\n\n        Summary:", "input": "", "output": "Whether all the columns in a DataFrame have the same type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _repr_html_ ( self ) : if self . _info_repr ( ) : buf = StringIO ( \"\" ) self . info ( buf = buf ) # need to escape the <class>, should be the first line. val = buf . getvalue ( ) . replace ( '<' , r'&lt;' , 1 ) val = val . replace ( '>' , r'&gt;' , 1 ) return '<pre>' + val + '</pre>' if get_option ( \"display.notebook_repr_html\" ) : max_rows = get_option ( \"display.max_rows\" ) max_cols = get_option ( \"display.max_columns\" ) show_dimensions = get_option ( \"display.show_dimensions\" ) return self . to_html ( max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , notebook = True ) else : return None\n\n\n        Summary:", "input": "", "output": "Return a html representation for a particular DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_string ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , justify = None , max_rows = None , max_cols = None , show_dimensions = False , decimal = '.' , line_width = None ) : formatter = fmt . DataFrameFormatter ( self , buf = buf , columns = columns , col_space = col_space , na_rep = na_rep , formatters = formatters , float_format = float_format , sparsify = sparsify , justify = justify , index_names = index_names , header = header , index = index , max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , decimal = decimal , line_width = line_width ) formatter . to_string ( ) if buf is None : result = formatter . buf . getvalue ( ) return result\n\n\n        Summary:", "input": "", "output": "Render a DataFrame to a console - friendly tabular output . % ( shared_params ) s line_width : int optional Width to wrap a line in characters . % ( returns ) s See Also -------- to_html : Convert DataFrame to HTML ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def iteritems ( self ) : if self . columns . is_unique and hasattr ( self , '_item_cache' ) : for k in self . columns : yield k , self . _get_item_cache ( k ) else : for i , k in enumerate ( self . columns ) : yield k , self . _ixs ( i , axis = 1 )\n\n\n        Summary:", "input": "", "output": "r Iterator over ( column name Series ) pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def iterrows ( self ) : columns = self . columns klass = self . _constructor_sliced for k , v in zip ( self . index , self . values ) : s = klass ( v , index = columns , name = k ) yield k , s\n\n\n        Summary:", "input": "", "output": "Iterate over DataFrame rows as ( index Series ) pairs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def itertuples ( self , index = True , name = \"Pandas\" ) : arrays = [ ] fields = list ( self . columns ) if index : arrays . append ( self . index ) fields . insert ( 0 , \"Index\" ) # use integer indexing because of possible duplicate column names arrays . extend ( self . iloc [ : , k ] for k in range ( len ( self . columns ) ) ) # Python 3 supports at most 255 arguments to constructor if name is not None and len ( self . columns ) + index < 256 : itertuple = collections . namedtuple ( name , fields , rename = True ) return map ( itertuple . _make , zip ( * arrays ) ) # fallback to regular tuples return zip ( * arrays )\n\n\n        Summary:", "input": "", "output": "Iterate over DataFrame rows as namedtuples ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dot ( self , other ) : if isinstance ( other , ( Series , DataFrame ) ) : common = self . columns . union ( other . index ) if ( len ( common ) > len ( self . columns ) or len ( common ) > len ( other . index ) ) : raise ValueError ( 'matrices are not aligned' ) left = self . reindex ( columns = common , copy = False ) right = other . reindex ( index = common , copy = False ) lvals = left . values rvals = right . values else : left = self lvals = self . values rvals = np . asarray ( other ) if lvals . shape [ 1 ] != rvals . shape [ 0 ] : raise ValueError ( 'Dot product shape mismatch, ' '{s} vs {r}' . format ( s = lvals . shape , r = rvals . shape ) ) if isinstance ( other , DataFrame ) : return self . _constructor ( np . dot ( lvals , rvals ) , index = left . index , columns = other . columns ) elif isinstance ( other , Series ) : return Series ( np . dot ( lvals , rvals ) , index = left . index ) elif isinstance ( rvals , ( np . ndarray , Index ) ) : result = np . dot ( lvals , rvals ) if result . ndim == 2 : return self . _constructor ( result , index = left . index ) else : return Series ( result , index = left . index ) else : # pragma: no cover raise TypeError ( 'unsupported type: {oth}' . format ( oth = type ( other ) ) )\n\n\n        Summary:", "input": "", "output": "Compute the matrix mutiplication between the DataFrame and other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_dict ( cls , data , orient = 'columns' , dtype = None , columns = None ) : index = None orient = orient . lower ( ) if orient == 'index' : if len ( data ) > 0 : # TODO speed up Series case if isinstance ( list ( data . values ( ) ) [ 0 ] , ( Series , dict ) ) : data = _from_nested_dict ( data ) else : data , index = list ( data . values ( ) ) , list ( data . keys ( ) ) elif orient == 'columns' : if columns is not None : raise ValueError ( \"cannot use columns parameter with \" \"orient='columns'\" ) else : # pragma: no cover raise ValueError ( 'only recognize index or columns for orient' ) return cls ( data , index = index , columns = columns , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Construct DataFrame from dict of array - like or dicts ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_numpy ( self , dtype = None , copy = False ) : result = np . array ( self . values , dtype = dtype , copy = copy ) return result\n\n\n        Summary:", "input": "", "output": "Convert the DataFrame to a NumPy array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_dict ( self , orient = 'dict' , into = dict ) : if not self . columns . is_unique : warnings . warn ( \"DataFrame columns are not unique, some \" \"columns will be omitted.\" , UserWarning , stacklevel = 2 ) # GH16122 into_c = com . standardize_mapping ( into ) if orient . lower ( ) . startswith ( 'd' ) : return into_c ( ( k , v . to_dict ( into ) ) for k , v in self . items ( ) ) elif orient . lower ( ) . startswith ( 'l' ) : return into_c ( ( k , v . tolist ( ) ) for k , v in self . items ( ) ) elif orient . lower ( ) . startswith ( 'sp' ) : return into_c ( ( ( 'index' , self . index . tolist ( ) ) , ( 'columns' , self . columns . tolist ( ) ) , ( 'data' , [ list ( map ( com . maybe_box_datetimelike , t ) ) for t in self . itertuples ( index = False , name = None ) ] ) ) ) elif orient . lower ( ) . startswith ( 's' ) : return into_c ( ( k , com . maybe_box_datetimelike ( v ) ) for k , v in self . items ( ) ) elif orient . lower ( ) . startswith ( 'r' ) : columns = self . columns . tolist ( ) rows = ( dict ( zip ( columns , row ) ) for row in self . itertuples ( index = False , name = None ) ) return [ into_c ( ( k , com . maybe_box_datetimelike ( v ) ) for k , v in row . items ( ) ) for row in rows ] elif orient . lower ( ) . startswith ( 'i' ) : if not self . index . is_unique : raise ValueError ( \"DataFrame index must be unique for orient='index'.\" ) return into_c ( ( t [ 0 ] , dict ( zip ( self . columns , t [ 1 : ] ) ) ) for t in self . itertuples ( name = None ) ) else : raise ValueError ( \"orient '{o}' not understood\" . format ( o = orient ) )\n\n\n        Summary:", "input": "", "output": "Convert the DataFrame to a dictionary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_gbq ( self , destination_table , project_id = None , chunksize = None , reauth = False , if_exists = 'fail' , auth_local_webserver = False , table_schema = None , location = None , progress_bar = True , credentials = None , verbose = None , private_key = None ) : from pandas . io import gbq return gbq . to_gbq ( self , destination_table , project_id = project_id , chunksize = chunksize , reauth = reauth , if_exists = if_exists , auth_local_webserver = auth_local_webserver , table_schema = table_schema , location = location , progress_bar = progress_bar , credentials = credentials , verbose = verbose , private_key = private_key )\n\n\n        Summary:", "input": "", "output": "Write a DataFrame to a Google BigQuery table ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_records ( cls , data , index = None , exclude = None , columns = None , coerce_float = False , nrows = None ) : # Make a copy of the input columns so we can modify it if columns is not None : columns = ensure_index ( columns ) if is_iterator ( data ) : if nrows == 0 : return cls ( ) try : first_row = next ( data ) except StopIteration : return cls ( index = index , columns = columns ) dtype = None if hasattr ( first_row , 'dtype' ) and first_row . dtype . names : dtype = first_row . dtype values = [ first_row ] if nrows is None : values += data else : values . extend ( itertools . islice ( data , nrows - 1 ) ) if dtype is not None : data = np . array ( values , dtype = dtype ) else : data = values if isinstance ( data , dict ) : if columns is None : columns = arr_columns = ensure_index ( sorted ( data ) ) arrays = [ data [ k ] for k in columns ] else : arrays = [ ] arr_columns = [ ] for k , v in data . items ( ) : if k in columns : arr_columns . append ( k ) arrays . append ( v ) arrays , arr_columns = reorder_arrays ( arrays , arr_columns , columns ) elif isinstance ( data , ( np . ndarray , DataFrame ) ) : arrays , columns = to_arrays ( data , columns ) if columns is not None : columns = ensure_index ( columns ) arr_columns = columns else : arrays , arr_columns = to_arrays ( data , columns , coerce_float = coerce_float ) arr_columns = ensure_index ( arr_columns ) if columns is not None : columns = ensure_index ( columns ) else : columns = arr_columns if exclude is None : exclude = set ( ) else : exclude = set ( exclude ) result_index = None if index is not None : if ( isinstance ( index , str ) or not hasattr ( index , \"__iter__\" ) ) : i = columns . get_loc ( index ) exclude . add ( index ) if len ( arrays ) > 0 : result_index = Index ( arrays [ i ] , name = index ) else : result_index = Index ( [ ] , name = index ) else : try : index_data = [ arrays [ arr_columns . get_loc ( field ) ] for field in index ] result_index = ensure_index_from_sequences ( index_data , names = index ) exclude . update ( index ) except Exception : result_index = index if any ( exclude ) : arr_exclude = [ x for x in exclude if x in arr_columns ] to_remove = [ arr_columns . get_loc ( col ) for col in arr_exclude ] arrays = [ v for i , v in enumerate ( arrays ) if i not in to_remove ] arr_columns = arr_columns . drop ( arr_exclude ) columns = columns . drop ( exclude ) mgr = arrays_to_mgr ( arrays , arr_columns , result_index , columns ) return cls ( mgr )\n\n\n        Summary:", "input": "", "output": "Convert structured or record ndarray to DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_records ( self , index = True , convert_datetime64 = None , column_dtypes = None , index_dtypes = None ) : if convert_datetime64 is not None : warnings . warn ( \"The 'convert_datetime64' parameter is \" \"deprecated and will be removed in a future \" \"version\" , FutureWarning , stacklevel = 2 ) if index : if is_datetime64_any_dtype ( self . index ) and convert_datetime64 : ix_vals = [ self . index . to_pydatetime ( ) ] else : if isinstance ( self . index , MultiIndex ) : # array of tuples to numpy cols. copy copy copy ix_vals = lmap ( np . array , zip ( * self . index . values ) ) else : ix_vals = [ self . index . values ] arrays = ix_vals + [ self [ c ] . get_values ( ) for c in self . columns ] count = 0 index_names = list ( self . index . names ) if isinstance ( self . index , MultiIndex ) : for i , n in enumerate ( index_names ) : if n is None : index_names [ i ] = 'level_%d' % count count += 1 elif index_names [ 0 ] is None : index_names = [ 'index' ] names = lmap ( str , index_names ) + lmap ( str , self . columns ) else : arrays = [ self [ c ] . get_values ( ) for c in self . columns ] names = lmap ( str , self . columns ) index_names = [ ] index_len = len ( index_names ) formats = [ ] for i , v in enumerate ( arrays ) : index = i # When the names and arrays are collected, we # first collect those in the DataFrame's index, # followed by those in its columns. # # Thus, the total length of the array is: # len(index_names) + len(DataFrame.columns). # # This check allows us to see whether we are # handling a name / array in the index or column. if index < index_len : dtype_mapping = index_dtypes name = index_names [ index ] else : index -= index_len dtype_mapping = column_dtypes name = self . columns [ index ] # We have a dictionary, so we get the data type # associated with the index or column (which can # be denoted by its name in the DataFrame or its # position in DataFrame's array of indices or # columns, whichever is applicable. if is_dict_like ( dtype_mapping ) : if name in dtype_mapping : dtype_mapping = dtype_mapping [ name ] elif index in dtype_mapping : dtype_mapping = dtype_mapping [ index ] else : dtype_mapping = None # If no mapping can be found, use the array's # dtype attribute for formatting. # # A valid dtype must either be a type or # string naming a type. if dtype_mapping is None : formats . append ( v . dtype ) elif isinstance ( dtype_mapping , ( type , np . dtype , str ) ) : formats . append ( dtype_mapping ) else : element = \"row\" if i < index_len else \"column\" msg = ( \"Invalid dtype {dtype} specified for \" \"{element} {name}\" ) . format ( dtype = dtype_mapping , element = element , name = name ) raise ValueError ( msg ) return np . rec . fromarrays ( arrays , dtype = { 'names' : names , 'formats' : formats } )\n\n\n        Summary:", "input": "", "output": "Convert DataFrame to a NumPy record array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_items ( cls , items , columns = None , orient = 'columns' ) : warnings . warn ( \"from_items is deprecated. Please use \" \"DataFrame.from_dict(dict(items), ...) instead. \" \"DataFrame.from_dict(OrderedDict(items)) may be used to \" \"preserve the key order.\" , FutureWarning , stacklevel = 2 ) keys , values = lzip ( * items ) if orient == 'columns' : if columns is not None : columns = ensure_index ( columns ) idict = dict ( items ) if len ( idict ) < len ( items ) : if not columns . equals ( ensure_index ( keys ) ) : raise ValueError ( 'With non-unique item names, passed ' 'columns must be identical' ) arrays = values else : arrays = [ idict [ k ] for k in columns if k in idict ] else : columns = ensure_index ( keys ) arrays = values # GH 17312 # Provide more informative error msg when scalar values passed try : return cls . _from_arrays ( arrays , columns , None ) except ValueError : if not is_nested_list_like ( values ) : raise ValueError ( 'The value in each (key, value) pair ' 'must be an array, Series, or dict' ) elif orient == 'index' : if columns is None : raise TypeError ( \"Must pass columns with orient='index'\" ) keys = ensure_index ( keys ) # GH 17312 # Provide more informative error msg when scalar values passed try : arr = np . array ( values , dtype = object ) . T data = [ lib . maybe_convert_objects ( v ) for v in arr ] return cls . _from_arrays ( data , columns , keys ) except TypeError : if not is_nested_list_like ( values ) : raise ValueError ( 'The value in each (key, value) pair ' 'must be an array, Series, or dict' ) else : # pragma: no cover raise ValueError ( \"'orient' must be either 'columns' or 'index'\" )\n\n\n        Summary:", "input": "", "output": "Construct a DataFrame from a list of tuples ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_csv ( cls , path , header = 0 , sep = ',' , index_col = 0 , parse_dates = True , encoding = None , tupleize_cols = None , infer_datetime_format = False ) : warnings . warn ( \"from_csv is deprecated. Please use read_csv(...) \" \"instead. Note that some of the default arguments are \" \"different, so please refer to the documentation \" \"for from_csv when changing your function calls\" , FutureWarning , stacklevel = 2 ) from pandas . io . parsers import read_csv return read_csv ( path , header = header , sep = sep , parse_dates = parse_dates , index_col = index_col , encoding = encoding , tupleize_cols = tupleize_cols , infer_datetime_format = infer_datetime_format )\n\n\n        Summary:", "input": "", "output": "Read CSV file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_sparse ( self , fill_value = None , kind = 'block' ) : from pandas . core . sparse . api import SparseDataFrame return SparseDataFrame ( self . _series , index = self . index , columns = self . columns , default_kind = kind , default_fill_value = fill_value )\n\n\n        Summary:", "input": "", "output": "Convert to SparseDataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_stata ( self , fname , convert_dates = None , write_index = True , encoding = \"latin-1\" , byteorder = None , time_stamp = None , data_label = None , variable_labels = None , version = 114 , convert_strl = None ) : kwargs = { } if version not in ( 114 , 117 ) : raise ValueError ( 'Only formats 114 and 117 supported.' ) if version == 114 : if convert_strl is not None : raise ValueError ( 'strl support is only available when using ' 'format 117' ) from pandas . io . stata import StataWriter as statawriter else : from pandas . io . stata import StataWriter117 as statawriter kwargs [ 'convert_strl' ] = convert_strl writer = statawriter ( fname , self , convert_dates = convert_dates , byteorder = byteorder , time_stamp = time_stamp , data_label = data_label , write_index = write_index , variable_labels = variable_labels , * * kwargs ) writer . write_file ( )\n\n\n        Summary:", "input": "", "output": "Export DataFrame object to Stata dta format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_feather ( self , fname ) : from pandas . io . feather_format import to_feather to_feather ( self , fname )\n\n\n        Summary:", "input": "", "output": "Write out the binary feather - format for DataFrames ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_parquet ( self , fname , engine = 'auto' , compression = 'snappy' , index = None , partition_cols = None , * * kwargs ) : from pandas . io . parquet import to_parquet to_parquet ( self , fname , engine , compression = compression , index = index , partition_cols = partition_cols , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Write a DataFrame to the binary parquet format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_html ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , justify = None , max_rows = None , max_cols = None , show_dimensions = False , decimal = '.' , bold_rows = True , classes = None , escape = True , notebook = False , border = None , table_id = None , render_links = False ) : if ( justify is not None and justify not in fmt . _VALID_JUSTIFY_PARAMETERS ) : raise ValueError ( \"Invalid value for justify parameter\" ) formatter = fmt . DataFrameFormatter ( self , buf = buf , columns = columns , col_space = col_space , na_rep = na_rep , formatters = formatters , float_format = float_format , sparsify = sparsify , justify = justify , index_names = index_names , header = header , index = index , bold_rows = bold_rows , escape = escape , max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , decimal = decimal , table_id = table_id , render_links = render_links ) # TODO: a generic formatter wld b in DataFrameFormatter formatter . to_html ( classes = classes , notebook = notebook , border = border ) if buf is None : return formatter . buf . getvalue ( )\n\n\n        Summary:", "input": "", "output": "Render a DataFrame as an HTML table . % ( shared_params ) s bold_rows : bool default True Make the row labels bold in the output . classes : str or list or tuple default None CSS class ( es ) to apply to the resulting html table . escape : bool default True Convert the characters < > and & to HTML - safe sequences . notebook : { True False } default False Whether the generated HTML is for IPython Notebook . border : int A border = border attribute is included in the opening <table > tag . Default pd . options . html . border ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def info ( self , verbose = None , buf = None , max_cols = None , memory_usage = None , null_counts = None ) : if buf is None : # pragma: no cover buf = sys . stdout lines = [ ] lines . append ( str ( type ( self ) ) ) lines . append ( self . index . _summary ( ) ) if len ( self . columns ) == 0 : lines . append ( 'Empty {name}' . format ( name = type ( self ) . __name__ ) ) fmt . buffer_put_lines ( buf , lines ) return cols = self . columns # hack if max_cols is None : max_cols = get_option ( 'display.max_info_columns' , len ( self . columns ) + 1 ) max_rows = get_option ( 'display.max_info_rows' , len ( self ) + 1 ) if null_counts is None : show_counts = ( ( len ( self . columns ) <= max_cols ) and ( len ( self ) < max_rows ) ) else : show_counts = null_counts exceeds_info_cols = len ( self . columns ) > max_cols def _verbose_repr ( ) : lines . append ( 'Data columns (total %d columns):' % len ( self . columns ) ) space = max ( len ( pprint_thing ( k ) ) for k in self . columns ) + 4 counts = None tmpl = \"{count}{dtype}\" if show_counts : counts = self . count ( ) if len ( cols ) != len ( counts ) : # pragma: no cover raise AssertionError ( 'Columns must equal counts ' '({cols:d} != {counts:d})' . format ( cols = len ( cols ) , counts = len ( counts ) ) ) tmpl = \"{count} non-null {dtype}\" dtypes = self . dtypes for i , col in enumerate ( self . columns ) : dtype = dtypes . iloc [ i ] col = pprint_thing ( col ) count = \"\" if show_counts : count = counts . iloc [ i ] lines . append ( _put_str ( col , space ) + tmpl . format ( count = count , dtype = dtype ) ) def _non_verbose_repr ( ) : lines . append ( self . columns . _summary ( name = 'Columns' ) ) def _sizeof_fmt ( num , size_qualifier ) : # returns size in human readable format for x in [ 'bytes' , 'KB' , 'MB' , 'GB' , 'TB' ] : if num < 1024.0 : return ( \"{num:3.1f}{size_q} \" \"{x}\" . format ( num = num , size_q = size_qualifier , x = x ) ) num /= 1024.0 return \"{num:3.1f}{size_q} {pb}\" . format ( num = num , size_q = size_qualifier , pb = 'PB' ) if verbose : _verbose_repr ( ) elif verbose is False : # specifically set to False, not nesc None _non_verbose_repr ( ) else : if exceeds_info_cols : _non_verbose_repr ( ) else : _verbose_repr ( ) counts = self . get_dtype_counts ( ) dtypes = [ '{k}({kk:d})' . format ( k = k [ 0 ] , kk = k [ 1 ] ) for k in sorted ( counts . items ( ) ) ] lines . append ( 'dtypes: {types}' . format ( types = ', ' . join ( dtypes ) ) ) if memory_usage is None : memory_usage = get_option ( 'display.memory_usage' ) if memory_usage : # append memory usage of df to display size_qualifier = '' if memory_usage == 'deep' : deep = True else : # size_qualifier is just a best effort; not guaranteed to catch # all cases (e.g., it misses categorical data even with object # categories) deep = False if ( 'object' in counts or self . index . _is_memory_usage_qualified ( ) ) : size_qualifier = '+' mem_usage = self . memory_usage ( index = True , deep = deep ) . sum ( ) lines . append ( \"memory usage: {mem}\\n\" . format ( mem = _sizeof_fmt ( mem_usage , size_qualifier ) ) ) fmt . buffer_put_lines ( buf , lines )\n\n\n        Summary:", "input": "", "output": "Print a concise summary of a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def memory_usage ( self , index = True , deep = False ) : result = Series ( [ c . memory_usage ( index = False , deep = deep ) for col , c in self . iteritems ( ) ] , index = self . columns ) if index : result = Series ( self . index . memory_usage ( deep = deep ) , index = [ 'Index' ] ) . append ( result ) return result\n\n\n        Summary:", "input": "", "output": "Return the memory usage of each column in bytes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transpose ( self , * args , * * kwargs ) : nv . validate_transpose ( args , dict ( ) ) return super ( ) . transpose ( 1 , 0 , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Transpose index and columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_value ( self , index , col , takeable = False ) : warnings . warn ( \"get_value is deprecated and will be removed \" \"in a future release. Please use \" \".at[] or .iat[] accessors instead\" , FutureWarning , stacklevel = 2 ) return self . _get_value ( index , col , takeable = takeable )\n\n\n        Summary:", "input": "", "output": "Quickly retrieve single value at passed column and index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_value ( self , index , col , value , takeable = False ) : warnings . warn ( \"set_value is deprecated and will be removed \" \"in a future release. Please use \" \".at[] or .iat[] accessors instead\" , FutureWarning , stacklevel = 2 ) return self . _set_value ( index , col , value , takeable = takeable )\n\n\n        Summary:", "input": "", "output": "Put single value at passed column and index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ixs ( self , i , axis = 0 ) : # irow if axis == 0 : if isinstance ( i , slice ) : return self [ i ] else : label = self . index [ i ] if isinstance ( label , Index ) : # a location index by definition result = self . take ( i , axis = axis ) copy = True else : new_values = self . _data . fast_xs ( i ) if is_scalar ( new_values ) : return new_values # if we are a copy, mark as such copy = ( isinstance ( new_values , np . ndarray ) and new_values . base is None ) result = self . _constructor_sliced ( new_values , index = self . columns , name = self . index [ i ] , dtype = new_values . dtype ) result . _set_is_copy ( self , copy = copy ) return result # icol else : label = self . columns [ i ] if isinstance ( i , slice ) : # need to return view lab_slice = slice ( label [ 0 ] , label [ - 1 ] ) return self . loc [ : , lab_slice ] else : if isinstance ( label , Index ) : return self . _take ( i , axis = 1 ) index_len = len ( self . index ) # if the values returned are not the same length # as the index (iow a not found value), iget returns # a 0-len ndarray. This is effectively catching # a numpy error (as numpy should really raise) values = self . _data . iget ( i ) if index_len and not len ( values ) : values = np . array ( [ np . nan ] * index_len , dtype = object ) result = self . _box_col_values ( values , label ) # this is a cached value, mark it so result . _set_as_cached ( label , self ) return result\n\n\n        Summary:", "input": "", "output": "Parameters ---------- i : int slice or sequence of integers axis : int"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def query ( self , expr , inplace = False , * * kwargs ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if not isinstance ( expr , str ) : msg = \"expr must be a string to be evaluated, {0} given\" raise ValueError ( msg . format ( type ( expr ) ) ) kwargs [ 'level' ] = kwargs . pop ( 'level' , 0 ) + 1 kwargs [ 'target' ] = None res = self . eval ( expr , * * kwargs ) try : new_data = self . loc [ res ] except ValueError : # when res is multi-dimensional loc raises, but this is sometimes a # valid query new_data = self [ res ] if inplace : self . _update_inplace ( new_data ) else : return new_data\n\n\n        Summary:", "input": "", "output": "Query the columns of a DataFrame with a boolean expression ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def eval ( self , expr , inplace = False , * * kwargs ) : from pandas . core . computation . eval import eval as _eval inplace = validate_bool_kwarg ( inplace , 'inplace' ) resolvers = kwargs . pop ( 'resolvers' , None ) kwargs [ 'level' ] = kwargs . pop ( 'level' , 0 ) + 1 if resolvers is None : index_resolvers = self . _get_index_resolvers ( ) column_resolvers = self . _get_space_character_free_column_resolvers ( ) resolvers = column_resolvers , index_resolvers if 'target' not in kwargs : kwargs [ 'target' ] = self kwargs [ 'resolvers' ] = kwargs . get ( 'resolvers' , ( ) ) + tuple ( resolvers ) return _eval ( expr , inplace = inplace , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Evaluate a string describing operations on DataFrame columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select_dtypes ( self , include = None , exclude = None ) : def _get_info_slice ( obj , indexer ) : \"\"\"Slice the info axis of `obj` with `indexer`.\"\"\" if not hasattr ( obj , '_info_axis_number' ) : msg = 'object of type {typ!r} has no info axis' raise TypeError ( msg . format ( typ = type ( obj ) . __name__ ) ) slices = [ slice ( None ) ] * obj . ndim slices [ obj . _info_axis_number ] = indexer return tuple ( slices ) if not is_list_like ( include ) : include = ( include , ) if include is not None else ( ) if not is_list_like ( exclude ) : exclude = ( exclude , ) if exclude is not None else ( ) selection = tuple ( map ( frozenset , ( include , exclude ) ) ) if not any ( selection ) : raise ValueError ( 'at least one of include or exclude must be ' 'nonempty' ) # convert the myriad valid dtypes object to a single representation include , exclude = map ( lambda x : frozenset ( map ( infer_dtype_from_object , x ) ) , selection ) for dtypes in ( include , exclude ) : invalidate_string_dtypes ( dtypes ) # can't both include AND exclude! if not include . isdisjoint ( exclude ) : raise ValueError ( 'include and exclude overlap on {inc_ex}' . format ( inc_ex = ( include & exclude ) ) ) # empty include/exclude -> defaults to True # three cases (we've already raised if both are empty) # case 1: empty include, nonempty exclude # we have True, True, ... True for include, same for exclude # in the loop below we get the excluded # and when we call '&' below we get only the excluded # case 2: nonempty include, empty exclude # same as case 1, but with include # case 3: both nonempty # the \"union\" of the logic of case 1 and case 2: # we get the included and excluded, and return their logical and include_these = Series ( not bool ( include ) , index = self . columns ) exclude_these = Series ( not bool ( exclude ) , index = self . columns ) def is_dtype_instance_mapper ( idx , dtype ) : return idx , functools . partial ( issubclass , dtype . type ) for idx , f in itertools . starmap ( is_dtype_instance_mapper , enumerate ( self . dtypes ) ) : if include : # checks for the case of empty include or exclude include_these . iloc [ idx ] = any ( map ( f , include ) ) if exclude : exclude_these . iloc [ idx ] = not any ( map ( f , exclude ) ) dtype_indexer = include_these & exclude_these return self . loc [ _get_info_slice ( self , dtype_indexer ) ]\n\n\n        Summary:", "input": "", "output": "Return a subset of the DataFrame s columns based on the column dtypes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _box_col_values ( self , values , items ) : klass = self . _constructor_sliced return klass ( values , index = self . index , name = items , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Provide boxed values for a column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_valid_index ( self , value ) : # GH5632, make sure that we are a Series convertible if not len ( self . index ) and is_list_like ( value ) : try : value = Series ( value ) except ( ValueError , NotImplementedError , TypeError ) : raise ValueError ( 'Cannot set a frame with no defined index ' 'and a value that cannot be converted to a ' 'Series' ) self . _data = self . _data . reindex_axis ( value . index . copy ( ) , axis = 1 , fill_value = np . nan )\n\n\n        Summary:", "input": "", "output": "Ensure that if we don t have an index that we can create one from the passed value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_item ( self , key , value ) : self . _ensure_valid_index ( value ) value = self . _sanitize_column ( key , value ) NDFrame . _set_item ( self , key , value ) # check if we are modifying a copy # try to set first as we want an invalid # value exception to occur first if len ( self ) : self . _check_setitem_copy ( )\n\n\n        Summary:", "input": "", "output": "Add series to DataFrame in specified column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def insert ( self , loc , column , value , allow_duplicates = False ) : self . _ensure_valid_index ( value ) value = self . _sanitize_column ( column , value , broadcast = False ) self . _data . insert ( loc , column , value , allow_duplicates = allow_duplicates )\n\n\n        Summary:", "input": "", "output": "Insert column into DataFrame at specified location ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def assign ( self , * * kwargs ) : data = self . copy ( ) # >= 3.6 preserve order of kwargs if PY36 : for k , v in kwargs . items ( ) : data [ k ] = com . apply_if_callable ( v , data ) else : # <= 3.5: do all calculations first... results = OrderedDict ( ) for k , v in kwargs . items ( ) : results [ k ] = com . apply_if_callable ( v , data ) # <= 3.5 and earlier results = sorted ( results . items ( ) ) # ... and then assign for k , v in results : data [ k ] = v return data\n\n\n        Summary:", "input": "", "output": "r Assign new columns to a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sanitize_column ( self , key , value , broadcast = True ) : def reindexer ( value ) : # reindex if necessary if value . index . equals ( self . index ) or not len ( self . index ) : value = value . _values . copy ( ) else : # GH 4107 try : value = value . reindex ( self . index ) . _values except Exception as e : # duplicate axis if not value . index . is_unique : raise e # other raise TypeError ( 'incompatible index of inserted column ' 'with frame index' ) return value if isinstance ( value , Series ) : value = reindexer ( value ) elif isinstance ( value , DataFrame ) : # align right-hand-side columns if self.columns # is multi-index and self[key] is a sub-frame if isinstance ( self . columns , MultiIndex ) and key in self . columns : loc = self . columns . get_loc ( key ) if isinstance ( loc , ( slice , Series , np . ndarray , Index ) ) : cols = maybe_droplevels ( self . columns [ loc ] , key ) if len ( cols ) and not cols . equals ( value . columns ) : value = value . reindex ( cols , axis = 1 ) # now align rows value = reindexer ( value ) . T elif isinstance ( value , ExtensionArray ) : # Explicitly copy here, instead of in sanitize_index, # as sanitize_index won't copy an EA, even with copy=True value = value . copy ( ) value = sanitize_index ( value , self . index , copy = False ) elif isinstance ( value , Index ) or is_sequence ( value ) : # turn me into an ndarray value = sanitize_index ( value , self . index , copy = False ) if not isinstance ( value , ( np . ndarray , Index ) ) : if isinstance ( value , list ) and len ( value ) > 0 : value = maybe_convert_platform ( value ) else : value = com . asarray_tuplesafe ( value ) elif value . ndim == 2 : value = value . copy ( ) . T elif isinstance ( value , Index ) : value = value . copy ( deep = True ) else : value = value . copy ( ) # possibly infer to datetimelike if is_object_dtype ( value . dtype ) : value = maybe_infer_to_datetimelike ( value ) else : # cast ignores pandas dtypes. so save the dtype first infer_dtype , _ = infer_dtype_from_scalar ( value , pandas_dtype = True ) # upcast value = cast_scalar_to_array ( len ( self . index ) , value ) value = maybe_cast_to_datetime ( value , infer_dtype ) # return internal types directly if is_extension_type ( value ) or is_extension_array_dtype ( value ) : return value # broadcast across multiple columns if necessary if broadcast and key in self . columns and value . ndim == 1 : if ( not self . columns . is_unique or isinstance ( self . columns , MultiIndex ) ) : existing_piece = self [ key ] if isinstance ( existing_piece , DataFrame ) : value = np . tile ( value , ( len ( existing_piece . columns ) , 1 ) ) return np . atleast_2d ( np . asarray ( value ) )\n\n\n        Summary:", "input": "", "output": "Ensures new columns ( which go into the BlockManager as new blocks ) are always copied and converted into an array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def lookup ( self , row_labels , col_labels ) : n = len ( row_labels ) if n != len ( col_labels ) : raise ValueError ( 'Row labels must have same size as column labels' ) thresh = 1000 if not self . _is_mixed_type or n > thresh : values = self . values ridx = self . index . get_indexer ( row_labels ) cidx = self . columns . get_indexer ( col_labels ) if ( ridx == - 1 ) . any ( ) : raise KeyError ( 'One or more row labels was not found' ) if ( cidx == - 1 ) . any ( ) : raise KeyError ( 'One or more column labels was not found' ) flat_index = ridx * len ( self . columns ) + cidx result = values . flat [ flat_index ] else : result = np . empty ( n , dtype = 'O' ) for i , ( r , c ) in enumerate ( zip ( row_labels , col_labels ) ) : result [ i ] = self . _get_value ( r , c ) if is_object_dtype ( result ) : result = lib . maybe_convert_objects ( result ) return result\n\n\n        Summary:", "input": "", "output": "Label - based fancy indexing function for DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reindex_multi ( self , axes , copy , fill_value ) : new_index , row_indexer = self . index . reindex ( axes [ 'index' ] ) new_columns , col_indexer = self . columns . reindex ( axes [ 'columns' ] ) if row_indexer is not None and col_indexer is not None : indexer = row_indexer , col_indexer new_values = algorithms . take_2d_multi ( self . values , indexer , fill_value = fill_value ) return self . _constructor ( new_values , index = new_index , columns = new_columns ) else : return self . _reindex_with_indexers ( { 0 : [ new_index , row_indexer ] , 1 : [ new_columns , col_indexer ] } , copy = copy , fill_value = fill_value )\n\n\n        Summary:", "input": "", "output": "We are guaranteed non - Nones in the axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def drop ( self , labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = 'raise' ) : return super ( ) . drop ( labels = labels , axis = axis , index = index , columns = columns , level = level , inplace = inplace , errors = errors )\n\n\n        Summary:", "input": "", "output": "Drop specified labels from rows or columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename ( self , * args , * * kwargs ) : axes = validate_axis_style_args ( self , args , kwargs , 'mapper' , 'rename' ) kwargs . update ( axes ) # Pop these, since the values are in `kwargs` under different names kwargs . pop ( 'axis' , None ) kwargs . pop ( 'mapper' , None ) return super ( ) . rename ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "Alter axes labels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_index ( self , keys , drop = True , append = False , inplace = False , verify_integrity = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if not isinstance ( keys , list ) : keys = [ keys ] err_msg = ( 'The parameter \"keys\" may be a column key, one-dimensional ' 'array, or a list containing only valid column keys and ' 'one-dimensional arrays.' ) missing = [ ] for col in keys : if isinstance ( col , ( ABCIndexClass , ABCSeries , np . ndarray , list , abc . Iterator ) ) : # arrays are fine as long as they are one-dimensional # iterators get converted to list below if getattr ( col , 'ndim' , 1 ) != 1 : raise ValueError ( err_msg ) else : # everything else gets tried as a key; see GH 24969 try : found = col in self . columns except TypeError : raise TypeError ( err_msg + ' Received column of ' 'type {}' . format ( type ( col ) ) ) else : if not found : missing . append ( col ) if missing : raise KeyError ( 'None of {} are in the columns' . format ( missing ) ) if inplace : frame = self else : frame = self . copy ( ) arrays = [ ] names = [ ] if append : names = [ x for x in self . index . names ] if isinstance ( self . index , ABCMultiIndex ) : for i in range ( self . index . nlevels ) : arrays . append ( self . index . _get_level_values ( i ) ) else : arrays . append ( self . index ) to_remove = [ ] for col in keys : if isinstance ( col , ABCMultiIndex ) : for n in range ( col . nlevels ) : arrays . append ( col . _get_level_values ( n ) ) names . extend ( col . names ) elif isinstance ( col , ( ABCIndexClass , ABCSeries ) ) : # if Index then not MultiIndex (treated above) arrays . append ( col ) names . append ( col . name ) elif isinstance ( col , ( list , np . ndarray ) ) : arrays . append ( col ) names . append ( None ) elif isinstance ( col , abc . Iterator ) : arrays . append ( list ( col ) ) names . append ( None ) # from here, col can only be a column label else : arrays . append ( frame [ col ] . _values ) names . append ( col ) if drop : to_remove . append ( col ) if len ( arrays [ - 1 ] ) != len ( self ) : # check newest element against length of calling frame, since # ensure_index_from_sequences would not raise for append=False. raise ValueError ( 'Length mismatch: Expected {len_self} rows, ' 'received array of length {len_col}' . format ( len_self = len ( self ) , len_col = len ( arrays [ - 1 ] ) ) ) index = ensure_index_from_sequences ( arrays , names ) if verify_integrity and not index . is_unique : duplicates = index [ index . duplicated ( ) ] . unique ( ) raise ValueError ( 'Index has duplicate keys: {dup}' . format ( dup = duplicates ) ) # use set to handle duplicate column names gracefully in case of drop for c in set ( to_remove ) : del frame [ c ] # clear up memory usage index . _cleanup ( ) frame . index = index if not inplace : return frame\n\n\n        Summary:", "input": "", "output": "Set the DataFrame index using existing columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reset_index ( self , level = None , drop = False , inplace = False , col_level = 0 , col_fill = '' ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if inplace : new_obj = self else : new_obj = self . copy ( ) def _maybe_casted_values ( index , labels = None ) : values = index . _values if not isinstance ( index , ( PeriodIndex , DatetimeIndex ) ) : if values . dtype == np . object_ : values = lib . maybe_convert_objects ( values ) # if we have the labels, extract the values with a mask if labels is not None : mask = labels == - 1 # we can have situations where the whole mask is -1, # meaning there is nothing found in labels, so make all nan's if mask . all ( ) : values = np . empty ( len ( mask ) ) values . fill ( np . nan ) else : values = values . take ( labels ) # TODO(https://github.com/pandas-dev/pandas/issues/24206) # Push this into maybe_upcast_putmask? # We can't pass EAs there right now. Looks a bit # complicated. # So we unbox the ndarray_values, op, re-box. values_type = type ( values ) values_dtype = values . dtype if issubclass ( values_type , DatetimeLikeArray ) : values = values . _data if mask . any ( ) : values , changed = maybe_upcast_putmask ( values , mask , np . nan ) if issubclass ( values_type , DatetimeLikeArray ) : values = values_type ( values , dtype = values_dtype ) return values new_index = ibase . default_index ( len ( new_obj ) ) if level is not None : if not isinstance ( level , ( tuple , list ) ) : level = [ level ] level = [ self . index . _get_level_number ( lev ) for lev in level ] if len ( level ) < self . index . nlevels : new_index = self . index . droplevel ( level ) if not drop : if isinstance ( self . index , MultiIndex ) : names = [ n if n is not None else ( 'level_%d' % i ) for ( i , n ) in enumerate ( self . index . names ) ] to_insert = lzip ( self . index . levels , self . index . codes ) else : default = 'index' if 'index' not in self else 'level_0' names = ( [ default ] if self . index . name is None else [ self . index . name ] ) to_insert = ( ( self . index , None ) , ) multi_col = isinstance ( self . columns , MultiIndex ) for i , ( lev , lab ) in reversed ( list ( enumerate ( to_insert ) ) ) : if not ( level is None or i in level ) : continue name = names [ i ] if multi_col : col_name = ( list ( name ) if isinstance ( name , tuple ) else [ name ] ) if col_fill is None : if len ( col_name ) not in ( 1 , self . columns . nlevels ) : raise ValueError ( \"col_fill=None is incompatible \" \"with incomplete column name \" \"{}\" . format ( name ) ) col_fill = col_name [ 0 ] lev_num = self . columns . _get_level_number ( col_level ) name_lst = [ col_fill ] * lev_num + col_name missing = self . columns . nlevels - len ( name_lst ) name_lst += [ col_fill ] * missing name = tuple ( name_lst ) # to ndarray and maybe infer different dtype level_values = _maybe_casted_values ( lev , lab ) new_obj . insert ( 0 , name , level_values ) new_obj . index = new_index if not inplace : return new_obj\n\n\n        Summary:", "input": "", "output": "Reset the index or a level of it ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dropna ( self , axis = 0 , how = 'any' , thresh = None , subset = None , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if isinstance ( axis , ( tuple , list ) ) : # GH20987 msg = ( \"supplying multiple axes to axis is deprecated and \" \"will be removed in a future version.\" ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) result = self for ax in axis : result = result . dropna ( how = how , thresh = thresh , subset = subset , axis = ax ) else : axis = self . _get_axis_number ( axis ) agg_axis = 1 - axis agg_obj = self if subset is not None : ax = self . _get_axis ( agg_axis ) indices = ax . get_indexer_for ( subset ) check = indices == - 1 if check . any ( ) : raise KeyError ( list ( np . compress ( check , subset ) ) ) agg_obj = self . take ( indices , axis = agg_axis ) count = agg_obj . count ( axis = agg_axis ) if thresh is not None : mask = count >= thresh elif how == 'any' : mask = count == len ( agg_obj . _get_axis ( agg_axis ) ) elif how == 'all' : mask = count > 0 else : if how is not None : raise ValueError ( 'invalid how option: {h}' . format ( h = how ) ) else : raise TypeError ( 'must specify how or thresh' ) result = self . loc ( axis = axis ) [ mask ] if inplace : self . _update_inplace ( result ) else : return result\n\n\n        Summary:", "input": "", "output": "Remove missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def drop_duplicates ( self , subset = None , keep = 'first' , inplace = False ) : if self . empty : return self . copy ( ) inplace = validate_bool_kwarg ( inplace , 'inplace' ) duplicated = self . duplicated ( subset , keep = keep ) if inplace : inds , = ( - duplicated ) . _ndarray_values . nonzero ( ) new_data = self . _data . take ( inds ) self . _update_inplace ( new_data ) else : return self [ - duplicated ]\n\n\n        Summary:", "input": "", "output": "Return DataFrame with duplicate rows removed optionally only considering certain columns . Indexes including time indexes are ignored ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def duplicated ( self , subset = None , keep = 'first' ) : from pandas . core . sorting import get_group_index from pandas . _libs . hashtable import duplicated_int64 , _SIZE_HINT_LIMIT if self . empty : return Series ( dtype = bool ) def f ( vals ) : labels , shape = algorithms . factorize ( vals , size_hint = min ( len ( self ) , _SIZE_HINT_LIMIT ) ) return labels . astype ( 'i8' , copy = False ) , len ( shape ) if subset is None : subset = self . columns elif ( not np . iterable ( subset ) or isinstance ( subset , str ) or isinstance ( subset , tuple ) and subset in self . columns ) : subset = subset , # Verify all columns in subset exist in the queried dataframe # Otherwise, raise a KeyError, same as if you try to __getitem__ with a # key that doesn't exist. diff = Index ( subset ) . difference ( self . columns ) if not diff . empty : raise KeyError ( diff ) vals = ( col . values for name , col in self . iteritems ( ) if name in subset ) labels , shape = map ( list , zip ( * map ( f , vals ) ) ) ids = get_group_index ( labels , shape , sort = False , xnull = False ) return Series ( duplicated_int64 ( ids , keep ) , index = self . index )\n\n\n        Summary:", "input": "", "output": "Return boolean Series denoting duplicate rows optionally only considering certain columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nlargest ( self , n , columns , keep = 'first' ) : return algorithms . SelectNFrame ( self , n = n , keep = keep , columns = columns ) . nlargest ( )\n\n\n        Summary:", "input": "", "output": "Return the first n rows ordered by columns in descending order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nsmallest ( self , n , columns , keep = 'first' ) : return algorithms . SelectNFrame ( self , n = n , keep = keep , columns = columns ) . nsmallest ( )\n\n\n        Summary:", "input": "", "output": "Return the first n rows ordered by columns in ascending order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def swaplevel ( self , i = - 2 , j = - 1 , axis = 0 ) : result = self . copy ( ) axis = self . _get_axis_number ( axis ) if axis == 0 : result . index = result . index . swaplevel ( i , j ) else : result . columns = result . columns . swaplevel ( i , j ) return result\n\n\n        Summary:", "input": "", "output": "Swap levels i and j in a MultiIndex on a particular axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reorder_levels ( self , order , axis = 0 ) : axis = self . _get_axis_number ( axis ) if not isinstance ( self . _get_axis ( axis ) , MultiIndex ) : # pragma: no cover raise TypeError ( 'Can only reorder levels on a hierarchical axis.' ) result = self . copy ( ) if axis == 0 : result . index = result . index . reorder_levels ( order ) else : result . columns = result . columns . reorder_levels ( order ) return result\n\n\n        Summary:", "input": "", "output": "Rearrange index levels using input order . May not drop or duplicate levels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine ( self , other , func , fill_value = None , overwrite = True ) : other_idxlen = len ( other . index ) # save for compare this , other = self . align ( other , copy = False ) new_index = this . index if other . empty and len ( new_index ) == len ( self . index ) : return self . copy ( ) if self . empty and len ( other ) == other_idxlen : return other . copy ( ) # sorts if possible new_columns = this . columns . union ( other . columns ) do_fill = fill_value is not None result = { } for col in new_columns : series = this [ col ] otherSeries = other [ col ] this_dtype = series . dtype other_dtype = otherSeries . dtype this_mask = isna ( series ) other_mask = isna ( otherSeries ) # don't overwrite columns unecessarily # DO propagate if this column is not in the intersection if not overwrite and other_mask . all ( ) : result [ col ] = this [ col ] . copy ( ) continue if do_fill : series = series . copy ( ) otherSeries = otherSeries . copy ( ) series [ this_mask ] = fill_value otherSeries [ other_mask ] = fill_value if col not in self . columns : # If self DataFrame does not have col in other DataFrame, # try to promote series, which is all NaN, as other_dtype. new_dtype = other_dtype try : series = series . astype ( new_dtype , copy = False ) except ValueError : # e.g. new_dtype is integer types pass else : # if we have different dtypes, possibly promote new_dtype = find_common_type ( [ this_dtype , other_dtype ] ) if not is_dtype_equal ( this_dtype , new_dtype ) : series = series . astype ( new_dtype ) if not is_dtype_equal ( other_dtype , new_dtype ) : otherSeries = otherSeries . astype ( new_dtype ) arr = func ( series , otherSeries ) arr = maybe_downcast_to_dtype ( arr , this_dtype ) result [ col ] = arr # convert_objects just in case return self . _constructor ( result , index = new_index , columns = new_columns )\n\n\n        Summary:", "input": "", "output": "Perform column - wise combine with another DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine_first ( self , other ) : import pandas . core . computation . expressions as expressions def extract_values ( arr ) : # Does two things: # 1. maybe gets the values from the Series / Index # 2. convert datelike to i8 if isinstance ( arr , ( ABCIndexClass , ABCSeries ) ) : arr = arr . _values if needs_i8_conversion ( arr ) : if is_extension_array_dtype ( arr . dtype ) : arr = arr . asi8 else : arr = arr . view ( 'i8' ) return arr def combiner ( x , y ) : mask = isna ( x ) if isinstance ( mask , ( ABCIndexClass , ABCSeries ) ) : mask = mask . _values x_values = extract_values ( x ) y_values = extract_values ( y ) # If the column y in other DataFrame is not in first DataFrame, # just return y_values. if y . name not in self . columns : return y_values return expressions . where ( mask , y_values , x_values ) return self . combine ( other , combiner , overwrite = False )\n\n\n        Summary:", "input": "", "output": "Update null elements with value in the same location in other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update ( self , other , join = 'left' , overwrite = True , filter_func = None , errors = 'ignore' ) : import pandas . core . computation . expressions as expressions # TODO: Support other joins if join != 'left' : # pragma: no cover raise NotImplementedError ( \"Only left join is supported\" ) if errors not in [ 'ignore' , 'raise' ] : raise ValueError ( \"The parameter errors must be either \" \"'ignore' or 'raise'\" ) if not isinstance ( other , DataFrame ) : other = DataFrame ( other ) other = other . reindex_like ( self ) for col in self . columns : this = self [ col ] . _values that = other [ col ] . _values if filter_func is not None : with np . errstate ( all = 'ignore' ) : mask = ~ filter_func ( this ) | isna ( that ) else : if errors == 'raise' : mask_this = notna ( that ) mask_that = notna ( this ) if any ( mask_this & mask_that ) : raise ValueError ( \"Data overlaps.\" ) if overwrite : mask = isna ( that ) else : mask = notna ( this ) # don't overwrite columns unecessarily if mask . all ( ) : continue self [ col ] = expressions . where ( mask , this , that )\n\n\n        Summary:", "input": "", "output": "Modify in place using non - NA values from another DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stack ( self , level = - 1 , dropna = True ) : from pandas . core . reshape . reshape import stack , stack_multiple if isinstance ( level , ( tuple , list ) ) : return stack_multiple ( self , level , dropna = dropna ) else : return stack ( self , level , dropna = dropna )\n\n\n        Summary:", "input": "", "output": "Stack the prescribed level ( s ) from columns to index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unstack ( self , level = - 1 , fill_value = None ) : from pandas . core . reshape . reshape import unstack return unstack ( self , level , fill_value )\n\n\n        Summary:", "input": "", "output": "Pivot a level of the ( necessarily hierarchical ) index labels returning a DataFrame having a new level of column labels whose inner - most level consists of the pivoted index labels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def diff ( self , periods = 1 , axis = 0 ) : bm_axis = self . _get_block_manager_axis ( axis ) new_data = self . _data . diff ( n = periods , axis = bm_axis ) return self . _constructor ( new_data )\n\n\n        Summary:", "input": "", "output": "First discrete difference of element ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gotitem ( self , key : Union [ str , List [ str ] ] , ndim : int , subset : Optional [ Union [ Series , ABCDataFrame ] ] = None , ) -> Union [ Series , ABCDataFrame ] : if subset is None : subset = self elif subset . ndim == 1 : # is Series return subset # TODO: _shallow_copy(subset)? return subset [ key ]\n\n\n        Summary:", "input": "", "output": "Sub - classes to define . Return a sliced object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , func , axis = 0 , broadcast = None , raw = False , reduce = None , result_type = None , args = ( ) , * * kwds ) : from pandas . core . apply import frame_apply op = frame_apply ( self , func = func , axis = axis , broadcast = broadcast , raw = raw , reduce = reduce , result_type = result_type , args = args , kwds = kwds ) return op . get_result ( )\n\n\n        Summary:", "input": "", "output": "Apply a function along an axis of the DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def applymap ( self , func ) : # if we have a dtype == 'M8[ns]', provide boxed values def infer ( x ) : if x . empty : return lib . map_infer ( x , func ) return lib . map_infer ( x . astype ( object ) . values , func ) return self . apply ( infer )\n\n\n        Summary:", "input": "", "output": "Apply a function to a Dataframe elementwise ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def append ( self , other , ignore_index = False , verify_integrity = False , sort = None ) : if isinstance ( other , ( Series , dict ) ) : if isinstance ( other , dict ) : other = Series ( other ) if other . name is None and not ignore_index : raise TypeError ( 'Can only append a Series if ignore_index=True' ' or if the Series has a name' ) if other . name is None : index = None else : # other must have the same index name as self, otherwise # index name will be reset index = Index ( [ other . name ] , name = self . index . name ) idx_diff = other . index . difference ( self . columns ) try : combined_columns = self . columns . append ( idx_diff ) except TypeError : combined_columns = self . columns . astype ( object ) . append ( idx_diff ) other = other . reindex ( combined_columns , copy = False ) other = DataFrame ( other . values . reshape ( ( 1 , len ( other ) ) ) , index = index , columns = combined_columns ) other = other . _convert ( datetime = True , timedelta = True ) if not self . columns . equals ( combined_columns ) : self = self . reindex ( columns = combined_columns ) elif isinstance ( other , list ) and not isinstance ( other [ 0 ] , DataFrame ) : other = DataFrame ( other ) if ( self . columns . get_indexer ( other . columns ) >= 0 ) . all ( ) : other = other . reindex ( columns = self . columns ) from pandas . core . reshape . concat import concat if isinstance ( other , ( list , tuple ) ) : to_concat = [ self ] + other else : to_concat = [ self , other ] return concat ( to_concat , ignore_index = ignore_index , verify_integrity = verify_integrity , sort = sort )\n\n\n        Summary:", "input": "", "output": "Append rows of other to the end of caller returning a new object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def join ( self , other , on = None , how = 'left' , lsuffix = '' , rsuffix = '' , sort = False ) : # For SparseDataFrame's benefit return self . _join_compat ( other , on = on , how = how , lsuffix = lsuffix , rsuffix = rsuffix , sort = sort )\n\n\n        Summary:", "input": "", "output": "Join columns of another DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def round ( self , decimals = 0 , * args , * * kwargs ) : from pandas . core . reshape . concat import concat def _dict_round ( df , decimals ) : for col , vals in df . iteritems ( ) : try : yield _series_round ( vals , decimals [ col ] ) except KeyError : yield vals def _series_round ( s , decimals ) : if is_integer_dtype ( s ) or is_float_dtype ( s ) : return s . round ( decimals ) return s nv . validate_round ( args , kwargs ) if isinstance ( decimals , ( dict , Series ) ) : if isinstance ( decimals , Series ) : if not decimals . index . is_unique : raise ValueError ( \"Index of decimals must be unique\" ) new_cols = [ col for col in _dict_round ( self , decimals ) ] elif is_integer ( decimals ) : # Dispatch to Series.round new_cols = [ _series_round ( v , decimals ) for _ , v in self . iteritems ( ) ] else : raise TypeError ( \"decimals must be an integer, a dict-like or a \" \"Series\" ) if len ( new_cols ) > 0 : return self . _constructor ( concat ( new_cols , axis = 1 ) , index = self . index , columns = self . columns ) else : return self\n\n\n        Summary:", "input": "", "output": "Round a DataFrame to a variable number of decimal places ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corr ( self , method = 'pearson' , min_periods = 1 ) : numeric_df = self . _get_numeric_data ( ) cols = numeric_df . columns idx = cols . copy ( ) mat = numeric_df . values if method == 'pearson' : correl = libalgos . nancorr ( ensure_float64 ( mat ) , minp = min_periods ) elif method == 'spearman' : correl = libalgos . nancorr_spearman ( ensure_float64 ( mat ) , minp = min_periods ) elif method == 'kendall' or callable ( method ) : if min_periods is None : min_periods = 1 mat = ensure_float64 ( mat ) . T corrf = nanops . get_corr_func ( method ) K = len ( cols ) correl = np . empty ( ( K , K ) , dtype = float ) mask = np . isfinite ( mat ) for i , ac in enumerate ( mat ) : for j , bc in enumerate ( mat ) : if i > j : continue valid = mask [ i ] & mask [ j ] if valid . sum ( ) < min_periods : c = np . nan elif i == j : c = 1. elif not valid . all ( ) : c = corrf ( ac [ valid ] , bc [ valid ] ) else : c = corrf ( ac , bc ) correl [ i , j ] = c correl [ j , i ] = c else : raise ValueError ( \"method must be either 'pearson', \" \"'spearman', 'kendall', or a callable, \" \"'{method}' was supplied\" . format ( method = method ) ) return self . _constructor ( correl , index = idx , columns = cols )\n\n\n        Summary:", "input": "", "output": "Compute pairwise correlation of columns excluding NA / null values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cov ( self , min_periods = None ) : numeric_df = self . _get_numeric_data ( ) cols = numeric_df . columns idx = cols . copy ( ) mat = numeric_df . values if notna ( mat ) . all ( ) : if min_periods is not None and min_periods > len ( mat ) : baseCov = np . empty ( ( mat . shape [ 1 ] , mat . shape [ 1 ] ) ) baseCov . fill ( np . nan ) else : baseCov = np . cov ( mat . T ) baseCov = baseCov . reshape ( ( len ( cols ) , len ( cols ) ) ) else : baseCov = libalgos . nancorr ( ensure_float64 ( mat ) , cov = True , minp = min_periods ) return self . _constructor ( baseCov , index = idx , columns = cols )\n\n\n        Summary:", "input": "", "output": "Compute pairwise covariance of columns excluding NA / null values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corrwith ( self , other , axis = 0 , drop = False , method = 'pearson' ) : axis = self . _get_axis_number ( axis ) this = self . _get_numeric_data ( ) if isinstance ( other , Series ) : return this . apply ( lambda x : other . corr ( x , method = method ) , axis = axis ) other = other . _get_numeric_data ( ) left , right = this . align ( other , join = 'inner' , copy = False ) if axis == 1 : left = left . T right = right . T if method == 'pearson' : # mask missing values left = left + right * 0 right = right + left * 0 # demeaned data ldem = left - left . mean ( ) rdem = right - right . mean ( ) num = ( ldem * rdem ) . sum ( ) dom = ( left . count ( ) - 1 ) * left . std ( ) * right . std ( ) correl = num / dom elif method in [ 'kendall' , 'spearman' ] or callable ( method ) : def c ( x ) : return nanops . nancorr ( x [ 0 ] , x [ 1 ] , method = method ) correl = Series ( map ( c , zip ( left . values . T , right . values . T ) ) , index = left . columns ) else : raise ValueError ( \"Invalid method {method} was passed, \" \"valid methods are: 'pearson', 'kendall', \" \"'spearman', or callable\" . format ( method = method ) ) if not drop : # Find non-matching labels along the given axis # and append missing correlations (GH 22375) raxis = 1 if axis == 0 else 0 result_index = ( this . _get_axis ( raxis ) . union ( other . _get_axis ( raxis ) ) ) idx_diff = result_index . difference ( correl . index ) if len ( idx_diff ) > 0 : correl = correl . append ( Series ( [ np . nan ] * len ( idx_diff ) , index = idx_diff ) ) return correl\n\n\n        Summary:", "input": "", "output": "Compute pairwise correlation between rows or columns of DataFrame with rows or columns of Series or DataFrame . DataFrames are first aligned along both axes before computing the correlations ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def count ( self , axis = 0 , level = None , numeric_only = False ) : axis = self . _get_axis_number ( axis ) if level is not None : return self . _count_level ( level , axis = axis , numeric_only = numeric_only ) if numeric_only : frame = self . _get_numeric_data ( ) else : frame = self # GH #423 if len ( frame . _get_axis ( axis ) ) == 0 : result = Series ( 0 , index = frame . _get_agg_axis ( axis ) ) else : if frame . _is_mixed_type or frame . _data . any_extension_types : # the or any_extension_types is really only hit for single- # column frames with an extension array result = notna ( frame ) . sum ( axis = axis ) else : # GH13407 series_counts = notna ( frame ) . sum ( axis = axis ) counts = series_counts . values result = Series ( counts , index = frame . _get_agg_axis ( axis ) ) return result . astype ( 'int64' )\n\n\n        Summary:", "input": "", "output": "Count non - NA cells for each column or row ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nunique ( self , axis = 0 , dropna = True ) : return self . apply ( Series . nunique , axis = axis , dropna = dropna )\n\n\n        Summary:", "input": "", "output": "Count distinct observations over requested axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def idxmin ( self , axis = 0 , skipna = True ) : axis = self . _get_axis_number ( axis ) indices = nanops . nanargmin ( self . values , axis = axis , skipna = skipna ) index = self . _get_axis ( axis ) result = [ index [ i ] if i >= 0 else np . nan for i in indices ] return Series ( result , index = self . _get_agg_axis ( axis ) )\n\n\n        Summary:", "input": "", "output": "Return index of first occurrence of minimum over requested axis . NA / null values are excluded ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_agg_axis ( self , axis_num ) : if axis_num == 0 : return self . columns elif axis_num == 1 : return self . index else : raise ValueError ( 'Axis must be 0 or 1 (got %r)' % axis_num )\n\n\n        Summary:", "input": "", "output": "Let s be explicit about this ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mode ( self , axis = 0 , numeric_only = False , dropna = True ) : data = self if not numeric_only else self . _get_numeric_data ( ) def f ( s ) : return s . mode ( dropna = dropna ) return data . apply ( f , axis = axis )\n\n\n        Summary:", "input": "", "output": "Get the mode ( s ) of each element along the selected axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def quantile ( self , q = 0.5 , axis = 0 , numeric_only = True , interpolation = 'linear' ) : self . _check_percentile ( q ) data = self . _get_numeric_data ( ) if numeric_only else self axis = self . _get_axis_number ( axis ) is_transposed = axis == 1 if is_transposed : data = data . T result = data . _data . quantile ( qs = q , axis = 1 , interpolation = interpolation , transposed = is_transposed ) if result . ndim == 2 : result = self . _constructor ( result ) else : result = self . _constructor_sliced ( result , name = q ) if is_transposed : result = result . T return result\n\n\n        Summary:", "input": "", "output": "Return values at the given quantile over requested axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_timestamp ( self , freq = None , how = 'start' , axis = 0 , copy = True ) : new_data = self . _data if copy : new_data = new_data . copy ( ) axis = self . _get_axis_number ( axis ) if axis == 0 : new_data . set_axis ( 1 , self . index . to_timestamp ( freq = freq , how = how ) ) elif axis == 1 : new_data . set_axis ( 0 , self . columns . to_timestamp ( freq = freq , how = how ) ) else : # pragma: no cover raise AssertionError ( 'Axis must be 0 or 1. Got {ax!s}' . format ( ax = axis ) ) return self . _constructor ( new_data )\n\n\n        Summary:", "input": "", "output": "Cast to DatetimeIndex of timestamps at * beginning * of period ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isin ( self , values ) : if isinstance ( values , dict ) : from pandas . core . reshape . concat import concat values = collections . defaultdict ( list , values ) return concat ( ( self . iloc [ : , [ i ] ] . isin ( values [ col ] ) for i , col in enumerate ( self . columns ) ) , axis = 1 ) elif isinstance ( values , Series ) : if not values . index . is_unique : raise ValueError ( \"cannot compute isin with \" \"a duplicate axis.\" ) return self . eq ( values . reindex_like ( self ) , axis = 'index' ) elif isinstance ( values , DataFrame ) : if not ( values . columns . is_unique and values . index . is_unique ) : raise ValueError ( \"cannot compute isin with \" \"a duplicate axis.\" ) return self . eq ( values . reindex_like ( self ) ) else : if not is_list_like ( values ) : raise TypeError ( \"only list-like or dict-like objects are \" \"allowed to be passed to DataFrame.isin(), \" \"you passed a \" \"{0!r}\" . format ( type ( values ) . __name__ ) ) return DataFrame ( algorithms . isin ( self . values . ravel ( ) , values ) . reshape ( self . shape ) , self . index , self . columns )\n\n\n        Summary:", "input": "", "output": "Whether each element in the DataFrame is contained in values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def integer_array ( values , dtype = None , copy = False ) : values , mask = coerce_to_array ( values , dtype = dtype , copy = copy ) return IntegerArray ( values , mask )\n\n\n        Summary:", "input": "", "output": "Infer and return an integer array of the values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def safe_cast ( values , dtype , copy ) : try : return values . astype ( dtype , casting = 'safe' , copy = copy ) except TypeError : casted = values . astype ( dtype , copy = copy ) if ( casted == values ) . all ( ) : return casted raise TypeError ( \"cannot safely cast non-equivalent {} to {}\" . format ( values . dtype , np . dtype ( dtype ) ) )\n\n\n        Summary:", "input": "", "output": "Safely cast the values to the dtype if they are equivalent meaning floats must be equivalent to the ints ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def coerce_to_array ( values , dtype , mask = None , copy = False ) : # if values is integer numpy array, preserve it's dtype if dtype is None and hasattr ( values , 'dtype' ) : if is_integer_dtype ( values . dtype ) : dtype = values . dtype if dtype is not None : if ( isinstance ( dtype , str ) and ( dtype . startswith ( \"Int\" ) or dtype . startswith ( \"UInt\" ) ) ) : # Avoid DeprecationWarning from NumPy about np.dtype(\"Int64\") # https://github.com/numpy/numpy/pull/7476 dtype = dtype . lower ( ) if not issubclass ( type ( dtype ) , _IntegerDtype ) : try : dtype = _dtypes [ str ( np . dtype ( dtype ) ) ] except KeyError : raise ValueError ( \"invalid dtype specified {}\" . format ( dtype ) ) if isinstance ( values , IntegerArray ) : values , mask = values . _data , values . _mask if dtype is not None : values = values . astype ( dtype . numpy_dtype , copy = False ) if copy : values = values . copy ( ) mask = mask . copy ( ) return values , mask values = np . array ( values , copy = copy ) if is_object_dtype ( values ) : inferred_type = lib . infer_dtype ( values , skipna = True ) if inferred_type == 'empty' : values = np . empty ( len ( values ) ) values . fill ( np . nan ) elif inferred_type not in [ 'floating' , 'integer' , 'mixed-integer' , 'mixed-integer-float' ] : raise TypeError ( \"{} cannot be converted to an IntegerDtype\" . format ( values . dtype ) ) elif not ( is_integer_dtype ( values ) or is_float_dtype ( values ) ) : raise TypeError ( \"{} cannot be converted to an IntegerDtype\" . format ( values . dtype ) ) if mask is None : mask = isna ( values ) else : assert len ( mask ) == len ( values ) if not values . ndim == 1 : raise TypeError ( \"values must be a 1D list-like\" ) if not mask . ndim == 1 : raise TypeError ( \"mask must be a 1D list-like\" ) # infer dtype if needed if dtype is None : dtype = np . dtype ( 'int64' ) else : dtype = dtype . type # if we are float, let's make sure that we can # safely cast # we copy as need to coerce here if mask . any ( ) : values = values . copy ( ) values [ mask ] = 1 values = safe_cast ( values , dtype , copy = False ) else : values = safe_cast ( values , dtype , copy = False ) return values , mask\n\n\n        Summary:", "input": "", "output": "Coerce the input values array to numpy arrays with a mask"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def construct_from_string ( cls , string ) : if string == cls . name : return cls ( ) raise TypeError ( \"Cannot construct a '{}' from \" \"'{}'\" . format ( cls , string ) )\n\n\n        Summary:", "input": "", "output": "Construction from a string raise a TypeError if not possible"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _coerce_to_ndarray ( self ) : # TODO(jreback) make this better data = self . _data . astype ( object ) data [ self . _mask ] = self . _na_value return data\n\n\n        Summary:", "input": "", "output": "coerce to an ndarary of object dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def astype ( self , dtype , copy = True ) : # if we are astyping to an existing IntegerDtype we can fastpath if isinstance ( dtype , _IntegerDtype ) : result = self . _data . astype ( dtype . numpy_dtype , copy = False ) return type ( self ) ( result , mask = self . _mask , copy = False ) # coerce data = self . _coerce_to_ndarray ( ) return astype_nansafe ( data , dtype , copy = None )\n\n\n        Summary:", "input": "", "output": "Cast to a NumPy array or IntegerArray with dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def value_counts ( self , dropna = True ) : from pandas import Index , Series # compute counts on the data with no nans data = self . _data [ ~ self . _mask ] value_counts = Index ( data ) . value_counts ( ) array = value_counts . values # TODO(extension) # if we have allow Index to hold an ExtensionArray # this is easier index = value_counts . index . astype ( object ) # if we want nans, count the mask if not dropna : # TODO(extension) # appending to an Index *always* infers # w/o passing the dtype array = np . append ( array , [ self . _mask . sum ( ) ] ) index = Index ( np . concatenate ( [ index . values , np . array ( [ np . nan ] , dtype = object ) ] ) , dtype = object ) return Series ( array , index = index )\n\n\n        Summary:", "input": "", "output": "Returns a Series containing counts of each category ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _values_for_argsort ( self ) -> np . ndarray : data = self . _data . copy ( ) data [ self . _mask ] = data . min ( ) - 1 return data\n\n\n        Summary:", "input": "", "output": "Return values for sorting ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_mask_result ( self , result , mask , other , op_name ) : # may need to fill infs # and mask wraparound if is_float_dtype ( result ) : mask |= ( result == np . inf ) | ( result == - np . inf ) # if we have a float operand we are by-definition # a float result # or our op is a divide if ( ( is_float_dtype ( other ) or is_float ( other ) ) or ( op_name in [ 'rtruediv' , 'truediv' , 'rdiv' , 'div' ] ) ) : result [ mask ] = np . nan return result return type ( self ) ( result , mask , copy = False )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- result : array - like mask : array - like bool other : scalar or array - like op_name : str"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def length_of_indexer ( indexer , target = None ) : if target is not None and isinstance ( indexer , slice ) : target_len = len ( target ) start = indexer . start stop = indexer . stop step = indexer . step if start is None : start = 0 elif start < 0 : start += target_len if stop is None or stop > target_len : stop = target_len elif stop < 0 : stop += target_len if step is None : step = 1 elif step < 0 : step = - step return ( stop - start + step - 1 ) // step elif isinstance ( indexer , ( ABCSeries , Index , np . ndarray , list ) ) : return len ( indexer ) elif not is_list_like_indexer ( indexer ) : return 1 raise AssertionError ( \"cannot find the length of the indexer\" )\n\n\n        Summary:", "input": "", "output": "return the length of a single non - tuple indexer which could be a slice"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_to_index_sliceable ( obj , key ) : idx = obj . index if isinstance ( key , slice ) : return idx . _convert_slice_indexer ( key , kind = 'getitem' ) elif isinstance ( key , str ) : # we are an actual column if obj . _data . items . contains ( key ) : return None # We might have a datetimelike string that we can translate to a # slice here via partial string indexing if idx . is_all_dates : try : return idx . _get_string_slice ( key ) except ( KeyError , ValueError , NotImplementedError ) : return None return None\n\n\n        Summary:", "input": "", "output": "if we are index sliceable then return my slicer otherwise return None"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def check_setitem_lengths ( indexer , value , values ) : # boolean with truth values == len of the value is ok too if isinstance ( indexer , ( np . ndarray , list ) ) : if is_list_like ( value ) and len ( indexer ) != len ( value ) : if not ( isinstance ( indexer , np . ndarray ) and indexer . dtype == np . bool_ and len ( indexer [ indexer ] ) == len ( value ) ) : raise ValueError ( \"cannot set using a list-like indexer \" \"with a different length than the value\" ) # slice elif isinstance ( indexer , slice ) : if is_list_like ( value ) and len ( values ) : if len ( value ) != length_of_indexer ( indexer , values ) : raise ValueError ( \"cannot set using a slice indexer with a \" \"different length than the value\" )\n\n\n        Summary:", "input": "", "output": "Validate that value and indexer are the same length ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_missing_indexer ( indexer ) : if isinstance ( indexer , dict ) : # a missing key (but not a tuple indexer) indexer = indexer [ 'key' ] if isinstance ( indexer , bool ) : raise KeyError ( \"cannot use a single bool to index into setitem\" ) return indexer , True return indexer , False\n\n\n        Summary:", "input": "", "output": "reverse convert a missing indexer which is a dict return the scalar indexer and a boolean indicating if we converted"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_from_missing_indexer_tuple ( indexer , axes ) : def get_indexer ( _i , _idx ) : return ( axes [ _i ] . get_loc ( _idx [ 'key' ] ) if isinstance ( _idx , dict ) else _idx ) return tuple ( get_indexer ( _i , _idx ) for _i , _idx in enumerate ( indexer ) )\n\n\n        Summary:", "input": "", "output": "create a filtered indexer that doesn t have any missing indexers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_convert_indices ( indices , n ) : if isinstance ( indices , list ) : indices = np . array ( indices ) if len ( indices ) == 0 : # If list is empty, np.array will return float and cause indexing # errors. return np . empty ( 0 , dtype = np . intp ) mask = indices < 0 if mask . any ( ) : indices = indices . copy ( ) indices [ mask ] += n mask = ( indices >= n ) | ( indices < 0 ) if mask . any ( ) : raise IndexError ( \"indices are out-of-bounds\" ) return indices\n\n\n        Summary:", "input": "", "output": "Attempt to convert indices into valid positive indices ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_indices ( indices , n ) : if len ( indices ) : min_idx = indices . min ( ) if min_idx < - 1 : msg = ( \"'indices' contains values less than allowed ({} < {})\" . format ( min_idx , - 1 ) ) raise ValueError ( msg ) max_idx = indices . max ( ) if max_idx >= n : raise IndexError ( \"indices are out-of-bounds\" )\n\n\n        Summary:", "input": "", "output": "Perform bounds - checking for an indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_convert_ix ( * args ) : ixify = True for arg in args : if not isinstance ( arg , ( np . ndarray , list , ABCSeries , Index ) ) : ixify = False if ixify : return np . ix_ ( * args ) else : return args\n\n\n        Summary:", "input": "", "output": "We likely want to take the cross - product"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _non_reducing_slice ( slice_ ) : # default to column slice, like DataFrame # ['A', 'B'] -> IndexSlices[:, ['A', 'B']] kinds = ( ABCSeries , np . ndarray , Index , list , str ) if isinstance ( slice_ , kinds ) : slice_ = IndexSlice [ : , slice_ ] def pred ( part ) : # true when slice does *not* reduce, False when part is a tuple, # i.e. MultiIndex slice return ( ( isinstance ( part , slice ) or is_list_like ( part ) ) and not isinstance ( part , tuple ) ) if not is_list_like ( slice_ ) : if not isinstance ( slice_ , slice ) : # a 1-d slice, like df.loc[1] slice_ = [ [ slice_ ] ] else : # slice(a, b, c) slice_ = [ slice_ ] # to tuplize later else : slice_ = [ part if pred ( part ) else [ part ] for part in slice_ ] return tuple ( slice_ )\n\n\n        Summary:", "input": "", "output": "Ensurse that a slice doesn t reduce to a Series or Scalar ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_numeric_slice ( df , slice_ , include_bool = False ) : if slice_ is None : dtypes = [ np . number ] if include_bool : dtypes . append ( bool ) slice_ = IndexSlice [ : , df . select_dtypes ( include = dtypes ) . columns ] return slice_\n\n\n        Summary:", "input": "", "output": "want nice defaults for background_gradient that don t break with non - numeric data . But if slice_ is passed go with that ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _has_valid_tuple ( self , key ) : for i , k in enumerate ( key ) : if i >= self . obj . ndim : raise IndexingError ( 'Too many indexers' ) try : self . _validate_key ( k , i ) except ValueError : raise ValueError ( \"Location based indexing can only have \" \"[{types}] types\" . format ( types = self . _valid_types ) )\n\n\n        Summary:", "input": "", "output": "check the key for valid keys across my indexer"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _has_valid_positional_setitem_indexer ( self , indexer ) : if isinstance ( indexer , dict ) : raise IndexError ( \"{0} cannot enlarge its target object\" . format ( self . name ) ) else : if not isinstance ( indexer , tuple ) : indexer = self . _tuplify ( indexer ) for ax , i in zip ( self . obj . axes , indexer ) : if isinstance ( i , slice ) : # should check the stop slice? pass elif is_list_like_indexer ( i ) : # should check the elements? pass elif is_integer ( i ) : if i >= len ( ax ) : raise IndexError ( \"{name} cannot enlarge its target \" \"object\" . format ( name = self . name ) ) elif isinstance ( i , dict ) : raise IndexError ( \"{name} cannot enlarge its target object\" . format ( name = self . name ) ) return True\n\n\n        Summary:", "input": "", "output": "validate that an positional indexer cannot enlarge its target will raise if needed does not modify the indexer externally"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _align_series ( self , indexer , ser , multiindex_indexer = False ) : if isinstance ( indexer , ( slice , np . ndarray , list , Index ) ) : indexer = tuple ( [ indexer ] ) if isinstance ( indexer , tuple ) : # flatten np.ndarray indexers def ravel ( i ) : return i . ravel ( ) if isinstance ( i , np . ndarray ) else i indexer = tuple ( map ( ravel , indexer ) ) aligners = [ not com . is_null_slice ( idx ) for idx in indexer ] sum_aligners = sum ( aligners ) single_aligner = sum_aligners == 1 is_frame = self . obj . ndim == 2 is_panel = self . obj . ndim >= 3 obj = self . obj # are we a single alignable value on a non-primary # dim (e.g. panel: 1,2, or frame: 0) ? # hence need to align to a single axis dimension # rather that find all valid dims # frame if is_frame : single_aligner = single_aligner and aligners [ 0 ] # panel elif is_panel : single_aligner = ( single_aligner and ( aligners [ 1 ] or aligners [ 2 ] ) ) # we have a frame, with multiple indexers on both axes; and a # series, so need to broadcast (see GH5206) if ( sum_aligners == self . ndim and all ( is_sequence ( _ ) for _ in indexer ) ) : ser = ser . reindex ( obj . axes [ 0 ] [ indexer [ 0 ] ] , copy = True ) . _values # single indexer if len ( indexer ) > 1 and not multiindex_indexer : len_indexer = len ( indexer [ 1 ] ) ser = np . tile ( ser , len_indexer ) . reshape ( len_indexer , - 1 ) . T return ser for i , idx in enumerate ( indexer ) : ax = obj . axes [ i ] # multiple aligners (or null slices) if is_sequence ( idx ) or isinstance ( idx , slice ) : if single_aligner and com . is_null_slice ( idx ) : continue new_ix = ax [ idx ] if not is_list_like_indexer ( new_ix ) : new_ix = Index ( [ new_ix ] ) else : new_ix = Index ( new_ix ) if ser . index . equals ( new_ix ) or not len ( new_ix ) : return ser . _values . copy ( ) return ser . reindex ( new_ix ) . _values # 2 dims elif single_aligner and is_frame : # reindex along index ax = self . obj . axes [ 1 ] if ser . index . equals ( ax ) or not len ( ax ) : return ser . _values . copy ( ) return ser . reindex ( ax ) . _values # >2 dims elif single_aligner : broadcast = [ ] for n , labels in enumerate ( self . obj . _get_plane_axes ( i ) ) : # reindex along the matching dimensions if len ( labels & ser . index ) : ser = ser . reindex ( labels ) else : broadcast . append ( ( n , len ( labels ) ) ) # broadcast along other dims ser = ser . _values . copy ( ) for ( axis , l ) in broadcast : shape = [ - 1 ] * ( len ( broadcast ) + 1 ) shape [ axis ] = l ser = np . tile ( ser , l ) . reshape ( shape ) if self . obj . ndim == 3 : ser = ser . T return ser elif is_scalar ( indexer ) : ax = self . obj . _get_axis ( 1 ) if ser . index . equals ( ax ) : return ser . _values . copy ( ) return ser . reindex ( ax ) . _values raise ValueError ( 'Incompatible indexer with Series' )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- indexer : tuple slice scalar The indexer used to get the locations that will be set to ser"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _multi_take_opportunity ( self , tup ) : if not all ( is_list_like_indexer ( x ) for x in tup ) : return False # just too complicated if any ( com . is_bool_indexer ( x ) for x in tup ) : return False return True\n\n\n        Summary:", "input": "", "output": "Check whether there is the possibility to use _multi_take . Currently the limit is that all axes being indexed must be indexed with list - likes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _multi_take ( self , tup ) : # GH 836 o = self . obj d = { axis : self . _get_listlike_indexer ( key , axis ) for ( key , axis ) in zip ( tup , o . _AXIS_ORDERS ) } return o . _reindex_with_indexers ( d , copy = True , allow_dups = True )\n\n\n        Summary:", "input": "", "output": "Create the indexers for the passed tuple of keys and execute the take operation . This allows the take operation to be executed all at once - rather than once for each dimension - improving efficiency ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_listlike_indexer ( self , key , axis , raise_missing = False ) : o = self . obj ax = o . _get_axis ( axis ) # Have the index compute an indexer or return None # if it cannot handle: indexer , keyarr = ax . _convert_listlike_indexer ( key , kind = self . name ) # We only act on all found values: if indexer is not None and ( indexer != - 1 ) . all ( ) : self . _validate_read_indexer ( key , indexer , axis , raise_missing = raise_missing ) return ax [ indexer ] , indexer if ax . is_unique : # If we are trying to get actual keys from empty Series, we # patiently wait for a KeyError later on - otherwise, convert if len ( ax ) or not len ( key ) : key = self . _convert_for_reindex ( key , axis ) indexer = ax . get_indexer_for ( key ) keyarr = ax . reindex ( keyarr ) [ 0 ] else : keyarr , indexer , new_indexer = ax . _reindex_non_unique ( keyarr ) self . _validate_read_indexer ( keyarr , indexer , o . _get_axis_number ( axis ) , raise_missing = raise_missing ) return keyarr , indexer\n\n\n        Summary:", "input": "", "output": "Transform a list - like of keys into a new index and an indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _getitem_iterable ( self , key , axis = None ) : if axis is None : axis = self . axis or 0 self . _validate_key ( key , axis ) labels = self . obj . _get_axis ( axis ) if com . is_bool_indexer ( key ) : # A boolean indexer key = check_bool_indexer ( labels , key ) inds , = key . nonzero ( ) return self . obj . _take ( inds , axis = axis ) else : # A collection of keys keyarr , indexer = self . _get_listlike_indexer ( key , axis , raise_missing = False ) return self . obj . _reindex_with_indexers ( { axis : [ keyarr , indexer ] } , copy = True , allow_dups = True )\n\n\n        Summary:", "input": "", "output": "Index current object with an an iterable key ( which can be a boolean indexer or a collection of keys ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_read_indexer ( self , key , indexer , axis , raise_missing = False ) : ax = self . obj . _get_axis ( axis ) if len ( key ) == 0 : return # Count missing values: missing = ( indexer < 0 ) . sum ( ) if missing : if missing == len ( indexer ) : raise KeyError ( \"None of [{key}] are in the [{axis}]\" . format ( key = key , axis = self . obj . _get_axis_name ( axis ) ) ) # We (temporarily) allow for some missing keys with .loc, except in # some cases (e.g. setting) in which \"raise_missing\" will be False if not ( self . name == 'loc' and not raise_missing ) : not_found = list ( set ( key ) - set ( ax ) ) raise KeyError ( \"{} not in index\" . format ( not_found ) ) # we skip the warning on Categorical/Interval # as this check is actually done (check for # non-missing values), but a bit later in the # code, so we want to avoid warning & then # just raising _missing_key_warning = textwrap . dedent ( \"\"\" Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\"\"\" ) # noqa if not ( ax . is_categorical ( ) or ax . is_interval ( ) ) : warnings . warn ( _missing_key_warning , FutureWarning , stacklevel = 6 )\n\n\n        Summary:", "input": "", "output": "Check that indexer can be used to return a result ( e . g . at least one element was found unless the list of keys was actually empty ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_to_indexer ( self , obj , axis = None , is_setter = False , raise_missing = False ) : if axis is None : axis = self . axis or 0 labels = self . obj . _get_axis ( axis ) if isinstance ( obj , slice ) : return self . _convert_slice_indexer ( obj , axis ) # try to find out correct indexer, if not type correct raise try : obj = self . _convert_scalar_indexer ( obj , axis ) except TypeError : # but we will allow setting if is_setter : pass # see if we are positional in nature is_int_index = labels . is_integer ( ) is_int_positional = is_integer ( obj ) and not is_int_index # if we are a label return me try : return labels . get_loc ( obj ) except LookupError : if isinstance ( obj , tuple ) and isinstance ( labels , MultiIndex ) : if is_setter and len ( obj ) == labels . nlevels : return { 'key' : obj } raise except TypeError : pass except ( ValueError ) : if not is_int_positional : raise # a positional if is_int_positional : # if we are setting and its not a valid location # its an insert which fails by definition if is_setter : # always valid if self . name == 'loc' : return { 'key' : obj } # a positional if ( obj >= self . obj . shape [ axis ] and not isinstance ( labels , MultiIndex ) ) : raise ValueError ( \"cannot set by positional indexing with \" \"enlargement\" ) return obj if is_nested_tuple ( obj , labels ) : return labels . get_locs ( obj ) elif is_list_like_indexer ( obj ) : if com . is_bool_indexer ( obj ) : obj = check_bool_indexer ( labels , obj ) inds , = obj . nonzero ( ) return inds else : # When setting, missing keys are not allowed, even with .loc: kwargs = { 'raise_missing' : True if is_setter else raise_missing } return self . _get_listlike_indexer ( obj , axis , * * kwargs ) [ 1 ] else : try : return labels . get_loc ( obj ) except LookupError : # allow a not found key only if we are a setter if not is_list_like_indexer ( obj ) and is_setter : return { 'key' : obj } raise\n\n\n        Summary:", "input": "", "output": "Convert indexing key into something we can use to do actual fancy indexing on an ndarray"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_for_reindex ( self , key , axis = None ) : if axis is None : axis = self . axis or 0 labels = self . obj . _get_axis ( axis ) if com . is_bool_indexer ( key ) : key = check_bool_indexer ( labels , key ) return labels [ key ] if isinstance ( key , Index ) : keyarr = labels . _convert_index_indexer ( key ) else : # asarray can be unsafe, NumPy strings are weird keyarr = com . asarray_tuplesafe ( key ) if is_integer_dtype ( keyarr ) : # Cast the indexer to uint64 if possible so # that the values returned from indexing are # also uint64. keyarr = labels . _convert_arr_indexer ( keyarr ) if not labels . is_integer ( ) : keyarr = ensure_platform_int ( keyarr ) return labels . take ( keyarr ) return keyarr\n\n\n        Summary:", "input": "", "output": "Transform a list of keys into a new array ready to be used as axis of the object we return ( e . g . including NaNs ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_slice_axis ( self , slice_obj , axis = None ) : if axis is None : axis = self . axis or 0 obj = self . obj if not need_slice ( slice_obj ) : return obj . copy ( deep = False ) labels = obj . _get_axis ( axis ) indexer = labels . slice_indexer ( slice_obj . start , slice_obj . stop , slice_obj . step , kind = self . name ) if isinstance ( indexer , slice ) : return self . _slice ( indexer , axis = axis , kind = 'iloc' ) else : return self . obj . _take ( indexer , axis = axis )\n\n\n        Summary:", "input": "", "output": "this is pretty simple as we just have to deal with labels"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_partial_string_timestamp_match_key ( self , key , labels ) : if isinstance ( labels , MultiIndex ) : if ( isinstance ( key , str ) and labels . levels [ 0 ] . is_all_dates ) : # Convert key '2016-01-01' to # ('2016-01-01'[, slice(None, None, None)]+) key = tuple ( [ key ] + [ slice ( None ) ] * ( len ( labels . levels ) - 1 ) ) if isinstance ( key , tuple ) : # Convert (..., '2016-01-01', ...) in tuple to # (..., slice('2016-01-01', '2016-01-01', None), ...) new_key = [ ] for i , component in enumerate ( key ) : if ( isinstance ( component , str ) and labels . levels [ i ] . is_all_dates ) : new_key . append ( slice ( component , component , None ) ) else : new_key . append ( component ) key = tuple ( new_key ) return key\n\n\n        Summary:", "input": "", "output": "Translate any partial string timestamp matches in key returning the new key ( GH 10331 )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_integer ( self , key , axis ) : len_axis = len ( self . obj . _get_axis ( axis ) ) if key >= len_axis or key < - len_axis : raise IndexError ( \"single positional indexer is out-of-bounds\" )\n\n\n        Summary:", "input": "", "output": "Check that key is a valid position in the desired axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_list_axis ( self , key , axis = None ) : if axis is None : axis = self . axis or 0 try : return self . obj . _take ( key , axis = axis ) except IndexError : # re-raise with different error message raise IndexError ( \"positional indexers are out-of-bounds\" )\n\n\n        Summary:", "input": "", "output": "Return Series values by list or array of integers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_to_indexer ( self , obj , axis = None , is_setter = False ) : if axis is None : axis = self . axis or 0 # make need to convert a float key if isinstance ( obj , slice ) : return self . _convert_slice_indexer ( obj , axis ) elif is_float ( obj ) : return self . _convert_scalar_indexer ( obj , axis ) try : self . _validate_key ( obj , axis ) return obj except ValueError : raise ValueError ( \"Can only index by location with \" \"a [{types}]\" . format ( types = self . _valid_types ) )\n\n\n        Summary:", "input": "", "output": "much simpler as we only have to deal with our valid types"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_key ( self , key , is_setter = False ) : # allow arbitrary setting if is_setter : return list ( key ) for ax , i in zip ( self . obj . axes , key ) : if ax . is_integer ( ) : if not is_integer ( i ) : raise ValueError ( \"At based indexing on an integer index \" \"can only have integer indexers\" ) else : if is_integer ( i ) and not ax . holds_integer ( ) : raise ValueError ( \"At based indexing on an non-integer \" \"index can only have non-integer \" \"indexers\" ) return key\n\n\n        Summary:", "input": "", "output": "require they keys to be the same type as the index ( so we don t fallback )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_key ( self , key , is_setter = False ) : for a , i in zip ( self . obj . axes , key ) : if not is_integer ( i ) : raise ValueError ( \"iAt based indexing can only have integer \" \"indexers\" ) return key\n\n\n        Summary:", "input": "", "output": "require integer args ( and convert to label arguments )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_manager ( sdf , columns , index ) : # from BlockManager perspective axes = [ ensure_index ( columns ) , ensure_index ( index ) ] return create_block_manager_from_arrays ( [ sdf [ c ] for c in columns ] , columns , axes )\n\n\n        Summary:", "input": "", "output": "create and return the block manager from a dataframe of series columns index"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def stack_sparse_frame ( frame ) : lengths = [ s . sp_index . npoints for _ , s in frame . items ( ) ] nobs = sum ( lengths ) # this is pretty fast minor_codes = np . repeat ( np . arange ( len ( frame . columns ) ) , lengths ) inds_to_concat = [ ] vals_to_concat = [ ] # TODO: Figure out whether this can be reached. # I think this currently can't be reached because you can't build a # SparseDataFrame with a non-np.NaN fill value (fails earlier). for _ , series in frame . items ( ) : if not np . isnan ( series . fill_value ) : raise TypeError ( 'This routine assumes NaN fill value' ) int_index = series . sp_index . to_int_index ( ) inds_to_concat . append ( int_index . indices ) vals_to_concat . append ( series . sp_values ) major_codes = np . concatenate ( inds_to_concat ) stacked_values = np . concatenate ( vals_to_concat ) index = MultiIndex ( levels = [ frame . index , frame . columns ] , codes = [ major_codes , minor_codes ] , verify_integrity = False ) lp = DataFrame ( stacked_values . reshape ( ( nobs , 1 ) ) , index = index , columns = [ 'foo' ] ) return lp . sort_index ( level = 0 )\n\n\n        Summary:", "input": "", "output": "Only makes sense when fill_value is NaN"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def homogenize ( series_dict ) : index = None need_reindex = False for _ , series in series_dict . items ( ) : if not np . isnan ( series . fill_value ) : raise TypeError ( 'this method is only valid with NaN fill values' ) if index is None : index = series . sp_index elif not series . sp_index . equals ( index ) : need_reindex = True index = index . intersect ( series . sp_index ) if need_reindex : output = { } for name , series in series_dict . items ( ) : if not series . sp_index . equals ( index ) : series = series . sparse_reindex ( index ) output [ name ] = series else : output = series_dict return output\n\n\n        Summary:", "input": "", "output": "Conform a set of SparseSeries ( with NaN fill_value ) to a common SparseIndex corresponding to the locations where they all have data"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _init_matrix ( self , data , index , columns , dtype = None ) : data = prep_ndarray ( data , copy = False ) index , columns = self . _prep_index ( data , index , columns ) data = { idx : data [ : , i ] for i , idx in enumerate ( columns ) } return self . _init_dict ( data , index , columns , dtype )\n\n\n        Summary:", "input": "", "output": "Init self from ndarray or list of lists ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _init_spmatrix ( self , data , index , columns , dtype = None , fill_value = None ) : index , columns = self . _prep_index ( data , index , columns ) data = data . tocoo ( ) N = len ( index ) # Construct a dict of SparseSeries sdict = { } values = Series ( data . data , index = data . row , copy = False ) for col , rowvals in values . groupby ( data . col ) : # get_blocks expects int32 row indices in sorted order rowvals = rowvals . sort_index ( ) rows = rowvals . index . values . astype ( np . int32 ) blocs , blens = get_blocks ( rows ) sdict [ columns [ col ] ] = SparseSeries ( rowvals . values , index = index , fill_value = fill_value , sparse_index = BlockIndex ( N , blocs , blens ) ) # Add any columns that were empty and thus not grouped on above sdict . update ( { column : SparseSeries ( index = index , fill_value = fill_value , sparse_index = BlockIndex ( N , [ ] , [ ] ) ) for column in columns if column not in sdict } ) return self . _init_dict ( sdict , index , columns , dtype )\n\n\n        Summary:", "input": "", "output": "Init self from scipy . sparse matrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_coo ( self ) : try : from scipy . sparse import coo_matrix except ImportError : raise ImportError ( 'Scipy is not installed' ) dtype = find_common_type ( self . dtypes ) if isinstance ( dtype , SparseDtype ) : dtype = dtype . subtype cols , rows , datas = [ ] , [ ] , [ ] for col , name in enumerate ( self ) : s = self [ name ] row = s . sp_index . to_int_index ( ) . indices cols . append ( np . repeat ( col , len ( row ) ) ) rows . append ( row ) datas . append ( s . sp_values . astype ( dtype , copy = False ) ) cols = np . concatenate ( cols ) rows = np . concatenate ( rows ) datas = np . concatenate ( datas ) return coo_matrix ( ( datas , ( rows , cols ) ) , shape = self . shape )\n\n\n        Summary:", "input": "", "output": "Return the contents of the frame as a sparse SciPy COO matrix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unpickle_sparse_frame_compat ( self , state ) : series , cols , idx , fv , kind = state if not isinstance ( cols , Index ) : # pragma: no cover from pandas . io . pickle import _unpickle_array columns = _unpickle_array ( cols ) else : columns = cols if not isinstance ( idx , Index ) : # pragma: no cover from pandas . io . pickle import _unpickle_array index = _unpickle_array ( idx ) else : index = idx series_dict = DataFrame ( ) for col , ( sp_index , sp_values ) in series . items ( ) : series_dict [ col ] = SparseSeries ( sp_values , sparse_index = sp_index , fill_value = fv ) self . _data = to_manager ( series_dict , columns , index ) self . _default_fill_value = fv self . _default_kind = kind\n\n\n        Summary:", "input": "", "output": "Original pickle format"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_dense ( self ) : data = { k : v . to_dense ( ) for k , v in self . items ( ) } return DataFrame ( data , index = self . index , columns = self . columns )\n\n\n        Summary:", "input": "", "output": "Convert to dense DataFrame"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_columns ( self , func ) : new_data = { col : func ( series ) for col , series in self . items ( ) } return self . _constructor ( data = new_data , index = self . index , columns = self . columns , default_fill_value = self . default_fill_value ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Get new SparseDataFrame applying func to each columns"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , deep = True ) : result = super ( ) . copy ( deep = deep ) result . _default_fill_value = self . _default_fill_value result . _default_kind = self . _default_kind return result\n\n\n        Summary:", "input": "", "output": "Make a copy of this SparseDataFrame"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def density ( self ) : tot_nonsparse = sum ( ser . sp_index . npoints for _ , ser in self . items ( ) ) tot = len ( self . index ) * len ( self . columns ) return tot_nonsparse / float ( tot )\n\n\n        Summary:", "input": "", "output": "Ratio of non - sparse points to total ( dense ) data points represented in the frame"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sanitize_column ( self , key , value , * * kwargs ) : def sp_maker ( x , index = None ) : return SparseArray ( x , index = index , fill_value = self . _default_fill_value , kind = self . _default_kind ) if isinstance ( value , SparseSeries ) : clean = value . reindex ( self . index ) . as_sparse_array ( fill_value = self . _default_fill_value , kind = self . _default_kind ) elif isinstance ( value , SparseArray ) : if len ( value ) != len ( self . index ) : raise ValueError ( 'Length of values does not match ' 'length of index' ) clean = value elif hasattr ( value , '__iter__' ) : if isinstance ( value , Series ) : clean = value . reindex ( self . index ) if not isinstance ( value , SparseSeries ) : clean = sp_maker ( clean ) else : if len ( value ) != len ( self . index ) : raise ValueError ( 'Length of values does not match ' 'length of index' ) clean = sp_maker ( value ) # Scalar else : clean = sp_maker ( value , self . index ) # always return a SparseArray! return clean\n\n\n        Summary:", "input": "", "output": "Creates a new SparseArray from the input value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def xs ( self , key , axis = 0 , copy = False ) : if axis == 1 : data = self [ key ] return data i = self . index . get_loc ( key ) data = self . take ( [ i ] ) . get_values ( ) [ 0 ] return Series ( data , index = self . columns )\n\n\n        Summary:", "input": "", "output": "Returns a row ( cross - section ) from the SparseDataFrame as a Series object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transpose ( self , * args , * * kwargs ) : nv . validate_transpose ( args , kwargs ) return self . _constructor ( self . values . T , index = self . columns , columns = self . index , default_fill_value = self . _default_fill_value , default_kind = self . _default_kind ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Returns a DataFrame with the rows / columns switched ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cumsum ( self , axis = 0 , * args , * * kwargs ) : nv . validate_cumsum ( args , kwargs ) if axis is None : axis = self . _stat_axis_number return self . apply ( lambda x : x . cumsum ( ) , axis = axis )\n\n\n        Summary:", "input": "", "output": "Return SparseDataFrame of cumulative sums over requested axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , func , axis = 0 , broadcast = None , reduce = None , result_type = None ) : if not len ( self . columns ) : return self axis = self . _get_axis_number ( axis ) if isinstance ( func , np . ufunc ) : new_series = { } for k , v in self . items ( ) : applied = func ( v ) applied . fill_value = func ( v . fill_value ) new_series [ k ] = applied return self . _constructor ( new_series , index = self . index , columns = self . columns , default_fill_value = self . _default_fill_value , default_kind = self . _default_kind ) . __finalize__ ( self ) from pandas . core . apply import frame_apply op = frame_apply ( self , func = func , axis = axis , reduce = reduce , broadcast = broadcast , result_type = result_type ) return op . get_result ( )\n\n\n        Summary:", "input": "", "output": "Analogous to DataFrame . apply for SparseDataFrame"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def conda_package_to_pip ( package ) : if package in EXCLUDE : return package = re . sub ( '(?<=[^<>])=' , '==' , package ) . strip ( ) for compare in ( '<=' , '>=' , '==' ) : if compare not in package : continue pkg , version = package . split ( compare ) if pkg in RENAME : return '' . join ( ( RENAME [ pkg ] , compare , version ) ) break return package\n\n\n        Summary:", "input": "", "output": "Convert a conda package to its pip equivalent ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def main ( conda_fname , pip_fname , compare = False ) : with open ( conda_fname ) as conda_fd : deps = yaml . safe_load ( conda_fd ) [ 'dependencies' ] pip_deps = [ ] for dep in deps : if isinstance ( dep , str ) : conda_dep = conda_package_to_pip ( dep ) if conda_dep : pip_deps . append ( conda_dep ) elif isinstance ( dep , dict ) and len ( dep ) == 1 and 'pip' in dep : pip_deps += dep [ 'pip' ] else : raise ValueError ( 'Unexpected dependency {}' . format ( dep ) ) pip_content = '\\n' . join ( pip_deps ) if compare : with open ( pip_fname ) as pip_fd : return pip_content != pip_fd . read ( ) else : with open ( pip_fname , 'w' ) as pip_fd : pip_fd . write ( pip_content ) return False\n\n\n        Summary:", "input": "", "output": "Generate the pip dependencies file from the conda file or compare that they are synchronized ( compare = True ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_convert_platform ( values ) : if isinstance ( values , ( list , tuple ) ) : values = construct_1d_object_array_from_listlike ( list ( values ) ) if getattr ( values , 'dtype' , None ) == np . object_ : if hasattr ( values , '_values' ) : values = values . _values values = lib . maybe_convert_objects ( values ) return values\n\n\n        Summary:", "input": "", "output": "try to do platform conversion allow ndarray or list here"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_nested_object ( obj ) : if isinstance ( obj , ABCSeries ) and is_object_dtype ( obj ) : if any ( isinstance ( v , ABCSeries ) for v in obj . values ) : return True return False\n\n\n        Summary:", "input": "", "output": "return a boolean if we have a nested object e . g . a Series with 1 or more Series elements"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_downcast_to_dtype ( result , dtype ) : if is_scalar ( result ) : return result def trans ( x ) : return x if isinstance ( dtype , str ) : if dtype == 'infer' : inferred_type = lib . infer_dtype ( ensure_object ( result . ravel ( ) ) , skipna = False ) if inferred_type == 'boolean' : dtype = 'bool' elif inferred_type == 'integer' : dtype = 'int64' elif inferred_type == 'datetime64' : dtype = 'datetime64[ns]' elif inferred_type == 'timedelta64' : dtype = 'timedelta64[ns]' # try to upcast here elif inferred_type == 'floating' : dtype = 'int64' if issubclass ( result . dtype . type , np . number ) : def trans ( x ) : # noqa return x . round ( ) else : dtype = 'object' if isinstance ( dtype , str ) : dtype = np . dtype ( dtype ) try : # don't allow upcasts here (except if empty) if dtype . kind == result . dtype . kind : if ( result . dtype . itemsize <= dtype . itemsize and np . prod ( result . shape ) ) : return result if is_bool_dtype ( dtype ) or is_integer_dtype ( dtype ) : # if we don't have any elements, just astype it if not np . prod ( result . shape ) : return trans ( result ) . astype ( dtype ) # do a test on the first element, if it fails then we are done r = result . ravel ( ) arr = np . array ( [ r [ 0 ] ] ) # if we have any nulls, then we are done if ( isna ( arr ) . any ( ) or not np . allclose ( arr , trans ( arr ) . astype ( dtype ) , rtol = 0 ) ) : return result # a comparable, e.g. a Decimal may slip in here elif not isinstance ( r [ 0 ] , ( np . integer , np . floating , np . bool , int , float , bool ) ) : return result if ( issubclass ( result . dtype . type , ( np . object_ , np . number ) ) and notna ( result ) . all ( ) ) : new_result = trans ( result ) . astype ( dtype ) try : if np . allclose ( new_result , result , rtol = 0 ) : return new_result except Exception : # comparison of an object dtype with a number type could # hit here if ( new_result == result ) . all ( ) : return new_result elif ( issubclass ( dtype . type , np . floating ) and not is_bool_dtype ( result . dtype ) ) : return result . astype ( dtype ) # a datetimelike # GH12821, iNaT is casted to float elif dtype . kind in [ 'M' , 'm' ] and result . dtype . kind in [ 'i' , 'f' ] : try : result = result . astype ( dtype ) except Exception : if dtype . tz : # convert to datetime and change timezone from pandas import to_datetime result = to_datetime ( result ) . tz_localize ( 'utc' ) result = result . tz_convert ( dtype . tz ) elif dtype . type == Period : # TODO(DatetimeArray): merge with previous elif from pandas . core . arrays import PeriodArray return PeriodArray ( result , freq = dtype . freq ) except Exception : pass return result\n\n\n        Summary:", "input": "", "output": "try to cast to the specified dtype ( e . g . convert back to bool / int or could be an astype of float64 - > float32"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_upcast_putmask ( result , mask , other ) : if not isinstance ( result , np . ndarray ) : raise ValueError ( \"The result input must be a ndarray.\" ) if mask . any ( ) : # Two conversions for date-like dtypes that can't be done automatically # in np.place: # NaN -> NaT # integer or integer array -> date-like array if is_datetimelike ( result . dtype ) : if is_scalar ( other ) : if isna ( other ) : other = result . dtype . type ( 'nat' ) elif is_integer ( other ) : other = np . array ( other , dtype = result . dtype ) elif is_integer_dtype ( other ) : other = np . array ( other , dtype = result . dtype ) def changeit ( ) : # try to directly set by expanding our array to full # length of the boolean try : om = other [ mask ] om_at = om . astype ( result . dtype ) if ( om == om_at ) . all ( ) : new_result = result . values . copy ( ) new_result [ mask ] = om_at result [ : ] = new_result return result , False except Exception : pass # we are forced to change the dtype of the result as the input # isn't compatible r , _ = maybe_upcast ( result , fill_value = other , copy = True ) np . place ( r , mask , other ) return r , True # we want to decide whether place will work # if we have nans in the False portion of our mask then we need to # upcast (possibly), otherwise we DON't want to upcast (e.g. if we # have values, say integers, in the success portion then it's ok to not # upcast) new_dtype , _ = maybe_promote ( result . dtype , other ) if new_dtype != result . dtype : # we have a scalar or len 0 ndarray # and its nan and we are changing some values if ( is_scalar ( other ) or ( isinstance ( other , np . ndarray ) and other . ndim < 1 ) ) : if isna ( other ) : return changeit ( ) # we have an ndarray and the masking has nans in it else : if isna ( other ) . any ( ) : return changeit ( ) try : np . place ( result , mask , other ) except Exception : return changeit ( ) return result , False\n\n\n        Summary:", "input": "", "output": "A safe version of putmask that potentially upcasts the result . The result is replaced with the first N elements of other where N is the number of True values in mask . If the length of other is shorter than N other will be repeated ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer_dtype_from ( val , pandas_dtype = False ) : if is_scalar ( val ) : return infer_dtype_from_scalar ( val , pandas_dtype = pandas_dtype ) return infer_dtype_from_array ( val , pandas_dtype = pandas_dtype )\n\n\n        Summary:", "input": "", "output": "interpret the dtype from a scalar or array . This is a convenience routines to infer dtype from a scalar or an array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer_dtype_from_scalar ( val , pandas_dtype = False ) : dtype = np . object_ # a 1-element ndarray if isinstance ( val , np . ndarray ) : msg = \"invalid ndarray passed to infer_dtype_from_scalar\" if val . ndim != 0 : raise ValueError ( msg ) dtype = val . dtype val = val . item ( ) elif isinstance ( val , str ) : # If we create an empty array using a string to infer # the dtype, NumPy will only allocate one character per entry # so this is kind of bad. Alternately we could use np.repeat # instead of np.empty (but then you still don't want things # coming out as np.str_! dtype = np . object_ elif isinstance ( val , ( np . datetime64 , datetime ) ) : val = tslibs . Timestamp ( val ) if val is tslibs . NaT or val . tz is None : dtype = np . dtype ( 'M8[ns]' ) else : if pandas_dtype : dtype = DatetimeTZDtype ( unit = 'ns' , tz = val . tz ) else : # return datetimetz as object return np . object_ , val val = val . value elif isinstance ( val , ( np . timedelta64 , timedelta ) ) : val = tslibs . Timedelta ( val ) . value dtype = np . dtype ( 'm8[ns]' ) elif is_bool ( val ) : dtype = np . bool_ elif is_integer ( val ) : if isinstance ( val , np . integer ) : dtype = type ( val ) else : dtype = np . int64 elif is_float ( val ) : if isinstance ( val , np . floating ) : dtype = type ( val ) else : dtype = np . float64 elif is_complex ( val ) : dtype = np . complex_ elif pandas_dtype : if lib . is_period ( val ) : dtype = PeriodDtype ( freq = val . freq ) val = val . ordinal return dtype , val\n\n\n        Summary:", "input": "", "output": "interpret the dtype from a scalar"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer_dtype_from_array ( arr , pandas_dtype = False ) : if isinstance ( arr , np . ndarray ) : return arr . dtype , arr if not is_list_like ( arr ) : arr = [ arr ] if pandas_dtype and is_extension_type ( arr ) : return arr . dtype , arr elif isinstance ( arr , ABCSeries ) : return arr . dtype , np . asarray ( arr ) # don't force numpy coerce with nan's inferred = lib . infer_dtype ( arr , skipna = False ) if inferred in [ 'string' , 'bytes' , 'unicode' , 'mixed' , 'mixed-integer' ] : return ( np . object_ , arr ) arr = np . asarray ( arr ) return arr . dtype , arr\n\n\n        Summary:", "input": "", "output": "infer the dtype from a scalar or array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_infer_dtype_type ( element ) : tipo = None if hasattr ( element , 'dtype' ) : tipo = element . dtype elif is_list_like ( element ) : element = np . asarray ( element ) tipo = element . dtype return tipo\n\n\n        Summary:", "input": "", "output": "Try to infer an object s dtype for use in arithmetic ops"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_upcast ( values , fill_value = np . nan , dtype = None , copy = False ) : if is_extension_type ( values ) : if copy : values = values . copy ( ) else : if dtype is None : dtype = values . dtype new_dtype , fill_value = maybe_promote ( dtype , fill_value ) if new_dtype != values . dtype : values = values . astype ( new_dtype ) elif copy : values = values . copy ( ) return values , fill_value\n\n\n        Summary:", "input": "", "output": "provide explicit type promotion and coercion"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def invalidate_string_dtypes ( dtype_set ) : non_string_dtypes = dtype_set - { np . dtype ( 'S' ) . type , np . dtype ( '<U' ) . type } if non_string_dtypes != dtype_set : raise TypeError ( \"string dtypes are not allowed, use 'object' instead\" )\n\n\n        Summary:", "input": "", "output": "Change string like dtypes to object for DataFrame . select_dtypes () ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def coerce_indexer_dtype ( indexer , categories ) : length = len ( categories ) if length < _int8_max : return ensure_int8 ( indexer ) elif length < _int16_max : return ensure_int16 ( indexer ) elif length < _int32_max : return ensure_int32 ( indexer ) return ensure_int64 ( indexer )\n\n\n        Summary:", "input": "", "output": "coerce the indexer input array to the smallest dtype possible"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def coerce_to_dtypes ( result , dtypes ) : if len ( result ) != len ( dtypes ) : raise AssertionError ( \"_coerce_to_dtypes requires equal len arrays\" ) def conv ( r , dtype ) : try : if isna ( r ) : pass elif dtype == _NS_DTYPE : r = tslibs . Timestamp ( r ) elif dtype == _TD_DTYPE : r = tslibs . Timedelta ( r ) elif dtype == np . bool_ : # messy. non 0/1 integers do not get converted. if is_integer ( r ) and r not in [ 0 , 1 ] : return int ( r ) r = bool ( r ) elif dtype . kind == 'f' : r = float ( r ) elif dtype . kind == 'i' : r = int ( r ) except Exception : pass return r return [ conv ( r , dtype ) for r , dtype in zip ( result , dtypes ) ]\n\n\n        Summary:", "input": "", "output": "given a dtypes and a result set coerce the result elements to the dtypes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def astype_nansafe ( arr , dtype , copy = True , skipna = False ) : # dispatch on extension dtype if needed if is_extension_array_dtype ( dtype ) : return dtype . construct_array_type ( ) . _from_sequence ( arr , dtype = dtype , copy = copy ) if not isinstance ( dtype , np . dtype ) : dtype = pandas_dtype ( dtype ) if issubclass ( dtype . type , str ) : return lib . astype_str ( arr . ravel ( ) , skipna = skipna ) . reshape ( arr . shape ) elif is_datetime64_dtype ( arr ) : if is_object_dtype ( dtype ) : return tslib . ints_to_pydatetime ( arr . view ( np . int64 ) ) elif dtype == np . int64 : return arr . view ( dtype ) # allow frequency conversions if dtype . kind == 'M' : return arr . astype ( dtype ) raise TypeError ( \"cannot astype a datetimelike from [{from_dtype}] \" \"to [{to_dtype}]\" . format ( from_dtype = arr . dtype , to_dtype = dtype ) ) elif is_timedelta64_dtype ( arr ) : if is_object_dtype ( dtype ) : return tslibs . ints_to_pytimedelta ( arr . view ( np . int64 ) ) elif dtype == np . int64 : return arr . view ( dtype ) if dtype not in [ _INT64_DTYPE , _TD_DTYPE ] : # allow frequency conversions # we return a float here! if dtype . kind == 'm' : mask = isna ( arr ) result = arr . astype ( dtype ) . astype ( np . float64 ) result [ mask ] = np . nan return result elif dtype == _TD_DTYPE : return arr . astype ( _TD_DTYPE , copy = copy ) raise TypeError ( \"cannot astype a timedelta from [{from_dtype}] \" \"to [{to_dtype}]\" . format ( from_dtype = arr . dtype , to_dtype = dtype ) ) elif ( np . issubdtype ( arr . dtype , np . floating ) and np . issubdtype ( dtype , np . integer ) ) : if not np . isfinite ( arr ) . all ( ) : raise ValueError ( 'Cannot convert non-finite values (NA or inf) to ' 'integer' ) elif is_object_dtype ( arr ) : # work around NumPy brokenness, #1987 if np . issubdtype ( dtype . type , np . integer ) : return lib . astype_intsafe ( arr . ravel ( ) , dtype ) . reshape ( arr . shape ) # if we have a datetime/timedelta array of objects # then coerce to a proper dtype and recall astype_nansafe elif is_datetime64_dtype ( dtype ) : from pandas import to_datetime return astype_nansafe ( to_datetime ( arr ) . values , dtype , copy = copy ) elif is_timedelta64_dtype ( dtype ) : from pandas import to_timedelta return astype_nansafe ( to_timedelta ( arr ) . values , dtype , copy = copy ) if dtype . name in ( \"datetime64\" , \"timedelta64\" ) : msg = ( \"The '{dtype}' dtype has no unit. \" \"Please pass in '{dtype}[ns]' instead.\" ) raise ValueError ( msg . format ( dtype = dtype . name ) ) if copy or is_object_dtype ( arr ) or is_object_dtype ( dtype ) : # Explicit copy, or required since NumPy can't view from / to object. return arr . astype ( dtype , copy = True ) return arr . view ( dtype )\n\n\n        Summary:", "input": "", "output": "Cast the elements of an array to a given dtype a nan - safe manner ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_convert_objects ( values , convert_dates = True , convert_numeric = True , convert_timedeltas = True , copy = True ) : # if we have passed in a list or scalar if isinstance ( values , ( list , tuple ) ) : values = np . array ( values , dtype = np . object_ ) if not hasattr ( values , 'dtype' ) : values = np . array ( [ values ] , dtype = np . object_ ) # convert dates if convert_dates and values . dtype == np . object_ : # we take an aggressive stance and convert to datetime64[ns] if convert_dates == 'coerce' : new_values = maybe_cast_to_datetime ( values , 'M8[ns]' , errors = 'coerce' ) # if we are all nans then leave me alone if not isna ( new_values ) . all ( ) : values = new_values else : values = lib . maybe_convert_objects ( values , convert_datetime = convert_dates ) # convert timedeltas if convert_timedeltas and values . dtype == np . object_ : if convert_timedeltas == 'coerce' : from pandas . core . tools . timedeltas import to_timedelta new_values = to_timedelta ( values , errors = 'coerce' ) # if we are all nans then leave me alone if not isna ( new_values ) . all ( ) : values = new_values else : values = lib . maybe_convert_objects ( values , convert_timedelta = convert_timedeltas ) # convert to numeric if values . dtype == np . object_ : if convert_numeric : try : new_values = lib . maybe_convert_numeric ( values , set ( ) , coerce_numeric = True ) # if we are all nans then leave me alone if not isna ( new_values ) . all ( ) : values = new_values except Exception : pass else : # soft-conversion values = lib . maybe_convert_objects ( values ) values = values . copy ( ) if copy else values return values\n\n\n        Summary:", "input": "", "output": "if we have an object dtype try to coerce dates and / or numbers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def soft_convert_objects ( values , datetime = True , numeric = True , timedelta = True , coerce = False , copy = True ) : conversion_count = sum ( ( datetime , numeric , timedelta ) ) if conversion_count == 0 : raise ValueError ( 'At least one of datetime, numeric or timedelta must ' 'be True.' ) elif conversion_count > 1 and coerce : raise ValueError ( \"Only one of 'datetime', 'numeric' or \" \"'timedelta' can be True when when coerce=True.\" ) if isinstance ( values , ( list , tuple ) ) : # List or scalar values = np . array ( values , dtype = np . object_ ) elif not hasattr ( values , 'dtype' ) : values = np . array ( [ values ] , dtype = np . object_ ) elif not is_object_dtype ( values . dtype ) : # If not object, do not attempt conversion values = values . copy ( ) if copy else values return values # If 1 flag is coerce, ensure 2 others are False if coerce : # Immediate return if coerce if datetime : from pandas import to_datetime return to_datetime ( values , errors = 'coerce' ) . to_numpy ( ) elif timedelta : from pandas import to_timedelta return to_timedelta ( values , errors = 'coerce' ) . to_numpy ( ) elif numeric : from pandas import to_numeric return to_numeric ( values , errors = 'coerce' ) # Soft conversions if datetime : # GH 20380, when datetime is beyond year 2262, hence outside # bound of nanosecond-resolution 64-bit integers. try : values = lib . maybe_convert_objects ( values , convert_datetime = datetime ) except OutOfBoundsDatetime : pass if timedelta and is_object_dtype ( values . dtype ) : # Object check to ensure only run if previous did not convert values = lib . maybe_convert_objects ( values , convert_timedelta = timedelta ) if numeric and is_object_dtype ( values . dtype ) : try : converted = lib . maybe_convert_numeric ( values , set ( ) , coerce_numeric = True ) # If all NaNs, then do not-alter values = converted if not isna ( converted ) . all ( ) else values values = values . copy ( ) if copy else values except Exception : pass return values\n\n\n        Summary:", "input": "", "output": "if we have an object dtype try to coerce dates and / or numbers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_infer_to_datetimelike ( value , convert_dates = False ) : # TODO: why not timedelta? if isinstance ( value , ( ABCDatetimeIndex , ABCPeriodIndex , ABCDatetimeArray , ABCPeriodArray ) ) : return value elif isinstance ( value , ABCSeries ) : if isinstance ( value . _values , ABCDatetimeIndex ) : return value . _values v = value if not is_list_like ( v ) : v = [ v ] v = np . array ( v , copy = False ) # we only care about object dtypes if not is_object_dtype ( v ) : return value shape = v . shape if not v . ndim == 1 : v = v . ravel ( ) if not len ( v ) : return value def try_datetime ( v ) : # safe coerce to datetime64 try : # GH19671 v = tslib . array_to_datetime ( v , require_iso8601 = True , errors = 'raise' ) [ 0 ] except ValueError : # we might have a sequence of the same-datetimes with tz's # if so coerce to a DatetimeIndex; if they are not the same, # then these stay as object dtype, xref GH19671 try : from pandas . _libs . tslibs import conversion from pandas import DatetimeIndex values , tz = conversion . datetime_to_datetime64 ( v ) return DatetimeIndex ( values ) . tz_localize ( 'UTC' ) . tz_convert ( tz = tz ) except ( ValueError , TypeError ) : pass except Exception : pass return v . reshape ( shape ) def try_timedelta ( v ) : # safe coerce to timedelta64 # will try first with a string & object conversion from pandas import to_timedelta try : return to_timedelta ( v ) . _ndarray_values . reshape ( shape ) except Exception : return v . reshape ( shape ) inferred_type = lib . infer_datetimelike_array ( ensure_object ( v ) ) if inferred_type == 'date' and convert_dates : value = try_datetime ( v ) elif inferred_type == 'datetime' : value = try_datetime ( v ) elif inferred_type == 'timedelta' : value = try_timedelta ( v ) elif inferred_type == 'nat' : # if all NaT, return as datetime if isna ( v ) . all ( ) : value = try_datetime ( v ) else : # We have at least a NaT and a string # try timedelta first to avoid spurious datetime conversions # e.g. '00:00:01' is a timedelta but technically is also a datetime value = try_timedelta ( v ) if lib . infer_dtype ( value , skipna = False ) in [ 'mixed' ] : # cannot skip missing values, as NaT implies that the string # is actually a datetime value = try_datetime ( v ) return value\n\n\n        Summary:", "input": "", "output": "we might have a array ( or single object ) that is datetime like and no dtype is passed don t change the value unless we find a datetime / timedelta set"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_cast_to_datetime ( value , dtype , errors = 'raise' ) : from pandas . core . tools . timedeltas import to_timedelta from pandas . core . tools . datetimes import to_datetime if dtype is not None : if isinstance ( dtype , str ) : dtype = np . dtype ( dtype ) is_datetime64 = is_datetime64_dtype ( dtype ) is_datetime64tz = is_datetime64tz_dtype ( dtype ) is_timedelta64 = is_timedelta64_dtype ( dtype ) if is_datetime64 or is_datetime64tz or is_timedelta64 : # Force the dtype if needed. msg = ( \"The '{dtype}' dtype has no unit. \" \"Please pass in '{dtype}[ns]' instead.\" ) if is_datetime64 and not is_dtype_equal ( dtype , _NS_DTYPE ) : if dtype . name in ( 'datetime64' , 'datetime64[ns]' ) : if dtype . name == 'datetime64' : raise ValueError ( msg . format ( dtype = dtype . name ) ) dtype = _NS_DTYPE else : raise TypeError ( \"cannot convert datetimelike to \" \"dtype [{dtype}]\" . format ( dtype = dtype ) ) elif is_datetime64tz : # our NaT doesn't support tz's # this will coerce to DatetimeIndex with # a matching dtype below if is_scalar ( value ) and isna ( value ) : value = [ value ] elif is_timedelta64 and not is_dtype_equal ( dtype , _TD_DTYPE ) : if dtype . name in ( 'timedelta64' , 'timedelta64[ns]' ) : if dtype . name == 'timedelta64' : raise ValueError ( msg . format ( dtype = dtype . name ) ) dtype = _TD_DTYPE else : raise TypeError ( \"cannot convert timedeltalike to \" \"dtype [{dtype}]\" . format ( dtype = dtype ) ) if is_scalar ( value ) : if value == iNaT or isna ( value ) : value = iNaT else : value = np . array ( value , copy = False ) # have a scalar array-like (e.g. NaT) if value . ndim == 0 : value = iNaT # we have an array of datetime or timedeltas & nulls elif np . prod ( value . shape ) or not is_dtype_equal ( value . dtype , dtype ) : try : if is_datetime64 : value = to_datetime ( value , errors = errors ) # GH 25843: Remove tz information since the dtype # didn't specify one if value . tz is not None : value = value . tz_localize ( None ) value = value . _values elif is_datetime64tz : # The string check can be removed once issue #13712 # is solved. String data that is passed with a # datetime64tz is assumed to be naive which should # be localized to the timezone. is_dt_string = is_string_dtype ( value ) value = to_datetime ( value , errors = errors ) . array if is_dt_string : # Strings here are naive, so directly localize value = value . tz_localize ( dtype . tz ) else : # Numeric values are UTC at this point, # so localize and convert value = ( value . tz_localize ( 'UTC' ) . tz_convert ( dtype . tz ) ) elif is_timedelta64 : value = to_timedelta ( value , errors = errors ) . _values except ( AttributeError , ValueError , TypeError ) : pass # coerce datetimelike to object elif is_datetime64_dtype ( value ) and not is_datetime64_dtype ( dtype ) : if is_object_dtype ( dtype ) : if value . dtype != _NS_DTYPE : value = value . astype ( _NS_DTYPE ) ints = np . asarray ( value ) . view ( 'i8' ) return tslib . ints_to_pydatetime ( ints ) # we have a non-castable dtype that was passed raise TypeError ( 'Cannot cast datetime64 to {dtype}' . format ( dtype = dtype ) ) else : is_array = isinstance ( value , np . ndarray ) # catch a datetime/timedelta that is not of ns variety # and no coercion specified if is_array and value . dtype . kind in [ 'M' , 'm' ] : dtype = value . dtype if dtype . kind == 'M' and dtype != _NS_DTYPE : value = value . astype ( _NS_DTYPE ) elif dtype . kind == 'm' and dtype != _TD_DTYPE : value = to_timedelta ( value ) # only do this if we have an array and the dtype of the array is not # setup already we are not an integer/object, so don't bother with this # conversion elif not ( is_array and not ( issubclass ( value . dtype . type , np . integer ) or value . dtype == np . object_ ) ) : value = maybe_infer_to_datetimelike ( value ) return value\n\n\n        Summary:", "input": "", "output": "try to cast the array / value to a datetimelike dtype converting float nan to iNaT"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def find_common_type ( types ) : if len ( types ) == 0 : raise ValueError ( 'no types given' ) first = types [ 0 ] # workaround for find_common_type([np.dtype('datetime64[ns]')] * 2) # => object if all ( is_dtype_equal ( first , t ) for t in types [ 1 : ] ) : return first if any ( isinstance ( t , ( PandasExtensionDtype , ExtensionDtype ) ) for t in types ) : return np . object # take lowest unit if all ( is_datetime64_dtype ( t ) for t in types ) : return np . dtype ( 'datetime64[ns]' ) if all ( is_timedelta64_dtype ( t ) for t in types ) : return np . dtype ( 'timedelta64[ns]' ) # don't mix bool / int or float or complex # this is different from numpy, which casts bool with float/int as int has_bools = any ( is_bool_dtype ( t ) for t in types ) if has_bools : for t in types : if is_integer_dtype ( t ) or is_float_dtype ( t ) or is_complex_dtype ( t ) : return np . object return np . find_common_type ( types , [ ] )\n\n\n        Summary:", "input": "", "output": "Find a common data type among the given dtypes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cast_scalar_to_array ( shape , value , dtype = None ) : if dtype is None : dtype , fill_value = infer_dtype_from_scalar ( value ) else : fill_value = value values = np . empty ( shape , dtype = dtype ) values . fill ( fill_value ) return values\n\n\n        Summary:", "input": "", "output": "create np . ndarray of specified shape and dtype filled with values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def construct_1d_arraylike_from_scalar ( value , length , dtype ) : if is_datetime64tz_dtype ( dtype ) : from pandas import DatetimeIndex subarr = DatetimeIndex ( [ value ] * length , dtype = dtype ) elif is_categorical_dtype ( dtype ) : from pandas import Categorical subarr = Categorical ( [ value ] * length , dtype = dtype ) else : if not isinstance ( dtype , ( np . dtype , type ( np . dtype ) ) ) : dtype = dtype . dtype if length and is_integer_dtype ( dtype ) and isna ( value ) : # coerce if we have nan for an integer dtype dtype = np . dtype ( 'float64' ) elif isinstance ( dtype , np . dtype ) and dtype . kind in ( \"U\" , \"S\" ) : # we need to coerce to object dtype to avoid # to allow numpy to take our string as a scalar value dtype = object if not isna ( value ) : value = to_str ( value ) subarr = np . empty ( length , dtype = dtype ) subarr . fill ( value ) return subarr\n\n\n        Summary:", "input": "", "output": "create a np . ndarray / pandas type of specified shape and dtype filled with values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def construct_1d_object_array_from_listlike ( values ) : # numpy will try to interpret nested lists as further dimensions, hence # making a 1D array that contains list-likes is a bit tricky: result = np . empty ( len ( values ) , dtype = 'object' ) result [ : ] = values return result\n\n\n        Summary:", "input": "", "output": "Transform any list - like object in a 1 - dimensional numpy array of object dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def construct_1d_ndarray_preserving_na ( values , dtype = None , copy = False ) : subarr = np . array ( values , dtype = dtype , copy = copy ) if dtype is not None and dtype . kind in ( \"U\" , \"S\" ) : # GH-21083 # We can't just return np.array(subarr, dtype='str') since # NumPy will convert the non-string objects into strings # Including NA values. Se we have to go # string -> object -> update NA, which requires an # additional pass over the data. na_values = isna ( values ) subarr2 = subarr . astype ( object ) subarr2 [ na_values ] = np . asarray ( values , dtype = object ) [ na_values ] subarr = subarr2 return subarr\n\n\n        Summary:", "input": "", "output": "Construct a new ndarray coercing values to dtype preserving NA ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_cast_to_integer_array ( arr , dtype , copy = False ) : try : if not hasattr ( arr , \"astype\" ) : casted = np . array ( arr , dtype = dtype , copy = copy ) else : casted = arr . astype ( dtype , copy = copy ) except OverflowError : raise OverflowError ( \"The elements provided in the data cannot all be \" \"casted to the dtype {dtype}\" . format ( dtype = dtype ) ) if np . array_equal ( arr , casted ) : return casted # We do this casting to allow for proper # data and dtype checking. # # We didn't do this earlier because NumPy # doesn't handle `uint64` correctly. arr = np . asarray ( arr ) if is_unsigned_integer_dtype ( dtype ) and ( arr < 0 ) . any ( ) : raise OverflowError ( \"Trying to coerce negative values \" \"to unsigned integers\" ) if is_integer_dtype ( dtype ) and ( is_float_dtype ( arr ) or is_object_dtype ( arr ) ) : raise ValueError ( \"Trying to coerce float values to integers\" )\n\n\n        Summary:", "input": "", "output": "Takes any dtype and returns the casted version raising for when data is incompatible with integer / unsigned integer dtypes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def scatter_plot ( data , x , y , by = None , ax = None , figsize = None , grid = False , * * kwargs ) : import matplotlib . pyplot as plt kwargs . setdefault ( 'edgecolors' , 'none' ) def plot_group ( group , ax ) : xvals = group [ x ] . values yvals = group [ y ] . values ax . scatter ( xvals , yvals , * * kwargs ) ax . grid ( grid ) if by is not None : fig = _grouped_plot ( plot_group , data , by = by , figsize = figsize , ax = ax ) else : if ax is None : fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) else : fig = ax . get_figure ( ) plot_group ( data , ax ) ax . set_ylabel ( pprint_thing ( y ) ) ax . set_xlabel ( pprint_thing ( x ) ) ax . grid ( grid ) return fig\n\n\n        Summary:", "input": "", "output": "Make a scatter plot from two DataFrame columns"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hist_frame ( data , column = None , by = None , grid = True , xlabelsize = None , xrot = None , ylabelsize = None , yrot = None , ax = None , sharex = False , sharey = False , figsize = None , layout = None , bins = 10 , * * kwds ) : _raise_if_no_mpl ( ) _converter . _WARN = False if by is not None : axes = grouped_hist ( data , column = column , by = by , ax = ax , grid = grid , figsize = figsize , sharex = sharex , sharey = sharey , layout = layout , bins = bins , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot , * * kwds ) return axes if column is not None : if not isinstance ( column , ( list , np . ndarray , ABCIndexClass ) ) : column = [ column ] data = data [ column ] data = data . _get_numeric_data ( ) naxes = len ( data . columns ) fig , axes = _subplots ( naxes = naxes , ax = ax , squeeze = False , sharex = sharex , sharey = sharey , figsize = figsize , layout = layout ) _axes = _flatten ( axes ) for i , col in enumerate ( com . try_sort ( data . columns ) ) : ax = _axes [ i ] ax . hist ( data [ col ] . dropna ( ) . values , bins = bins , * * kwds ) ax . set_title ( col ) ax . grid ( grid ) _set_ticks_props ( axes , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot ) fig . subplots_adjust ( wspace = 0.3 , hspace = 0.3 ) return axes\n\n\n        Summary:", "input": "", "output": "Make a histogram of the DataFrame s ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hist_series ( self , by = None , ax = None , grid = True , xlabelsize = None , xrot = None , ylabelsize = None , yrot = None , figsize = None , bins = 10 , * * kwds ) : import matplotlib . pyplot as plt if by is None : if kwds . get ( 'layout' , None ) is not None : raise ValueError ( \"The 'layout' keyword is not supported when \" \"'by' is None\" ) # hack until the plotting interface is a bit more unified fig = kwds . pop ( 'figure' , plt . gcf ( ) if plt . get_fignums ( ) else plt . figure ( figsize = figsize ) ) if ( figsize is not None and tuple ( figsize ) != tuple ( fig . get_size_inches ( ) ) ) : fig . set_size_inches ( * figsize , forward = True ) if ax is None : ax = fig . gca ( ) elif ax . get_figure ( ) != fig : raise AssertionError ( 'passed axis not bound to passed figure' ) values = self . dropna ( ) . values ax . hist ( values , bins = bins , * * kwds ) ax . grid ( grid ) axes = np . array ( [ ax ] ) _set_ticks_props ( axes , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot ) else : if 'figure' in kwds : raise ValueError ( \"Cannot pass 'figure' when using the \" \"'by' argument, since a new 'Figure' instance \" \"will be created\" ) axes = grouped_hist ( self , by = by , ax = ax , grid = grid , figsize = figsize , bins = bins , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot , * * kwds ) if hasattr ( axes , 'ndim' ) : if axes . ndim == 1 and len ( axes ) == 1 : return axes [ 0 ] return axes\n\n\n        Summary:", "input": "", "output": "Draw histogram of the input series using matplotlib ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def grouped_hist ( data , column = None , by = None , ax = None , bins = 50 , figsize = None , layout = None , sharex = False , sharey = False , rot = 90 , grid = True , xlabelsize = None , xrot = None , ylabelsize = None , yrot = None , * * kwargs ) : _raise_if_no_mpl ( ) _converter . _WARN = False def plot_group ( group , ax ) : ax . hist ( group . dropna ( ) . values , bins = bins , * * kwargs ) xrot = xrot or rot fig , axes = _grouped_plot ( plot_group , data , column = column , by = by , sharex = sharex , sharey = sharey , ax = ax , figsize = figsize , layout = layout , rot = rot ) _set_ticks_props ( axes , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot ) fig . subplots_adjust ( bottom = 0.15 , top = 0.9 , left = 0.1 , right = 0.9 , hspace = 0.5 , wspace = 0.3 ) return axes\n\n\n        Summary:", "input": "", "output": "Grouped histogram"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def boxplot_frame_groupby ( grouped , subplots = True , column = None , fontsize = None , rot = 0 , grid = True , ax = None , figsize = None , layout = None , sharex = False , sharey = True , * * kwds ) : _raise_if_no_mpl ( ) _converter . _WARN = False if subplots is True : naxes = len ( grouped ) fig , axes = _subplots ( naxes = naxes , squeeze = False , ax = ax , sharex = sharex , sharey = sharey , figsize = figsize , layout = layout ) axes = _flatten ( axes ) from pandas . core . series import Series ret = Series ( ) for ( key , group ) , ax in zip ( grouped , axes ) : d = group . boxplot ( ax = ax , column = column , fontsize = fontsize , rot = rot , grid = grid , * * kwds ) ax . set_title ( pprint_thing ( key ) ) ret . loc [ key ] = d fig . subplots_adjust ( bottom = 0.15 , top = 0.9 , left = 0.1 , right = 0.9 , wspace = 0.2 ) else : from pandas . core . reshape . concat import concat keys , frames = zip ( * grouped ) if grouped . axis == 0 : df = concat ( frames , keys = keys , axis = 1 ) else : if len ( frames ) > 1 : df = frames [ 0 ] . join ( frames [ 1 : : ] ) else : df = frames [ 0 ] ret = df . boxplot ( column = column , fontsize = fontsize , rot = rot , grid = grid , ax = ax , figsize = figsize , layout = layout , * * kwds ) return ret\n\n\n        Summary:", "input": "", "output": "Make box plots from DataFrameGroupBy data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _has_plotted_object ( self , ax ) : return ( len ( ax . lines ) != 0 or len ( ax . artists ) != 0 or len ( ax . containers ) != 0 )\n\n\n        Summary:", "input": "", "output": "check whether ax has data"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def result ( self ) : if self . subplots : if self . layout is not None and not is_list_like ( self . ax ) : return self . axes . reshape ( * self . layout ) else : return self . axes else : sec_true = isinstance ( self . secondary_y , bool ) and self . secondary_y all_sec = ( is_list_like ( self . secondary_y ) and len ( self . secondary_y ) == self . nseries ) if ( sec_true or all_sec ) : # if all data is plotted on secondary, return right axes return self . _get_ax_layer ( self . axes [ 0 ] , primary = False ) else : return self . axes [ 0 ]\n\n\n        Summary:", "input": "", "output": "Return result axes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _post_plot_logic_common ( self , ax , data ) : def get_label ( i ) : try : return pprint_thing ( data . index [ i ] ) except Exception : return '' if self . orientation == 'vertical' or self . orientation is None : if self . _need_to_set_index : xticklabels = [ get_label ( x ) for x in ax . get_xticks ( ) ] ax . set_xticklabels ( xticklabels ) self . _apply_axis_properties ( ax . xaxis , rot = self . rot , fontsize = self . fontsize ) self . _apply_axis_properties ( ax . yaxis , fontsize = self . fontsize ) if hasattr ( ax , 'right_ax' ) : self . _apply_axis_properties ( ax . right_ax . yaxis , fontsize = self . fontsize ) elif self . orientation == 'horizontal' : if self . _need_to_set_index : yticklabels = [ get_label ( y ) for y in ax . get_yticks ( ) ] ax . set_yticklabels ( yticklabels ) self . _apply_axis_properties ( ax . yaxis , rot = self . rot , fontsize = self . fontsize ) self . _apply_axis_properties ( ax . xaxis , fontsize = self . fontsize ) if hasattr ( ax , 'right_ax' ) : self . _apply_axis_properties ( ax . right_ax . yaxis , fontsize = self . fontsize ) else : # pragma no cover raise ValueError\n\n\n        Summary:", "input": "", "output": "Common post process for each axes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _adorn_subplots ( self ) : if len ( self . axes ) > 0 : all_axes = self . _get_subplots ( ) nrows , ncols = self . _get_axes_layout ( ) _handle_shared_axes ( axarr = all_axes , nplots = len ( all_axes ) , naxes = nrows * ncols , nrows = nrows , ncols = ncols , sharex = self . sharex , sharey = self . sharey ) for ax in self . axes : if self . yticks is not None : ax . set_yticks ( self . yticks ) if self . xticks is not None : ax . set_xticks ( self . xticks ) if self . ylim is not None : ax . set_ylim ( self . ylim ) if self . xlim is not None : ax . set_xlim ( self . xlim ) ax . grid ( self . grid ) if self . title : if self . subplots : if is_list_like ( self . title ) : if len ( self . title ) != self . nseries : msg = ( 'The length of `title` must equal the number ' 'of columns if using `title` of type `list` ' 'and `subplots=True`.\\n' 'length of title = {}\\n' 'number of columns = {}' ) . format ( len ( self . title ) , self . nseries ) raise ValueError ( msg ) for ( ax , title ) in zip ( self . axes , self . title ) : ax . set_title ( title ) else : self . fig . suptitle ( self . title ) else : if is_list_like ( self . title ) : msg = ( 'Using `title` of type `list` is not supported ' 'unless `subplots=True` is passed' ) raise ValueError ( msg ) self . axes [ 0 ] . set_title ( self . title )\n\n\n        Summary:", "input": "", "output": "Common post process unrelated to data"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_axis_properties ( self , axis , rot = None , fontsize = None ) : if rot is not None or fontsize is not None : # rot=0 is a valid setting, hence the explicit None check labels = axis . get_majorticklabels ( ) + axis . get_minorticklabels ( ) for label in labels : if rot is not None : label . set_rotation ( rot ) if fontsize is not None : label . set_fontsize ( fontsize )\n\n\n        Summary:", "input": "", "output": "Tick creation within matplotlib is reasonably expensive and is internally deferred until accessed as Ticks are created / destroyed multiple times per draw . It s therefore beneficial for us to avoid accessing unless we will act on the Tick ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_ax_layer ( cls , ax , primary = True ) : if primary : return getattr ( ax , 'left_ax' , ax ) else : return getattr ( ax , 'right_ax' , ax )\n\n\n        Summary:", "input": "", "output": "get left ( primary ) or right ( secondary ) axes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_style_colors ( self , colors , kwds , col_num , label ) : style = None if self . style is not None : if isinstance ( self . style , list ) : try : style = self . style [ col_num ] except IndexError : pass elif isinstance ( self . style , dict ) : style = self . style . get ( label , style ) else : style = self . style has_color = 'color' in kwds or self . colormap is not None nocolor_style = style is None or re . match ( '[a-z]+' , style ) is None if ( has_color or self . subplots ) and nocolor_style : kwds [ 'color' ] = colors [ col_num % len ( colors ) ] return style , kwds\n\n\n        Summary:", "input": "", "output": "Manage style and color based on column number and its label . Returns tuple of appropriate style and kwds which color may be added ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_errorbars ( self , label , err ) : if err is None : return None def match_labels ( data , e ) : e = e . reindex ( data . index ) return e # key-matched DataFrame if isinstance ( err , ABCDataFrame ) : err = match_labels ( self . data , err ) # key-matched dict elif isinstance ( err , dict ) : pass # Series of error values elif isinstance ( err , ABCSeries ) : # broadcast error series across data err = match_labels ( self . data , err ) err = np . atleast_2d ( err ) err = np . tile ( err , ( self . nseries , 1 ) ) # errors are a column in the dataframe elif isinstance ( err , str ) : evalues = self . data [ err ] . values self . data = self . data [ self . data . columns . drop ( err ) ] err = np . atleast_2d ( evalues ) err = np . tile ( err , ( self . nseries , 1 ) ) elif is_list_like ( err ) : if is_iterator ( err ) : err = np . atleast_2d ( list ( err ) ) else : # raw error values err = np . atleast_2d ( err ) err_shape = err . shape # asymmetrical error bars if err . ndim == 3 : if ( err_shape [ 0 ] != self . nseries ) or ( err_shape [ 1 ] != 2 ) or ( err_shape [ 2 ] != len ( self . data ) ) : msg = \"Asymmetrical error bars should be provided \" + \"with the shape (%u, 2, %u)\" % ( self . nseries , len ( self . data ) ) raise ValueError ( msg ) # broadcast errors to each data series if len ( err ) == 1 : err = np . tile ( err , ( self . nseries , 1 ) ) elif is_number ( err ) : err = np . tile ( [ err ] , ( self . nseries , len ( self . data ) ) ) else : msg = \"No valid {label} detected\" . format ( label = label ) raise ValueError ( msg ) return err\n\n\n        Summary:", "input": "", "output": "Look for error keyword arguments and return the actual errorbar data or return the error DataFrame / dict"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_plot_keywords ( self , kwds , y ) : # y is required for KdePlot kwds [ 'bottom' ] = self . bottom kwds [ 'bins' ] = self . bins return kwds\n\n\n        Summary:", "input": "", "output": "merge BoxPlot / KdePlot properties to passed kwds"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def line ( self , x = None , y = None , * * kwds ) : return self ( kind = 'line' , x = x , y = y , * * kwds )\n\n\n        Summary:", "input": "", "output": "Plot DataFrame columns as lines ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def bar ( self , x = None , y = None , * * kwds ) : return self ( kind = 'bar' , x = x , y = y , * * kwds )\n\n\n        Summary:", "input": "", "output": "Vertical bar plot ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def barh ( self , x = None , y = None , * * kwds ) : return self ( kind = 'barh' , x = x , y = y , * * kwds )\n\n\n        Summary:", "input": "", "output": "Make a horizontal bar plot ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hist ( self , by = None , bins = 10 , * * kwds ) : return self ( kind = 'hist' , by = by , bins = bins , * * kwds )\n\n\n        Summary:", "input": "", "output": "Draw one histogram of the DataFrame s columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def area ( self , x = None , y = None , * * kwds ) : return self ( kind = 'area' , x = x , y = y , * * kwds )\n\n\n        Summary:", "input": "", "output": "Draw a stacked area plot ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def scatter ( self , x , y , s = None , c = None , * * kwds ) : return self ( kind = 'scatter' , x = x , y = y , c = c , s = s , * * kwds )\n\n\n        Summary:", "input": "", "output": "Create a scatter plot with varying marker point size and color ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hexbin ( self , x , y , C = None , reduce_C_function = None , gridsize = None , * * kwds ) : if reduce_C_function is not None : kwds [ 'reduce_C_function' ] = reduce_C_function if gridsize is not None : kwds [ 'gridsize' ] = gridsize return self ( kind = 'hexbin' , x = x , y = y , C = C , * * kwds )\n\n\n        Summary:", "input": "", "output": "Generate a hexagonal binning plot ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_objs_combined_axis ( objs , intersect = False , axis = 0 , sort = True ) : obs_idxes = [ obj . _get_axis ( axis ) for obj in objs if hasattr ( obj , '_get_axis' ) ] if obs_idxes : return _get_combined_index ( obs_idxes , intersect = intersect , sort = sort )\n\n\n        Summary:", "input": "", "output": "Extract combined index : return intersection or union ( depending on the value of intersect ) of indexes on given axis or None if all objects lack indexes ( e . g . they are numpy arrays ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_distinct_objs ( objs ) : ids = set ( ) res = [ ] for obj in objs : if not id ( obj ) in ids : ids . add ( id ( obj ) ) res . append ( obj ) return res\n\n\n        Summary:", "input": "", "output": "Return a list with distinct elements of objs ( different ids ) . Preserves order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_combined_index ( indexes , intersect = False , sort = False ) : # TODO: handle index names! indexes = _get_distinct_objs ( indexes ) if len ( indexes ) == 0 : index = Index ( [ ] ) elif len ( indexes ) == 1 : index = indexes [ 0 ] elif intersect : index = indexes [ 0 ] for other in indexes [ 1 : ] : index = index . intersection ( other ) else : index = _union_indexes ( indexes , sort = sort ) index = ensure_index ( index ) if sort : try : index = index . sort_values ( ) except TypeError : pass return index\n\n\n        Summary:", "input": "", "output": "Return the union or intersection of indexes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _union_indexes ( indexes , sort = True ) : if len ( indexes ) == 0 : raise AssertionError ( 'Must have at least 1 Index to union' ) if len ( indexes ) == 1 : result = indexes [ 0 ] if isinstance ( result , list ) : result = Index ( sorted ( result ) ) return result indexes , kind = _sanitize_and_check ( indexes ) def _unique_indices ( inds ) : \"\"\" Convert indexes to lists and concatenate them, removing duplicates. The final dtype is inferred. Parameters ---------- inds : list of Index or list objects Returns ------- Index \"\"\" def conv ( i ) : if isinstance ( i , Index ) : i = i . tolist ( ) return i return Index ( lib . fast_unique_multiple_list ( [ conv ( i ) for i in inds ] , sort = sort ) ) if kind == 'special' : result = indexes [ 0 ] if hasattr ( result , 'union_many' ) : return result . union_many ( indexes [ 1 : ] ) else : for other in indexes [ 1 : ] : result = result . union ( other ) return result elif kind == 'array' : index = indexes [ 0 ] for other in indexes [ 1 : ] : if not index . equals ( other ) : if sort is None : # TODO: remove once pd.concat sort default changes warnings . warn ( _sort_msg , FutureWarning , stacklevel = 8 ) sort = True return _unique_indices ( indexes ) name = _get_consensus_names ( indexes ) [ 0 ] if name != index . name : index = index . _shallow_copy ( name = name ) return index else : # kind='list' return _unique_indices ( indexes )\n\n\n        Summary:", "input": "", "output": "Return the union of indexes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sanitize_and_check ( indexes ) : kinds = list ( { type ( index ) for index in indexes } ) if list in kinds : if len ( kinds ) > 1 : indexes = [ Index ( com . try_sort ( x ) ) if not isinstance ( x , Index ) else x for x in indexes ] kinds . remove ( list ) else : return indexes , 'list' if len ( kinds ) > 1 or Index not in kinds : return indexes , 'special' else : return indexes , 'array'\n\n\n        Summary:", "input": "", "output": "Verify the type of indexes and convert lists to Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_consensus_names ( indexes ) : # find the non-none names, need to tupleify to make # the set hashable, then reverse on return consensus_names = { tuple ( i . names ) for i in indexes if com . _any_not_none ( * i . names ) } if len ( consensus_names ) == 1 : return list ( list ( consensus_names ) [ 0 ] ) return [ None ] * indexes [ 0 ] . nlevels\n\n\n        Summary:", "input": "", "output": "Give a consensus names to indexes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _all_indexes_same ( indexes ) : first = indexes [ 0 ] for index in indexes [ 1 : ] : if not first . equals ( index ) : return False return True\n\n\n        Summary:", "input": "", "output": "Determine if all indexes contain the same elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_params ( sql , params ) : args = [ sql ] if params is not None : if hasattr ( params , 'keys' ) : # test if params is a mapping args += [ params ] else : args += [ list ( params ) ] return args\n\n\n        Summary:", "input": "", "output": "Convert SQL and params args to DBAPI2 . 0 compliant format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _process_parse_dates_argument ( parse_dates ) : # handle non-list entries for parse_dates gracefully if parse_dates is True or parse_dates is None or parse_dates is False : parse_dates = [ ] elif not hasattr ( parse_dates , '__iter__' ) : parse_dates = [ parse_dates ] return parse_dates\n\n\n        Summary:", "input": "", "output": "Process parse_dates argument for read_sql functions"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_date_columns ( data_frame , parse_dates ) : parse_dates = _process_parse_dates_argument ( parse_dates ) # we want to coerce datetime64_tz dtypes for now to UTC # we could in theory do a 'nice' conversion from a FixedOffset tz # GH11216 for col_name , df_col in data_frame . iteritems ( ) : if is_datetime64tz_dtype ( df_col ) or col_name in parse_dates : try : fmt = parse_dates [ col_name ] except TypeError : fmt = None data_frame [ col_name ] = _handle_date_column ( df_col , format = fmt ) return data_frame\n\n\n        Summary:", "input": "", "output": "Force non - datetime columns to be read as such . Supports both string formatted and integer timestamp columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_result ( data , columns , index_col = None , coerce_float = True , parse_dates = None ) : frame = DataFrame . from_records ( data , columns = columns , coerce_float = coerce_float ) frame = _parse_date_columns ( frame , parse_dates ) if index_col is not None : frame . set_index ( index_col , inplace = True ) return frame\n\n\n        Summary:", "input": "", "output": "Wrap result set of query in a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def execute ( sql , con , cur = None , params = None ) : if cur is None : pandas_sql = pandasSQL_builder ( con ) else : pandas_sql = pandasSQL_builder ( cur , is_cursor = True ) args = _convert_params ( sql , params ) return pandas_sql . execute ( * args )\n\n\n        Summary:", "input": "", "output": "Execute the given SQL query using the provided connection object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_sql_table ( table_name , con , schema = None , index_col = None , coerce_float = True , parse_dates = None , columns = None , chunksize = None ) : con = _engine_builder ( con ) if not _is_sqlalchemy_connectable ( con ) : raise NotImplementedError ( \"read_sql_table only supported for \" \"SQLAlchemy connectable.\" ) import sqlalchemy from sqlalchemy . schema import MetaData meta = MetaData ( con , schema = schema ) try : meta . reflect ( only = [ table_name ] , views = True ) except sqlalchemy . exc . InvalidRequestError : raise ValueError ( \"Table {name} not found\" . format ( name = table_name ) ) pandas_sql = SQLDatabase ( con , meta = meta ) table = pandas_sql . read_table ( table_name , index_col = index_col , coerce_float = coerce_float , parse_dates = parse_dates , columns = columns , chunksize = chunksize ) if table is not None : return table else : raise ValueError ( \"Table {name} not found\" . format ( name = table_name ) , con )\n\n\n        Summary:", "input": "", "output": "Read SQL database table into a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_sql_query ( sql , con , index_col = None , coerce_float = True , params = None , parse_dates = None , chunksize = None ) : pandas_sql = pandasSQL_builder ( con ) return pandas_sql . read_query ( sql , index_col = index_col , params = params , coerce_float = coerce_float , parse_dates = parse_dates , chunksize = chunksize )\n\n\n        Summary:", "input": "", "output": "Read SQL query into a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_sql ( sql , con , index_col = None , coerce_float = True , params = None , parse_dates = None , columns = None , chunksize = None ) : pandas_sql = pandasSQL_builder ( con ) if isinstance ( pandas_sql , SQLiteDatabase ) : return pandas_sql . read_query ( sql , index_col = index_col , params = params , coerce_float = coerce_float , parse_dates = parse_dates , chunksize = chunksize ) try : _is_table_name = pandas_sql . has_table ( sql ) except Exception : # using generic exception to catch errors from sql drivers (GH24988) _is_table_name = False if _is_table_name : pandas_sql . meta . reflect ( only = [ sql ] ) return pandas_sql . read_table ( sql , index_col = index_col , coerce_float = coerce_float , parse_dates = parse_dates , columns = columns , chunksize = chunksize ) else : return pandas_sql . read_query ( sql , index_col = index_col , params = params , coerce_float = coerce_float , parse_dates = parse_dates , chunksize = chunksize )\n\n\n        Summary:", "input": "", "output": "Read SQL query or database table into a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_sql ( frame , name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None ) : if if_exists not in ( 'fail' , 'replace' , 'append' ) : raise ValueError ( \"'{0}' is not valid for if_exists\" . format ( if_exists ) ) pandas_sql = pandasSQL_builder ( con , schema = schema ) if isinstance ( frame , Series ) : frame = frame . to_frame ( ) elif not isinstance ( frame , DataFrame ) : raise NotImplementedError ( \"'frame' argument should be either a \" \"Series or a DataFrame\" ) pandas_sql . to_sql ( frame , name , if_exists = if_exists , index = index , index_label = index_label , schema = schema , chunksize = chunksize , dtype = dtype , method = method )\n\n\n        Summary:", "input": "", "output": "Write records stored in a DataFrame to a SQL database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def has_table ( table_name , con , schema = None ) : pandas_sql = pandasSQL_builder ( con , schema = schema ) return pandas_sql . has_table ( table_name )\n\n\n        Summary:", "input": "", "output": "Check if DataBase has named table ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _engine_builder ( con ) : global _SQLALCHEMY_INSTALLED if isinstance ( con , str ) : try : import sqlalchemy except ImportError : _SQLALCHEMY_INSTALLED = False else : con = sqlalchemy . create_engine ( con ) return con return con\n\n\n        Summary:", "input": "", "output": "Returns a SQLAlchemy engine from a URI ( if con is a string ) else it just return con without modifying it ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pandasSQL_builder ( con , schema = None , meta = None , is_cursor = False ) : # When support for DBAPI connections is removed, # is_cursor should not be necessary. con = _engine_builder ( con ) if _is_sqlalchemy_connectable ( con ) : return SQLDatabase ( con , schema = schema , meta = meta ) elif isinstance ( con , str ) : raise ImportError ( \"Using URI string without sqlalchemy installed.\" ) else : return SQLiteDatabase ( con , is_cursor = is_cursor )\n\n\n        Summary:", "input": "", "output": "Convenience function to return the correct PandasSQL subclass based on the provided parameters ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_schema ( frame , name , keys = None , con = None , dtype = None ) : pandas_sql = pandasSQL_builder ( con = con ) return pandas_sql . _create_sql_schema ( frame , name , keys = keys , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Get the SQL db table schema for the given frame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _execute_insert ( self , conn , keys , data_iter ) : data = [ dict ( zip ( keys , row ) ) for row in data_iter ] conn . execute ( self . table . insert ( ) , data )\n\n\n        Summary:", "input": "", "output": "Execute SQL statement inserting data"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _query_iterator ( self , result , chunksize , columns , coerce_float = True , parse_dates = None ) : while True : data = result . fetchmany ( chunksize ) if not data : break else : self . frame = DataFrame . from_records ( data , columns = columns , coerce_float = coerce_float ) self . _harmonize_columns ( parse_dates = parse_dates ) if self . index is not None : self . frame . set_index ( self . index , inplace = True ) yield self . frame\n\n\n        Summary:", "input": "", "output": "Return generator through chunked result set ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _harmonize_columns ( self , parse_dates = None ) : parse_dates = _process_parse_dates_argument ( parse_dates ) for sql_col in self . table . columns : col_name = sql_col . name try : df_col = self . frame [ col_name ] # Handle date parsing upfront; don't try to convert columns # twice if col_name in parse_dates : try : fmt = parse_dates [ col_name ] except TypeError : fmt = None self . frame [ col_name ] = _handle_date_column ( df_col , format = fmt ) continue # the type the dataframe column should have col_type = self . _get_dtype ( sql_col . type ) if ( col_type is datetime or col_type is date or col_type is DatetimeTZDtype ) : # Convert tz-aware Datetime SQL columns to UTC utc = col_type is DatetimeTZDtype self . frame [ col_name ] = _handle_date_column ( df_col , utc = utc ) elif col_type is float : # floats support NA, can always convert! self . frame [ col_name ] = df_col . astype ( col_type , copy = False ) elif len ( df_col ) == df_col . count ( ) : # No NA values, can convert ints and bools if col_type is np . dtype ( 'int64' ) or col_type is bool : self . frame [ col_name ] = df_col . astype ( col_type , copy = False ) except KeyError : pass\n\n\n        Summary:", "input": "", "output": "Make the DataFrame s column types align with the SQL table column types . Need to work around limited NA value support . Floats are always fine ints must always be floats if there are Null values . Booleans are hard because converting bool column with None replaces all Nones with false . Therefore only convert bool if there are no NA values . Datetimes should already be converted to np . datetime64 if supported but here we also force conversion if required ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_table ( self , table_name , index_col = None , coerce_float = True , parse_dates = None , columns = None , schema = None , chunksize = None ) : table = SQLTable ( table_name , self , index = index_col , schema = schema ) return table . read ( coerce_float = coerce_float , parse_dates = parse_dates , columns = columns , chunksize = chunksize )\n\n\n        Summary:", "input": "", "output": "Read SQL database table into a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _query_iterator ( result , chunksize , columns , index_col = None , coerce_float = True , parse_dates = None ) : while True : data = result . fetchmany ( chunksize ) if not data : break else : yield _wrap_result ( data , columns , index_col = index_col , coerce_float = coerce_float , parse_dates = parse_dates )\n\n\n        Summary:", "input": "", "output": "Return generator through chunked result set"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_query ( self , sql , index_col = None , coerce_float = True , parse_dates = None , params = None , chunksize = None ) : args = _convert_params ( sql , params ) result = self . execute ( * args ) columns = result . keys ( ) if chunksize is not None : return self . _query_iterator ( result , chunksize , columns , index_col = index_col , coerce_float = coerce_float , parse_dates = parse_dates ) else : data = result . fetchall ( ) frame = _wrap_result ( data , columns , index_col = index_col , coerce_float = coerce_float , parse_dates = parse_dates ) return frame\n\n\n        Summary:", "input": "", "output": "Read SQL query into a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_sql ( self , frame , name , if_exists = 'fail' , index = True , index_label = None , schema = None , chunksize = None , dtype = None , method = None ) : if dtype and not is_dict_like ( dtype ) : dtype = { col_name : dtype for col_name in frame } if dtype is not None : from sqlalchemy . types import to_instance , TypeEngine for col , my_type in dtype . items ( ) : if not isinstance ( to_instance ( my_type ) , TypeEngine ) : raise ValueError ( 'The type of {column} is not a ' 'SQLAlchemy type ' . format ( column = col ) ) table = SQLTable ( name , self , frame = frame , index = index , if_exists = if_exists , index_label = index_label , schema = schema , dtype = dtype ) table . create ( ) table . insert ( chunksize , method = method ) if ( not name . isdigit ( ) and not name . islower ( ) ) : # check for potentially case sensitivity issues (GH7815) # Only check when name is not a number and name is not lower case engine = self . connectable . engine with self . connectable . connect ( ) as conn : table_names = engine . table_names ( schema = schema or self . meta . schema , connection = conn , ) if name not in table_names : msg = ( \"The provided table name '{0}' is not found exactly as \" \"such in the database after writing the table, possibly \" \"due to case sensitivity issues. Consider using lower \" \"case table names.\" ) . format ( name ) warnings . warn ( msg , UserWarning )\n\n\n        Summary:", "input": "", "output": "Write records stored in a DataFrame to a SQL database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_table_setup ( self ) : column_names_and_types = self . _get_column_names_and_types ( self . _sql_type_name ) pat = re . compile ( r'\\s+' ) column_names = [ col_name for col_name , _ , _ in column_names_and_types ] if any ( map ( pat . search , column_names ) ) : warnings . warn ( _SAFE_NAMES_WARNING , stacklevel = 6 ) escape = _get_valid_sqlite_name create_tbl_stmts = [ escape ( cname ) + ' ' + ctype for cname , ctype , _ in column_names_and_types ] if self . keys is not None and len ( self . keys ) : if not is_list_like ( self . keys ) : keys = [ self . keys ] else : keys = self . keys cnames_br = \", \" . join ( escape ( c ) for c in keys ) create_tbl_stmts . append ( \"CONSTRAINT {tbl}_pk PRIMARY KEY ({cnames_br})\" . format ( tbl = self . name , cnames_br = cnames_br ) ) create_stmts = [ \"CREATE TABLE \" + escape ( self . name ) + \" (\\n\" + ',\\n ' . join ( create_tbl_stmts ) + \"\\n)\" ] ix_cols = [ cname for cname , _ , is_index in column_names_and_types if is_index ] if len ( ix_cols ) : cnames = \"_\" . join ( ix_cols ) cnames_br = \",\" . join ( escape ( c ) for c in ix_cols ) create_stmts . append ( \"CREATE INDEX \" + escape ( \"ix_\" + self . name + \"_\" + cnames ) + \"ON \" + escape ( self . name ) + \" (\" + cnames_br + \")\" ) return create_stmts\n\n\n        Summary:", "input": "", "output": "Return a list of SQL statements that creates a table reflecting the structure of a DataFrame . The first entry will be a CREATE TABLE statement while the rest will be CREATE INDEX statements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _query_iterator ( cursor , chunksize , columns , index_col = None , coerce_float = True , parse_dates = None ) : while True : data = cursor . fetchmany ( chunksize ) if type ( data ) == tuple : data = list ( data ) if not data : cursor . close ( ) break else : yield _wrap_result ( data , columns , index_col = index_col , coerce_float = coerce_float , parse_dates = parse_dates )\n\n\n        Summary:", "input": "", "output": "Return generator through chunked result set"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_sql ( self , frame , name , if_exists = 'fail' , index = True , index_label = None , schema = None , chunksize = None , dtype = None , method = None ) : if dtype and not is_dict_like ( dtype ) : dtype = { col_name : dtype for col_name in frame } if dtype is not None : for col , my_type in dtype . items ( ) : if not isinstance ( my_type , str ) : raise ValueError ( '{column} ({type!s}) not a string' . format ( column = col , type = my_type ) ) table = SQLiteTable ( name , self , frame = frame , index = index , if_exists = if_exists , index_label = index_label , dtype = dtype ) table . create ( ) table . insert ( chunksize , method )\n\n\n        Summary:", "input": "", "output": "Write records stored in a DataFrame to a SQL database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_to_categorical ( array ) : if isinstance ( array , ( ABCSeries , ABCCategoricalIndex ) ) : return array . _values elif isinstance ( array , np . ndarray ) : return Categorical ( array ) return array\n\n\n        Summary:", "input": "", "output": "Coerce to a categorical if a series is given ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def contains ( cat , key , container ) : hash ( key ) # get location of key in categories. # If a KeyError, the key isn't in categories, so logically # can't be in container either. try : loc = cat . categories . get_loc ( key ) except KeyError : return False # loc is the location of key in categories, but also the *value* # for key in container. So, `key` may be in categories, # but still not in `container`. Example ('b' in categories, # but not in values): # 'b' in Categorical(['a'], categories=['a', 'b']) # False if is_scalar ( loc ) : return loc in container else : # if categories is an IntervalIndex, loc is an array. return any ( loc_ in container for loc_ in loc )\n\n\n        Summary:", "input": "", "output": "Helper for membership check for key in cat ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_codes_for_values ( values , categories ) : from pandas . core . algorithms import _get_data_algo , _hashtables dtype_equal = is_dtype_equal ( values . dtype , categories . dtype ) if dtype_equal : # To prevent erroneous dtype coercion in _get_data_algo, retrieve # the underlying numpy array. gh-22702 values = getattr ( values , '_ndarray_values' , values ) categories = getattr ( categories , '_ndarray_values' , categories ) elif ( is_extension_array_dtype ( categories . dtype ) and is_object_dtype ( values ) ) : # Support inferring the correct extension dtype from an array of # scalar objects. e.g. # Categorical(array[Period, Period], categories=PeriodIndex(...)) try : values = ( categories . dtype . construct_array_type ( ) . _from_sequence ( values ) ) except Exception : # but that may fail for any reason, so fall back to object values = ensure_object ( values ) categories = ensure_object ( categories ) else : values = ensure_object ( values ) categories = ensure_object ( categories ) ( hash_klass , vec_klass ) , vals = _get_data_algo ( values , _hashtables ) ( _ , _ ) , cats = _get_data_algo ( categories , _hashtables ) t = hash_klass ( len ( cats ) ) t . map_locations ( cats ) return coerce_indexer_dtype ( t . lookup ( vals ) , cats )\n\n\n        Summary:", "input": "", "output": "utility routine to turn values into codes given the specified categories"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _recode_for_categories ( codes , old_categories , new_categories ) : from pandas . core . algorithms import take_1d if len ( old_categories ) == 0 : # All null anyway, so just retain the nulls return codes . copy ( ) elif new_categories . equals ( old_categories ) : # Same categories, so no need to actually recode return codes . copy ( ) indexer = coerce_indexer_dtype ( new_categories . get_indexer ( old_categories ) , new_categories ) new_codes = take_1d ( indexer , codes . copy ( ) , fill_value = - 1 ) return new_codes\n\n\n        Summary:", "input": "", "output": "Convert a set of codes for to a new set of categories"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _factorize_from_iterable ( values ) : from pandas . core . indexes . category import CategoricalIndex if not is_list_like ( values ) : raise TypeError ( \"Input must be list-like\" ) if is_categorical ( values ) : if isinstance ( values , ( ABCCategoricalIndex , ABCSeries ) ) : values = values . _values categories = CategoricalIndex ( values . categories , dtype = values . dtype ) codes = values . codes else : # The value of ordered is irrelevant since we don't use cat as such, # but only the resulting categories, the order of which is independent # from ordered. Set ordered to False as default. See GH #15457 cat = Categorical ( values , ordered = False ) categories = cat . categories codes = cat . codes return codes , categories\n\n\n        Summary:", "input": "", "output": "Factorize an input values into categories and codes . Preserves categorical dtype in categories ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _factorize_from_iterables ( iterables ) : if len ( iterables ) == 0 : # For consistency, it should return a list of 2 lists. return [ [ ] , [ ] ] return map ( list , lzip ( * [ _factorize_from_iterable ( it ) for it in iterables ] ) )\n\n\n        Summary:", "input": "", "output": "A higher - level wrapper over _factorize_from_iterable ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self ) : return self . _constructor ( values = self . _codes . copy ( ) , dtype = self . dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Copy constructor ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def astype ( self , dtype , copy = True ) : if is_categorical_dtype ( dtype ) : # GH 10696/18593 dtype = self . dtype . update_dtype ( dtype ) self = self . copy ( ) if copy else self if dtype == self . dtype : return self return self . _set_dtype ( dtype ) return np . array ( self , dtype = dtype , copy = copy )\n\n\n        Summary:", "input": "", "output": "Coerce this type to another dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_inferred_categories ( cls , inferred_categories , inferred_codes , dtype , true_values = None ) : from pandas import Index , to_numeric , to_datetime , to_timedelta cats = Index ( inferred_categories ) known_categories = ( isinstance ( dtype , CategoricalDtype ) and dtype . categories is not None ) if known_categories : # Convert to a specialized type with `dtype` if specified. if dtype . categories . is_numeric ( ) : cats = to_numeric ( inferred_categories , errors = \"coerce\" ) elif is_datetime64_dtype ( dtype . categories ) : cats = to_datetime ( inferred_categories , errors = \"coerce\" ) elif is_timedelta64_dtype ( dtype . categories ) : cats = to_timedelta ( inferred_categories , errors = \"coerce\" ) elif dtype . categories . is_boolean ( ) : if true_values is None : true_values = [ \"True\" , \"TRUE\" , \"true\" ] cats = cats . isin ( true_values ) if known_categories : # Recode from observation order to dtype.categories order. categories = dtype . categories codes = _recode_for_categories ( inferred_codes , cats , categories ) elif not cats . is_monotonic_increasing : # Sort categories and recode for unknown categories. unsorted = cats . copy ( ) categories = cats . sort_values ( ) codes = _recode_for_categories ( inferred_codes , unsorted , categories ) dtype = CategoricalDtype ( categories , ordered = False ) else : dtype = CategoricalDtype ( cats , ordered = False ) codes = inferred_codes return cls ( codes , dtype = dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Construct a Categorical from inferred values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_codes ( cls , codes , categories = None , ordered = None , dtype = None ) : dtype = CategoricalDtype . _from_values_or_dtype ( categories = categories , ordered = ordered , dtype = dtype ) if dtype . categories is None : msg = ( \"The categories must be provided in 'categories' or \" \"'dtype'. Both were None.\" ) raise ValueError ( msg ) codes = np . asarray ( codes ) # #21767 if not is_integer_dtype ( codes ) : msg = \"codes need to be array-like integers\" if is_float_dtype ( codes ) : icodes = codes . astype ( 'i8' ) if ( icodes == codes ) . all ( ) : msg = None codes = icodes warn ( ( \"float codes will be disallowed in the future and \" \"raise a ValueError\" ) , FutureWarning , stacklevel = 2 ) if msg : raise ValueError ( msg ) if len ( codes ) and ( codes . max ( ) >= len ( dtype . categories ) or codes . min ( ) < - 1 ) : raise ValueError ( \"codes need to be between -1 and \" \"len(categories)-1\" ) return cls ( codes , dtype = dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Make a Categorical type from codes and categories or dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_codes ( self ) : v = self . _codes . view ( ) v . flags . writeable = False return v\n\n\n        Summary:", "input": "", "output": "Get the codes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_categories ( self , categories , fastpath = False ) : if fastpath : new_dtype = CategoricalDtype . _from_fastpath ( categories , self . ordered ) else : new_dtype = CategoricalDtype ( categories , ordered = self . ordered ) if ( not fastpath and self . dtype . categories is not None and len ( new_dtype . categories ) != len ( self . dtype . categories ) ) : raise ValueError ( \"new categories need to have the same number of \" \"items than the old categories!\" ) self . _dtype = new_dtype\n\n\n        Summary:", "input": "", "output": "Sets new categories inplace"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_dtype ( self , dtype ) : codes = _recode_for_categories ( self . codes , self . categories , dtype . categories ) return type ( self ) ( codes , dtype = dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Internal method for directly updating the CategoricalDtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_ordered ( self , value , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) new_dtype = CategoricalDtype ( self . categories , ordered = value ) cat = self if inplace else self . copy ( ) cat . _dtype = new_dtype if not inplace : return cat\n\n\n        Summary:", "input": "", "output": "Set the ordered attribute to the boolean value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def as_ordered ( self , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) return self . set_ordered ( True , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Set the Categorical to be ordered ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def as_unordered ( self , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) return self . set_ordered ( False , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Set the Categorical to be unordered ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_categories ( self , new_categories , ordered = None , rename = False , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if ordered is None : ordered = self . dtype . ordered new_dtype = CategoricalDtype ( new_categories , ordered = ordered ) cat = self if inplace else self . copy ( ) if rename : if ( cat . dtype . categories is not None and len ( new_dtype . categories ) < len ( cat . dtype . categories ) ) : # remove all _codes which are larger and set to -1/NaN cat . _codes [ cat . _codes >= len ( new_dtype . categories ) ] = - 1 else : codes = _recode_for_categories ( cat . codes , cat . categories , new_dtype . categories ) cat . _codes = codes cat . _dtype = new_dtype if not inplace : return cat\n\n\n        Summary:", "input": "", "output": "Set the categories to the specified new_categories ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename_categories ( self , new_categories , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) cat = self if inplace else self . copy ( ) if isinstance ( new_categories , ABCSeries ) : msg = ( \"Treating Series 'new_categories' as a list-like and using \" \"the values. In a future version, 'rename_categories' will \" \"treat Series like a dictionary.\\n\" \"For dict-like, use 'new_categories.to_dict()'\\n\" \"For list-like, use 'new_categories.values'.\" ) warn ( msg , FutureWarning , stacklevel = 2 ) new_categories = list ( new_categories ) if is_dict_like ( new_categories ) : cat . categories = [ new_categories . get ( item , item ) for item in cat . categories ] elif callable ( new_categories ) : cat . categories = [ new_categories ( item ) for item in cat . categories ] else : cat . categories = new_categories if not inplace : return cat\n\n\n        Summary:", "input": "", "output": "Rename categories ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reorder_categories ( self , new_categories , ordered = None , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if set ( self . dtype . categories ) != set ( new_categories ) : raise ValueError ( \"items in new_categories are not the same as in \" \"old categories\" ) return self . set_categories ( new_categories , ordered = ordered , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Reorder categories as specified in new_categories ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_categories ( self , new_categories , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if not is_list_like ( new_categories ) : new_categories = [ new_categories ] already_included = set ( new_categories ) & set ( self . dtype . categories ) if len ( already_included ) != 0 : msg = ( \"new categories must not include old categories: \" \"{already_included!s}\" ) raise ValueError ( msg . format ( already_included = already_included ) ) new_categories = list ( self . dtype . categories ) + list ( new_categories ) new_dtype = CategoricalDtype ( new_categories , self . ordered ) cat = self if inplace else self . copy ( ) cat . _dtype = new_dtype cat . _codes = coerce_indexer_dtype ( cat . _codes , new_dtype . categories ) if not inplace : return cat\n\n\n        Summary:", "input": "", "output": "Add new categories ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def remove_categories ( self , removals , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if not is_list_like ( removals ) : removals = [ removals ] removal_set = set ( list ( removals ) ) not_included = removal_set - set ( self . dtype . categories ) new_categories = [ c for c in self . dtype . categories if c not in removal_set ] # GH 10156 if any ( isna ( removals ) ) : not_included = [ x for x in not_included if notna ( x ) ] new_categories = [ x for x in new_categories if notna ( x ) ] if len ( not_included ) != 0 : msg = \"removals must all be in old categories: {not_included!s}\" raise ValueError ( msg . format ( not_included = not_included ) ) return self . set_categories ( new_categories , ordered = self . ordered , rename = False , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Remove the specified categories ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def remove_unused_categories ( self , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) cat = self if inplace else self . copy ( ) idx , inv = np . unique ( cat . _codes , return_inverse = True ) if idx . size != 0 and idx [ 0 ] == - 1 : # na sentinel idx , inv = idx [ 1 : ] , inv - 1 new_categories = cat . dtype . categories . take ( idx ) new_dtype = CategoricalDtype . _from_fastpath ( new_categories , ordered = self . ordered ) cat . _dtype = new_dtype cat . _codes = coerce_indexer_dtype ( inv , new_dtype . categories ) if not inplace : return cat\n\n\n        Summary:", "input": "", "output": "Remove categories which are not used ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map ( self , mapper ) : new_categories = self . categories . map ( mapper ) try : return self . from_codes ( self . _codes . copy ( ) , categories = new_categories , ordered = self . ordered ) except ValueError : # NA values are represented in self._codes with -1 # np.take causes NA values to take final element in new_categories if np . any ( self . _codes == - 1 ) : new_categories = new_categories . insert ( len ( new_categories ) , np . nan ) return np . take ( new_categories , self . _codes )\n\n\n        Summary:", "input": "", "output": "Map categories using input correspondence ( dict Series or function ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods , fill_value = None ) : # since categoricals always have ndim == 1, an axis parameter # doesn't make any sense here. codes = self . codes if codes . ndim > 1 : raise NotImplementedError ( \"Categorical with ndim > 1.\" ) if np . prod ( codes . shape ) and ( periods != 0 ) : codes = np . roll ( codes , ensure_platform_int ( periods ) , axis = 0 ) if isna ( fill_value ) : fill_value = - 1 elif fill_value in self . categories : fill_value = self . categories . get_loc ( fill_value ) else : raise ValueError ( \"'fill_value={}' is not present \" \"in this Categorical's \" \"categories\" . format ( fill_value ) ) if periods > 0 : codes [ : periods ] = fill_value else : codes [ periods : ] = fill_value return self . from_codes ( codes , dtype = self . dtype )\n\n\n        Summary:", "input": "", "output": "Shift Categorical by desired number of periods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def memory_usage ( self , deep = False ) : return self . _codes . nbytes + self . dtype . categories . memory_usage ( deep = deep )\n\n\n        Summary:", "input": "", "output": "Memory usage of my values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def value_counts ( self , dropna = True ) : from numpy import bincount from pandas import Series , CategoricalIndex code , cat = self . _codes , self . categories ncat , mask = len ( cat ) , 0 <= code ix , clean = np . arange ( ncat ) , mask . all ( ) if dropna or clean : obs = code if clean else code [ mask ] count = bincount ( obs , minlength = ncat or None ) else : count = bincount ( np . where ( mask , code , ncat ) ) ix = np . append ( ix , - 1 ) ix = self . _constructor ( ix , dtype = self . dtype , fastpath = True ) return Series ( count , index = CategoricalIndex ( ix ) , dtype = 'int64' )\n\n\n        Summary:", "input": "", "output": "Return a Series containing counts of each category ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_values ( self ) : # if we are a datetime and period index, return Index to keep metadata if is_datetimelike ( self . categories ) : return self . categories . take ( self . _codes , fill_value = np . nan ) elif is_integer_dtype ( self . categories ) and - 1 in self . _codes : return self . categories . astype ( \"object\" ) . take ( self . _codes , fill_value = np . nan ) return np . array ( self )\n\n\n        Summary:", "input": "", "output": "Return the values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_values ( self , inplace = False , ascending = True , na_position = 'last' ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if na_position not in [ 'last' , 'first' ] : msg = 'invalid na_position: {na_position!r}' raise ValueError ( msg . format ( na_position = na_position ) ) sorted_idx = nargsort ( self , ascending = ascending , na_position = na_position ) if inplace : self . _codes = self . _codes [ sorted_idx ] else : return self . _constructor ( values = self . _codes [ sorted_idx ] , dtype = self . dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Sort the Categorical by category value returning a new Categorical by default ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _values_for_rank ( self ) : from pandas import Series if self . ordered : values = self . codes mask = values == - 1 if mask . any ( ) : values = values . astype ( 'float64' ) values [ mask ] = np . nan elif self . categories . is_numeric ( ) : values = np . array ( self ) else : # reorder the categories (so rank can use the float codes) # instead of passing an object array to rank values = np . array ( self . rename_categories ( Series ( self . categories ) . rank ( ) . values ) ) return values\n\n\n        Summary:", "input": "", "output": "For correctly ranking ordered categorical data . See GH#15420"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fillna ( self , value = None , method = None , limit = None ) : value , method = validate_fillna_kwargs ( value , method , validate_scalar_dict_value = False ) if value is None : value = np . nan if limit is not None : raise NotImplementedError ( \"specifying a limit for fillna has not \" \"been implemented yet\" ) codes = self . _codes # pad / bfill if method is not None : values = self . to_dense ( ) . reshape ( - 1 , len ( self ) ) values = interpolate_2d ( values , method , 0 , None , value ) . astype ( self . categories . dtype ) [ 0 ] codes = _get_codes_for_values ( values , self . categories ) else : # If value is a dict or a Series (a dict value has already # been converted to a Series) if isinstance ( value , ABCSeries ) : if not value [ ~ value . isin ( self . categories ) ] . isna ( ) . all ( ) : raise ValueError ( \"fill value must be in categories\" ) values_codes = _get_codes_for_values ( value , self . categories ) indexer = np . where ( values_codes != - 1 ) codes [ indexer ] = values_codes [ values_codes != - 1 ] # If value is not a dict or Series it should be a scalar elif is_hashable ( value ) : if not isna ( value ) and value not in self . categories : raise ValueError ( \"fill value must be in categories\" ) mask = codes == - 1 if mask . any ( ) : codes = codes . copy ( ) if isna ( value ) : codes [ mask ] = - 1 else : codes [ mask ] = self . categories . get_loc ( value ) else : raise TypeError ( '\"value\" parameter must be a scalar, dict ' 'or Series, but you passed a ' '\"{0}\"' . format ( type ( value ) . __name__ ) ) return self . _constructor ( codes , dtype = self . dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Fill NA / NaN values using the specified method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take_nd ( self , indexer , allow_fill = None , fill_value = None ) : indexer = np . asarray ( indexer , dtype = np . intp ) if allow_fill is None : if ( indexer < 0 ) . any ( ) : warn ( _take_msg , FutureWarning , stacklevel = 2 ) allow_fill = True dtype = self . dtype if isna ( fill_value ) : fill_value = - 1 elif allow_fill : # convert user-provided `fill_value` to codes if fill_value in self . categories : fill_value = self . categories . get_loc ( fill_value ) else : msg = ( \"'fill_value' ('{}') is not in this Categorical's \" \"categories.\" ) raise TypeError ( msg . format ( fill_value ) ) codes = take ( self . _codes , indexer , allow_fill = allow_fill , fill_value = fill_value ) result = type ( self ) . from_codes ( codes , dtype = dtype ) return result\n\n\n        Summary:", "input": "", "output": "Take elements from the Categorical ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _slice ( self , slicer ) : # only allow 1 dimensional slicing, but can # in a 2-d case be passd (slice(None),....) if isinstance ( slicer , tuple ) and len ( slicer ) == 2 : if not com . is_null_slice ( slicer [ 0 ] ) : raise AssertionError ( \"invalid slicing for a 1-ndim \" \"categorical\" ) slicer = slicer [ 1 ] codes = self . _codes [ slicer ] return self . _constructor ( values = codes , dtype = self . dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Return a slice of myself ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _tidy_repr ( self , max_vals = 10 , footer = True ) : num = max_vals // 2 head = self [ : num ] . _get_repr ( length = False , footer = False ) tail = self [ - ( max_vals - num ) : ] . _get_repr ( length = False , footer = False ) result = '{head}, ..., {tail}' . format ( head = head [ : - 1 ] , tail = tail [ 1 : ] ) if footer : result = '{result}\\n{footer}' . format ( result = result , footer = self . _repr_footer ( ) ) return str ( result )\n\n\n        Summary:", "input": "", "output": "a short repr displaying only max_vals and an optional ( but default footer )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _repr_categories ( self ) : max_categories = ( 10 if get_option ( \"display.max_categories\" ) == 0 else get_option ( \"display.max_categories\" ) ) from pandas . io . formats import format as fmt if len ( self . categories ) > max_categories : num = max_categories // 2 head = fmt . format_array ( self . categories [ : num ] , None ) tail = fmt . format_array ( self . categories [ - num : ] , None ) category_strs = head + [ \"...\" ] + tail else : category_strs = fmt . format_array ( self . categories , None ) # Strip all leading spaces, which format_array adds for columns... category_strs = [ x . strip ( ) for x in category_strs ] return category_strs\n\n\n        Summary:", "input": "", "output": "return the base repr for the categories"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _repr_categories_info ( self ) : category_strs = self . _repr_categories ( ) dtype = getattr ( self . categories , 'dtype_str' , str ( self . categories . dtype ) ) levheader = \"Categories ({length}, {dtype}): \" . format ( length = len ( self . categories ) , dtype = dtype ) width , height = get_terminal_size ( ) max_width = get_option ( \"display.width\" ) or width if console . in_ipython_frontend ( ) : # 0 = no breaks max_width = 0 levstring = \"\" start = True cur_col_len = len ( levheader ) # header sep_len , sep = ( 3 , \" < \" ) if self . ordered else ( 2 , \", \" ) linesep = sep . rstrip ( ) + \"\\n\" # remove whitespace for val in category_strs : if max_width != 0 and cur_col_len + sep_len + len ( val ) > max_width : levstring += linesep + ( \" \" * ( len ( levheader ) + 1 ) ) cur_col_len = len ( levheader ) + 1 # header + a whitespace elif not start : levstring += sep cur_col_len += len ( val ) levstring += val start = False # replace to simple save space by return levheader + \"[\" + levstring . replace ( \" < ... < \" , \" ... \" ) + \"]\"\n\n\n        Summary:", "input": "", "output": "Returns a string representation of the footer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_coerce_indexer ( self , indexer ) : if isinstance ( indexer , np . ndarray ) and indexer . dtype . kind == 'i' : indexer = indexer . astype ( self . _codes . dtype ) return indexer\n\n\n        Summary:", "input": "", "output": "return an indexer coerced to the codes dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reverse_indexer ( self ) : categories = self . categories r , counts = libalgos . groupsort_indexer ( self . codes . astype ( 'int64' ) , categories . size ) counts = counts . cumsum ( ) result = ( r [ start : end ] for start , end in zip ( counts , counts [ 1 : ] ) ) result = dict ( zip ( categories , result ) ) return result\n\n\n        Summary:", "input": "", "output": "Compute the inverse of a categorical returning a dict of categories - > indexers ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def min ( self , numeric_only = None , * * kwargs ) : self . check_for_ordered ( 'min' ) if numeric_only : good = self . _codes != - 1 pointer = self . _codes [ good ] . min ( * * kwargs ) else : pointer = self . _codes . min ( * * kwargs ) if pointer == - 1 : return np . nan else : return self . categories [ pointer ]\n\n\n        Summary:", "input": "", "output": "The minimum value of the object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mode ( self , dropna = True ) : import pandas . _libs . hashtable as htable codes = self . _codes if dropna : good = self . _codes != - 1 codes = self . _codes [ good ] codes = sorted ( htable . mode_int64 ( ensure_int64 ( codes ) , dropna ) ) return self . _constructor ( values = codes , dtype = self . dtype , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Returns the mode ( s ) of the Categorical ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unique ( self ) : # unlike np.unique, unique1d does not sort unique_codes = unique1d ( self . codes ) cat = self . copy ( ) # keep nan in codes cat . _codes = unique_codes # exclude nan from indexer for categories take_codes = unique_codes [ unique_codes != - 1 ] if self . ordered : take_codes = np . sort ( take_codes ) return cat . set_categories ( cat . categories . take ( take_codes ) )\n\n\n        Summary:", "input": "", "output": "Return the Categorical which categories and codes are unique . Unused categories are NOT returned ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def equals ( self , other ) : if self . is_dtype_equal ( other ) : if self . categories . equals ( other . categories ) : # fastpath to avoid re-coding other_codes = other . _codes else : other_codes = _recode_for_categories ( other . codes , other . categories , self . categories ) return np . array_equal ( self . _codes , other_codes ) return False\n\n\n        Summary:", "input": "", "output": "Returns True if categorical arrays are equal ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_dtype_equal ( self , other ) : try : return hash ( self . dtype ) == hash ( other . dtype ) except ( AttributeError , TypeError ) : return False\n\n\n        Summary:", "input": "", "output": "Returns True if categoricals are the same dtype same categories and same ordered"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def describe ( self ) : counts = self . value_counts ( dropna = False ) freqs = counts / float ( counts . sum ( ) ) from pandas . core . reshape . concat import concat result = concat ( [ counts , freqs ] , axis = 1 ) result . columns = [ 'counts' , 'freqs' ] result . index . name = 'categories' return result\n\n\n        Summary:", "input": "", "output": "Describes this Categorical"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isin ( self , values ) : from pandas . core . internals . construction import sanitize_array if not is_list_like ( values ) : raise TypeError ( \"only list-like objects are allowed to be passed\" \" to isin(), you passed a [{values_type}]\" . format ( values_type = type ( values ) . __name__ ) ) values = sanitize_array ( values , None , None ) null_mask = np . asarray ( isna ( values ) ) code_values = self . categories . get_indexer ( values ) code_values = code_values [ null_mask | ( code_values >= 0 ) ] return algorithms . isin ( self . codes , code_values )\n\n\n        Summary:", "input": "", "output": "Check whether values are contained in Categorical ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_timedelta ( arg , unit = 'ns' , box = True , errors = 'raise' ) : unit = parse_timedelta_unit ( unit ) if errors not in ( 'ignore' , 'raise' , 'coerce' ) : raise ValueError ( \"errors must be one of 'ignore', \" \"'raise', or 'coerce'}\" ) if unit in { 'Y' , 'y' , 'M' } : warnings . warn ( \"M and Y units are deprecated and \" \"will be removed in a future version.\" , FutureWarning , stacklevel = 2 ) if arg is None : return arg elif isinstance ( arg , ABCSeries ) : values = _convert_listlike ( arg . _values , unit = unit , box = False , errors = errors ) return arg . _constructor ( values , index = arg . index , name = arg . name ) elif isinstance ( arg , ABCIndexClass ) : return _convert_listlike ( arg , unit = unit , box = box , errors = errors , name = arg . name ) elif isinstance ( arg , np . ndarray ) and arg . ndim == 0 : # extract array scalar and process below arg = arg . item ( ) elif is_list_like ( arg ) and getattr ( arg , 'ndim' , 1 ) == 1 : return _convert_listlike ( arg , unit = unit , box = box , errors = errors ) elif getattr ( arg , 'ndim' , 1 ) > 1 : raise TypeError ( 'arg must be a string, timedelta, list, tuple, ' '1-d array, or Series' ) # ...so it must be a scalar value. Return scalar. return _coerce_scalar_to_timedelta_type ( arg , unit = unit , box = box , errors = errors )\n\n\n        Summary:", "input": "", "output": "Convert argument to timedelta ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _coerce_scalar_to_timedelta_type ( r , unit = 'ns' , box = True , errors = 'raise' ) : try : result = Timedelta ( r , unit ) if not box : # explicitly view as timedelta64 for case when result is pd.NaT result = result . asm8 . view ( 'timedelta64[ns]' ) except ValueError : if errors == 'raise' : raise elif errors == 'ignore' : return r # coerce result = NaT return result\n\n\n        Summary:", "input": "", "output": "Convert string r to a timedelta object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_listlike ( arg , unit = 'ns' , box = True , errors = 'raise' , name = None ) : if isinstance ( arg , ( list , tuple ) ) or not hasattr ( arg , 'dtype' ) : # This is needed only to ensure that in the case where we end up # returning arg (errors == \"ignore\"), and where the input is a # generator, we return a useful list-like instead of a # used-up generator arg = np . array ( list ( arg ) , dtype = object ) try : value = sequence_to_td64ns ( arg , unit = unit , errors = errors , copy = False ) [ 0 ] except ValueError : if errors == 'ignore' : return arg else : # This else-block accounts for the cases when errors='raise' # and errors='coerce'. If errors == 'raise', these errors # should be raised. If errors == 'coerce', we shouldn't # expect any errors to be raised, since all parsing errors # cause coercion to pd.NaT. However, if an error / bug is # introduced that causes an Exception to be raised, we would # like to surface it. raise if box : from pandas import TimedeltaIndex value = TimedeltaIndex ( value , unit = 'ns' , name = name ) return value\n\n\n        Summary:", "input": "", "output": "Convert a list of objects to a timedelta index object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate_range ( start = None , end = None , periods = None , offset = BDay ( ) ) : from pandas . tseries . frequencies import to_offset offset = to_offset ( offset ) start = to_datetime ( start ) end = to_datetime ( end ) if start and not offset . onOffset ( start ) : start = offset . rollforward ( start ) elif end and not offset . onOffset ( end ) : end = offset . rollback ( end ) if periods is None and end < start and offset . n >= 0 : end = None periods = 0 if end is None : end = start + ( periods - 1 ) * offset if start is None : start = end - ( periods - 1 ) * offset cur = start if offset . n >= 0 : while cur <= end : yield cur # faster than cur + offset next_date = offset . apply ( cur ) if next_date <= cur : raise ValueError ( 'Offset {offset} did not increment date' . format ( offset = offset ) ) cur = next_date else : while cur >= end : yield cur # faster than cur + offset next_date = offset . apply ( cur ) if next_date >= cur : raise ValueError ( 'Offset {offset} did not decrement date' . format ( offset = offset ) ) cur = next_date\n\n\n        Summary:", "input": "", "output": "Generates a sequence of dates corresponding to the specified time offset . Similar to dateutil . rrule except uses pandas DateOffset objects to represent time increments ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply_index ( self , i ) : if type ( self ) is not DateOffset : raise NotImplementedError ( \"DateOffset subclass {name} \" \"does not have a vectorized \" \"implementation\" . format ( name = self . __class__ . __name__ ) ) kwds = self . kwds relativedelta_fast = { 'years' , 'months' , 'weeks' , 'days' , 'hours' , 'minutes' , 'seconds' , 'microseconds' } # relativedelta/_offset path only valid for base DateOffset if ( self . _use_relativedelta and set ( kwds ) . issubset ( relativedelta_fast ) ) : months = ( ( kwds . get ( 'years' , 0 ) * 12 + kwds . get ( 'months' , 0 ) ) * self . n ) if months : shifted = liboffsets . shift_months ( i . asi8 , months ) i = type ( i ) ( shifted , freq = i . freq , dtype = i . dtype ) weeks = ( kwds . get ( 'weeks' , 0 ) ) * self . n if weeks : # integer addition on PeriodIndex is deprecated, # so we directly use _time_shift instead asper = i . to_period ( 'W' ) if not isinstance ( asper . _data , np . ndarray ) : # unwrap PeriodIndex --> PeriodArray asper = asper . _data shifted = asper . _time_shift ( weeks ) i = shifted . to_timestamp ( ) + i . to_perioddelta ( 'W' ) timedelta_kwds = { k : v for k , v in kwds . items ( ) if k in [ 'days' , 'hours' , 'minutes' , 'seconds' , 'microseconds' ] } if timedelta_kwds : delta = Timedelta ( * * timedelta_kwds ) i = i + ( self . n * delta ) return i elif not self . _use_relativedelta and hasattr ( self , '_offset' ) : # timedelta return i + ( self . _offset * self . n ) else : # relativedelta with other keywords kwd = set ( kwds ) - relativedelta_fast raise NotImplementedError ( \"DateOffset with relativedelta \" \"keyword(s) {kwd} not able to be \" \"applied vectorized\" . format ( kwd = kwd ) )\n\n\n        Summary:", "input": "", "output": "Vectorized apply of DateOffset to DatetimeIndex raises NotImplentedError for offsets without a vectorized implementation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rollback ( self , dt ) : dt = as_timestamp ( dt ) if not self . onOffset ( dt ) : dt = dt - self . __class__ ( 1 , normalize = self . normalize , * * self . kwds ) return dt\n\n\n        Summary:", "input": "", "output": "Roll provided date backward to next offset only if not on offset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rollforward ( self , dt ) : dt = as_timestamp ( dt ) if not self . onOffset ( dt ) : dt = dt + self . __class__ ( 1 , normalize = self . normalize , * * self . kwds ) return dt\n\n\n        Summary:", "input": "", "output": "Roll provided date forward to next offset only if not on offset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def next_bday ( self ) : if self . n >= 0 : nb_offset = 1 else : nb_offset = - 1 if self . _prefix . startswith ( 'C' ) : # CustomBusinessHour return CustomBusinessDay ( n = nb_offset , weekmask = self . weekmask , holidays = self . holidays , calendar = self . calendar ) else : return BusinessDay ( n = nb_offset )\n\n\n        Summary:", "input": "", "output": "Used for moving to next business day ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _next_opening_time ( self , other ) : if not self . next_bday . onOffset ( other ) : other = other + self . next_bday else : if self . n >= 0 and self . start < other . time ( ) : other = other + self . next_bday elif self . n < 0 and other . time ( ) < self . start : other = other + self . next_bday return datetime ( other . year , other . month , other . day , self . start . hour , self . start . minute )\n\n\n        Summary:", "input": "", "output": "If n is positive return tomorrow s business day opening time . Otherwise yesterday s business day s opening time ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_business_hours_by_sec ( self ) : if self . _get_daytime_flag : # create dummy datetime to calculate businesshours in a day dtstart = datetime ( 2014 , 4 , 1 , self . start . hour , self . start . minute ) until = datetime ( 2014 , 4 , 1 , self . end . hour , self . end . minute ) return ( until - dtstart ) . total_seconds ( ) else : dtstart = datetime ( 2014 , 4 , 1 , self . start . hour , self . start . minute ) until = datetime ( 2014 , 4 , 2 , self . end . hour , self . end . minute ) return ( until - dtstart ) . total_seconds ( )\n\n\n        Summary:", "input": "", "output": "Return business hours in a day by seconds ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rollback ( self , dt ) : if not self . onOffset ( dt ) : businesshours = self . _get_business_hours_by_sec if self . n >= 0 : dt = self . _prev_opening_time ( dt ) + timedelta ( seconds = businesshours ) else : dt = self . _next_opening_time ( dt ) + timedelta ( seconds = businesshours ) return dt\n\n\n        Summary:", "input": "", "output": "Roll provided date backward to next offset only if not on offset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rollforward ( self , dt ) : if not self . onOffset ( dt ) : if self . n >= 0 : return self . _next_opening_time ( dt ) else : return self . _prev_opening_time ( dt ) return dt\n\n\n        Summary:", "input": "", "output": "Roll provided date forward to next offset only if not on offset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _onOffset ( self , dt , businesshours ) : # if self.normalize and not _is_normalized(dt): # return False # Valid BH can be on the different BusinessDay during midnight # Distinguish by the time spent from previous opening time if self . n >= 0 : op = self . _prev_opening_time ( dt ) else : op = self . _next_opening_time ( dt ) span = ( dt - op ) . total_seconds ( ) if span <= businesshours : return True else : return False\n\n\n        Summary:", "input": "", "output": "Slight speedups using calculated values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cbday_roll ( self ) : cbday = CustomBusinessDay ( n = self . n , normalize = False , * * self . kwds ) if self . _prefix . endswith ( 'S' ) : # MonthBegin roll_func = cbday . rollforward else : # MonthEnd roll_func = cbday . rollback return roll_func\n\n\n        Summary:", "input": "", "output": "Define default roll function to be called in apply method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def month_roll ( self ) : if self . _prefix . endswith ( 'S' ) : # MonthBegin roll_func = self . m_offset . rollback else : # MonthEnd roll_func = self . m_offset . rollforward return roll_func\n\n\n        Summary:", "input": "", "output": "Define default roll function to be called in apply method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_index_days ( self , i , roll ) : nanos = ( roll % 2 ) * Timedelta ( days = self . day_of_month - 1 ) . value return i + nanos . astype ( 'timedelta64[ns]' )\n\n\n        Summary:", "input": "", "output": "Add days portion of offset to DatetimeIndex i ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _end_apply_index ( self , dtindex ) : off = dtindex . to_perioddelta ( 'D' ) base , mult = libfrequencies . get_freq_code ( self . freqstr ) base_period = dtindex . to_period ( base ) if not isinstance ( base_period . _data , np . ndarray ) : # unwrap PeriodIndex --> PeriodArray base_period = base_period . _data if self . n > 0 : # when adding, dates on end roll to next normed = dtindex - off + Timedelta ( 1 , 'D' ) - Timedelta ( 1 , 'ns' ) roll = np . where ( base_period . to_timestamp ( how = 'end' ) == normed , self . n , self . n - 1 ) # integer-array addition on PeriodIndex is deprecated, # so we use _addsub_int_array directly shifted = base_period . _addsub_int_array ( roll , operator . add ) base = shifted . to_timestamp ( how = 'end' ) else : # integer addition on PeriodIndex is deprecated, # so we use _time_shift directly roll = self . n base = base_period . _time_shift ( roll ) . to_timestamp ( how = 'end' ) return base + off + Timedelta ( 1 , 'ns' ) - Timedelta ( 1 , 'D' )\n\n\n        Summary:", "input": "", "output": "Add self to the given DatetimeIndex specialized for case where self . weekday is non - null ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_offset_day ( self , other ) : mstart = datetime ( other . year , other . month , 1 ) wday = mstart . weekday ( ) shift_days = ( self . weekday - wday ) % 7 return 1 + shift_days + self . week * 7\n\n\n        Summary:", "input": "", "output": "Find the day in the same month as other that has the same weekday as self . weekday and is the self . week th such day in the month ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_offset_day ( self , other ) : dim = ccalendar . get_days_in_month ( other . year , other . month ) mend = datetime ( other . year , other . month , dim ) wday = mend . weekday ( ) shift_days = ( wday - self . weekday ) % 7 return dim - shift_days\n\n\n        Summary:", "input": "", "output": "Find the day in the same month as other that has the same weekday as self . weekday and is the last such day in the month ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _rollback_to_year ( self , other ) : num_qtrs = 0 norm = Timestamp ( other ) . tz_localize ( None ) start = self . _offset . rollback ( norm ) # Note: start <= norm and self._offset.onOffset(start) if start < norm : # roll adjustment qtr_lens = self . get_weeks ( norm ) # check thet qtr_lens is consistent with self._offset addition end = liboffsets . shift_day ( start , days = 7 * sum ( qtr_lens ) ) assert self . _offset . onOffset ( end ) , ( start , end , qtr_lens ) tdelta = norm - start for qlen in qtr_lens : if qlen * 7 <= tdelta . days : num_qtrs += 1 tdelta -= Timedelta ( days = qlen * 7 ) else : break else : tdelta = Timedelta ( 0 ) # Note: we always have tdelta.value >= 0 return start , num_qtrs , tdelta\n\n\n        Summary:", "input": "", "output": "Roll other back to the most recent date that was on a fiscal year end ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concat ( objs , axis = 0 , join = 'outer' , join_axes = None , ignore_index = False , keys = None , levels = None , names = None , verify_integrity = False , sort = None , copy = True ) : op = _Concatenator ( objs , axis = axis , join_axes = join_axes , ignore_index = ignore_index , join = join , keys = keys , levels = levels , names = names , verify_integrity = verify_integrity , copy = copy , sort = sort ) return op . get_result ( )\n\n\n        Summary:", "input": "", "output": "Concatenate pandas objects along a particular axis with optional set logic along the other axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_concat_axis ( self ) : if self . _is_series : if self . axis == 0 : indexes = [ x . index for x in self . objs ] elif self . ignore_index : idx = ibase . default_index ( len ( self . objs ) ) return idx elif self . keys is None : names = [ None ] * len ( self . objs ) num = 0 has_names = False for i , x in enumerate ( self . objs ) : if not isinstance ( x , Series ) : raise TypeError ( \"Cannot concatenate type 'Series' \" \"with object of type {type!r}\" . format ( type = type ( x ) . __name__ ) ) if x . name is not None : names [ i ] = x . name has_names = True else : names [ i ] = num num += 1 if has_names : return Index ( names ) else : return ibase . default_index ( len ( self . objs ) ) else : return ensure_index ( self . keys ) . set_names ( self . names ) else : indexes = [ x . _data . axes [ self . axis ] for x in self . objs ] if self . ignore_index : idx = ibase . default_index ( sum ( len ( i ) for i in indexes ) ) return idx if self . keys is None : concat_axis = _concat_indexes ( indexes ) else : concat_axis = _make_concat_multiindex ( indexes , self . keys , self . levels , self . names ) self . _maybe_check_integrity ( concat_axis ) return concat_axis\n\n\n        Summary:", "input": "", "output": "Return index to be used along concatenation axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _in ( x , y ) : try : return x . isin ( y ) except AttributeError : if is_list_like ( x ) : try : return y . isin ( x ) except AttributeError : pass return x in y\n\n\n        Summary:", "input": "", "output": "Compute the vectorized membership of x in y if possible otherwise use Python ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _not_in ( x , y ) : try : return ~ x . isin ( y ) except AttributeError : if is_list_like ( x ) : try : return ~ y . isin ( x ) except AttributeError : pass return x not in y\n\n\n        Summary:", "input": "", "output": "Compute the vectorized membership of x not in y if possible otherwise use Python ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cast_inplace ( terms , acceptable_dtypes , dtype ) : dt = np . dtype ( dtype ) for term in terms : if term . type in acceptable_dtypes : continue try : new_value = term . value . astype ( dt ) except AttributeError : new_value = dt . type ( term . value ) term . update ( new_value )\n\n\n        Summary:", "input": "", "output": "Cast an expression inplace ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update ( self , value ) : key = self . name # if it's a variable name (otherwise a constant) if isinstance ( key , str ) : self . env . swapkey ( self . local_name , key , new_value = value ) self . value = value\n\n\n        Summary:", "input": "", "output": "search order for local ( i . e . @variable ) variables :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def evaluate ( self , env , engine , parser , term_type , eval_in_python ) : if engine == 'python' : res = self ( env ) else : # recurse over the left/right nodes left = self . lhs . evaluate ( env , engine = engine , parser = parser , term_type = term_type , eval_in_python = eval_in_python ) right = self . rhs . evaluate ( env , engine = engine , parser = parser , term_type = term_type , eval_in_python = eval_in_python ) # base cases if self . op in eval_in_python : res = self . func ( left . value , right . value ) else : from pandas . core . computation . eval import eval res = eval ( self , local_dict = env , engine = engine , parser = parser ) name = env . add_tmp ( res ) return term_type ( name , env = env )\n\n\n        Summary:", "input": "", "output": "Evaluate a binary operation * before * being passed to the engine ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_values ( self ) : def stringify ( value ) : if self . encoding is not None : encoder = partial ( pprint_thing_encoded , encoding = self . encoding ) else : encoder = pprint_thing return encoder ( value ) lhs , rhs = self . lhs , self . rhs if is_term ( lhs ) and lhs . is_datetime and is_term ( rhs ) and rhs . is_scalar : v = rhs . value if isinstance ( v , ( int , float ) ) : v = stringify ( v ) v = Timestamp ( _ensure_decoded ( v ) ) if v . tz is not None : v = v . tz_convert ( 'UTC' ) self . rhs . update ( v ) if is_term ( rhs ) and rhs . is_datetime and is_term ( lhs ) and lhs . is_scalar : v = lhs . value if isinstance ( v , ( int , float ) ) : v = stringify ( v ) v = Timestamp ( _ensure_decoded ( v ) ) if v . tz is not None : v = v . tz_convert ( 'UTC' ) self . lhs . update ( v )\n\n\n        Summary:", "input": "", "output": "Convert datetimes to a comparable value in an expression ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def crosstab ( index , columns , values = None , rownames = None , colnames = None , aggfunc = None , margins = False , margins_name = 'All' , dropna = True , normalize = False ) : index = com . maybe_make_list ( index ) columns = com . maybe_make_list ( columns ) rownames = _get_names ( index , rownames , prefix = 'row' ) colnames = _get_names ( columns , colnames , prefix = 'col' ) common_idx = _get_objs_combined_axis ( index + columns , intersect = True , sort = False ) data = { } data . update ( zip ( rownames , index ) ) data . update ( zip ( colnames , columns ) ) if values is None and aggfunc is not None : raise ValueError ( \"aggfunc cannot be used without values.\" ) if values is not None and aggfunc is None : raise ValueError ( \"values cannot be used without an aggfunc.\" ) from pandas import DataFrame df = DataFrame ( data , index = common_idx ) if values is None : df [ '__dummy__' ] = 0 kwargs = { 'aggfunc' : len , 'fill_value' : 0 } else : df [ '__dummy__' ] = values kwargs = { 'aggfunc' : aggfunc } table = df . pivot_table ( '__dummy__' , index = rownames , columns = colnames , margins = margins , margins_name = margins_name , dropna = dropna , * * kwargs ) # Post-process if normalize is not False : table = _normalize ( table , normalize = normalize , margins = margins , margins_name = margins_name ) return table\n\n\n        Summary:", "input": "", "output": "Compute a simple cross tabulation of two ( or more ) factors . By default computes a frequency table of the factors unless an array of values and an aggregation function are passed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _shape ( self , df ) : row , col = df . shape return row + df . columns . nlevels , col + df . index . nlevels\n\n\n        Summary:", "input": "", "output": "Calculate table chape considering index levels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_cells ( self , left , right , vertical ) : if vertical : # calculate required number of cells vcells = max ( sum ( self . _shape ( l ) [ 0 ] for l in left ) , self . _shape ( right ) [ 0 ] ) hcells = ( max ( self . _shape ( l ) [ 1 ] for l in left ) + self . _shape ( right ) [ 1 ] ) else : vcells = max ( [ self . _shape ( l ) [ 0 ] for l in left ] + [ self . _shape ( right ) [ 0 ] ] ) hcells = sum ( [ self . _shape ( l ) [ 1 ] for l in left ] + [ self . _shape ( right ) [ 1 ] ] ) return hcells , vcells\n\n\n        Summary:", "input": "", "output": "Calculate appropriate figure size based on left and right data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def plot ( self , left , right , labels = None , vertical = True ) : import matplotlib . pyplot as plt import matplotlib . gridspec as gridspec if not isinstance ( left , list ) : left = [ left ] left = [ self . _conv ( l ) for l in left ] right = self . _conv ( right ) hcells , vcells = self . _get_cells ( left , right , vertical ) if vertical : figsize = self . cell_width * hcells , self . cell_height * vcells else : # include margin for titles figsize = self . cell_width * hcells , self . cell_height * vcells fig = plt . figure ( figsize = figsize ) if vertical : gs = gridspec . GridSpec ( len ( left ) , hcells ) # left max_left_cols = max ( self . _shape ( l ) [ 1 ] for l in left ) max_left_rows = max ( self . _shape ( l ) [ 0 ] for l in left ) for i , ( l , label ) in enumerate ( zip ( left , labels ) ) : ax = fig . add_subplot ( gs [ i , 0 : max_left_cols ] ) self . _make_table ( ax , l , title = label , height = 1.0 / max_left_rows ) # right ax = plt . subplot ( gs [ : , max_left_cols : ] ) self . _make_table ( ax , right , title = 'Result' , height = 1.05 / vcells ) fig . subplots_adjust ( top = 0.9 , bottom = 0.05 , left = 0.05 , right = 0.95 ) else : max_rows = max ( self . _shape ( df ) [ 0 ] for df in left + [ right ] ) height = 1.0 / np . max ( max_rows ) gs = gridspec . GridSpec ( 1 , hcells ) # left i = 0 for l , label in zip ( left , labels ) : sp = self . _shape ( l ) ax = fig . add_subplot ( gs [ 0 , i : i + sp [ 1 ] ] ) self . _make_table ( ax , l , title = label , height = height ) i += sp [ 1 ] # right ax = plt . subplot ( gs [ 0 , i : ] ) self . _make_table ( ax , right , title = 'Result' , height = height ) fig . subplots_adjust ( top = 0.85 , bottom = 0.05 , left = 0.05 , right = 0.95 ) return fig\n\n\n        Summary:", "input": "", "output": "Plot left / right DataFrames in specified layout ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _conv ( self , data ) : if isinstance ( data , pd . Series ) : if data . name is None : data = data . to_frame ( name = '' ) else : data = data . to_frame ( ) data = data . fillna ( 'NaN' ) return data\n\n\n        Summary:", "input": "", "output": "Convert each input to appropriate for table outplot"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cut ( x , bins , right = True , labels = None , retbins = False , precision = 3 , include_lowest = False , duplicates = 'raise' ) : # NOTE: this binning code is changed a bit from histogram for var(x) == 0 # for handling the cut for datetime and timedelta objects x_is_series , series_index , name , x = _preprocess_for_cut ( x ) x , dtype = _coerce_to_type ( x ) if not np . iterable ( bins ) : if is_scalar ( bins ) and bins < 1 : raise ValueError ( \"`bins` should be a positive integer.\" ) try : # for array-like sz = x . size except AttributeError : x = np . asarray ( x ) sz = x . size if sz == 0 : raise ValueError ( 'Cannot cut empty array' ) rng = ( nanops . nanmin ( x ) , nanops . nanmax ( x ) ) mn , mx = [ mi + 0.0 for mi in rng ] if np . isinf ( mn ) or np . isinf ( mx ) : # GH 24314 raise ValueError ( 'cannot specify integer `bins` when input data ' 'contains infinity' ) elif mn == mx : # adjust end points before binning mn -= .001 * abs ( mn ) if mn != 0 else .001 mx += .001 * abs ( mx ) if mx != 0 else .001 bins = np . linspace ( mn , mx , bins + 1 , endpoint = True ) else : # adjust end points after binning bins = np . linspace ( mn , mx , bins + 1 , endpoint = True ) adj = ( mx - mn ) * 0.001 # 0.1% of the range if right : bins [ 0 ] -= adj else : bins [ - 1 ] += adj elif isinstance ( bins , IntervalIndex ) : if bins . is_overlapping : raise ValueError ( 'Overlapping IntervalIndex is not accepted.' ) else : if is_datetime64tz_dtype ( bins ) : bins = np . asarray ( bins , dtype = _NS_DTYPE ) else : bins = np . asarray ( bins ) bins = _convert_bin_to_numeric_type ( bins , dtype ) # GH 26045: cast to float64 to avoid an overflow if ( np . diff ( bins . astype ( 'float64' ) ) < 0 ) . any ( ) : raise ValueError ( 'bins must increase monotonically.' ) fac , bins = _bins_to_cuts ( x , bins , right = right , labels = labels , precision = precision , include_lowest = include_lowest , dtype = dtype , duplicates = duplicates ) return _postprocess_for_cut ( fac , bins , retbins , x_is_series , series_index , name , dtype )\n\n\n        Summary:", "input": "", "output": "Bin values into discrete intervals ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def qcut ( x , q , labels = None , retbins = False , precision = 3 , duplicates = 'raise' ) : x_is_series , series_index , name , x = _preprocess_for_cut ( x ) x , dtype = _coerce_to_type ( x ) if is_integer ( q ) : quantiles = np . linspace ( 0 , 1 , q + 1 ) else : quantiles = q bins = algos . quantile ( x , quantiles ) fac , bins = _bins_to_cuts ( x , bins , labels = labels , precision = precision , include_lowest = True , dtype = dtype , duplicates = duplicates ) return _postprocess_for_cut ( fac , bins , retbins , x_is_series , series_index , name , dtype )\n\n\n        Summary:", "input": "", "output": "Quantile - based discretization function . Discretize variable into equal - sized buckets based on rank or based on sample quantiles . For example 1000 values for 10 quantiles would produce a Categorical object indicating quantile membership for each data point ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _coerce_to_type ( x ) : dtype = None if is_datetime64tz_dtype ( x ) : dtype = x . dtype elif is_datetime64_dtype ( x ) : x = to_datetime ( x ) dtype = np . dtype ( 'datetime64[ns]' ) elif is_timedelta64_dtype ( x ) : x = to_timedelta ( x ) dtype = np . dtype ( 'timedelta64[ns]' ) if dtype is not None : # GH 19768: force NaT to NaN during integer conversion x = np . where ( x . notna ( ) , x . view ( np . int64 ) , np . nan ) return x , dtype\n\n\n        Summary:", "input": "", "output": "if the passed data is of datetime / timedelta type this method converts it to numeric so that cut method can handle it"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_bin_to_numeric_type ( bins , dtype ) : bins_dtype = infer_dtype ( bins , skipna = False ) if is_timedelta64_dtype ( dtype ) : if bins_dtype in [ 'timedelta' , 'timedelta64' ] : bins = to_timedelta ( bins ) . view ( np . int64 ) else : raise ValueError ( \"bins must be of timedelta64 dtype\" ) elif is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) : if bins_dtype in [ 'datetime' , 'datetime64' ] : bins = to_datetime ( bins ) . view ( np . int64 ) else : raise ValueError ( \"bins must be of datetime64 dtype\" ) return bins\n\n\n        Summary:", "input": "", "output": "if the passed bin is of datetime / timedelta type this method converts it to integer"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_bin_to_datelike_type ( bins , dtype ) : if is_datetime64tz_dtype ( dtype ) : bins = to_datetime ( bins . astype ( np . int64 ) , utc = True ) . tz_convert ( dtype . tz ) elif is_datetime_or_timedelta_dtype ( dtype ) : bins = Index ( bins . astype ( np . int64 ) , dtype = dtype ) return bins\n\n\n        Summary:", "input": "", "output": "Convert bins to a DatetimeIndex or TimedeltaIndex if the orginal dtype is datelike"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_labels ( bins , precision , right = True , include_lowest = False , dtype = None ) : closed = 'right' if right else 'left' if is_datetime64tz_dtype ( dtype ) : formatter = partial ( Timestamp , tz = dtype . tz ) adjust = lambda x : x - Timedelta ( '1ns' ) elif is_datetime64_dtype ( dtype ) : formatter = Timestamp adjust = lambda x : x - Timedelta ( '1ns' ) elif is_timedelta64_dtype ( dtype ) : formatter = Timedelta adjust = lambda x : x - Timedelta ( '1ns' ) else : precision = _infer_precision ( precision , bins ) formatter = lambda x : _round_frac ( x , precision ) adjust = lambda x : x - 10 ** ( - precision ) breaks = [ formatter ( b ) for b in bins ] labels = IntervalIndex . from_breaks ( breaks , closed = closed ) if right and include_lowest : # we will adjust the left hand side by precision to # account that we are all right closed v = adjust ( labels [ 0 ] . left ) i = IntervalIndex ( [ Interval ( v , labels [ 0 ] . right , closed = 'right' ) ] ) labels = i . append ( labels [ 1 : ] ) return labels\n\n\n        Summary:", "input": "", "output": "based on the dtype return our labels"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _preprocess_for_cut ( x ) : x_is_series = isinstance ( x , Series ) series_index = None name = None if x_is_series : series_index = x . index name = x . name # Check that the passed array is a Pandas or Numpy object # We don't want to strip away a Pandas data-type here (e.g. datetimetz) ndim = getattr ( x , 'ndim' , None ) if ndim is None : x = np . asarray ( x ) if x . ndim != 1 : raise ValueError ( \"Input array must be 1 dimensional\" ) return x_is_series , series_index , name , x\n\n\n        Summary:", "input": "", "output": "handles preprocessing for cut where we convert passed input to array strip the index information and store it separately"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _postprocess_for_cut ( fac , bins , retbins , x_is_series , series_index , name , dtype ) : if x_is_series : fac = Series ( fac , index = series_index , name = name ) if not retbins : return fac bins = _convert_bin_to_datelike_type ( bins , dtype ) return fac , bins\n\n\n        Summary:", "input": "", "output": "handles post processing for the cut method where we combine the index information if the originally passed datatype was a series"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _round_frac ( x , precision ) : if not np . isfinite ( x ) or x == 0 : return x else : frac , whole = np . modf ( x ) if whole == 0 : digits = - int ( np . floor ( np . log10 ( abs ( frac ) ) ) ) - 1 + precision else : digits = precision return np . around ( x , digits )\n\n\n        Summary:", "input": "", "output": "Round the fractional part of the given number"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _infer_precision ( base_precision , bins ) : for precision in range ( base_precision , 20 ) : levels = [ _round_frac ( b , precision ) for b in bins ] if algos . unique ( levels ) . size == bins . size : return precision return base_precision\n\n\n        Summary:", "input": "", "output": "Infer an appropriate precision for _round_frac"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def detect_console_encoding ( ) : global _initial_defencoding encoding = None try : encoding = sys . stdout . encoding or sys . stdin . encoding except ( AttributeError , IOError ) : pass # try again for something better if not encoding or 'ascii' in encoding . lower ( ) : try : encoding = locale . getpreferredencoding ( ) except Exception : pass # when all else fails. this will usually be \"ascii\" if not encoding or 'ascii' in encoding . lower ( ) : encoding = sys . getdefaultencoding ( ) # GH#3360, save the reported defencoding at import time # MPL backends may change it. Make available for debugging. if not _initial_defencoding : _initial_defencoding = sys . getdefaultencoding ( ) return encoding\n\n\n        Summary:", "input": "", "output": "Try to find the most capable encoding supported by the console . slightly modified from the way IPython handles the same issue ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_arg_length ( fname , args , max_fname_arg_count , compat_args ) : if max_fname_arg_count < 0 : raise ValueError ( \"'max_fname_arg_count' must be non-negative\" ) if len ( args ) > len ( compat_args ) : max_arg_count = len ( compat_args ) + max_fname_arg_count actual_arg_count = len ( args ) + max_fname_arg_count argument = 'argument' if max_arg_count == 1 else 'arguments' raise TypeError ( \"{fname}() takes at most {max_arg} {argument} \" \"({given_arg} given)\" . format ( fname = fname , max_arg = max_arg_count , argument = argument , given_arg = actual_arg_count ) )\n\n\n        Summary:", "input": "", "output": "Checks whether args has length of at most compat_args . Raises a TypeError if that is not the case similar to in Python when a function is called with too many arguments ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_for_default_values ( fname , arg_val_dict , compat_args ) : for key in arg_val_dict : # try checking equality directly with '=' operator, # as comparison may have been overridden for the left # hand object try : v1 = arg_val_dict [ key ] v2 = compat_args [ key ] # check for None-ness otherwise we could end up # comparing a numpy array vs None if ( v1 is not None and v2 is None ) or ( v1 is None and v2 is not None ) : match = False else : match = ( v1 == v2 ) if not is_bool ( match ) : raise ValueError ( \"'match' is not a boolean\" ) # could not compare them directly, so try comparison # using the 'is' operator except ValueError : match = ( arg_val_dict [ key ] is compat_args [ key ] ) if not match : raise ValueError ( ( \"the '{arg}' parameter is not \" \"supported in the pandas \" \"implementation of {fname}()\" . format ( fname = fname , arg = key ) ) )\n\n\n        Summary:", "input": "", "output": "Check that the keys in arg_val_dict are mapped to their default values as specified in compat_args ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_args ( fname , args , max_fname_arg_count , compat_args ) : _check_arg_length ( fname , args , max_fname_arg_count , compat_args ) # We do this so that we can provide a more informative # error message about the parameters that we are not # supporting in the pandas implementation of 'fname' kwargs = dict ( zip ( compat_args , args ) ) _check_for_default_values ( fname , kwargs , compat_args )\n\n\n        Summary:", "input": "", "output": "Checks whether the length of the * args argument passed into a function has at most len ( compat_args ) arguments and whether or not all of these elements in args are set to their default values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_for_invalid_keys ( fname , kwargs , compat_args ) : # set(dict) --> set of the dictionary's keys diff = set ( kwargs ) - set ( compat_args ) if diff : bad_arg = list ( diff ) [ 0 ] raise TypeError ( ( \"{fname}() got an unexpected \" \"keyword argument '{arg}'\" . format ( fname = fname , arg = bad_arg ) ) )\n\n\n        Summary:", "input": "", "output": "Checks whether kwargs contains any keys that are not in compat_args and raises a TypeError if there is one ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_kwargs ( fname , kwargs , compat_args ) : kwds = kwargs . copy ( ) _check_for_invalid_keys ( fname , kwargs , compat_args ) _check_for_default_values ( fname , kwds , compat_args )\n\n\n        Summary:", "input": "", "output": "Checks whether parameters passed to the ** kwargs argument in a function fname are valid parameters as specified in * compat_args and whether or not they are set to their default values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_args_and_kwargs ( fname , args , kwargs , max_fname_arg_count , compat_args ) : # Check that the total number of arguments passed in (i.e. # args and kwargs) does not exceed the length of compat_args _check_arg_length ( fname , args + tuple ( kwargs . values ( ) ) , max_fname_arg_count , compat_args ) # Check there is no overlap with the positional and keyword # arguments, similar to what is done in actual Python functions args_dict = dict ( zip ( compat_args , args ) ) for key in args_dict : if key in kwargs : raise TypeError ( \"{fname}() got multiple values for keyword \" \"argument '{arg}'\" . format ( fname = fname , arg = key ) ) kwargs . update ( args_dict ) validate_kwargs ( fname , kwargs , compat_args )\n\n\n        Summary:", "input": "", "output": "Checks whether parameters passed to the * args and ** kwargs argument in a function fname are valid parameters as specified in * compat_args and whether or not they are set to their default values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_bool_kwarg ( value , arg_name ) : if not ( is_bool ( value ) or value is None ) : raise ValueError ( 'For argument \"{arg}\" expected type bool, received ' 'type {typ}.' . format ( arg = arg_name , typ = type ( value ) . __name__ ) ) return value\n\n\n        Summary:", "input": "", "output": "Ensures that argument passed in arg_name is of type bool ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_axis_style_args ( data , args , kwargs , arg_name , method_name ) : # TODO: Change to keyword-only args and remove all this out = { } # Goal: fill 'out' with index/columns-style arguments # like out = {'index': foo, 'columns': bar} # Start by validating for consistency if 'axis' in kwargs and any ( x in kwargs for x in data . _AXIS_NUMBERS ) : msg = \"Cannot specify both 'axis' and any of 'index' or 'columns'.\" raise TypeError ( msg ) # First fill with explicit values provided by the user... if arg_name in kwargs : if args : msg = ( \"{} got multiple values for argument \" \"'{}'\" . format ( method_name , arg_name ) ) raise TypeError ( msg ) axis = data . _get_axis_name ( kwargs . get ( 'axis' , 0 ) ) out [ axis ] = kwargs [ arg_name ] # More user-provided arguments, now from kwargs for k , v in kwargs . items ( ) : try : ax = data . _get_axis_name ( k ) except ValueError : pass else : out [ ax ] = v # All user-provided kwargs have been handled now. # Now we supplement with positional arguments, emitting warnings # when there's ambiguity and raising when there's conflicts if len ( args ) == 0 : pass # It's up to the function to decide if this is valid elif len ( args ) == 1 : axis = data . _get_axis_name ( kwargs . get ( 'axis' , 0 ) ) out [ axis ] = args [ 0 ] elif len ( args ) == 2 : if 'axis' in kwargs : # Unambiguously wrong msg = ( \"Cannot specify both 'axis' and any of 'index' \" \"or 'columns'\" ) raise TypeError ( msg ) msg = ( \"Interpreting call\\n\\t'.{method_name}(a, b)' as \" \"\\n\\t'.{method_name}(index=a, columns=b)'.\\nUse named \" \"arguments to remove any ambiguity. In the future, using \" \"positional arguments for 'index' or 'columns' will raise \" \" a 'TypeError'.\" ) warnings . warn ( msg . format ( method_name = method_name , ) , FutureWarning , stacklevel = 4 ) out [ data . _AXIS_NAMES [ 0 ] ] = args [ 0 ] out [ data . _AXIS_NAMES [ 1 ] ] = args [ 1 ] else : msg = \"Cannot specify all of '{}', 'index', 'columns'.\" raise TypeError ( msg . format ( arg_name ) ) return out\n\n\n        Summary:", "input": "", "output": "Argument handler for mixed index columns / axis functions"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_fillna_kwargs ( value , method , validate_scalar_dict_value = True ) : from pandas . core . missing import clean_fill_method if value is None and method is None : raise ValueError ( \"Must specify a fill 'value' or 'method'.\" ) elif value is None and method is not None : method = clean_fill_method ( method ) elif value is not None and method is None : if validate_scalar_dict_value and isinstance ( value , ( list , tuple ) ) : raise TypeError ( '\"value\" parameter must be a scalar or dict, but ' 'you passed a \"{0}\"' . format ( type ( value ) . __name__ ) ) elif value is not None and method is not None : raise ValueError ( \"Cannot specify both 'value' and 'method'.\" ) return value , method\n\n\n        Summary:", "input": "", "output": "Validate the keyword arguments to fillna ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_process_deprecations ( r , how = None , fill_method = None , limit = None ) : if how is not None : # .resample(..., how='sum') if isinstance ( how , str ) : method = \"{0}()\" . format ( how ) # .resample(..., how=lambda x: ....) else : method = \".apply(<func>)\" # if we have both a how and fill_method, then show # the following warning if fill_method is None : warnings . warn ( \"how in .resample() is deprecated\\n\" \"the new syntax is \" \".resample(...).{method}\" . format ( method = method ) , FutureWarning , stacklevel = 3 ) r = r . aggregate ( how ) if fill_method is not None : # show the prior function call method = '.' + method if how is not None else '' args = \"limit={0}\" . format ( limit ) if limit is not None else \"\" warnings . warn ( \"fill_method is deprecated to .resample()\\n\" \"the new syntax is .resample(...){method}\" \".{fill_method}({args})\" . format ( method = method , fill_method = fill_method , args = args ) , FutureWarning , stacklevel = 3 ) if how is not None : r = getattr ( r , fill_method ) ( limit = limit ) else : r = r . aggregate ( fill_method , limit = limit ) return r\n\n\n        Summary:", "input": "", "output": "Potentially we might have a deprecation warning show it but call the appropriate methods anyhow ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def resample ( obj , kind = None , * * kwds ) : tg = TimeGrouper ( * * kwds ) return tg . _get_resampler ( obj , kind = kind )\n\n\n        Summary:", "input": "", "output": "Create a TimeGrouper and return our resampler ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_resampler_for_grouping ( groupby , rule , how = None , fill_method = None , limit = None , kind = None , * * kwargs ) : # .resample uses 'on' similar to how .groupby uses 'key' kwargs [ 'key' ] = kwargs . pop ( 'on' , None ) tg = TimeGrouper ( freq = rule , * * kwargs ) resampler = tg . _get_resampler ( groupby . obj , kind = kind ) r = resampler . _get_resampler_for_grouping ( groupby = groupby ) return _maybe_process_deprecations ( r , how = how , fill_method = fill_method , limit = limit )\n\n\n        Summary:", "input": "", "output": "Return our appropriate resampler when grouping as well ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_timestamp_range_edges ( first , last , offset , closed = 'left' , base = 0 ) : if isinstance ( offset , Tick ) : if isinstance ( offset , Day ) : # _adjust_dates_anchored assumes 'D' means 24H, but first/last # might contain a DST transition (23H, 24H, or 25H). # So \"pretend\" the dates are naive when adjusting the endpoints tz = first . tz first = first . tz_localize ( None ) last = last . tz_localize ( None ) first , last = _adjust_dates_anchored ( first , last , offset , closed = closed , base = base ) if isinstance ( offset , Day ) : first = first . tz_localize ( tz ) last = last . tz_localize ( tz ) return first , last else : first = first . normalize ( ) last = last . normalize ( ) if closed == 'left' : first = Timestamp ( offset . rollback ( first ) ) else : first = Timestamp ( first - offset ) last = Timestamp ( last + offset ) return first , last\n\n\n        Summary:", "input": "", "output": "Adjust the first Timestamp to the preceeding Timestamp that resides on the provided offset . Adjust the last Timestamp to the following Timestamp that resides on the provided offset . Input Timestamps that already reside on the offset will be adjusted depending on the type of offset and the closed parameter ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_period_range_edges ( first , last , offset , closed = 'left' , base = 0 ) : if not all ( isinstance ( obj , pd . Period ) for obj in [ first , last ] ) : raise TypeError ( \"'first' and 'last' must be instances of type Period\" ) # GH 23882 first = first . to_timestamp ( ) last = last . to_timestamp ( ) adjust_first = not offset . onOffset ( first ) adjust_last = offset . onOffset ( last ) first , last = _get_timestamp_range_edges ( first , last , offset , closed = closed , base = base ) first = ( first + adjust_first * offset ) . to_period ( offset ) last = ( last - adjust_last * offset ) . to_period ( offset ) return first , last\n\n\n        Summary:", "input": "", "output": "Adjust the provided first and last Periods to the respective Period of the given offset that encompasses them ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asfreq ( obj , freq , method = None , how = None , normalize = False , fill_value = None ) : if isinstance ( obj . index , PeriodIndex ) : if method is not None : raise NotImplementedError ( \"'method' argument is not supported\" ) if how is None : how = 'E' new_obj = obj . copy ( ) new_obj . index = obj . index . asfreq ( freq , how = how ) elif len ( obj . index ) == 0 : new_obj = obj . copy ( ) new_obj . index = obj . index . _shallow_copy ( freq = to_offset ( freq ) ) else : dti = date_range ( obj . index [ 0 ] , obj . index [ - 1 ] , freq = freq ) dti . name = obj . index . name new_obj = obj . reindex ( dti , method = method , fill_value = fill_value ) if normalize : new_obj . index = new_obj . index . normalize ( ) return new_obj\n\n\n        Summary:", "input": "", "output": "Utility frequency conversion method for Series / DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_selection ( self ) : # upsampling and PeriodIndex resampling do not work # with selection, this state used to catch and raise an error return ( self . groupby is not None and ( self . groupby . key is not None or self . groupby . level is not None ) )\n\n\n        Summary:", "input": "", "output": "Is the resampling from a DataFrame column or MultiIndex level ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_binner ( self ) : if self . binner is None : self . binner , self . grouper = self . _get_binner ( )\n\n\n        Summary:", "input": "", "output": "Setup our binners ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_binner ( self ) : binner , bins , binlabels = self . _get_binner_for_time ( ) bin_grouper = BinGrouper ( bins , binlabels , indexer = self . groupby . indexer ) return binner , bin_grouper\n\n\n        Summary:", "input": "", "output": "Create the BinGrouper assume that self . set_grouper ( obj ) has already been called ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transform ( self , arg , * args , * * kwargs ) : return self . _selected_obj . groupby ( self . groupby ) . transform ( arg , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Call function producing a like - indexed Series on each group and return a Series with the transformed values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gotitem ( self , key , ndim , subset = None ) : self . _set_binner ( ) grouper = self . grouper if subset is None : subset = self . obj grouped = groupby ( subset , by = None , grouper = grouper , axis = self . axis ) # try the key selection try : return grouped [ key ] except KeyError : return grouped\n\n\n        Summary:", "input": "", "output": "Sub - classes to define . Return a sliced object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _groupby_and_aggregate ( self , how , grouper = None , * args , * * kwargs ) : if grouper is None : self . _set_binner ( ) grouper = self . grouper obj = self . _selected_obj grouped = groupby ( obj , by = None , grouper = grouper , axis = self . axis ) try : if isinstance ( obj , ABCDataFrame ) and callable ( how ) : # Check if the function is reducing or not. result = grouped . _aggregate_item_by_item ( how , * args , * * kwargs ) else : result = grouped . aggregate ( how , * args , * * kwargs ) except Exception : # we have a non-reducing function # try to evaluate result = grouped . apply ( how , * args , * * kwargs ) result = self . _apply_loffset ( result ) return self . _wrap_result ( result )\n\n\n        Summary:", "input": "", "output": "Re - evaluate the obj with a groupby aggregation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply_loffset ( self , result ) : needs_offset = ( isinstance ( self . loffset , ( DateOffset , timedelta , np . timedelta64 ) ) and isinstance ( result . index , DatetimeIndex ) and len ( result . index ) > 0 ) if needs_offset : result . index = result . index + self . loffset self . loffset = None return result\n\n\n        Summary:", "input": "", "output": "If loffset is set offset the result index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_resampler_for_grouping ( self , groupby , * * kwargs ) : return self . _resampler_for_grouping ( self , groupby = groupby , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return the correct class for resampling with groupby ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_result ( self , result ) : if isinstance ( result , ABCSeries ) and self . _selection is not None : result . name = self . _selection if isinstance ( result , ABCSeries ) and result . empty : obj = self . obj if isinstance ( obj . index , PeriodIndex ) : result . index = obj . index . asfreq ( self . freq ) else : result . index = obj . index . _shallow_copy ( freq = self . freq ) result . name = getattr ( obj , 'name' , None ) return result\n\n\n        Summary:", "input": "", "output": "Potentially wrap any results ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def interpolate ( self , method = 'linear' , axis = 0 , limit = None , inplace = False , limit_direction = 'forward' , limit_area = None , downcast = None , * * kwargs ) : result = self . _upsample ( None ) return result . interpolate ( method = method , axis = axis , limit = limit , inplace = inplace , limit_direction = limit_direction , limit_area = limit_area , downcast = downcast , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Interpolate values according to different methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def std ( self , ddof = 1 , * args , * * kwargs ) : nv . validate_resampler_func ( 'std' , args , kwargs ) return self . _downsample ( 'std' , ddof = ddof )\n\n\n        Summary:", "input": "", "output": "Compute standard deviation of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def var ( self , ddof = 1 , * args , * * kwargs ) : nv . validate_resampler_func ( 'var' , args , kwargs ) return self . _downsample ( 'var' , ddof = ddof )\n\n\n        Summary:", "input": "", "output": "Compute variance of groups excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _apply ( self , f , grouper = None , * args , * * kwargs ) : def func ( x ) : x = self . _shallow_copy ( x , groupby = self . groupby ) if isinstance ( f , str ) : return getattr ( x , f ) ( * * kwargs ) return x . apply ( f , * args , * * kwargs ) result = self . _groupby . apply ( func ) return self . _wrap_result ( result )\n\n\n        Summary:", "input": "", "output": "Dispatch to _upsample ; we are stripping all of the _upsample kwargs and performing the original function call on the grouped object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _downsample ( self , how , * * kwargs ) : self . _set_binner ( ) how = self . _is_cython_func ( how ) or how ax = self . ax obj = self . _selected_obj if not len ( ax ) : # reset to the new freq obj = obj . copy ( ) obj . index . freq = self . freq return obj # do we have a regular frequency if ax . freq is not None or ax . inferred_freq is not None : if len ( self . grouper . binlabels ) > len ( ax ) and how is None : # let's do an asfreq return self . asfreq ( ) # we are downsampling # we want to call the actual grouper method here result = obj . groupby ( self . grouper , axis = self . axis ) . aggregate ( how , * * kwargs ) result = self . _apply_loffset ( result ) return self . _wrap_result ( result )\n\n\n        Summary:", "input": "", "output": "Downsample the cython defined function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _adjust_binner_for_upsample ( self , binner ) : if self . closed == 'right' : binner = binner [ 1 : ] else : binner = binner [ : - 1 ] return binner\n\n\n        Summary:", "input": "", "output": "Adjust our binner when upsampling ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _upsample ( self , method , limit = None , fill_value = None ) : self . _set_binner ( ) if self . axis : raise AssertionError ( 'axis must be 0' ) if self . _from_selection : raise ValueError ( \"Upsampling from level= or on= selection\" \" is not supported, use .set_index(...)\" \" to explicitly set index to\" \" datetime-like\" ) ax = self . ax obj = self . _selected_obj binner = self . binner res_index = self . _adjust_binner_for_upsample ( binner ) # if we have the same frequency as our axis, then we are equal sampling if limit is None and to_offset ( ax . inferred_freq ) == self . freq : result = obj . copy ( ) result . index = res_index else : result = obj . reindex ( res_index , method = method , limit = limit , fill_value = fill_value ) result = self . _apply_loffset ( result ) return self . _wrap_result ( result )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- method : string { backfill bfill pad ffill asfreq } method for upsampling limit : int default None Maximum size gap to fill when reindexing fill_value : scalar default None Value to use for missing values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _downsample ( self , how , * * kwargs ) : # we may need to actually resample as if we are timestamps if self . kind == 'timestamp' : return super ( ) . _downsample ( how , * * kwargs ) how = self . _is_cython_func ( how ) or how ax = self . ax if is_subperiod ( ax . freq , self . freq ) : # Downsampling return self . _groupby_and_aggregate ( how , grouper = self . grouper , * * kwargs ) elif is_superperiod ( ax . freq , self . freq ) : if how == 'ohlc' : # GH #13083 # upsampling to subperiods is handled as an asfreq, which works # for pure aggregating/reducing methods # OHLC reduces along the time dimension, but creates multiple # values for each period -> handle by _groupby_and_aggregate() return self . _groupby_and_aggregate ( how , grouper = self . grouper ) return self . asfreq ( ) elif ax . freq == self . freq : return self . asfreq ( ) raise IncompatibleFrequency ( 'Frequency {} cannot be resampled to {}, as they are not ' 'sub or super periods' . format ( ax . freq , self . freq ) )\n\n\n        Summary:", "input": "", "output": "Downsample the cython defined function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _upsample ( self , method , limit = None , fill_value = None ) : # we may need to actually resample as if we are timestamps if self . kind == 'timestamp' : return super ( ) . _upsample ( method , limit = limit , fill_value = fill_value ) self . _set_binner ( ) ax = self . ax obj = self . obj new_index = self . binner # Start vs. end of period memb = ax . asfreq ( self . freq , how = self . convention ) # Get the fill indexer indexer = memb . get_indexer ( new_index , method = method , limit = limit ) return self . _wrap_result ( _take_new_index ( obj , indexer , new_index , axis = self . axis ) )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- method : string { backfill bfill pad ffill } method for upsampling limit : int default None Maximum size gap to fill when reindexing fill_value : scalar default None Value to use for missing values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_resampler ( self , obj , kind = None ) : self . _set_grouper ( obj ) ax = self . ax if isinstance ( ax , DatetimeIndex ) : return DatetimeIndexResampler ( obj , groupby = self , kind = kind , axis = self . axis ) elif isinstance ( ax , PeriodIndex ) or kind == 'period' : return PeriodIndexResampler ( obj , groupby = self , kind = kind , axis = self . axis ) elif isinstance ( ax , TimedeltaIndex ) : return TimedeltaIndexResampler ( obj , groupby = self , axis = self . axis ) raise TypeError ( \"Only valid with DatetimeIndex, \" \"TimedeltaIndex or PeriodIndex, \" \"but got an instance of %r\" % type ( ax ) . __name__ )\n\n\n        Summary:", "input": "", "output": "Return my resampler or raise if we have an invalid axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _combine_hash_arrays ( arrays , num_items ) : try : first = next ( arrays ) except StopIteration : return np . array ( [ ] , dtype = np . uint64 ) arrays = itertools . chain ( [ first ] , arrays ) mult = np . uint64 ( 1000003 ) out = np . zeros_like ( first ) + np . uint64 ( 0x345678 ) for i , a in enumerate ( arrays ) : inverse_i = num_items - i out ^= a out *= mult mult += np . uint64 ( 82520 + inverse_i + inverse_i ) assert i + 1 == num_items , 'Fed in wrong num_items' out += np . uint64 ( 97531 ) return out\n\n\n        Summary:", "input": "", "output": "Parameters ---------- arrays : generator num_items : int"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hash_pandas_object ( obj , index = True , encoding = 'utf8' , hash_key = None , categorize = True ) : from pandas import Series if hash_key is None : hash_key = _default_hash_key if isinstance ( obj , ABCMultiIndex ) : return Series ( hash_tuples ( obj , encoding , hash_key ) , dtype = 'uint64' , copy = False ) if isinstance ( obj , ABCIndexClass ) : h = hash_array ( obj . values , encoding , hash_key , categorize ) . astype ( 'uint64' , copy = False ) h = Series ( h , index = obj , dtype = 'uint64' , copy = False ) elif isinstance ( obj , ABCSeries ) : h = hash_array ( obj . values , encoding , hash_key , categorize ) . astype ( 'uint64' , copy = False ) if index : index_iter = ( hash_pandas_object ( obj . index , index = False , encoding = encoding , hash_key = hash_key , categorize = categorize ) . values for _ in [ None ] ) arrays = itertools . chain ( [ h ] , index_iter ) h = _combine_hash_arrays ( arrays , 2 ) h = Series ( h , index = obj . index , dtype = 'uint64' , copy = False ) elif isinstance ( obj , ABCDataFrame ) : hashes = ( hash_array ( series . values ) for _ , series in obj . iteritems ( ) ) num_items = len ( obj . columns ) if index : index_hash_generator = ( hash_pandas_object ( obj . index , index = False , encoding = encoding , hash_key = hash_key , categorize = categorize ) . values # noqa for _ in [ None ] ) num_items += 1 hashes = itertools . chain ( hashes , index_hash_generator ) h = _combine_hash_arrays ( hashes , num_items ) h = Series ( h , index = obj . index , dtype = 'uint64' , copy = False ) else : raise TypeError ( \"Unexpected type for hashing %s\" % type ( obj ) ) return h\n\n\n        Summary:", "input": "", "output": "Return a data hash of the Index / Series / DataFrame"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hash_tuples ( vals , encoding = 'utf8' , hash_key = None ) : is_tuple = False if isinstance ( vals , tuple ) : vals = [ vals ] is_tuple = True elif not is_list_like ( vals ) : raise TypeError ( \"must be convertible to a list-of-tuples\" ) from pandas import Categorical , MultiIndex if not isinstance ( vals , ABCMultiIndex ) : vals = MultiIndex . from_tuples ( vals ) # create a list-of-Categoricals vals = [ Categorical ( vals . codes [ level ] , vals . levels [ level ] , ordered = False , fastpath = True ) for level in range ( vals . nlevels ) ] # hash the list-of-ndarrays hashes = ( _hash_categorical ( cat , encoding = encoding , hash_key = hash_key ) for cat in vals ) h = _combine_hash_arrays ( hashes , len ( vals ) ) if is_tuple : h = h [ 0 ] return h\n\n\n        Summary:", "input": "", "output": "Hash an MultiIndex / list - of - tuples efficiently"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hash_tuple ( val , encoding = 'utf8' , hash_key = None ) : hashes = ( _hash_scalar ( v , encoding = encoding , hash_key = hash_key ) for v in val ) h = _combine_hash_arrays ( hashes , len ( val ) ) [ 0 ] return h\n\n\n        Summary:", "input": "", "output": "Hash a single tuple efficiently"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _hash_categorical ( c , encoding , hash_key ) : # Convert ExtensionArrays to ndarrays values = np . asarray ( c . categories . values ) hashed = hash_array ( values , encoding , hash_key , categorize = False ) # we have uint64, as we don't directly support missing values # we don't want to use take_nd which will coerce to float # instead, directly construct the result with a # max(np.uint64) as the missing value indicator # # TODO: GH 15362 mask = c . isna ( ) if len ( hashed ) : result = hashed . take ( c . codes ) else : result = np . zeros ( len ( mask ) , dtype = 'uint64' ) if mask . any ( ) : result [ mask ] = np . iinfo ( np . uint64 ) . max return result\n\n\n        Summary:", "input": "", "output": "Hash a Categorical by hashing its categories and then mapping the codes to the hashes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hash_array ( vals , encoding = 'utf8' , hash_key = None , categorize = True ) : if not hasattr ( vals , 'dtype' ) : raise TypeError ( \"must pass a ndarray-like\" ) dtype = vals . dtype if hash_key is None : hash_key = _default_hash_key # For categoricals, we hash the categories, then remap the codes to the # hash values. (This check is above the complex check so that we don't ask # numpy if categorical is a subdtype of complex, as it will choke). if is_categorical_dtype ( dtype ) : return _hash_categorical ( vals , encoding , hash_key ) elif is_extension_array_dtype ( dtype ) : vals , _ = vals . _values_for_factorize ( ) dtype = vals . dtype # we'll be working with everything as 64-bit values, so handle this # 128-bit value early if np . issubdtype ( dtype , np . complex128 ) : return hash_array ( vals . real ) + 23 * hash_array ( vals . imag ) # First, turn whatever array this is into unsigned 64-bit ints, if we can # manage it. elif isinstance ( dtype , np . bool ) : vals = vals . astype ( 'u8' ) elif issubclass ( dtype . type , ( np . datetime64 , np . timedelta64 ) ) : vals = vals . view ( 'i8' ) . astype ( 'u8' , copy = False ) elif issubclass ( dtype . type , np . number ) and dtype . itemsize <= 8 : vals = vals . view ( 'u{}' . format ( vals . dtype . itemsize ) ) . astype ( 'u8' ) else : # With repeated values, its MUCH faster to categorize object dtypes, # then hash and rename categories. We allow skipping the categorization # when the values are known/likely to be unique. if categorize : from pandas import factorize , Categorical , Index codes , categories = factorize ( vals , sort = False ) cat = Categorical ( codes , Index ( categories ) , ordered = False , fastpath = True ) return _hash_categorical ( cat , encoding , hash_key ) try : vals = hashing . hash_object_array ( vals , hash_key , encoding ) except TypeError : # we have mixed types vals = hashing . hash_object_array ( vals . astype ( str ) . astype ( object ) , hash_key , encoding ) # Then, redistribute these 64-bit ints within the space of 64-bit ints vals ^= vals >> 30 vals *= np . uint64 ( 0xbf58476d1ce4e5b9 ) vals ^= vals >> 27 vals *= np . uint64 ( 0x94d049bb133111eb ) vals ^= vals >> 31 return vals\n\n\n        Summary:", "input": "", "output": "Given a 1d array return an array of deterministic integers ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _hash_scalar ( val , encoding = 'utf8' , hash_key = None ) : if isna ( val ) : # this is to be consistent with the _hash_categorical implementation return np . array ( [ np . iinfo ( np . uint64 ) . max ] , dtype = 'u8' ) if getattr ( val , 'tzinfo' , None ) is not None : # for tz-aware datetimes, we need the underlying naive UTC value and # not the tz aware object or pd extension type (as # infer_dtype_from_scalar would do) if not isinstance ( val , tslibs . Timestamp ) : val = tslibs . Timestamp ( val ) val = val . tz_convert ( None ) dtype , val = infer_dtype_from_scalar ( val ) vals = np . array ( [ val ] , dtype = dtype ) return hash_array ( vals , hash_key = hash_key , encoding = encoding , categorize = False )\n\n\n        Summary:", "input": "", "output": "Hash scalar value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _process_single_doc ( self , single_doc ) : base_name , extension = os . path . splitext ( single_doc ) if extension in ( '.rst' , '.ipynb' ) : if os . path . exists ( os . path . join ( SOURCE_PATH , single_doc ) ) : return single_doc else : raise FileNotFoundError ( 'File {} not found' . format ( single_doc ) ) elif single_doc . startswith ( 'pandas.' ) : try : obj = pandas # noqa: F821 for name in single_doc . split ( '.' ) : obj = getattr ( obj , name ) except AttributeError : raise ImportError ( 'Could not import {}' . format ( single_doc ) ) else : return single_doc [ len ( 'pandas.' ) : ] else : raise ValueError ( ( '--single={} not understood. Value should be a ' 'valid path to a .rst or .ipynb file, or a ' 'valid pandas object (e.g. categorical.rst or ' 'pandas.DataFrame.head)' ) . format ( single_doc ) )\n\n\n        Summary:", "input": "", "output": "Make sure the provided value for -- single is a path to an existing . rst / . ipynb file or a pandas object that can be imported ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _run_os ( * args ) : subprocess . check_call ( args , stdout = sys . stdout , stderr = sys . stderr )\n\n\n        Summary:", "input": "", "output": "Execute a command as a OS terminal ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sphinx_build ( self , kind ) : if kind not in ( 'html' , 'latex' ) : raise ValueError ( 'kind must be html or latex, ' 'not {}' . format ( kind ) ) cmd = [ 'sphinx-build' , '-b' , kind ] if self . num_jobs : cmd += [ '-j' , str ( self . num_jobs ) ] if self . warnings_are_errors : cmd += [ '-W' , '--keep-going' ] if self . verbosity : cmd . append ( '-{}' . format ( 'v' * self . verbosity ) ) cmd += [ '-d' , os . path . join ( BUILD_PATH , 'doctrees' ) , SOURCE_PATH , os . path . join ( BUILD_PATH , kind ) ] return subprocess . call ( cmd )\n\n\n        Summary:", "input": "", "output": "Call sphinx to build documentation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _open_browser ( self , single_doc_html ) : url = os . path . join ( 'file://' , DOC_PATH , 'build' , 'html' , single_doc_html ) webbrowser . open ( url , new = 2 )\n\n\n        Summary:", "input": "", "output": "Open a browser tab showing single"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_page_title ( self , page ) : fname = os . path . join ( SOURCE_PATH , '{}.rst' . format ( page ) ) option_parser = docutils . frontend . OptionParser ( components = ( docutils . parsers . rst . Parser , ) ) doc = docutils . utils . new_document ( '<doc>' , option_parser . get_default_values ( ) ) with open ( fname ) as f : data = f . read ( ) parser = docutils . parsers . rst . Parser ( ) # do not generate any warning when parsing the rst with open ( os . devnull , 'a' ) as f : doc . reporter . stream = f parser . parse ( data , doc ) section = next ( node for node in doc . children if isinstance ( node , docutils . nodes . section ) ) title = next ( node for node in section . children if isinstance ( node , docutils . nodes . title ) ) return title . astext ( )\n\n\n        Summary:", "input": "", "output": "Open the rst file page and extract its title ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_redirects ( self ) : html = ''' <html> <head> <meta http-equiv=\"refresh\" content=\"0;URL={url}\"/> </head> <body> <p> The page has been moved to <a href=\"{url}\">{title}</a> </p> </body> <html> ''' with open ( REDIRECTS_FILE ) as mapping_fd : reader = csv . reader ( mapping_fd ) for row in reader : if not row or row [ 0 ] . strip ( ) . startswith ( '#' ) : continue path = os . path . join ( BUILD_PATH , 'html' , * row [ 0 ] . split ( '/' ) ) + '.html' try : title = self . _get_page_title ( row [ 1 ] ) except Exception : # the file can be an ipynb and not an rst, or docutils # may not be able to read the rst because it has some # sphinx specific stuff title = 'this page' if os . path . exists ( path ) : raise RuntimeError ( ( 'Redirection would overwrite an existing file: ' '{}' ) . format ( path ) ) with open ( path , 'w' ) as moved_page_fd : moved_page_fd . write ( html . format ( url = '{}.html' . format ( row [ 1 ] ) , title = title ) )\n\n\n        Summary:", "input": "", "output": "Create in the build directory an html file with a redirect for every row in REDIRECTS_FILE ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def html ( self ) : ret_code = self . _sphinx_build ( 'html' ) zip_fname = os . path . join ( BUILD_PATH , 'html' , 'pandas.zip' ) if os . path . exists ( zip_fname ) : os . remove ( zip_fname ) if self . single_doc_html is not None : self . _open_browser ( self . single_doc_html ) else : self . _add_redirects ( ) return ret_code\n\n\n        Summary:", "input": "", "output": "Build HTML documentation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def latex ( self , force = False ) : if sys . platform == 'win32' : sys . stderr . write ( 'latex build has not been tested on windows\\n' ) else : ret_code = self . _sphinx_build ( 'latex' ) os . chdir ( os . path . join ( BUILD_PATH , 'latex' ) ) if force : for i in range ( 3 ) : self . _run_os ( 'pdflatex' , '-interaction=nonstopmode' , 'pandas.tex' ) raise SystemExit ( 'You should check the file ' '\"build/latex/pandas.pdf\" for problems.' ) else : self . _run_os ( 'make' ) return ret_code\n\n\n        Summary:", "input": "", "output": "Build PDF documentation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def clean ( ) : shutil . rmtree ( BUILD_PATH , ignore_errors = True ) shutil . rmtree ( os . path . join ( SOURCE_PATH , 'reference' , 'api' ) , ignore_errors = True )\n\n\n        Summary:", "input": "", "output": "Clean documentation generated files ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def zip_html ( self ) : zip_fname = os . path . join ( BUILD_PATH , 'html' , 'pandas.zip' ) if os . path . exists ( zip_fname ) : os . remove ( zip_fname ) dirname = os . path . join ( BUILD_PATH , 'html' ) fnames = os . listdir ( dirname ) os . chdir ( dirname ) self . _run_os ( 'zip' , zip_fname , '-r' , '-q' , * fnames )\n\n\n        Summary:", "input": "", "output": "Compress HTML documentation into a zip file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_result ( self , buf ) : # string representation of the columns if len ( self . frame . columns ) == 0 or len ( self . frame . index ) == 0 : info_line = ( 'Empty {name}\\nColumns: {col}\\nIndex: {idx}' . format ( name = type ( self . frame ) . __name__ , col = self . frame . columns , idx = self . frame . index ) ) strcols = [ [ info_line ] ] else : strcols = self . fmt . _to_str_columns ( ) def get_col_type ( dtype ) : if issubclass ( dtype . type , np . number ) : return 'r' else : return 'l' # reestablish the MultiIndex that has been joined by _to_str_column if self . fmt . index and isinstance ( self . frame . index , ABCMultiIndex ) : out = self . frame . index . format ( adjoin = False , sparsify = self . fmt . sparsify , names = self . fmt . has_index_names , na_rep = self . fmt . na_rep ) # index.format will sparsify repeated entries with empty strings # so pad these with some empty space def pad_empties ( x ) : for pad in reversed ( x ) : if pad : break return [ x [ 0 ] ] + [ i if i else ' ' * len ( pad ) for i in x [ 1 : ] ] out = ( pad_empties ( i ) for i in out ) # Add empty spaces for each column level clevels = self . frame . columns . nlevels out = [ [ ' ' * len ( i [ - 1 ] ) ] * clevels + i for i in out ] # Add the column names to the last index column cnames = self . frame . columns . names if any ( cnames ) : new_names = [ i if i else '{}' for i in cnames ] out [ self . frame . index . nlevels - 1 ] [ : clevels ] = new_names # Get rid of old multiindex column and add new ones strcols = out + strcols [ 1 : ] column_format = self . column_format if column_format is None : dtypes = self . frame . dtypes . _values column_format = '' . join ( map ( get_col_type , dtypes ) ) if self . fmt . index : index_format = 'l' * self . frame . index . nlevels column_format = index_format + column_format elif not isinstance ( column_format , str ) : # pragma: no cover raise AssertionError ( 'column_format must be str or unicode, ' 'not {typ}' . format ( typ = type ( column_format ) ) ) if not self . longtable : buf . write ( '\\\\begin{{tabular}}{{{fmt}}}\\n' . format ( fmt = column_format ) ) buf . write ( '\\\\toprule\\n' ) else : buf . write ( '\\\\begin{{longtable}}{{{fmt}}}\\n' . format ( fmt = column_format ) ) buf . write ( '\\\\toprule\\n' ) ilevels = self . frame . index . nlevels clevels = self . frame . columns . nlevels nlevels = clevels if self . fmt . has_index_names and self . fmt . show_index_names : nlevels += 1 strrows = list ( zip ( * strcols ) ) self . clinebuf = [ ] for i , row in enumerate ( strrows ) : if i == nlevels and self . fmt . header : buf . write ( '\\\\midrule\\n' ) # End of header if self . longtable : buf . write ( '\\\\endhead\\n' ) buf . write ( '\\\\midrule\\n' ) buf . write ( '\\\\multicolumn{{{n}}}{{r}}{{{{Continued on next ' 'page}}}} \\\\\\\\\\n' . format ( n = len ( row ) ) ) buf . write ( '\\\\midrule\\n' ) buf . write ( '\\\\endfoot\\n\\n' ) buf . write ( '\\\\bottomrule\\n' ) buf . write ( '\\\\endlastfoot\\n' ) if self . fmt . kwds . get ( 'escape' , True ) : # escape backslashes first crow = [ ( x . replace ( '\\\\' , '\\\\textbackslash ' ) . replace ( '_' , '\\\\_' ) . replace ( '%' , '\\\\%' ) . replace ( '$' , '\\\\$' ) . replace ( '#' , '\\\\#' ) . replace ( '{' , '\\\\{' ) . replace ( '}' , '\\\\}' ) . replace ( '~' , '\\\\textasciitilde ' ) . replace ( '^' , '\\\\textasciicircum ' ) . replace ( '&' , '\\\\&' ) if ( x and x != '{}' ) else '{}' ) for x in row ] else : crow = [ x if x else '{}' for x in row ] if self . bold_rows and self . fmt . index : # bold row labels crow = [ '\\\\textbf{{{x}}}' . format ( x = x ) if j < ilevels and x . strip ( ) not in [ '' , '{}' ] else x for j , x in enumerate ( crow ) ] if i < clevels and self . fmt . header and self . multicolumn : # sum up columns to multicolumns crow = self . _format_multicolumn ( crow , ilevels ) if ( i >= nlevels and self . fmt . index and self . multirow and ilevels > 1 ) : # sum up rows to multirows crow = self . _format_multirow ( crow , ilevels , i , strrows ) buf . write ( ' & ' . join ( crow ) ) buf . write ( ' \\\\\\\\\\n' ) if self . multirow and i < len ( strrows ) - 1 : self . _print_cline ( buf , i , len ( strcols ) ) if not self . longtable : buf . write ( '\\\\bottomrule\\n' ) buf . write ( '\\\\end{tabular}\\n' ) else : buf . write ( '\\\\end{longtable}\\n' )\n\n\n        Summary:", "input": "", "output": "Render a DataFrame to a LaTeX tabular / longtable environment output ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_multicolumn ( self , row , ilevels ) : row2 = list ( row [ : ilevels ] ) ncol = 1 coltext = '' def append_col ( ) : # write multicolumn if needed if ncol > 1 : row2 . append ( '\\\\multicolumn{{{ncol:d}}}{{{fmt:s}}}{{{txt:s}}}' . format ( ncol = ncol , fmt = self . multicolumn_format , txt = coltext . strip ( ) ) ) # don't modify where not needed else : row2 . append ( coltext ) for c in row [ ilevels : ] : # if next col has text, write the previous if c . strip ( ) : if coltext : append_col ( ) coltext = c ncol = 1 # if not, add it to the previous multicolumn else : ncol += 1 # write last column name if coltext : append_col ( ) return row2\n\n\n        Summary:", "input": "", "output": "r Combine columns belonging to a group to a single multicolumn entry according to self . multicolumn_format"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_multirow ( self , row , ilevels , i , rows ) : for j in range ( ilevels ) : if row [ j ] . strip ( ) : nrow = 1 for r in rows [ i + 1 : ] : if not r [ j ] . strip ( ) : nrow += 1 else : break if nrow > 1 : # overwrite non-multirow entry row [ j ] = '\\\\multirow{{{nrow:d}}}{{*}}{{{row:s}}}' . format ( nrow = nrow , row = row [ j ] . strip ( ) ) # save when to end the current block with \\cline self . clinebuf . append ( [ i + nrow - 1 , j + 1 ] ) return row\n\n\n        Summary:", "input": "", "output": "r Check following rows whether row should be a multirow"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _print_cline ( self , buf , i , icol ) : for cl in self . clinebuf : if cl [ 0 ] == i : buf . write ( '\\\\cline{{{cl:d}-{icol:d}}}\\n' . format ( cl = cl [ 1 ] , icol = icol ) ) # remove entries that have been written to buffer self . clinebuf = [ x for x in self . clinebuf if x [ 0 ] != i ]\n\n\n        Summary:", "input": "", "output": "Print clines after multirow - blocks are finished"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_integer ( name , val , min_val = 0 ) : msg = \"'{name:s}' must be an integer >={min_val:d}\" . format ( name = name , min_val = min_val ) if val is not None : if is_float ( val ) : if int ( val ) != val : raise ValueError ( msg ) val = int ( val ) elif not ( is_integer ( val ) and val >= min_val ) : raise ValueError ( msg ) return val\n\n\n        Summary:", "input": "", "output": "Checks whether the name parameter for parsing is either an integer OR float that can SAFELY be cast to an integer without losing accuracy . Raises a ValueError if that is not the case ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_names ( names ) : if names is not None : if len ( names ) != len ( set ( names ) ) : msg = ( \"Duplicate names specified. This \" \"will raise an error in the future.\" ) warnings . warn ( msg , UserWarning , stacklevel = 3 ) return names\n\n\n        Summary:", "input": "", "output": "Check if the names parameter contains duplicates ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _read ( filepath_or_buffer : FilePathOrBuffer , kwds ) : encoding = kwds . get ( 'encoding' , None ) if encoding is not None : encoding = re . sub ( '_' , '-' , encoding ) . lower ( ) kwds [ 'encoding' ] = encoding compression = kwds . get ( 'compression' , 'infer' ) compression = _infer_compression ( filepath_or_buffer , compression ) # TODO: get_filepath_or_buffer could return # Union[FilePathOrBuffer, s3fs.S3File, gcsfs.GCSFile] # though mypy handling of conditional imports is difficult. # See https://github.com/python/mypy/issues/1297 fp_or_buf , _ , compression , should_close = get_filepath_or_buffer ( filepath_or_buffer , encoding , compression ) kwds [ 'compression' ] = compression if kwds . get ( 'date_parser' , None ) is not None : if isinstance ( kwds [ 'parse_dates' ] , bool ) : kwds [ 'parse_dates' ] = True # Extract some of the arguments (pass chunksize on). iterator = kwds . get ( 'iterator' , False ) chunksize = _validate_integer ( 'chunksize' , kwds . get ( 'chunksize' , None ) , 1 ) nrows = kwds . get ( 'nrows' , None ) # Check for duplicates in names. _validate_names ( kwds . get ( \"names\" , None ) ) # Create the parser. parser = TextFileReader ( fp_or_buf , * * kwds ) if chunksize or iterator : return parser try : data = parser . read ( nrows ) finally : parser . close ( ) if should_close : try : fp_or_buf . close ( ) except ValueError : pass return data\n\n\n        Summary:", "input": "", "output": "Generic reader of line files ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_fwf ( filepath_or_buffer : FilePathOrBuffer , colspecs = 'infer' , widths = None , infer_nrows = 100 , * * kwds ) : # Check input arguments. if colspecs is None and widths is None : raise ValueError ( \"Must specify either colspecs or widths\" ) elif colspecs not in ( None , 'infer' ) and widths is not None : raise ValueError ( \"You must specify only one of 'widths' and \" \"'colspecs'\" ) # Compute 'colspecs' from 'widths', if specified. if widths is not None : colspecs , col = [ ] , 0 for w in widths : colspecs . append ( ( col , col + w ) ) col += w kwds [ 'colspecs' ] = colspecs kwds [ 'infer_nrows' ] = infer_nrows kwds [ 'engine' ] = 'python-fwf' return _read ( filepath_or_buffer , kwds )\n\n\n        Summary:", "input": "", "output": "r Read a table of fixed - width formatted lines into DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_potential_multi_index ( columns ) : return ( len ( columns ) and not isinstance ( columns , MultiIndex ) and all ( isinstance ( c , tuple ) for c in columns ) )\n\n\n        Summary:", "input": "", "output": "Check whether or not the columns parameter could be converted into a MultiIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _evaluate_usecols ( usecols , names ) : if callable ( usecols ) : return { i for i , name in enumerate ( names ) if usecols ( name ) } return usecols\n\n\n        Summary:", "input": "", "output": "Check whether or not the usecols parameter is a callable . If so enumerates the names parameter and returns a set of indices for each entry in names that evaluates to True . If not a callable returns usecols ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_usecols_names ( usecols , names ) : missing = [ c for c in usecols if c not in names ] if len ( missing ) > 0 : raise ValueError ( \"Usecols do not match columns, \" \"columns expected but not found: {missing}\" . format ( missing = missing ) ) return usecols\n\n\n        Summary:", "input": "", "output": "Validates that all usecols are present in a given list of names . If not raise a ValueError that shows what usecols are missing ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_usecols_arg ( usecols ) : msg = ( \"'usecols' must either be list-like of all strings, all unicode, \" \"all integers or a callable.\" ) if usecols is not None : if callable ( usecols ) : return usecols , None if not is_list_like ( usecols ) : # see gh-20529 # # Ensure it is iterable container but not string. raise ValueError ( msg ) usecols_dtype = lib . infer_dtype ( usecols , skipna = False ) if usecols_dtype not in ( \"empty\" , \"integer\" , \"string\" , \"unicode\" ) : raise ValueError ( msg ) usecols = set ( usecols ) return usecols , usecols_dtype return usecols , None\n\n\n        Summary:", "input": "", "output": "Validate the usecols parameter ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_parse_dates_arg ( parse_dates ) : msg = ( \"Only booleans, lists, and \" \"dictionaries are accepted \" \"for the 'parse_dates' parameter\" ) if parse_dates is not None : if is_scalar ( parse_dates ) : if not lib . is_bool ( parse_dates ) : raise TypeError ( msg ) elif not isinstance ( parse_dates , ( list , dict ) ) : raise TypeError ( msg ) return parse_dates\n\n\n        Summary:", "input": "", "output": "Check whether or not the parse_dates parameter is a non - boolean scalar . Raises a ValueError if that is the case ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _stringify_na_values ( na_values ) : result = [ ] for x in na_values : result . append ( str ( x ) ) result . append ( x ) try : v = float ( x ) # we are like 999 here if v == int ( v ) : v = int ( v ) result . append ( \"{value}.0\" . format ( value = v ) ) result . append ( str ( v ) ) result . append ( v ) except ( TypeError , ValueError , OverflowError ) : pass try : result . append ( int ( x ) ) except ( TypeError , ValueError , OverflowError ) : pass return set ( result )\n\n\n        Summary:", "input": "", "output": "return a stringified and numeric for these values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_na_values ( col , na_values , na_fvalues , keep_default_na ) : if isinstance ( na_values , dict ) : if col in na_values : return na_values [ col ] , na_fvalues [ col ] else : if keep_default_na : return _NA_VALUES , set ( ) return set ( ) , set ( ) else : return na_values , na_fvalues\n\n\n        Summary:", "input": "", "output": "Get the NaN values for a given column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _extract_multi_indexer_columns ( self , header , index_names , col_names , passed_names = False ) : if len ( header ) < 2 : return header [ 0 ] , index_names , col_names , passed_names # the names are the tuples of the header that are not the index cols # 0 is the name of the index, assuming index_col is a list of column # numbers ic = self . index_col if ic is None : ic = [ ] if not isinstance ( ic , ( list , tuple , np . ndarray ) ) : ic = [ ic ] sic = set ( ic ) # clean the index_names index_names = header . pop ( - 1 ) index_names , names , index_col = _clean_index_names ( index_names , self . index_col , self . unnamed_cols ) # extract the columns field_count = len ( header [ 0 ] ) def extract ( r ) : return tuple ( r [ i ] for i in range ( field_count ) if i not in sic ) columns = lzip ( * [ extract ( r ) for r in header ] ) names = ic + columns # If we find unnamed columns all in a single # level, then our header was too long. for n in range ( len ( columns [ 0 ] ) ) : if all ( compat . to_str ( c [ n ] ) in self . unnamed_cols for c in columns ) : raise ParserError ( \"Passed header=[{header}] are too many rows for this \" \"multi_index of columns\" . format ( header = ',' . join ( str ( x ) for x in self . header ) ) ) # Clean the column names (if we have an index_col). if len ( ic ) : col_names = [ r [ 0 ] if ( len ( r [ 0 ] ) and r [ 0 ] not in self . unnamed_cols ) else None for r in header ] else : col_names = [ None ] * len ( header ) passed_names = True return names , index_names , col_names , passed_names\n\n\n        Summary:", "input": "", "output": "extract and return the names index_names col_names header is a list - of - lists returned from the parsers"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _infer_types ( self , values , na_values , try_num_bool = True ) : na_count = 0 if issubclass ( values . dtype . type , ( np . number , np . bool_ ) ) : mask = algorithms . isin ( values , list ( na_values ) ) na_count = mask . sum ( ) if na_count > 0 : if is_integer_dtype ( values ) : values = values . astype ( np . float64 ) np . putmask ( values , mask , np . nan ) return values , na_count if try_num_bool : try : result = lib . maybe_convert_numeric ( values , na_values , False ) na_count = isna ( result ) . sum ( ) except Exception : result = values if values . dtype == np . object_ : na_count = parsers . sanitize_objects ( result , na_values , False ) else : result = values if values . dtype == np . object_ : na_count = parsers . sanitize_objects ( values , na_values , False ) if result . dtype == np . object_ and try_num_bool : result = libops . maybe_convert_bool ( np . asarray ( values ) , true_values = self . true_values , false_values = self . false_values ) return result , na_count\n\n\n        Summary:", "input": "", "output": "Infer types of values possibly casting"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cast_types ( self , values , cast_type , column ) : if is_categorical_dtype ( cast_type ) : known_cats = ( isinstance ( cast_type , CategoricalDtype ) and cast_type . categories is not None ) if not is_object_dtype ( values ) and not known_cats : # XXX this is for consistency with # c-parser which parses all categories # as strings values = astype_nansafe ( values , str ) cats = Index ( values ) . unique ( ) . dropna ( ) values = Categorical . _from_inferred_categories ( cats , cats . get_indexer ( values ) , cast_type , true_values = self . true_values ) # use the EA's implementation of casting elif is_extension_array_dtype ( cast_type ) : # ensure cast_type is an actual dtype and not a string cast_type = pandas_dtype ( cast_type ) array_type = cast_type . construct_array_type ( ) try : return array_type . _from_sequence_of_strings ( values , dtype = cast_type ) except NotImplementedError : raise NotImplementedError ( \"Extension Array: {ea} must implement \" \"_from_sequence_of_strings in order \" \"to be used in parser methods\" . format ( ea = array_type ) ) else : try : values = astype_nansafe ( values , cast_type , copy = True , skipna = True ) except ValueError : raise ValueError ( \"Unable to convert column {column} to type \" \"{cast_type}\" . format ( column = column , cast_type = cast_type ) ) return values\n\n\n        Summary:", "input": "", "output": "Cast values to specified type"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_noconvert_columns ( self ) : names = self . orig_names if self . usecols_dtype == 'integer' : # A set of integers will be converted to a list in # the correct order every single time. usecols = list ( self . usecols ) usecols . sort ( ) elif ( callable ( self . usecols ) or self . usecols_dtype not in ( 'empty' , None ) ) : # The names attribute should have the correct columns # in the proper order for indexing with parse_dates. usecols = self . names [ : ] else : # Usecols is empty. usecols = None def _set ( x ) : if usecols is not None and is_integer ( x ) : x = usecols [ x ] if not is_integer ( x ) : x = names . index ( x ) self . _reader . set_noconvert ( x ) if isinstance ( self . parse_dates , list ) : for val in self . parse_dates : if isinstance ( val , list ) : for k in val : _set ( k ) else : _set ( val ) elif isinstance ( self . parse_dates , dict ) : for val in self . parse_dates . values ( ) : if isinstance ( val , list ) : for k in val : _set ( k ) else : _set ( val ) elif self . parse_dates : if isinstance ( self . index_col , list ) : for k in self . index_col : _set ( k ) elif self . index_col is not None : _set ( self . index_col )\n\n\n        Summary:", "input": "", "output": "Set the columns that should not undergo dtype conversions ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _handle_usecols ( self , columns , usecols_key ) : if self . usecols is not None : if callable ( self . usecols ) : col_indices = _evaluate_usecols ( self . usecols , usecols_key ) elif any ( isinstance ( u , str ) for u in self . usecols ) : if len ( columns ) > 1 : raise ValueError ( \"If using multiple headers, usecols must \" \"be integers.\" ) col_indices = [ ] for col in self . usecols : if isinstance ( col , str ) : try : col_indices . append ( usecols_key . index ( col ) ) except ValueError : _validate_usecols_names ( self . usecols , usecols_key ) else : col_indices . append ( col ) else : col_indices = self . usecols columns = [ [ n for i , n in enumerate ( column ) if i in col_indices ] for column in columns ] self . _col_indices = col_indices return columns\n\n\n        Summary:", "input": "", "output": "Sets self . _col_indices"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_for_bom ( self , first_row ) : # first_row will be a list, so we need to check # that that list is not empty before proceeding. if not first_row : return first_row # The first element of this row is the one that could have the # BOM that we want to remove. Check that the first element is a # string before proceeding. if not isinstance ( first_row [ 0 ] , str ) : return first_row # Check that the string is not empty, as that would # obviously not have a BOM at the start of it. if not first_row [ 0 ] : return first_row # Since the string is non-empty, check that it does # in fact begin with a BOM. first_elt = first_row [ 0 ] [ 0 ] if first_elt != _BOM : return first_row first_row = first_row [ 0 ] if len ( first_row ) > 1 and first_row [ 1 ] == self . quotechar : start = 2 quote = first_row [ 1 ] end = first_row [ 2 : ] . index ( quote ) + 2 # Extract the data between the quotation marks new_row = first_row [ start : end ] # Extract any remaining data after the second # quotation mark. if len ( first_row ) > end + 1 : new_row += first_row [ end + 1 : ] return [ new_row ] elif len ( first_row ) > 1 : return [ first_row [ 1 : ] ] else : # First row is just the BOM, so we # return an empty string. return [ \"\" ]\n\n\n        Summary:", "input": "", "output": "Checks whether the file begins with the BOM character . If it does remove it . In addition if there is quoting in the field subsequent to the BOM remove it as well because it technically takes place at the beginning of the name not the middle of it ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _alert_malformed ( self , msg , row_num ) : if self . error_bad_lines : raise ParserError ( msg ) elif self . warn_bad_lines : base = 'Skipping line {row_num}: ' . format ( row_num = row_num ) sys . stderr . write ( base + msg + '\\n' )\n\n\n        Summary:", "input": "", "output": "Alert a user about a malformed row ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _next_iter_line ( self , row_num ) : try : return next ( self . data ) except csv . Error as e : if self . warn_bad_lines or self . error_bad_lines : msg = str ( e ) if 'NULL byte' in msg : msg = ( 'NULL byte detected. This byte ' 'cannot be processed in Python\\'s ' 'native csv library at the moment, ' 'so please pass in engine=\\'c\\' instead' ) if self . skipfooter > 0 : reason = ( 'Error could possibly be due to ' 'parsing errors in the skipped footer rows ' '(the skipfooter keyword is only applied ' 'after Python\\'s csv library has parsed ' 'all rows).' ) msg += '. ' + reason self . _alert_malformed ( msg , row_num ) return None\n\n\n        Summary:", "input": "", "output": "Wrapper around iterating through self . data ( CSV source ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _remove_empty_lines ( self , lines ) : ret = [ ] for l in lines : # Remove empty lines and lines with only one whitespace value if ( len ( l ) > 1 or len ( l ) == 1 and ( not isinstance ( l [ 0 ] , str ) or l [ 0 ] . strip ( ) ) ) : ret . append ( l ) return ret\n\n\n        Summary:", "input": "", "output": "Iterate through the lines and remove any that are either empty or contain only one whitespace value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_index_name ( self , columns ) : orig_names = list ( columns ) columns = list ( columns ) try : line = self . _next_line ( ) except StopIteration : line = None try : next_line = self . _next_line ( ) except StopIteration : next_line = None # implicitly index_col=0 b/c 1 fewer column names implicit_first_cols = 0 if line is not None : # leave it 0, #2442 # Case 1 if self . index_col is not False : implicit_first_cols = len ( line ) - self . num_original_columns # Case 0 if next_line is not None : if len ( next_line ) == len ( line ) + self . num_original_columns : # column and index names on diff rows self . index_col = lrange ( len ( line ) ) self . buf = self . buf [ 1 : ] for c in reversed ( line ) : columns . insert ( 0 , c ) # Update list of original names to include all indices. orig_names = list ( columns ) self . num_original_columns = len ( columns ) return line , orig_names , columns if implicit_first_cols > 0 : # Case 1 self . _implicit_index = True if self . index_col is None : self . index_col = lrange ( implicit_first_cols ) index_name = None else : # Case 2 ( index_name , columns_ , self . index_col ) = _clean_index_names ( columns , self . index_col , self . unnamed_cols ) return index_name , orig_names , columns\n\n\n        Summary:", "input": "", "output": "Try several cases to get lines :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_rows ( self , infer_nrows , skiprows = None ) : if skiprows is None : skiprows = set ( ) buffer_rows = [ ] detect_rows = [ ] for i , row in enumerate ( self . f ) : if i not in skiprows : detect_rows . append ( row ) buffer_rows . append ( row ) if len ( detect_rows ) >= infer_nrows : break self . buffer = iter ( buffer_rows ) return detect_rows\n\n\n        Summary:", "input": "", "output": "Read rows from self . f skipping as specified ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def linkcode_resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except AttributeError : return None try : # inspect.unwrap() was added in Python version 3.4 if sys . version_info >= ( 3 , 5 ) : fn = inspect . getsourcefile ( inspect . unwrap ( obj ) ) else : fn = inspect . getsourcefile ( obj ) except TypeError : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except OSError : lineno = None if lineno : linespec = \"#L{:d}-L{:d}\" . format ( lineno , lineno + len ( source ) - 1 ) else : linespec = \"\" fn = os . path . relpath ( fn , start = os . path . dirname ( pandas . __file__ ) ) if '+' in pandas . __version__ : return ( \"http://github.com/pandas-dev/pandas/blob/master/pandas/\" \"{}{}\" . format ( fn , linespec ) ) else : return ( \"http://github.com/pandas-dev/pandas/blob/\" \"v{}/pandas/{}{}\" . format ( pandas . __version__ , fn , linespec ) )\n\n\n        Summary:", "input": "", "output": "Determine the URL corresponding to Python object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def process_class_docstrings ( app , what , name , obj , options , lines ) : if what == \"class\" : joined = '\\n' . join ( lines ) templates = [ \"\"\".. rubric:: Attributes .. autosummary:: :toctree: None \"\"\" , \"\"\".. rubric:: Methods .. autosummary:: :toctree: None \"\"\" ] for template in templates : if template in joined : joined = joined . replace ( template , '' ) lines [ : ] = joined . split ( '\\n' )\n\n\n        Summary:", "input": "", "output": "For those classes for which we use ::"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pack ( o , stream , * * kwargs ) : packer = Packer ( * * kwargs ) stream . write ( packer . pack ( o ) )\n\n\n        Summary:", "input": "", "output": "Pack object o and write it to stream"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_mgr_concatenation_plan ( mgr , indexers ) : # Calculate post-reindex shape , save for item axis which will be separate # for each block anyway. mgr_shape = list ( mgr . shape ) for ax , indexer in indexers . items ( ) : mgr_shape [ ax ] = len ( indexer ) mgr_shape = tuple ( mgr_shape ) if 0 in indexers : ax0_indexer = indexers . pop ( 0 ) blknos = algos . take_1d ( mgr . _blknos , ax0_indexer , fill_value = - 1 ) blklocs = algos . take_1d ( mgr . _blklocs , ax0_indexer , fill_value = - 1 ) else : if mgr . _is_single_block : blk = mgr . blocks [ 0 ] return [ ( blk . mgr_locs , JoinUnit ( blk , mgr_shape , indexers ) ) ] ax0_indexer = None blknos = mgr . _blknos blklocs = mgr . _blklocs plan = [ ] for blkno , placements in libinternals . get_blkno_placements ( blknos , mgr . nblocks , group = False ) : assert placements . is_slice_like join_unit_indexers = indexers . copy ( ) shape = list ( mgr_shape ) shape [ 0 ] = len ( placements ) shape = tuple ( shape ) if blkno == - 1 : unit = JoinUnit ( None , shape ) else : blk = mgr . blocks [ blkno ] ax0_blk_indexer = blklocs [ placements . indexer ] unit_no_ax0_reindexing = ( len ( placements ) == len ( blk . mgr_locs ) and # Fastpath detection of join unit not # needing to reindex its block: no ax0 # reindexing took place and block # placement was sequential before. ( ( ax0_indexer is None and blk . mgr_locs . is_slice_like and blk . mgr_locs . as_slice . step == 1 ) or # Slow-ish detection: all indexer locs # are sequential (and length match is # checked above). ( np . diff ( ax0_blk_indexer ) == 1 ) . all ( ) ) ) # Omit indexer if no item reindexing is required. if unit_no_ax0_reindexing : join_unit_indexers . pop ( 0 , None ) else : join_unit_indexers [ 0 ] = ax0_blk_indexer unit = JoinUnit ( blk , shape , join_unit_indexers ) plan . append ( ( placements , unit ) ) return plan\n\n\n        Summary:", "input": "", "output": "Construct concatenation plan for given block manager and indexers ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def concatenate_join_units ( join_units , concat_axis , copy ) : if concat_axis == 0 and len ( join_units ) > 1 : # Concatenating join units along ax0 is handled in _merge_blocks. raise AssertionError ( \"Concatenating join units along axis0\" ) empty_dtype , upcasted_na = get_empty_dtype_and_na ( join_units ) to_concat = [ ju . get_reindexed_values ( empty_dtype = empty_dtype , upcasted_na = upcasted_na ) for ju in join_units ] if len ( to_concat ) == 1 : # Only one block, nothing to concatenate. concat_values = to_concat [ 0 ] if copy : if isinstance ( concat_values , np . ndarray ) : # non-reindexed (=not yet copied) arrays are made into a view # in JoinUnit.get_reindexed_values if concat_values . base is not None : concat_values = concat_values . copy ( ) else : concat_values = concat_values . copy ( ) else : concat_values = _concat . _concat_compat ( to_concat , axis = concat_axis ) return concat_values\n\n\n        Summary:", "input": "", "output": "Concatenate values from several join units along selected axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_empty_dtype_and_na ( join_units ) : if len ( join_units ) == 1 : blk = join_units [ 0 ] . block if blk is None : return np . float64 , np . nan if is_uniform_reindex ( join_units ) : # XXX: integrate property empty_dtype = join_units [ 0 ] . block . dtype upcasted_na = join_units [ 0 ] . block . fill_value return empty_dtype , upcasted_na has_none_blocks = False dtypes = [ None ] * len ( join_units ) for i , unit in enumerate ( join_units ) : if unit . block is None : has_none_blocks = True else : dtypes [ i ] = unit . dtype upcast_classes = defaultdict ( list ) null_upcast_classes = defaultdict ( list ) for dtype , unit in zip ( dtypes , join_units ) : if dtype is None : continue if is_categorical_dtype ( dtype ) : upcast_cls = 'category' elif is_datetime64tz_dtype ( dtype ) : upcast_cls = 'datetimetz' elif issubclass ( dtype . type , np . bool_ ) : upcast_cls = 'bool' elif issubclass ( dtype . type , np . object_ ) : upcast_cls = 'object' elif is_datetime64_dtype ( dtype ) : upcast_cls = 'datetime' elif is_timedelta64_dtype ( dtype ) : upcast_cls = 'timedelta' elif is_sparse ( dtype ) : upcast_cls = dtype . subtype . name elif is_extension_array_dtype ( dtype ) : upcast_cls = 'object' elif is_float_dtype ( dtype ) or is_numeric_dtype ( dtype ) : upcast_cls = dtype . name else : upcast_cls = 'float' # Null blocks should not influence upcast class selection, unless there # are only null blocks, when same upcasting rules must be applied to # null upcast classes. if unit . is_na : null_upcast_classes [ upcast_cls ] . append ( dtype ) else : upcast_classes [ upcast_cls ] . append ( dtype ) if not upcast_classes : upcast_classes = null_upcast_classes # create the result if 'object' in upcast_classes : return np . dtype ( np . object_ ) , np . nan elif 'bool' in upcast_classes : if has_none_blocks : return np . dtype ( np . object_ ) , np . nan else : return np . dtype ( np . bool_ ) , None elif 'category' in upcast_classes : return np . dtype ( np . object_ ) , np . nan elif 'datetimetz' in upcast_classes : # GH-25014. We use NaT instead of iNaT, since this eventually # ends up in DatetimeArray.take, which does not allow iNaT. dtype = upcast_classes [ 'datetimetz' ] return dtype [ 0 ] , tslibs . NaT elif 'datetime' in upcast_classes : return np . dtype ( 'M8[ns]' ) , tslibs . iNaT elif 'timedelta' in upcast_classes : return np . dtype ( 'm8[ns]' ) , tslibs . iNaT else : # pragma try : g = np . find_common_type ( upcast_classes , [ ] ) except TypeError : # At least one is an ExtensionArray return np . dtype ( np . object_ ) , np . nan else : if is_float_dtype ( g ) : return g , g . type ( np . nan ) elif is_numeric_dtype ( g ) : if has_none_blocks : return np . float64 , np . nan else : return g , None msg = \"invalid dtype determination in get_concat_dtype\" raise AssertionError ( msg )\n\n\n        Summary:", "input": "", "output": "Return dtype and N / A values to use when concatenating specified units ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_uniform_join_units ( join_units ) : return ( # all blocks need to have the same type all ( type ( ju . block ) is type ( join_units [ 0 ] . block ) for ju in join_units ) and # noqa # no blocks that would get missing values (can lead to type upcasts) # unless we're an extension dtype. all ( not ju . is_na or ju . block . is_extension for ju in join_units ) and # no blocks with indexers (as then the dimensions do not fit) all ( not ju . indexers for ju in join_units ) and # disregard Panels all ( ju . block . ndim <= 2 for ju in join_units ) and # only use this path when there is something to concatenate len ( join_units ) > 1 )\n\n\n        Summary:", "input": "", "output": "Check if the join units consist of blocks of uniform type that can be concatenated using Block . concat_same_type instead of the generic concatenate_join_units ( which uses _concat . _concat_compat ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def trim_join_unit ( join_unit , length ) : if 0 not in join_unit . indexers : extra_indexers = join_unit . indexers if join_unit . block is None : extra_block = None else : extra_block = join_unit . block . getitem_block ( slice ( length , None ) ) join_unit . block = join_unit . block . getitem_block ( slice ( length ) ) else : extra_block = join_unit . block extra_indexers = copy . copy ( join_unit . indexers ) extra_indexers [ 0 ] = extra_indexers [ 0 ] [ length : ] join_unit . indexers [ 0 ] = join_unit . indexers [ 0 ] [ : length ] extra_shape = ( join_unit . shape [ 0 ] - length , ) + join_unit . shape [ 1 : ] join_unit . shape = ( length , ) + join_unit . shape [ 1 : ] return JoinUnit ( block = extra_block , indexers = extra_indexers , shape = extra_shape )\n\n\n        Summary:", "input": "", "output": "Reduce join_unit s shape along item axis to length ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine_concat_plans ( plans , concat_axis ) : if len ( plans ) == 1 : for p in plans [ 0 ] : yield p [ 0 ] , [ p [ 1 ] ] elif concat_axis == 0 : offset = 0 for plan in plans : last_plc = None for plc , unit in plan : yield plc . add ( offset ) , [ unit ] last_plc = plc if last_plc is not None : offset += last_plc . as_slice . stop else : num_ended = [ 0 ] def _next_or_none ( seq ) : retval = next ( seq , None ) if retval is None : num_ended [ 0 ] += 1 return retval plans = list ( map ( iter , plans ) ) next_items = list ( map ( _next_or_none , plans ) ) while num_ended [ 0 ] != len ( next_items ) : if num_ended [ 0 ] > 0 : raise ValueError ( \"Plan shapes are not aligned\" ) placements , units = zip ( * next_items ) lengths = list ( map ( len , placements ) ) min_len , max_len = min ( lengths ) , max ( lengths ) if min_len == max_len : yield placements [ 0 ] , units next_items [ : ] = map ( _next_or_none , plans ) else : yielded_placement = None yielded_units = [ None ] * len ( next_items ) for i , ( plc , unit ) in enumerate ( next_items ) : yielded_units [ i ] = unit if len ( plc ) > min_len : # trim_join_unit updates unit in place, so only # placement needs to be sliced to skip min_len. next_items [ i ] = ( plc [ min_len : ] , trim_join_unit ( unit , min_len ) ) else : yielded_placement = plc next_items [ i ] = _next_or_none ( plans [ i ] ) yield yielded_placement , yielded_units\n\n\n        Summary:", "input": "", "output": "Combine multiple concatenation plans into one ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def use ( self , key , value ) : old_value = self [ key ] try : self [ key ] = value yield self finally : self [ key ] = old_value\n\n\n        Summary:", "input": "", "output": "Temporarily set a parameter value using the with statement . Aliasing allowed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _stata_elapsed_date_to_datetime_vec ( dates , fmt ) : MIN_YEAR , MAX_YEAR = Timestamp . min . year , Timestamp . max . year MAX_DAY_DELTA = ( Timestamp . max - datetime . datetime ( 1960 , 1 , 1 ) ) . days MIN_DAY_DELTA = ( Timestamp . min - datetime . datetime ( 1960 , 1 , 1 ) ) . days MIN_MS_DELTA = MIN_DAY_DELTA * 24 * 3600 * 1000 MAX_MS_DELTA = MAX_DAY_DELTA * 24 * 3600 * 1000 def convert_year_month_safe ( year , month ) : \"\"\" Convert year and month to datetimes, using pandas vectorized versions when the date range falls within the range supported by pandas. Otherwise it falls back to a slower but more robust method using datetime. \"\"\" if year . max ( ) < MAX_YEAR and year . min ( ) > MIN_YEAR : return to_datetime ( 100 * year + month , format = '%Y%m' ) else : index = getattr ( year , 'index' , None ) return Series ( [ datetime . datetime ( y , m , 1 ) for y , m in zip ( year , month ) ] , index = index ) def convert_year_days_safe ( year , days ) : \"\"\" Converts year (e.g. 1999) and days since the start of the year to a datetime or datetime64 Series \"\"\" if year . max ( ) < ( MAX_YEAR - 1 ) and year . min ( ) > MIN_YEAR : return ( to_datetime ( year , format = '%Y' ) + to_timedelta ( days , unit = 'd' ) ) else : index = getattr ( year , 'index' , None ) value = [ datetime . datetime ( y , 1 , 1 ) + relativedelta ( days = int ( d ) ) for y , d in zip ( year , days ) ] return Series ( value , index = index ) def convert_delta_safe ( base , deltas , unit ) : \"\"\" Convert base dates and deltas to datetimes, using pandas vectorized versions if the deltas satisfy restrictions required to be expressed as dates in pandas. \"\"\" index = getattr ( deltas , 'index' , None ) if unit == 'd' : if deltas . max ( ) > MAX_DAY_DELTA or deltas . min ( ) < MIN_DAY_DELTA : values = [ base + relativedelta ( days = int ( d ) ) for d in deltas ] return Series ( values , index = index ) elif unit == 'ms' : if deltas . max ( ) > MAX_MS_DELTA or deltas . min ( ) < MIN_MS_DELTA : values = [ base + relativedelta ( microseconds = ( int ( d ) * 1000 ) ) for d in deltas ] return Series ( values , index = index ) else : raise ValueError ( 'format not understood' ) base = to_datetime ( base ) deltas = to_timedelta ( deltas , unit = unit ) return base + deltas # TODO: If/when pandas supports more than datetime64[ns], this should be # improved to use correct range, e.g. datetime[Y] for yearly bad_locs = np . isnan ( dates ) has_bad_values = False if bad_locs . any ( ) : has_bad_values = True data_col = Series ( dates ) data_col [ bad_locs ] = 1.0 # Replace with NaT dates = dates . astype ( np . int64 ) if fmt . startswith ( ( \"%tc\" , \"tc\" ) ) : # Delta ms relative to base base = stata_epoch ms = dates conv_dates = convert_delta_safe ( base , ms , 'ms' ) elif fmt . startswith ( ( \"%tC\" , \"tC\" ) ) : warnings . warn ( \"Encountered %tC format. Leaving in Stata \" \"Internal Format.\" ) conv_dates = Series ( dates , dtype = np . object ) if has_bad_values : conv_dates [ bad_locs ] = NaT return conv_dates # Delta days relative to base elif fmt . startswith ( ( \"%td\" , \"td\" , \"%d\" , \"d\" ) ) : base = stata_epoch days = dates conv_dates = convert_delta_safe ( base , days , 'd' ) # does not count leap days - 7 days is a week. # 52nd week may have more than 7 days elif fmt . startswith ( ( \"%tw\" , \"tw\" ) ) : year = stata_epoch . year + dates // 52 days = ( dates % 52 ) * 7 conv_dates = convert_year_days_safe ( year , days ) elif fmt . startswith ( ( \"%tm\" , \"tm\" ) ) : # Delta months relative to base year = stata_epoch . year + dates // 12 month = ( dates % 12 ) + 1 conv_dates = convert_year_month_safe ( year , month ) elif fmt . startswith ( ( \"%tq\" , \"tq\" ) ) : # Delta quarters relative to base year = stata_epoch . year + dates // 4 month = ( dates % 4 ) * 3 + 1 conv_dates = convert_year_month_safe ( year , month ) elif fmt . startswith ( ( \"%th\" , \"th\" ) ) : # Delta half-years relative to base year = stata_epoch . year + dates // 2 month = ( dates % 2 ) * 6 + 1 conv_dates = convert_year_month_safe ( year , month ) elif fmt . startswith ( ( \"%ty\" , \"ty\" ) ) : # Years -- not delta year = dates month = np . ones_like ( dates ) conv_dates = convert_year_month_safe ( year , month ) else : raise ValueError ( \"Date fmt {fmt} not understood\" . format ( fmt = fmt ) ) if has_bad_values : # Restore NaT for bad values conv_dates [ bad_locs ] = NaT return conv_dates\n\n\n        Summary:", "input": "", "output": "Convert from SIF to datetime . http : // www . stata . com / help . cgi?datetime"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _datetime_to_stata_elapsed_vec ( dates , fmt ) : index = dates . index NS_PER_DAY = 24 * 3600 * 1000 * 1000 * 1000 US_PER_DAY = NS_PER_DAY / 1000 def parse_dates_safe ( dates , delta = False , year = False , days = False ) : d = { } if is_datetime64_dtype ( dates . values ) : if delta : delta = dates - stata_epoch d [ 'delta' ] = delta . values . astype ( np . int64 ) // 1000 # microseconds if days or year : dates = DatetimeIndex ( dates ) d [ 'year' ] , d [ 'month' ] = dates . year , dates . month if days : days = ( dates . astype ( np . int64 ) - to_datetime ( d [ 'year' ] , format = '%Y' ) . astype ( np . int64 ) ) d [ 'days' ] = days // NS_PER_DAY elif infer_dtype ( dates , skipna = False ) == 'datetime' : if delta : delta = dates . values - stata_epoch f = lambda x : US_PER_DAY * x . days + 1000000 * x . seconds + x . microseconds v = np . vectorize ( f ) d [ 'delta' ] = v ( delta ) if year : year_month = dates . apply ( lambda x : 100 * x . year + x . month ) d [ 'year' ] = year_month . values // 100 d [ 'month' ] = ( year_month . values - d [ 'year' ] * 100 ) if days : f = lambda x : ( x - datetime . datetime ( x . year , 1 , 1 ) ) . days v = np . vectorize ( f ) d [ 'days' ] = v ( dates ) else : raise ValueError ( 'Columns containing dates must contain either ' 'datetime64, datetime.datetime or null values.' ) return DataFrame ( d , index = index ) bad_loc = isna ( dates ) index = dates . index if bad_loc . any ( ) : dates = Series ( dates ) if is_datetime64_dtype ( dates ) : dates [ bad_loc ] = to_datetime ( stata_epoch ) else : dates [ bad_loc ] = stata_epoch if fmt in [ \"%tc\" , \"tc\" ] : d = parse_dates_safe ( dates , delta = True ) conv_dates = d . delta / 1000 elif fmt in [ \"%tC\" , \"tC\" ] : warnings . warn ( \"Stata Internal Format tC not supported.\" ) conv_dates = dates elif fmt in [ \"%td\" , \"td\" ] : d = parse_dates_safe ( dates , delta = True ) conv_dates = d . delta // US_PER_DAY elif fmt in [ \"%tw\" , \"tw\" ] : d = parse_dates_safe ( dates , year = True , days = True ) conv_dates = ( 52 * ( d . year - stata_epoch . year ) + d . days // 7 ) elif fmt in [ \"%tm\" , \"tm\" ] : d = parse_dates_safe ( dates , year = True ) conv_dates = ( 12 * ( d . year - stata_epoch . year ) + d . month - 1 ) elif fmt in [ \"%tq\" , \"tq\" ] : d = parse_dates_safe ( dates , year = True ) conv_dates = 4 * ( d . year - stata_epoch . year ) + ( d . month - 1 ) // 3 elif fmt in [ \"%th\" , \"th\" ] : d = parse_dates_safe ( dates , year = True ) conv_dates = ( 2 * ( d . year - stata_epoch . year ) + ( d . month > 6 ) . astype ( np . int ) ) elif fmt in [ \"%ty\" , \"ty\" ] : d = parse_dates_safe ( dates , year = True ) conv_dates = d . year else : raise ValueError ( \"Format {fmt} is not a known Stata date format\" . format ( fmt = fmt ) ) conv_dates = Series ( conv_dates , dtype = np . float64 ) missing_value = struct . unpack ( '<d' , b'\\x00\\x00\\x00\\x00\\x00\\x00\\xe0\\x7f' ) [ 0 ] conv_dates [ bad_loc ] = missing_value return Series ( conv_dates , index = index )\n\n\n        Summary:", "input": "", "output": "Convert from datetime to SIF . http : // www . stata . com / help . cgi?datetime"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cast_to_stata_types ( data ) : ws = '' # original, if small, if large conversion_data = ( ( np . bool , np . int8 , np . int8 ) , ( np . uint8 , np . int8 , np . int16 ) , ( np . uint16 , np . int16 , np . int32 ) , ( np . uint32 , np . int32 , np . int64 ) ) float32_max = struct . unpack ( '<f' , b'\\xff\\xff\\xff\\x7e' ) [ 0 ] float64_max = struct . unpack ( '<d' , b'\\xff\\xff\\xff\\xff\\xff\\xff\\xdf\\x7f' ) [ 0 ] for col in data : dtype = data [ col ] . dtype # Cast from unsupported types to supported types for c_data in conversion_data : if dtype == c_data [ 0 ] : if data [ col ] . max ( ) <= np . iinfo ( c_data [ 1 ] ) . max : dtype = c_data [ 1 ] else : dtype = c_data [ 2 ] if c_data [ 2 ] == np . float64 : # Warn if necessary if data [ col ] . max ( ) >= 2 ** 53 : ws = precision_loss_doc % ( 'uint64' , 'float64' ) data [ col ] = data [ col ] . astype ( dtype ) # Check values and upcast if necessary if dtype == np . int8 : if data [ col ] . max ( ) > 100 or data [ col ] . min ( ) < - 127 : data [ col ] = data [ col ] . astype ( np . int16 ) elif dtype == np . int16 : if data [ col ] . max ( ) > 32740 or data [ col ] . min ( ) < - 32767 : data [ col ] = data [ col ] . astype ( np . int32 ) elif dtype == np . int64 : if ( data [ col ] . max ( ) <= 2147483620 and data [ col ] . min ( ) >= - 2147483647 ) : data [ col ] = data [ col ] . astype ( np . int32 ) else : data [ col ] = data [ col ] . astype ( np . float64 ) if data [ col ] . max ( ) >= 2 ** 53 or data [ col ] . min ( ) <= - 2 ** 53 : ws = precision_loss_doc % ( 'int64' , 'float64' ) elif dtype in ( np . float32 , np . float64 ) : value = data [ col ] . max ( ) if np . isinf ( value ) : raise ValueError ( 'Column {col} has a maximum value of ' 'infinity which is outside the range ' 'supported by Stata.' . format ( col = col ) ) if dtype == np . float32 and value > float32_max : data [ col ] = data [ col ] . astype ( np . float64 ) elif dtype == np . float64 : if value > float64_max : raise ValueError ( 'Column {col} has a maximum value ' '({val}) outside the range supported by ' 'Stata ({float64_max})' . format ( col = col , val = value , float64_max = float64_max ) ) if ws : warnings . warn ( ws , PossiblePrecisionLoss ) return data\n\n\n        Summary:", "input": "", "output": "Checks the dtypes of the columns of a pandas DataFrame for compatibility with the data types and ranges supported by Stata and converts if necessary ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _dtype_to_stata_type ( dtype , column ) : # TODO: expand to handle datetime to integer conversion if dtype . type == np . object_ : # try to coerce it to the biggest string # not memory efficient, what else could we # do? itemsize = max_len_string_array ( ensure_object ( column . values ) ) return max ( itemsize , 1 ) elif dtype == np . float64 : return 255 elif dtype == np . float32 : return 254 elif dtype == np . int32 : return 253 elif dtype == np . int16 : return 252 elif dtype == np . int8 : return 251 else : # pragma : no cover raise NotImplementedError ( \"Data type {dtype} not supported.\" . format ( dtype = dtype ) )\n\n\n        Summary:", "input": "", "output": "Convert dtype types to stata types . Returns the byte of the given ordinal . See TYPE_MAP and comments for an explanation . This is also explained in the dta spec . 1 - 244 are strings of this length Pandas Stata 251 - for int8 byte 252 - for int16 int 253 - for int32 long 254 - for float32 float 255 - for double double"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _dtype_to_default_stata_fmt ( dtype , column , dta_version = 114 , force_strl = False ) : # TODO: Refactor to combine type with format # TODO: expand this to handle a default datetime format? if dta_version < 117 : max_str_len = 244 else : max_str_len = 2045 if force_strl : return '%9s' if dtype . type == np . object_ : inferred_dtype = infer_dtype ( column , skipna = True ) if not ( inferred_dtype in ( 'string' , 'unicode' ) or len ( column ) == 0 ) : raise ValueError ( 'Column `{col}` cannot be exported.\\n\\nOnly ' 'string-like object arrays containing all ' 'strings or a mix of strings and None can be ' 'exported. Object arrays containing only null ' 'values are prohibited. Other object types' 'cannot be exported and must first be converted ' 'to one of the supported ' 'types.' . format ( col = column . name ) ) itemsize = max_len_string_array ( ensure_object ( column . values ) ) if itemsize > max_str_len : if dta_version >= 117 : return '%9s' else : raise ValueError ( excessive_string_length_error % column . name ) return \"%\" + str ( max ( itemsize , 1 ) ) + \"s\" elif dtype == np . float64 : return \"%10.0g\" elif dtype == np . float32 : return \"%9.0g\" elif dtype == np . int32 : return \"%12.0g\" elif dtype == np . int8 or dtype == np . int16 : return \"%8.0g\" else : # pragma : no cover raise NotImplementedError ( \"Data type {dtype} not supported.\" . format ( dtype = dtype ) )\n\n\n        Summary:", "input": "", "output": "Map numpy dtype to stata s default format for this type . Not terribly important since users can change this in Stata . Semantics are"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _pad_bytes_new ( name , length ) : if isinstance ( name , str ) : name = bytes ( name , 'utf-8' ) return name + b'\\x00' * ( length - len ( name ) )\n\n\n        Summary:", "input": "", "output": "Takes a bytes instance and pads it with null bytes until it s length chars ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate_value_label ( self , byteorder , encoding ) : self . _encoding = encoding bio = BytesIO ( ) null_string = '\\x00' null_byte = b'\\x00' # len bio . write ( struct . pack ( byteorder + 'i' , self . len ) ) # labname labname = self . _encode ( _pad_bytes ( self . labname [ : 32 ] , 33 ) ) bio . write ( labname ) # padding - 3 bytes for i in range ( 3 ) : bio . write ( struct . pack ( 'c' , null_byte ) ) # value_label_table # n - int32 bio . write ( struct . pack ( byteorder + 'i' , self . n ) ) # textlen - int32 bio . write ( struct . pack ( byteorder + 'i' , self . text_len ) ) # off - int32 array (n elements) for offset in self . off : bio . write ( struct . pack ( byteorder + 'i' , offset ) ) # val - int32 array (n elements) for value in self . val : bio . write ( struct . pack ( byteorder + 'i' , value ) ) # txt - Text labels, null terminated for text in self . txt : bio . write ( self . _encode ( text + null_string ) ) bio . seek ( 0 ) return bio . read ( )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- byteorder : str Byte order of the output encoding : str File encoding"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _setup_dtype ( self ) : if self . _dtype is not None : return self . _dtype dtype = [ ] # Convert struct data types to numpy data type for i , typ in enumerate ( self . typlist ) : if typ in self . NUMPY_TYPE_MAP : dtype . append ( ( 's' + str ( i ) , self . byteorder + self . NUMPY_TYPE_MAP [ typ ] ) ) else : dtype . append ( ( 's' + str ( i ) , 'S' + str ( typ ) ) ) dtype = np . dtype ( dtype ) self . _dtype = dtype return self . _dtype\n\n\n        Summary:", "input": "", "output": "Map between numpy and state dtypes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _do_convert_categoricals ( self , data , value_label_dict , lbllist , order_categoricals ) : value_labels = list ( value_label_dict . keys ( ) ) cat_converted_data = [ ] for col , label in zip ( data , lbllist ) : if label in value_labels : # Explicit call with ordered=True cat_data = Categorical ( data [ col ] , ordered = order_categoricals ) categories = [ ] for category in cat_data . categories : if category in value_label_dict [ label ] : categories . append ( value_label_dict [ label ] [ category ] ) else : categories . append ( category ) # Partially labeled try : cat_data . categories = categories except ValueError : vc = Series ( categories ) . value_counts ( ) repeats = list ( vc . index [ vc > 1 ] ) repeats = '-' * 80 + '\\n' + '\\n' . join ( repeats ) # GH 25772 msg = \"\"\" Value labels for column {col} are not unique. These cannot be converted to pandas categoricals. Either read the file with `convert_categoricals` set to False or use the low level interface in `StataReader` to separately read the values and the value_labels. The repeated labels are: {repeats} \"\"\" raise ValueError ( msg . format ( col = col , repeats = repeats ) ) # TODO: is the next line needed above in the data(...) method? cat_data = Series ( cat_data , index = data . index ) cat_converted_data . append ( ( col , cat_data ) ) else : cat_converted_data . append ( ( col , data [ col ] ) ) data = DataFrame . from_dict ( OrderedDict ( cat_converted_data ) ) return data\n\n\n        Summary:", "input": "", "output": "Converts categorical columns to Categorical type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _write ( self , to_write ) : self . _file . write ( to_write . encode ( self . _encoding or self . _default_encoding ) )\n\n\n        Summary:", "input": "", "output": "Helper to call encode before writing to file for Python 3 compat ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _prepare_categoricals ( self , data ) : is_cat = [ is_categorical_dtype ( data [ col ] ) for col in data ] self . _is_col_cat = is_cat self . _value_labels = [ ] if not any ( is_cat ) : return data get_base_missing_value = StataMissingValue . get_base_missing_value data_formatted = [ ] for col , col_is_cat in zip ( data , is_cat ) : if col_is_cat : self . _value_labels . append ( StataValueLabel ( data [ col ] ) ) dtype = data [ col ] . cat . codes . dtype if dtype == np . int64 : raise ValueError ( 'It is not possible to export ' 'int64-based categorical data to Stata.' ) values = data [ col ] . cat . codes . values . copy ( ) # Upcast if needed so that correct missing values can be set if values . max ( ) >= get_base_missing_value ( dtype ) : if dtype == np . int8 : dtype = np . int16 elif dtype == np . int16 : dtype = np . int32 else : dtype = np . float64 values = np . array ( values , dtype = dtype ) # Replace missing values with Stata missing value for type values [ values == - 1 ] = get_base_missing_value ( dtype ) data_formatted . append ( ( col , values ) ) else : data_formatted . append ( ( col , data [ col ] ) ) return DataFrame . from_dict ( OrderedDict ( data_formatted ) )\n\n\n        Summary:", "input": "", "output": "Check for categorical columns retain categorical information for Stata file and convert categorical data to int"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replace_nans ( self , data ) : # return data for c in data : dtype = data [ c ] . dtype if dtype in ( np . float32 , np . float64 ) : if dtype == np . float32 : replacement = self . MISSING_VALUES [ 'f' ] else : replacement = self . MISSING_VALUES [ 'd' ] data [ c ] = data [ c ] . fillna ( replacement ) return data\n\n\n        Summary:", "input": "", "output": "Checks floating point data columns for nans and replaces these with the generic Stata for missing value ( . )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_column_names ( self , data ) : converted_names = { } columns = list ( data . columns ) original_columns = columns [ : ] duplicate_var_id = 0 for j , name in enumerate ( columns ) : orig_name = name if not isinstance ( name , str ) : name = str ( name ) for c in name : if ( ( c < 'A' or c > 'Z' ) and ( c < 'a' or c > 'z' ) and ( c < '0' or c > '9' ) and c != '_' ) : name = name . replace ( c , '_' ) # Variable name must not be a reserved word if name in self . RESERVED_WORDS : name = '_' + name # Variable name may not start with a number if name [ 0 ] >= '0' and name [ 0 ] <= '9' : name = '_' + name name = name [ : min ( len ( name ) , 32 ) ] if not name == orig_name : # check for duplicates while columns . count ( name ) > 0 : # prepend ascending number to avoid duplicates name = '_' + str ( duplicate_var_id ) + name name = name [ : min ( len ( name ) , 32 ) ] duplicate_var_id += 1 converted_names [ orig_name ] = name columns [ j ] = name data . columns = columns # Check date conversion, and fix key if needed if self . _convert_dates : for c , o in zip ( columns , original_columns ) : if c != o : self . _convert_dates [ c ] = self . _convert_dates [ o ] del self . _convert_dates [ o ] if converted_names : conversion_warning = [ ] for orig_name , name in converted_names . items ( ) : # need to possibly encode the orig name if its unicode try : orig_name = orig_name . encode ( 'utf-8' ) except ( UnicodeDecodeError , AttributeError ) : pass msg = '{0} -> {1}' . format ( orig_name , name ) conversion_warning . append ( msg ) ws = invalid_name_doc . format ( '\\n ' . join ( conversion_warning ) ) warnings . warn ( ws , InvalidColumnName ) self . _converted_names = converted_names self . _update_strl_names ( ) return data\n\n\n        Summary:", "input": "", "output": "Checks column names to ensure that they are valid Stata column names . This includes checks for : * Non - string names * Stata keywords * Variables that start with numbers * Variables with names that are too long"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _close ( self ) : # Some file-like objects might not support flush try : self . _file . flush ( ) except AttributeError : pass if self . _own_file : self . _file . close ( )\n\n\n        Summary:", "input": "", "output": "Close the file if it was created by the writer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate_table ( self ) : gso_table = self . _gso_table gso_df = self . df columns = list ( gso_df . columns ) selected = gso_df [ self . columns ] col_index = [ ( col , columns . index ( col ) ) for col in self . columns ] keys = np . empty ( selected . shape , dtype = np . uint64 ) for o , ( idx , row ) in enumerate ( selected . iterrows ( ) ) : for j , ( col , v ) in enumerate ( col_index ) : val = row [ col ] # Allow columns with mixed str and None (GH 23633) val = '' if val is None else val key = gso_table . get ( val , None ) if key is None : # Stata prefers human numbers key = ( v + 1 , o + 1 ) gso_table [ val ] = key keys [ o , j ] = self . _convert_key ( key ) for i , col in enumerate ( self . columns ) : gso_df [ col ] = keys [ : , i ] return gso_table , gso_df\n\n\n        Summary:", "input": "", "output": "Generates the GSO lookup table for the DataFRame"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate_blob ( self , gso_table ) : # Format information # Length includes null term # 117 # GSOvvvvooootllllxxxxxxxxxxxxxxx...x # 3 u4 u4 u1 u4 string + null term # # 118, 119 # GSOvvvvooooooootllllxxxxxxxxxxxxxxx...x # 3 u4 u8 u1 u4 string + null term bio = BytesIO ( ) gso = bytes ( 'GSO' , 'ascii' ) gso_type = struct . pack ( self . _byteorder + 'B' , 130 ) null = struct . pack ( self . _byteorder + 'B' , 0 ) v_type = self . _byteorder + self . _gso_v_type o_type = self . _byteorder + self . _gso_o_type len_type = self . _byteorder + 'I' for strl , vo in gso_table . items ( ) : if vo == ( 0 , 0 ) : continue v , o = vo # GSO bio . write ( gso ) # vvvv bio . write ( struct . pack ( v_type , v ) ) # oooo / oooooooo bio . write ( struct . pack ( o_type , o ) ) # t bio . write ( gso_type ) # llll utf8_string = bytes ( strl , 'utf-8' ) bio . write ( struct . pack ( len_type , len ( utf8_string ) + 1 ) ) # xxx...xxx bio . write ( utf8_string ) bio . write ( null ) bio . seek ( 0 ) return bio . read ( )\n\n\n        Summary:", "input": "", "output": "Generates the binary blob of GSOs that is written to the dta file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _tag ( val , tag ) : if isinstance ( val , str ) : val = bytes ( val , 'utf-8' ) return ( bytes ( '<' + tag + '>' , 'utf-8' ) + val + bytes ( '</' + tag + '>' , 'utf-8' ) )\n\n\n        Summary:", "input": "", "output": "Surround val with <tag > < / tag >"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _write_header ( self , data_label = None , time_stamp = None ) : byteorder = self . _byteorder self . _file . write ( bytes ( '<stata_dta>' , 'utf-8' ) ) bio = BytesIO ( ) # ds_format - 117 bio . write ( self . _tag ( bytes ( '117' , 'utf-8' ) , 'release' ) ) # byteorder bio . write ( self . _tag ( byteorder == \">\" and \"MSF\" or \"LSF\" , 'byteorder' ) ) # number of vars, 2 bytes assert self . nvar < 2 ** 16 bio . write ( self . _tag ( struct . pack ( byteorder + \"H\" , self . nvar ) , 'K' ) ) # number of obs, 4 bytes bio . write ( self . _tag ( struct . pack ( byteorder + \"I\" , self . nobs ) , 'N' ) ) # data label 81 bytes, char, null terminated label = data_label [ : 80 ] if data_label is not None else '' label_len = struct . pack ( byteorder + \"B\" , len ( label ) ) label = label_len + bytes ( label , 'utf-8' ) bio . write ( self . _tag ( label , 'label' ) ) # time stamp, 18 bytes, char, null terminated # format dd Mon yyyy hh:mm if time_stamp is None : time_stamp = datetime . datetime . now ( ) elif not isinstance ( time_stamp , datetime . datetime ) : raise ValueError ( \"time_stamp should be datetime type\" ) # Avoid locale-specific month conversion months = [ 'Jan' , 'Feb' , 'Mar' , 'Apr' , 'May' , 'Jun' , 'Jul' , 'Aug' , 'Sep' , 'Oct' , 'Nov' , 'Dec' ] month_lookup = { i + 1 : month for i , month in enumerate ( months ) } ts = ( time_stamp . strftime ( \"%d \" ) + month_lookup [ time_stamp . month ] + time_stamp . strftime ( \" %Y %H:%M\" ) ) # '\\x11' added due to inspection of Stata file ts = b'\\x11' + bytes ( ts , 'utf8' ) bio . write ( self . _tag ( ts , 'timestamp' ) ) bio . seek ( 0 ) self . _file . write ( self . _tag ( bio . read ( ) , 'header' ) )\n\n\n        Summary:", "input": "", "output": "Write the file header"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _write_map ( self ) : if self . _map is None : self . _map = OrderedDict ( ( ( 'stata_data' , 0 ) , ( 'map' , self . _file . tell ( ) ) , ( 'variable_types' , 0 ) , ( 'varnames' , 0 ) , ( 'sortlist' , 0 ) , ( 'formats' , 0 ) , ( 'value_label_names' , 0 ) , ( 'variable_labels' , 0 ) , ( 'characteristics' , 0 ) , ( 'data' , 0 ) , ( 'strls' , 0 ) , ( 'value_labels' , 0 ) , ( 'stata_data_close' , 0 ) , ( 'end-of-file' , 0 ) ) ) # Move to start of map self . _file . seek ( self . _map [ 'map' ] ) bio = BytesIO ( ) for val in self . _map . values ( ) : bio . write ( struct . pack ( self . _byteorder + 'Q' , val ) ) bio . seek ( 0 ) self . _file . write ( self . _tag ( bio . read ( ) , 'map' ) )\n\n\n        Summary:", "input": "", "output": "Called twice during file write . The first populates the values in the map with 0s . The second call writes the final map locations when all blocks have been written ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _update_strl_names ( self ) : # Update convert_strl if names changed for orig , new in self . _converted_names . items ( ) : if orig in self . _convert_strl : idx = self . _convert_strl . index ( orig ) self . _convert_strl [ idx ] = new\n\n\n        Summary:", "input": "", "output": "Update column names for conversion to strl if they might have been changed to comply with Stata naming rules"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_strls ( self , data ) : convert_cols = [ col for i , col in enumerate ( data ) if self . typlist [ i ] == 32768 or col in self . _convert_strl ] if convert_cols : ssw = StataStrLWriter ( data , convert_cols ) tab , new_data = ssw . generate_table ( ) data = new_data self . _strl_blob = ssw . generate_blob ( tab ) return data\n\n\n        Summary:", "input": "", "output": "Convert columns to StrLs if either very large or in the convert_strl variable"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def register ( explicit = True ) : # Renamed in pandas.plotting.__init__ global _WARN if explicit : _WARN = False pairs = get_pairs ( ) for type_ , cls in pairs : converter = cls ( ) if type_ in units . registry : previous = units . registry [ type_ ] _mpl_units [ type_ ] = previous units . registry [ type_ ] = converter\n\n\n        Summary:", "input": "", "output": "Register Pandas Formatters and Converters with matplotlib"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def deregister ( ) : # Renamed in pandas.plotting.__init__ for type_ , cls in get_pairs ( ) : # We use type to catch our classes directly, no inheritance if type ( units . registry . get ( type_ ) ) is cls : units . registry . pop ( type_ ) # restore the old keys for unit , formatter in _mpl_units . items ( ) : if type ( formatter ) not in { DatetimeConverter , PeriodConverter , TimeConverter } : # make it idempotent by excluding ours. units . registry [ unit ] = formatter\n\n\n        Summary:", "input": "", "output": "Remove pandas formatters and converters"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _dt_to_float_ordinal ( dt ) : if ( isinstance ( dt , ( np . ndarray , Index , ABCSeries ) ) and is_datetime64_ns_dtype ( dt ) ) : base = dates . epoch2num ( dt . asi8 / 1.0E9 ) else : base = dates . date2num ( dt ) return base\n\n\n        Summary:", "input": "", "output": "Convert : mod : datetime to the Gregorian date as UTC float days preserving hours minutes seconds and microseconds . Return value is a : func : float ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_default_annual_spacing ( nyears ) : if nyears < 11 : ( min_spacing , maj_spacing ) = ( 1 , 1 ) elif nyears < 20 : ( min_spacing , maj_spacing ) = ( 1 , 2 ) elif nyears < 50 : ( min_spacing , maj_spacing ) = ( 1 , 5 ) elif nyears < 100 : ( min_spacing , maj_spacing ) = ( 5 , 10 ) elif nyears < 200 : ( min_spacing , maj_spacing ) = ( 5 , 25 ) elif nyears < 600 : ( min_spacing , maj_spacing ) = ( 10 , 50 ) else : factor = nyears // 1000 + 1 ( min_spacing , maj_spacing ) = ( factor * 20 , factor * 100 ) return ( min_spacing , maj_spacing )\n\n\n        Summary:", "input": "", "output": "Returns a default spacing between consecutive ticks for annual data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def period_break ( dates , period ) : current = getattr ( dates , period ) previous = getattr ( dates - 1 * dates . freq , period ) return np . nonzero ( current - previous ) [ 0 ]\n\n\n        Summary:", "input": "", "output": "Returns the indices where the given period changes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def has_level_label ( label_flags , vmin ) : if label_flags . size == 0 or ( label_flags . size == 1 and label_flags [ 0 ] == 0 and vmin % 1 > 0.0 ) : return False else : return True\n\n\n        Summary:", "input": "", "output": "Returns true if the label_flags indicate there is at least one label for this level ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def axisinfo ( unit , axis ) : tz = unit majloc = PandasAutoDateLocator ( tz = tz ) majfmt = PandasAutoDateFormatter ( majloc , tz = tz ) datemin = pydt . date ( 2000 , 1 , 1 ) datemax = pydt . date ( 2010 , 1 , 1 ) return units . AxisInfo ( majloc = majloc , majfmt = majfmt , label = '' , default_limits = ( datemin , datemax ) )\n\n\n        Summary:", "input": "", "output": "Return the : class : ~matplotlib . units . AxisInfo for * unit * ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_locator ( self , dmin , dmax ) : _check_implicitly_registered ( ) delta = relativedelta ( dmax , dmin ) num_days = ( delta . years * 12.0 + delta . months ) * 31.0 + delta . days num_sec = ( delta . hours * 60.0 + delta . minutes ) * 60.0 + delta . seconds tot_sec = num_days * 86400. + num_sec if abs ( tot_sec ) < self . minticks : self . _freq = - 1 locator = MilliSecondLocator ( self . tz ) locator . set_axis ( self . axis ) locator . set_view_interval ( * self . axis . get_view_interval ( ) ) locator . set_data_interval ( * self . axis . get_data_interval ( ) ) return locator return dates . AutoDateLocator . get_locator ( self , dmin , dmax )\n\n\n        Summary:", "input": "", "output": "Pick the best locator based on a distance ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def autoscale ( self ) : dmin , dmax = self . datalim_to_dt ( ) if dmin > dmax : dmax , dmin = dmin , dmax # We need to cap at the endpoints of valid datetime # TODO(wesm): unused? # delta = relativedelta(dmax, dmin) # try: # start = dmin - delta # except ValueError: # start = _from_ordinal(1.0) # try: # stop = dmax + delta # except ValueError: # # The magic number! # stop = _from_ordinal(3652059.9999999) dmin , dmax = self . datalim_to_dt ( ) vmin = dates . date2num ( dmin ) vmax = dates . date2num ( dmax ) return self . nonsingular ( vmin , vmax )\n\n\n        Summary:", "input": "", "output": "Set the view limits to include the data range ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_default_locs ( self , vmin , vmax ) : if self . plot_obj . date_axis_info is None : self . plot_obj . date_axis_info = self . finder ( vmin , vmax , self . freq ) locator = self . plot_obj . date_axis_info if self . isminor : return np . compress ( locator [ 'min' ] , locator [ 'val' ] ) return np . compress ( locator [ 'maj' ] , locator [ 'val' ] )\n\n\n        Summary:", "input": "", "output": "Returns the default locations of ticks ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def autoscale ( self ) : # requires matplotlib >= 0.98.0 ( vmin , vmax ) = self . axis . get_data_interval ( ) locs = self . _get_default_locs ( vmin , vmax ) ( vmin , vmax ) = locs [ [ 0 , - 1 ] ] if vmin == vmax : vmin -= 1 vmax += 1 return nonsingular ( vmin , vmax )\n\n\n        Summary:", "input": "", "output": "Sets the view limits to the nearest multiples of base that contain the data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_default_format ( self , vmin , vmax ) : if self . plot_obj . date_axis_info is None : self . plot_obj . date_axis_info = self . finder ( vmin , vmax , self . freq ) info = self . plot_obj . date_axis_info if self . isminor : format = np . compress ( info [ 'min' ] & np . logical_not ( info [ 'maj' ] ) , info ) else : format = np . compress ( info [ 'maj' ] , info ) self . formatdict = { x : f for ( x , _ , _ , f ) in format } return self . formatdict\n\n\n        Summary:", "input": "", "output": "Returns the default ticks spacing ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_locs ( self , locs ) : # don't actually use the locs. This is just needed to work with # matplotlib. Force to use vmin, vmax _check_implicitly_registered ( ) self . locs = locs ( vmin , vmax ) = vi = tuple ( self . axis . get_view_interval ( ) ) if vi != self . plot_obj . view_interval : self . plot_obj . date_axis_info = None self . plot_obj . view_interval = vi if vmax < vmin : ( vmin , vmax ) = ( vmax , vmin ) self . _set_default_format ( vmin , vmax )\n\n\n        Summary:", "input": "", "output": "Sets the locations of the ticks"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_default_names ( data ) : if com . _all_not_none ( * data . index . names ) : nms = data . index . names if len ( nms ) == 1 and data . index . name == 'index' : warnings . warn ( \"Index name of 'index' is not round-trippable\" ) elif len ( nms ) > 1 and any ( x . startswith ( 'level_' ) for x in nms ) : warnings . warn ( \"Index names beginning with 'level_' are not \" \"round-trippable\" ) return data data = data . copy ( ) if data . index . nlevels > 1 : names = [ name if name is not None else 'level_{}' . format ( i ) for i , name in enumerate ( data . index . names ) ] data . index . names = names else : data . index . name = data . index . name or 'index' return data\n\n\n        Summary:", "input": "", "output": "Sets index names to index for regular or level_x for Multi"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_json_field_to_pandas_type ( field ) : typ = field [ 'type' ] if typ == 'string' : return 'object' elif typ == 'integer' : return 'int64' elif typ == 'number' : return 'float64' elif typ == 'boolean' : return 'bool' elif typ == 'duration' : return 'timedelta64' elif typ == 'datetime' : if field . get ( 'tz' ) : return 'datetime64[ns, {tz}]' . format ( tz = field [ 'tz' ] ) else : return 'datetime64[ns]' elif typ == 'any' : if 'constraints' in field and 'ordered' in field : return CategoricalDtype ( categories = field [ 'constraints' ] [ 'enum' ] , ordered = field [ 'ordered' ] ) else : return 'object' raise ValueError ( \"Unsupported or invalid field type: {}\" . format ( typ ) )\n\n\n        Summary:", "input": "", "output": "Converts a JSON field descriptor into its corresponding NumPy / pandas type"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def build_table_schema ( data , index = True , primary_key = None , version = True ) : if index is True : data = set_default_names ( data ) schema = { } fields = [ ] if index : if data . index . nlevels > 1 : for level in data . index . levels : fields . append ( convert_pandas_type_to_json_field ( level ) ) else : fields . append ( convert_pandas_type_to_json_field ( data . index ) ) if data . ndim > 1 : for column , s in data . iteritems ( ) : fields . append ( convert_pandas_type_to_json_field ( s ) ) else : fields . append ( convert_pandas_type_to_json_field ( data ) ) schema [ 'fields' ] = fields if index and data . index . is_unique and primary_key is None : if data . index . nlevels == 1 : schema [ 'primaryKey' ] = [ data . index . name ] else : schema [ 'primaryKey' ] = data . index . names elif primary_key is not None : schema [ 'primaryKey' ] = primary_key if version : schema [ 'pandas_version' ] = '0.20.0' return schema\n\n\n        Summary:", "input": "", "output": "Create a Table schema from data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parse_table_schema ( json , precise_float ) : table = loads ( json , precise_float = precise_float ) col_order = [ field [ 'name' ] for field in table [ 'schema' ] [ 'fields' ] ] df = DataFrame ( table [ 'data' ] , columns = col_order ) [ col_order ] dtypes = { field [ 'name' ] : convert_json_field_to_pandas_type ( field ) for field in table [ 'schema' ] [ 'fields' ] } # Cannot directly use as_type with timezone data on object; raise for now if any ( str ( x ) . startswith ( 'datetime64[ns, ' ) for x in dtypes . values ( ) ) : raise NotImplementedError ( 'table=\"orient\" can not yet read timezone ' 'data' ) # No ISO constructor for Timedelta as of yet, so need to raise if 'timedelta64' in dtypes . values ( ) : raise NotImplementedError ( 'table=\"orient\" can not yet read ' 'ISO-formatted Timedelta data' ) df = df . astype ( dtypes ) if 'primaryKey' in table [ 'schema' ] : df = df . set_index ( table [ 'schema' ] [ 'primaryKey' ] ) if len ( df . index . names ) == 1 : if df . index . name == 'index' : df . index . name = None else : df . index . names = [ None if x . startswith ( 'level_' ) else x for x in df . index . names ] return df\n\n\n        Summary:", "input": "", "output": "Builds a DataFrame from a given schema"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_op_result_name ( left , right ) : # `left` is always a pd.Series when called from within ops if isinstance ( right , ( ABCSeries , pd . Index ) ) : name = _maybe_match_name ( left , right ) else : name = left . name return name\n\n\n        Summary:", "input": "", "output": "Find the appropriate name to pin to an operation result . This result should always be either an Index or a Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_match_name ( a , b ) : a_has = hasattr ( a , 'name' ) b_has = hasattr ( b , 'name' ) if a_has and b_has : if a . name == b . name : return a . name else : # TODO: what if they both have np.nan for their names? return None elif a_has : return a . name elif b_has : return b . name return None\n\n\n        Summary:", "input": "", "output": "Try to find a name to attach to the result of an operation between a and b . If only one of these has a name attribute return that name . Otherwise return a consensus name if they match of None if they have different names ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_upcast_for_op ( obj ) : if type ( obj ) is datetime . timedelta : # GH#22390 cast up to Timedelta to rely on Timedelta # implementation; otherwise operation against numeric-dtype # raises TypeError return pd . Timedelta ( obj ) elif isinstance ( obj , np . timedelta64 ) and not isna ( obj ) : # In particular non-nanosecond timedelta64 needs to be cast to # nanoseconds, or else we get undesired behavior like # np.timedelta64(3, 'D') / 2 == np.timedelta64(1, 'D') # The isna check is to avoid casting timedelta64(\"NaT\"), which would # return NaT and incorrectly be treated as a datetime-NaT. return pd . Timedelta ( obj ) elif isinstance ( obj , np . ndarray ) and is_timedelta64_dtype ( obj ) : # GH#22390 Unfortunately we need to special-case right-hand # timedelta64 dtypes because numpy casts integer dtypes to # timedelta64 when operating with timedelta64 return pd . TimedeltaIndex ( obj ) return obj\n\n\n        Summary:", "input": "", "output": "Cast non - pandas objects to pandas types to unify behavior of arithmetic and comparison operations ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_invalid_op ( name ) : def invalid_op ( self , other = None ) : raise TypeError ( \"cannot perform {name} with this index type: \" \"{typ}\" . format ( name = name , typ = type ( self ) . __name__ ) ) invalid_op . __name__ = name return invalid_op\n\n\n        Summary:", "input": "", "output": "Return a binary method that always raises a TypeError ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gen_eval_kwargs ( name ) : kwargs = { } # Series and Panel appear to only pass __add__, __radd__, ... # but DataFrame gets both these dunder names _and_ non-dunder names # add, radd, ... name = name . replace ( '__' , '' ) if name . startswith ( 'r' ) : if name not in [ 'radd' , 'rand' , 'ror' , 'rxor' ] : # Exclude commutative operations kwargs [ 'reversed' ] = True if name in [ 'truediv' , 'rtruediv' ] : kwargs [ 'truediv' ] = True if name in [ 'ne' ] : kwargs [ 'masker' ] = True return kwargs\n\n\n        Summary:", "input": "", "output": "Find the keyword arguments to pass to numexpr for the given operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gen_fill_zeros ( name ) : name = name . strip ( '__' ) if 'div' in name : # truediv, floordiv, div, and reversed variants fill_value = np . inf elif 'mod' in name : # mod, rmod fill_value = np . nan else : fill_value = None return fill_value\n\n\n        Summary:", "input": "", "output": "Find the appropriate fill value to use when filling in undefined values in the results of the given operation caused by operating on ( generally dividing by ) zero ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_opstr ( op , cls ) : # numexpr is available for non-sparse classes subtyp = getattr ( cls , '_subtyp' , '' ) use_numexpr = 'sparse' not in subtyp if not use_numexpr : # if we're not using numexpr, then don't pass a str_rep return None return { operator . add : '+' , radd : '+' , operator . mul : '*' , rmul : '*' , operator . sub : '-' , rsub : '-' , operator . truediv : '/' , rtruediv : '/' , operator . floordiv : '//' , rfloordiv : '//' , operator . mod : None , # TODO: Why None for mod but '%' for rmod? rmod : '%' , operator . pow : '**' , rpow : '**' , operator . eq : '==' , operator . ne : '!=' , operator . le : '<=' , operator . lt : '<' , operator . ge : '>=' , operator . gt : '>' , operator . and_ : '&' , rand_ : '&' , operator . or_ : '|' , ror_ : '|' , operator . xor : '^' , rxor : '^' , divmod : None , rdivmod : None } [ op ]\n\n\n        Summary:", "input": "", "output": "Find the operation string if any to pass to numexpr for this operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_op_name ( op , special ) : opname = op . __name__ . strip ( '_' ) if special : opname = '__{opname}__' . format ( opname = opname ) return opname\n\n\n        Summary:", "input": "", "output": "Find the name to attach to this method according to conventions for special and non - special methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _make_flex_doc ( op_name , typ ) : op_name = op_name . replace ( '__' , '' ) op_desc = _op_descriptions [ op_name ] if op_desc [ 'reversed' ] : equiv = 'other ' + op_desc [ 'op' ] + ' ' + typ else : equiv = typ + ' ' + op_desc [ 'op' ] + ' other' if typ == 'series' : base_doc = _flex_doc_SERIES doc_no_examples = base_doc . format ( desc = op_desc [ 'desc' ] , op_name = op_name , equiv = equiv , reverse = op_desc [ 'reverse' ] ) if op_desc [ 'series_examples' ] : doc = doc_no_examples + op_desc [ 'series_examples' ] else : doc = doc_no_examples elif typ == 'dataframe' : base_doc = _flex_doc_FRAME doc = base_doc . format ( desc = op_desc [ 'desc' ] , op_name = op_name , equiv = equiv , reverse = op_desc [ 'reverse' ] ) elif typ == 'panel' : base_doc = _flex_doc_PANEL doc = base_doc . format ( desc = op_desc [ 'desc' ] , op_name = op_name , equiv = equiv , reverse = op_desc [ 'reverse' ] ) else : raise AssertionError ( 'Invalid typ argument.' ) return doc\n\n\n        Summary:", "input": "", "output": "Make the appropriate substitutions for the given operation and class - typ into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring to attach to a generated method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fill_binop ( left , right , fill_value ) : # TODO: can we make a no-copy implementation? if fill_value is not None : left_mask = isna ( left ) right_mask = isna ( right ) left = left . copy ( ) right = right . copy ( ) # one but not both mask = left_mask ^ right_mask left [ left_mask & mask ] = fill_value right [ right_mask & mask ] = fill_value return left , right\n\n\n        Summary:", "input": "", "output": "If a non - None fill_value is given replace null entries in left and right with this value but only in positions where _one_ of left / right is null not both ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mask_cmp_op ( x , y , op , allowed_types ) : # TODO: Can we make the allowed_types arg unnecessary? xrav = x . ravel ( ) result = np . empty ( x . size , dtype = bool ) if isinstance ( y , allowed_types ) : yrav = y . ravel ( ) mask = notna ( xrav ) & notna ( yrav ) result [ mask ] = op ( np . array ( list ( xrav [ mask ] ) ) , np . array ( list ( yrav [ mask ] ) ) ) else : mask = notna ( xrav ) result [ mask ] = op ( np . array ( list ( xrav [ mask ] ) ) , y ) if op == operator . ne : # pragma: no cover np . putmask ( result , ~ mask , True ) else : np . putmask ( result , ~ mask , False ) result = result . reshape ( x . shape ) return result\n\n\n        Summary:", "input": "", "output": "Apply the function op to only non - null points in x and y ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def masked_arith_op ( x , y , op ) : # For Series `x` is 1D so ravel() is a no-op; calling it anyway makes # the logic valid for both Series and DataFrame ops. xrav = x . ravel ( ) assert isinstance ( x , ( np . ndarray , ABCSeries ) ) , type ( x ) if isinstance ( y , ( np . ndarray , ABCSeries , ABCIndexClass ) ) : dtype = find_common_type ( [ x . dtype , y . dtype ] ) result = np . empty ( x . size , dtype = dtype ) # PeriodIndex.ravel() returns int64 dtype, so we have # to work around that case. See GH#19956 yrav = y if is_period_dtype ( y ) else y . ravel ( ) mask = notna ( xrav ) & notna ( yrav ) if yrav . shape != mask . shape : # FIXME: GH#5284, GH#5035, GH#19448 # Without specifically raising here we get mismatched # errors in Py3 (TypeError) vs Py2 (ValueError) # Note: Only = an issue in DataFrame case raise ValueError ( 'Cannot broadcast operands together.' ) if mask . any ( ) : with np . errstate ( all = 'ignore' ) : result [ mask ] = op ( xrav [ mask ] , com . values_from_object ( yrav [ mask ] ) ) else : assert is_scalar ( y ) , type ( y ) assert isinstance ( x , np . ndarray ) , type ( x ) # mask is only meaningful for x result = np . empty ( x . size , dtype = x . dtype ) mask = notna ( xrav ) # 1 ** np.nan is 1. So we have to unmask those. if op == pow : mask = np . where ( x == 1 , False , mask ) elif op == rpow : mask = np . where ( y == 1 , False , mask ) if mask . any ( ) : with np . errstate ( all = 'ignore' ) : result [ mask ] = op ( xrav [ mask ] , y ) result , changed = maybe_upcast_putmask ( result , ~ mask , np . nan ) result = result . reshape ( x . shape ) # 2D compat return result\n\n\n        Summary:", "input": "", "output": "If the given arithmetic operation fails attempt it again on only the non - null elements of the input array ( s ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def invalid_comparison ( left , right , op ) : if op is operator . eq : res_values = np . zeros ( left . shape , dtype = bool ) elif op is operator . ne : res_values = np . ones ( left . shape , dtype = bool ) else : raise TypeError ( \"Invalid comparison between dtype={dtype} and {typ}\" . format ( dtype = left . dtype , typ = type ( right ) . __name__ ) ) return res_values\n\n\n        Summary:", "input": "", "output": "If a comparison has mismatched types and is not necessarily meaningful follow python3 conventions by :"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def should_series_dispatch ( left , right , op ) : if left . _is_mixed_type or right . _is_mixed_type : return True if not len ( left . columns ) or not len ( right . columns ) : # ensure obj.dtypes[0] exists for each obj return False ldtype = left . dtypes . iloc [ 0 ] rdtype = right . dtypes . iloc [ 0 ] if ( ( is_timedelta64_dtype ( ldtype ) and is_integer_dtype ( rdtype ) ) or ( is_timedelta64_dtype ( rdtype ) and is_integer_dtype ( ldtype ) ) ) : # numpy integer dtypes as timedelta64 dtypes in this scenario return True if is_datetime64_dtype ( ldtype ) and is_object_dtype ( rdtype ) : # in particular case where right is an array of DateOffsets return True return False\n\n\n        Summary:", "input": "", "output": "Identify cases where a DataFrame operation should dispatch to its Series counterpart ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dispatch_to_series ( left , right , func , str_rep = None , axis = None ) : # Note: we use iloc to access columns for compat with cases # with non-unique columns. import pandas . core . computation . expressions as expressions right = lib . item_from_zerodim ( right ) if lib . is_scalar ( right ) or np . ndim ( right ) == 0 : def column_op ( a , b ) : return { i : func ( a . iloc [ : , i ] , b ) for i in range ( len ( a . columns ) ) } elif isinstance ( right , ABCDataFrame ) : assert right . _indexed_same ( left ) def column_op ( a , b ) : return { i : func ( a . iloc [ : , i ] , b . iloc [ : , i ] ) for i in range ( len ( a . columns ) ) } elif isinstance ( right , ABCSeries ) and axis == \"columns\" : # We only get here if called via left._combine_match_columns, # in which case we specifically want to operate row-by-row assert right . index . equals ( left . columns ) def column_op ( a , b ) : return { i : func ( a . iloc [ : , i ] , b . iloc [ i ] ) for i in range ( len ( a . columns ) ) } elif isinstance ( right , ABCSeries ) : assert right . index . equals ( left . index ) # Handle other cases later def column_op ( a , b ) : return { i : func ( a . iloc [ : , i ] , b ) for i in range ( len ( a . columns ) ) } else : # Remaining cases have less-obvious dispatch rules raise NotImplementedError ( right ) new_data = expressions . evaluate ( column_op , str_rep , left , right ) result = left . _constructor ( new_data , index = left . index , copy = False ) # Pin columns instead of passing to constructor for compat with # non-unique columns case result . columns = left . columns return result\n\n\n        Summary:", "input": "", "output": "Evaluate the frame operation func ( left right ) by evaluating column - by - column dispatching to the Series implementation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dispatch_to_index_op ( op , left , right , index_class ) : left_idx = index_class ( left ) # avoid accidentally allowing integer add/sub. For datetime64[tz] dtypes, # left_idx may inherit a freq from a cached DatetimeIndex. # See discussion in GH#19147. if getattr ( left_idx , 'freq' , None ) is not None : left_idx = left_idx . _shallow_copy ( freq = None ) try : result = op ( left_idx , right ) except NullFrequencyError : # DatetimeIndex and TimedeltaIndex with freq == None raise ValueError # on add/sub of integers (or int-like). We re-raise as a TypeError. raise TypeError ( 'incompatible type for a datetime/timedelta ' 'operation [{name}]' . format ( name = op . __name__ ) ) return result\n\n\n        Summary:", "input": "", "output": "Wrap Series left in the given index_class to delegate the operation op to the index implementation . DatetimeIndex and TimedeltaIndex perform type checking timezone handling overflow checks etc ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dispatch_to_extension_op ( op , left , right ) : # The op calls will raise TypeError if the op is not defined # on the ExtensionArray # unbox Series and Index to arrays if isinstance ( left , ( ABCSeries , ABCIndexClass ) ) : new_left = left . _values else : new_left = left if isinstance ( right , ( ABCSeries , ABCIndexClass ) ) : new_right = right . _values else : new_right = right res_values = op ( new_left , new_right ) res_name = get_op_result_name ( left , right ) if op . __name__ in [ 'divmod' , 'rdivmod' ] : return _construct_divmod_result ( left , res_values , left . index , res_name ) return _construct_result ( left , res_values , left . index , res_name )\n\n\n        Summary:", "input": "", "output": "Assume that left or right is a Series backed by an ExtensionArray apply the operator defined by op ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_method_wrappers ( cls ) : if issubclass ( cls , ABCSparseSeries ) : # Be sure to catch this before ABCSeries and ABCSparseArray, # as they will both come see SparseSeries as a subclass arith_flex = _flex_method_SERIES comp_flex = _flex_method_SERIES arith_special = _arith_method_SPARSE_SERIES comp_special = _arith_method_SPARSE_SERIES bool_special = _bool_method_SERIES # TODO: I don't think the functions defined by bool_method are tested elif issubclass ( cls , ABCSeries ) : # Just Series; SparseSeries is caught above arith_flex = _flex_method_SERIES comp_flex = _flex_method_SERIES arith_special = _arith_method_SERIES comp_special = _comp_method_SERIES bool_special = _bool_method_SERIES elif issubclass ( cls , ABCSparseArray ) : arith_flex = None comp_flex = None arith_special = _arith_method_SPARSE_ARRAY comp_special = _arith_method_SPARSE_ARRAY bool_special = _arith_method_SPARSE_ARRAY elif issubclass ( cls , ABCPanel ) : arith_flex = _flex_method_PANEL comp_flex = _comp_method_PANEL arith_special = _arith_method_PANEL comp_special = _comp_method_PANEL bool_special = _arith_method_PANEL elif issubclass ( cls , ABCDataFrame ) : # Same for DataFrame and SparseDataFrame arith_flex = _arith_method_FRAME comp_flex = _flex_comp_method_FRAME arith_special = _arith_method_FRAME comp_special = _comp_method_FRAME bool_special = _arith_method_FRAME return arith_flex , comp_flex , arith_special , comp_special , bool_special\n\n\n        Summary:", "input": "", "output": "Find the appropriate operation - wrappers to use when defining flex / special arithmetic boolean and comparison operations with the given class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_special_arithmetic_methods ( cls ) : _ , _ , arith_method , comp_method , bool_method = _get_method_wrappers ( cls ) new_methods = _create_methods ( cls , arith_method , comp_method , bool_method , special = True ) # inplace operators (I feel like these should get passed an `inplace=True` # or just be removed def _wrap_inplace_method ( method ) : \"\"\" return an inplace wrapper for this method \"\"\" def f ( self , other ) : result = method ( self , other ) # this makes sure that we are aligned like the input # we are updating inplace so we want to ignore is_copy self . _update_inplace ( result . reindex_like ( self , copy = False ) . _data , verify_is_copy = False ) return self f . __name__ = \"__i{name}__\" . format ( name = method . __name__ . strip ( \"__\" ) ) return f new_methods . update ( dict ( __iadd__ = _wrap_inplace_method ( new_methods [ \"__add__\" ] ) , __isub__ = _wrap_inplace_method ( new_methods [ \"__sub__\" ] ) , __imul__ = _wrap_inplace_method ( new_methods [ \"__mul__\" ] ) , __itruediv__ = _wrap_inplace_method ( new_methods [ \"__truediv__\" ] ) , __ifloordiv__ = _wrap_inplace_method ( new_methods [ \"__floordiv__\" ] ) , __imod__ = _wrap_inplace_method ( new_methods [ \"__mod__\" ] ) , __ipow__ = _wrap_inplace_method ( new_methods [ \"__pow__\" ] ) ) ) new_methods . update ( dict ( __iand__ = _wrap_inplace_method ( new_methods [ \"__and__\" ] ) , __ior__ = _wrap_inplace_method ( new_methods [ \"__or__\" ] ) , __ixor__ = _wrap_inplace_method ( new_methods [ \"__xor__\" ] ) ) ) add_methods ( cls , new_methods = new_methods )\n\n\n        Summary:", "input": "", "output": "Adds the full suite of special arithmetic methods ( __add__ __sub__ etc . ) to the class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_flex_arithmetic_methods ( cls ) : flex_arith_method , flex_comp_method , _ , _ , _ = _get_method_wrappers ( cls ) new_methods = _create_methods ( cls , flex_arith_method , flex_comp_method , bool_method = None , special = False ) new_methods . update ( dict ( multiply = new_methods [ 'mul' ] , subtract = new_methods [ 'sub' ] , divide = new_methods [ 'div' ] ) ) # opt out of bool flex methods for now assert not any ( kname in new_methods for kname in ( 'ror_' , 'rxor' , 'rand_' ) ) add_methods ( cls , new_methods = new_methods )\n\n\n        Summary:", "input": "", "output": "Adds the full suite of flex arithmetic methods ( pow mul add ) to the class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _align_method_SERIES ( left , right , align_asobject = False ) : # ToDo: Different from _align_method_FRAME, list, tuple and ndarray # are not coerced here # because Series has inconsistencies described in #13637 if isinstance ( right , ABCSeries ) : # avoid repeated alignment if not left . index . equals ( right . index ) : if align_asobject : # to keep original value's dtype for bool ops left = left . astype ( object ) right = right . astype ( object ) left , right = left . align ( right , copy = False ) return left , right\n\n\n        Summary:", "input": "", "output": "align lhs and rhs Series"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_result ( left , result , index , name , dtype = None ) : out = left . _constructor ( result , index = index , dtype = dtype ) out = out . __finalize__ ( left ) out . name = name return out\n\n\n        Summary:", "input": "", "output": "If the raw op result has a non - None name ( e . g . it is an Index object ) and the name argument is None then passing name to the constructor will not be enough ; we still need to override the name attribute ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_divmod_result ( left , result , index , name , dtype = None ) : return ( _construct_result ( left , result [ 0 ] , index = index , name = name , dtype = dtype ) , _construct_result ( left , result [ 1 ] , index = index , name = name , dtype = dtype ) , )\n\n\n        Summary:", "input": "", "output": "divmod returns a tuple of like indexed series instead of a single series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _arith_method_SERIES ( cls , op , special ) : str_rep = _get_opstr ( op , cls ) op_name = _get_op_name ( op , special ) eval_kwargs = _gen_eval_kwargs ( op_name ) fill_zeros = _gen_fill_zeros ( op_name ) construct_result = ( _construct_divmod_result if op in [ divmod , rdivmod ] else _construct_result ) def na_op ( x , y ) : import pandas . core . computation . expressions as expressions try : result = expressions . evaluate ( op , str_rep , x , y , * * eval_kwargs ) except TypeError : result = masked_arith_op ( x , y , op ) result = missing . fill_zeros ( result , x , y , op_name , fill_zeros ) return result def safe_na_op ( lvalues , rvalues ) : \"\"\" return the result of evaluating na_op on the passed in values try coercion to object type if the native types are not compatible Parameters ---------- lvalues : array-like rvalues : array-like Raises ------ TypeError: invalid operation \"\"\" try : with np . errstate ( all = 'ignore' ) : return na_op ( lvalues , rvalues ) except Exception : if is_object_dtype ( lvalues ) : return libalgos . arrmap_object ( lvalues , lambda x : op ( x , rvalues ) ) raise def wrapper ( left , right ) : if isinstance ( right , ABCDataFrame ) : return NotImplemented left , right = _align_method_SERIES ( left , right ) res_name = get_op_result_name ( left , right ) right = maybe_upcast_for_op ( right ) if is_categorical_dtype ( left ) : raise TypeError ( \"{typ} cannot perform the operation \" \"{op}\" . format ( typ = type ( left ) . __name__ , op = str_rep ) ) elif is_datetime64_dtype ( left ) or is_datetime64tz_dtype ( left ) : # Give dispatch_to_index_op a chance for tests like # test_dt64_series_add_intlike, which the index dispatching handles # specifically. result = dispatch_to_index_op ( op , left , right , pd . DatetimeIndex ) return construct_result ( left , result , index = left . index , name = res_name , dtype = result . dtype ) elif ( is_extension_array_dtype ( left ) or ( is_extension_array_dtype ( right ) and not is_scalar ( right ) ) ) : # GH#22378 disallow scalar to exclude e.g. \"category\", \"Int64\" return dispatch_to_extension_op ( op , left , right ) elif is_timedelta64_dtype ( left ) : result = dispatch_to_index_op ( op , left , right , pd . TimedeltaIndex ) return construct_result ( left , result , index = left . index , name = res_name ) elif is_timedelta64_dtype ( right ) : # We should only get here with non-scalar or timedelta64('NaT') # values for right # Note: we cannot use dispatch_to_index_op because # that may incorrectly raise TypeError when we # should get NullFrequencyError result = op ( pd . Index ( left ) , right ) return construct_result ( left , result , index = left . index , name = res_name , dtype = result . dtype ) lvalues = left . values rvalues = right if isinstance ( rvalues , ABCSeries ) : rvalues = rvalues . values result = safe_na_op ( lvalues , rvalues ) return construct_result ( left , result , index = left . index , name = res_name , dtype = None ) wrapper . __name__ = op_name return wrapper\n\n\n        Summary:", "input": "", "output": "Wrapper function for Series arithmetic operations to avoid code duplication ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _comp_method_SERIES ( cls , op , special ) : op_name = _get_op_name ( op , special ) masker = _gen_eval_kwargs ( op_name ) . get ( 'masker' , False ) def na_op ( x , y ) : # TODO: # should have guarantess on what x, y can be type-wise # Extension Dtypes are not called here # Checking that cases that were once handled here are no longer # reachable. assert not ( is_categorical_dtype ( y ) and not is_scalar ( y ) ) if is_object_dtype ( x . dtype ) : result = _comp_method_OBJECT_ARRAY ( op , x , y ) elif is_datetimelike_v_numeric ( x , y ) : return invalid_comparison ( x , y , op ) else : # we want to compare like types # we only want to convert to integer like if # we are not NotImplemented, otherwise # we would allow datetime64 (but viewed as i8) against # integer comparisons # we have a datetime/timedelta and may need to convert assert not needs_i8_conversion ( x ) mask = None if not is_scalar ( y ) and needs_i8_conversion ( y ) : mask = isna ( x ) | isna ( y ) y = y . view ( 'i8' ) x = x . view ( 'i8' ) method = getattr ( x , op_name , None ) if method is not None : with np . errstate ( all = 'ignore' ) : result = method ( y ) if result is NotImplemented : return invalid_comparison ( x , y , op ) else : result = op ( x , y ) if mask is not None and mask . any ( ) : result [ mask ] = masker return result def wrapper ( self , other , axis = None ) : # Validate the axis parameter if axis is not None : self . _get_axis_number ( axis ) res_name = get_op_result_name ( self , other ) if isinstance ( other , list ) : # TODO: same for tuples? other = np . asarray ( other ) if isinstance ( other , ABCDataFrame ) : # pragma: no cover # Defer to DataFrame implementation; fail early return NotImplemented elif isinstance ( other , ABCSeries ) and not self . _indexed_same ( other ) : raise ValueError ( \"Can only compare identically-labeled \" \"Series objects\" ) elif is_categorical_dtype ( self ) : # Dispatch to Categorical implementation; pd.CategoricalIndex # behavior is non-canonical GH#19513 res_values = dispatch_to_index_op ( op , self , other , pd . Categorical ) return self . _constructor ( res_values , index = self . index , name = res_name ) elif is_datetime64_dtype ( self ) or is_datetime64tz_dtype ( self ) : # Dispatch to DatetimeIndex to ensure identical # Series/Index behavior if ( isinstance ( other , datetime . date ) and not isinstance ( other , datetime . datetime ) ) : # https://github.com/pandas-dev/pandas/issues/21152 # Compatibility for difference between Series comparison w/ # datetime and date msg = ( \"Comparing Series of datetimes with 'datetime.date'. \" \"Currently, the 'datetime.date' is coerced to a \" \"datetime. In the future pandas will not coerce, \" \"and {future}. \" \"To retain the current behavior, \" \"convert the 'datetime.date' to a datetime with \" \"'pd.Timestamp'.\" ) if op in { operator . lt , operator . le , operator . gt , operator . ge } : future = \"a TypeError will be raised\" else : future = ( \"'the values will not compare equal to the \" \"'datetime.date'\" ) msg = '\\n' . join ( textwrap . wrap ( msg . format ( future = future ) ) ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) other = pd . Timestamp ( other ) res_values = dispatch_to_index_op ( op , self , other , pd . DatetimeIndex ) return self . _constructor ( res_values , index = self . index , name = res_name ) elif is_timedelta64_dtype ( self ) : res_values = dispatch_to_index_op ( op , self , other , pd . TimedeltaIndex ) return self . _constructor ( res_values , index = self . index , name = res_name ) elif ( is_extension_array_dtype ( self ) or ( is_extension_array_dtype ( other ) and not is_scalar ( other ) ) ) : # Note: the `not is_scalar(other)` condition rules out # e.g. other == \"category\" return dispatch_to_extension_op ( op , self , other ) elif isinstance ( other , ABCSeries ) : # By this point we have checked that self._indexed_same(other) res_values = na_op ( self . values , other . values ) # rename is needed in case res_name is None and res_values.name # is not. return self . _constructor ( res_values , index = self . index , name = res_name ) . rename ( res_name ) elif isinstance ( other , ( np . ndarray , pd . Index ) ) : # do not check length of zerodim array # as it will broadcast if other . ndim != 0 and len ( self ) != len ( other ) : raise ValueError ( 'Lengths must match to compare' ) res_values = na_op ( self . values , np . asarray ( other ) ) result = self . _constructor ( res_values , index = self . index ) # rename is needed in case res_name is None and self.name # is not. return result . __finalize__ ( self ) . rename ( res_name ) elif is_scalar ( other ) and isna ( other ) : # numpy does not like comparisons vs None if op is operator . ne : res_values = np . ones ( len ( self ) , dtype = bool ) else : res_values = np . zeros ( len ( self ) , dtype = bool ) return self . _constructor ( res_values , index = self . index , name = res_name , dtype = 'bool' ) else : values = self . get_values ( ) with np . errstate ( all = 'ignore' ) : res = na_op ( values , other ) if is_scalar ( res ) : raise TypeError ( 'Could not compare {typ} type with Series' . format ( typ = type ( other ) ) ) # always return a full value series here res_values = com . values_from_object ( res ) return self . _constructor ( res_values , index = self . index , name = res_name , dtype = 'bool' ) wrapper . __name__ = op_name return wrapper\n\n\n        Summary:", "input": "", "output": "Wrapper function for Series arithmetic operations to avoid code duplication ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _bool_method_SERIES ( cls , op , special ) : op_name = _get_op_name ( op , special ) def na_op ( x , y ) : try : result = op ( x , y ) except TypeError : assert not isinstance ( y , ( list , ABCSeries , ABCIndexClass ) ) if isinstance ( y , np . ndarray ) : # bool-bool dtype operations should be OK, should not get here assert not ( is_bool_dtype ( x ) and is_bool_dtype ( y ) ) x = ensure_object ( x ) y = ensure_object ( y ) result = libops . vec_binop ( x , y , op ) else : # let null fall thru assert lib . is_scalar ( y ) if not isna ( y ) : y = bool ( y ) try : result = libops . scalar_binop ( x , y , op ) except ( TypeError , ValueError , AttributeError , OverflowError , NotImplementedError ) : raise TypeError ( \"cannot compare a dtyped [{dtype}] array \" \"with a scalar of type [{typ}]\" . format ( dtype = x . dtype , typ = type ( y ) . __name__ ) ) return result fill_int = lambda x : x . fillna ( 0 ) fill_bool = lambda x : x . fillna ( False ) . astype ( bool ) def wrapper ( self , other ) : is_self_int_dtype = is_integer_dtype ( self . dtype ) self , other = _align_method_SERIES ( self , other , align_asobject = True ) res_name = get_op_result_name ( self , other ) if isinstance ( other , ABCDataFrame ) : # Defer to DataFrame implementation; fail early return NotImplemented elif isinstance ( other , ( ABCSeries , ABCIndexClass ) ) : is_other_int_dtype = is_integer_dtype ( other . dtype ) other = fill_int ( other ) if is_other_int_dtype else fill_bool ( other ) ovalues = other . values finalizer = lambda x : x else : # scalars, list, tuple, np.array is_other_int_dtype = is_integer_dtype ( np . asarray ( other ) ) if is_list_like ( other ) and not isinstance ( other , np . ndarray ) : # TODO: Can we do this before the is_integer_dtype check? # could the is_integer_dtype check be checking the wrong # thing? e.g. other = [[0, 1], [2, 3], [4, 5]]? other = construct_1d_object_array_from_listlike ( other ) ovalues = other finalizer = lambda x : x . __finalize__ ( self ) # For int vs int `^`, `|`, `&` are bitwise operators and return # integer dtypes. Otherwise these are boolean ops filler = ( fill_int if is_self_int_dtype and is_other_int_dtype else fill_bool ) res_values = na_op ( self . values , ovalues ) unfilled = self . _constructor ( res_values , index = self . index , name = res_name ) filled = filler ( unfilled ) return finalizer ( filled ) wrapper . __name__ = op_name return wrapper\n\n\n        Summary:", "input": "", "output": "Wrapper function for Series arithmetic operations to avoid code duplication ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _combine_series_frame ( self , other , func , fill_value = None , axis = None , level = None ) : if fill_value is not None : raise NotImplementedError ( \"fill_value {fill} not supported.\" . format ( fill = fill_value ) ) if axis is not None : axis = self . _get_axis_number ( axis ) if axis == 0 : return self . _combine_match_index ( other , func , level = level ) else : return self . _combine_match_columns ( other , func , level = level ) else : if not len ( other ) : return self * np . nan if not len ( self ) : # Ambiguous case, use _series so works with DataFrame return self . _constructor ( data = self . _series , index = self . index , columns = self . columns ) # default axis is columns return self . _combine_match_columns ( other , func , level = level )\n\n\n        Summary:", "input": "", "output": "Apply binary operator func to self other using alignment and fill conventions determined by the fill_value axis and level kwargs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _align_method_FRAME ( left , right , axis ) : def to_series ( right ) : msg = ( 'Unable to coerce to Series, length must be {req_len}: ' 'given {given_len}' ) if axis is not None and left . _get_axis_name ( axis ) == 'index' : if len ( left . index ) != len ( right ) : raise ValueError ( msg . format ( req_len = len ( left . index ) , given_len = len ( right ) ) ) right = left . _constructor_sliced ( right , index = left . index ) else : if len ( left . columns ) != len ( right ) : raise ValueError ( msg . format ( req_len = len ( left . columns ) , given_len = len ( right ) ) ) right = left . _constructor_sliced ( right , index = left . columns ) return right if isinstance ( right , np . ndarray ) : if right . ndim == 1 : right = to_series ( right ) elif right . ndim == 2 : if right . shape == left . shape : right = left . _constructor ( right , index = left . index , columns = left . columns ) elif right . shape [ 0 ] == left . shape [ 0 ] and right . shape [ 1 ] == 1 : # Broadcast across columns right = np . broadcast_to ( right , left . shape ) right = left . _constructor ( right , index = left . index , columns = left . columns ) elif right . shape [ 1 ] == left . shape [ 1 ] and right . shape [ 0 ] == 1 : # Broadcast along rows right = to_series ( right [ 0 , : ] ) else : raise ValueError ( \"Unable to coerce to DataFrame, shape \" \"must be {req_shape}: given {given_shape}\" . format ( req_shape = left . shape , given_shape = right . shape ) ) elif right . ndim > 2 : raise ValueError ( 'Unable to coerce to Series/DataFrame, dim ' 'must be <= 2: {dim}' . format ( dim = right . shape ) ) elif ( is_list_like ( right ) and not isinstance ( right , ( ABCSeries , ABCDataFrame ) ) ) : # GH17901 right = to_series ( right ) return right\n\n\n        Summary:", "input": "", "output": "convert rhs to meet lhs dims if input is list tuple or np . ndarray"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _cast_sparse_series_op ( left , right , opname ) : from pandas . core . sparse . api import SparseDtype opname = opname . strip ( '_' ) # TODO: This should be moved to the array? if is_integer_dtype ( left ) and is_integer_dtype ( right ) : # series coerces to float64 if result should have NaN/inf if opname in ( 'floordiv' , 'mod' ) and ( right . values == 0 ) . any ( ) : left = left . astype ( SparseDtype ( np . float64 , left . fill_value ) ) right = right . astype ( SparseDtype ( np . float64 , right . fill_value ) ) elif opname in ( 'rfloordiv' , 'rmod' ) and ( left . values == 0 ) . any ( ) : left = left . astype ( SparseDtype ( np . float64 , left . fill_value ) ) right = right . astype ( SparseDtype ( np . float64 , right . fill_value ) ) return left , right\n\n\n        Summary:", "input": "", "output": "For SparseSeries operation coerce to float64 if the result is expected to have NaN or inf values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _arith_method_SPARSE_SERIES ( cls , op , special ) : op_name = _get_op_name ( op , special ) def wrapper ( self , other ) : if isinstance ( other , ABCDataFrame ) : return NotImplemented elif isinstance ( other , ABCSeries ) : if not isinstance ( other , ABCSparseSeries ) : other = other . to_sparse ( fill_value = self . fill_value ) return _sparse_series_op ( self , other , op , op_name ) elif is_scalar ( other ) : with np . errstate ( all = 'ignore' ) : new_values = op ( self . values , other ) return self . _constructor ( new_values , index = self . index , name = self . name ) else : # pragma: no cover raise TypeError ( 'operation with {other} not supported' . format ( other = type ( other ) ) ) wrapper . __name__ = op_name return wrapper\n\n\n        Summary:", "input": "", "output": "Wrapper function for Series arithmetic operations to avoid code duplication ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _arith_method_SPARSE_ARRAY ( cls , op , special ) : op_name = _get_op_name ( op , special ) def wrapper ( self , other ) : from pandas . core . arrays . sparse . array import ( SparseArray , _sparse_array_op , _wrap_result , _get_fill ) if isinstance ( other , np . ndarray ) : if len ( self ) != len ( other ) : raise AssertionError ( \"length mismatch: {self} vs. {other}\" . format ( self = len ( self ) , other = len ( other ) ) ) if not isinstance ( other , SparseArray ) : dtype = getattr ( other , 'dtype' , None ) other = SparseArray ( other , fill_value = self . fill_value , dtype = dtype ) return _sparse_array_op ( self , other , op , op_name ) elif is_scalar ( other ) : with np . errstate ( all = 'ignore' ) : fill = op ( _get_fill ( self ) , np . asarray ( other ) ) result = op ( self . sp_values , other ) return _wrap_result ( op_name , result , self . sp_index , fill ) else : # pragma: no cover raise TypeError ( 'operation with {other} not supported' . format ( other = type ( other ) ) ) wrapper . __name__ = op_name return wrapper\n\n\n        Summary:", "input": "", "output": "Wrapper function for Series arithmetic operations to avoid code duplication ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_periods ( periods ) : if periods is not None : if lib . is_float ( periods ) : periods = int ( periods ) elif not lib . is_integer ( periods ) : raise TypeError ( 'periods must be a number, got {periods}' . format ( periods = periods ) ) return periods\n\n\n        Summary:", "input": "", "output": "If a periods argument is passed to the Datetime / Timedelta Array / Index constructor cast it to an integer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_endpoints ( closed ) : left_closed = False right_closed = False if closed is None : left_closed = True right_closed = True elif closed == \"left\" : left_closed = True elif closed == \"right\" : right_closed = True else : raise ValueError ( \"Closed has to be either 'left', 'right' or None\" ) return left_closed , right_closed\n\n\n        Summary:", "input": "", "output": "Check that the closed argument is among [ None left right ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_inferred_freq ( freq , inferred_freq , freq_infer ) : if inferred_freq is not None : if freq is not None and freq != inferred_freq : raise ValueError ( 'Inferred frequency {inferred} from passed ' 'values does not conform to passed frequency ' '{passed}' . format ( inferred = inferred_freq , passed = freq . freqstr ) ) elif freq is None : freq = inferred_freq freq_infer = False return freq , freq_infer\n\n\n        Summary:", "input": "", "output": "If the user passes a freq and another freq is inferred from passed data require that they match ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_infer_freq ( freq ) : freq_infer = False if not isinstance ( freq , DateOffset ) : # if a passed freq is None, don't infer automatically if freq != 'infer' : freq = frequencies . to_offset ( freq ) else : freq_infer = True freq = None return freq , freq_infer\n\n\n        Summary:", "input": "", "output": "Comparing a DateOffset to the string infer raises so we need to be careful about comparisons . Make a dummy variable freq_infer to signify the case where the given freq is infer and set freq to None to avoid comparison trouble later on ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_datetimelike_to_i8 ( other , to_utc = False ) : from pandas import Index from pandas . core . arrays import PeriodArray if lib . is_scalar ( other ) and isna ( other ) : return iNaT elif isinstance ( other , ( PeriodArray , ABCIndexClass , DatetimeLikeArrayMixin ) ) : # convert tz if needed if getattr ( other , 'tz' , None ) is not None : if to_utc : other = other . tz_convert ( 'UTC' ) else : other = other . tz_localize ( None ) else : try : return np . array ( other , copy = False ) . view ( 'i8' ) except TypeError : # period array cannot be coerced to int other = Index ( other ) return other . asi8\n\n\n        Summary:", "input": "", "output": "Helper for coercing an input scalar or array to i8 ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _scalar_from_string ( self , value : str , ) -> Union [ Period , Timestamp , Timedelta , NaTType ] : raise AbstractMethodError ( self )\n\n\n        Summary:", "input": "", "output": "Construct a scalar type from a string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unbox_scalar ( self , value : Union [ Period , Timestamp , Timedelta , NaTType ] , ) -> int : raise AbstractMethodError ( self )\n\n\n        Summary:", "input": "", "output": "Unbox the integer value of a scalar value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_compatible_with ( self , other : Union [ Period , Timestamp , Timedelta , NaTType ] , ) -> None : raise AbstractMethodError ( self )\n\n\n        Summary:", "input": "", "output": "Verify that self and other are compatible ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def strftime ( self , date_format ) : from pandas import Index return Index ( self . _format_native_types ( date_format = date_format ) )\n\n\n        Summary:", "input": "", "output": "Convert to Index using specified date_format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def searchsorted ( self , value , side = 'left' , sorter = None ) : if isinstance ( value , str ) : value = self . _scalar_from_string ( value ) if not ( isinstance ( value , ( self . _scalar_type , type ( self ) ) ) or isna ( value ) ) : raise ValueError ( \"Unexpected type for 'value': {valtype}\" . format ( valtype = type ( value ) ) ) self . _check_compatible_with ( value ) if isinstance ( value , type ( self ) ) : value = value . asi8 else : value = self . _unbox_scalar ( value ) return self . asi8 . searchsorted ( value , side = side , sorter = sorter )\n\n\n        Summary:", "input": "", "output": "Find indices where elements should be inserted to maintain order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def repeat ( self , repeats , * args , * * kwargs ) : nv . validate_repeat ( args , kwargs ) values = self . _data . repeat ( repeats ) return type ( self ) ( values . view ( 'i8' ) , dtype = self . dtype )\n\n\n        Summary:", "input": "", "output": "Repeat elements of an array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def value_counts ( self , dropna = False ) : from pandas import Series , Index if dropna : values = self [ ~ self . isna ( ) ] . _data else : values = self . _data cls = type ( self ) result = value_counts ( values , sort = False , dropna = dropna ) index = Index ( cls ( result . index . view ( 'i8' ) , dtype = self . dtype ) , name = result . index . name ) return Series ( result . values , index = index , name = result . name )\n\n\n        Summary:", "input": "", "output": "Return a Series containing counts of unique values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_mask_results ( self , result , fill_value = iNaT , convert = None ) : if self . _hasnans : if convert : result = result . astype ( convert ) if fill_value is None : fill_value = np . nan result [ self . _isnan ] = fill_value return result\n\n\n        Summary:", "input": "", "output": "Parameters ---------- result : a ndarray fill_value : object default iNaT convert : string / dtype or None"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_frequency ( cls , index , freq , * * kwargs ) : if is_period_dtype ( cls ) : # Frequency validation is not meaningful for Period Array/Index return None inferred = index . inferred_freq if index . size == 0 or inferred == freq . freqstr : return None try : on_freq = cls . _generate_range ( start = index [ 0 ] , end = None , periods = len ( index ) , freq = freq , * * kwargs ) if not np . array_equal ( index . asi8 , on_freq . asi8 ) : raise ValueError except ValueError as e : if \"non-fixed\" in str ( e ) : # non-fixed frequencies are not meaningful for timedelta64; # we retain that error message raise e # GH#11587 the main way this is reached is if the `np.array_equal` # check above is False. This can also be reached if index[0] # is `NaT`, in which case the call to `cls._generate_range` will # raise a ValueError, which we re-raise with a more targeted # message. raise ValueError ( 'Inferred frequency {infer} from passed values ' 'does not conform to passed frequency {passed}' . format ( infer = inferred , passed = freq . freqstr ) )\n\n\n        Summary:", "input": "", "output": "Validate that a frequency is compatible with the values of a given Datetime Array / Index or Timedelta Array / Index"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_delta ( self , other ) : if isinstance ( other , ( Tick , timedelta , np . timedelta64 ) ) : new_values = self . _add_timedeltalike_scalar ( other ) elif is_timedelta64_dtype ( other ) : # ndarray[timedelta64] or TimedeltaArray/index new_values = self . _add_delta_tdi ( other ) return new_values\n\n\n        Summary:", "input": "", "output": "Add a timedelta - like Tick or TimedeltaIndex - like object to self yielding an int64 numpy array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_timedeltalike_scalar ( self , other ) : if isna ( other ) : # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds new_values = np . empty ( len ( self ) , dtype = 'i8' ) new_values [ : ] = iNaT return new_values inc = delta_to_nanoseconds ( other ) new_values = checked_add_with_arr ( self . asi8 , inc , arr_mask = self . _isnan ) . view ( 'i8' ) new_values = self . _maybe_mask_results ( new_values ) return new_values . view ( 'i8' )\n\n\n        Summary:", "input": "", "output": "Add a delta of a timedeltalike return the i8 result view"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_delta_tdi ( self , other ) : if len ( self ) != len ( other ) : raise ValueError ( \"cannot add indices of unequal length\" ) if isinstance ( other , np . ndarray ) : # ndarray[timedelta64]; wrap in TimedeltaIndex for op from pandas import TimedeltaIndex other = TimedeltaIndex ( other ) self_i8 = self . asi8 other_i8 = other . asi8 new_values = checked_add_with_arr ( self_i8 , other_i8 , arr_mask = self . _isnan , b_mask = other . _isnan ) if self . _hasnans or other . _hasnans : mask = ( self . _isnan ) | ( other . _isnan ) new_values [ mask ] = iNaT return new_values . view ( 'i8' )\n\n\n        Summary:", "input": "", "output": "Add a delta of a TimedeltaIndex return the i8 result view"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_nat ( self ) : if is_period_dtype ( self ) : raise TypeError ( 'Cannot add {cls} and {typ}' . format ( cls = type ( self ) . __name__ , typ = type ( NaT ) . __name__ ) ) # GH#19124 pd.NaT is treated like a timedelta for both timedelta # and datetime dtypes result = np . zeros ( len ( self ) , dtype = np . int64 ) result . fill ( iNaT ) return type ( self ) ( result , dtype = self . dtype , freq = None )\n\n\n        Summary:", "input": "", "output": "Add pd . NaT to self"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sub_nat ( self ) : # GH#19124 Timedelta - datetime is not in general well-defined. # We make an exception for pd.NaT, which in this case quacks # like a timedelta. # For datetime64 dtypes by convention we treat NaT as a datetime, so # this subtraction returns a timedelta64 dtype. # For period dtype, timedelta64 is a close-enough return dtype. result = np . zeros ( len ( self ) , dtype = np . int64 ) result . fill ( iNaT ) return result . view ( 'timedelta64[ns]' )\n\n\n        Summary:", "input": "", "output": "Subtract pd . NaT from self"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sub_period_array ( self , other ) : if not is_period_dtype ( self ) : raise TypeError ( \"cannot subtract {dtype}-dtype from {cls}\" . format ( dtype = other . dtype , cls = type ( self ) . __name__ ) ) if len ( self ) != len ( other ) : raise ValueError ( \"cannot subtract arrays/indices of \" \"unequal length\" ) if self . freq != other . freq : msg = DIFFERENT_FREQ . format ( cls = type ( self ) . __name__ , own_freq = self . freqstr , other_freq = other . freqstr ) raise IncompatibleFrequency ( msg ) new_values = checked_add_with_arr ( self . asi8 , - other . asi8 , arr_mask = self . _isnan , b_mask = other . _isnan ) new_values = np . array ( [ self . freq . base * x for x in new_values ] ) if self . _hasnans or other . _hasnans : mask = ( self . _isnan ) | ( other . _isnan ) new_values [ mask ] = NaT return new_values\n\n\n        Summary:", "input": "", "output": "Subtract a Period Array / Index from self . This is only valid if self is itself a Period Array / Index raises otherwise . Both objects must have the same frequency ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _addsub_int_array ( self , other , op ) : # _addsub_int_array is overriden by PeriodArray assert not is_period_dtype ( self ) assert op in [ operator . add , operator . sub ] if self . freq is None : # GH#19123 raise NullFrequencyError ( \"Cannot shift with no freq\" ) elif isinstance ( self . freq , Tick ) : # easy case where we can convert to timedelta64 operation td = Timedelta ( self . freq ) return op ( self , td * other ) # We should only get here with DatetimeIndex; dispatch # to _addsub_offset_array assert not is_timedelta64_dtype ( self ) return op ( self , np . array ( other ) * self . freq )\n\n\n        Summary:", "input": "", "output": "Add or subtract array - like of integers equivalent to applying _time_shift pointwise ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _addsub_offset_array ( self , other , op ) : assert op in [ operator . add , operator . sub ] if len ( other ) == 1 : return op ( self , other [ 0 ] ) warnings . warn ( \"Adding/subtracting array of DateOffsets to \" \"{cls} not vectorized\" . format ( cls = type ( self ) . __name__ ) , PerformanceWarning ) # For EA self.astype('O') returns a numpy array, not an Index left = lib . values_from_object ( self . astype ( 'O' ) ) res_values = op ( left , np . array ( other ) ) kwargs = { } if not is_period_dtype ( self ) : kwargs [ 'freq' ] = 'infer' return self . _from_sequence ( res_values , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Add or subtract array - like of DateOffset objects"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _time_shift ( self , periods , freq = None ) : if freq is not None and freq != self . freq : if isinstance ( freq , str ) : freq = frequencies . to_offset ( freq ) offset = periods * freq result = self + offset return result if periods == 0 : # immutable so OK return self . copy ( ) if self . freq is None : raise NullFrequencyError ( \"Cannot shift with no freq\" ) start = self [ 0 ] + periods * self . freq end = self [ - 1 ] + periods * self . freq # Note: in the DatetimeTZ case, _generate_range will infer the # appropriate timezone from `start` and `end`, so tz does not need # to be passed explicitly. return self . _generate_range ( start = start , end = end , periods = None , freq = self . freq )\n\n\n        Summary:", "input": "", "output": "Shift each value by periods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_localized ( self , arg , ambiguous = 'raise' , nonexistent = 'raise' , from_utc = False ) : # reconvert to local tz tz = getattr ( self , 'tz' , None ) if tz is not None : if not isinstance ( arg , type ( self ) ) : arg = self . _simple_new ( arg ) if from_utc : arg = arg . tz_localize ( 'UTC' ) . tz_convert ( self . tz ) else : arg = arg . tz_localize ( self . tz , ambiguous = ambiguous , nonexistent = nonexistent ) return arg\n\n\n        Summary:", "input": "", "output": "Ensure that we are re - localized ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def min ( self , axis = None , skipna = True , * args , * * kwargs ) : nv . validate_min ( args , kwargs ) nv . validate_minmax_axis ( axis ) result = nanops . nanmin ( self . asi8 , skipna = skipna , mask = self . isna ( ) ) if isna ( result ) : # Period._from_ordinal does not handle np.nan gracefully return NaT return self . _box_func ( result )\n\n\n        Summary:", "input": "", "output": "Return the minimum value of the Array or minimum along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def max ( self , axis = None , skipna = True , * args , * * kwargs ) : # TODO: skipna is broken with max. # See https://github.com/pandas-dev/pandas/issues/24265 nv . validate_max ( args , kwargs ) nv . validate_minmax_axis ( axis ) mask = self . isna ( ) if skipna : values = self [ ~ mask ] . asi8 elif mask . any ( ) : return NaT else : values = self . asi8 if not len ( values ) : # short-circut for empty max / min return NaT result = nanops . nanmax ( values , skipna = skipna ) # Don't have to worry about NA `result`, since no NA went in. return self . _box_func ( result )\n\n\n        Summary:", "input": "", "output": "Return the maximum value of the Array or maximum along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _period_array_cmp ( cls , op ) : opname = '__{name}__' . format ( name = op . __name__ ) nat_result = opname == '__ne__' def wrapper ( self , other ) : op = getattr ( self . asi8 , opname ) if isinstance ( other , ( ABCDataFrame , ABCSeries , ABCIndexClass ) ) : return NotImplemented if is_list_like ( other ) and len ( other ) != len ( self ) : raise ValueError ( \"Lengths must match\" ) if isinstance ( other , Period ) : self . _check_compatible_with ( other ) result = op ( other . ordinal ) elif isinstance ( other , cls ) : self . _check_compatible_with ( other ) result = op ( other . asi8 ) mask = self . _isnan | other . _isnan if mask . any ( ) : result [ mask ] = nat_result return result elif other is NaT : result = np . empty ( len ( self . asi8 ) , dtype = bool ) result . fill ( nat_result ) else : other = Period ( other , freq = self . freq ) result = op ( other . ordinal ) if self . _hasnans : result [ self . _isnan ] = nat_result return result return compat . set_function_name ( wrapper , opname , cls )\n\n\n        Summary:", "input": "", "output": "Wrap comparison operations to convert Period - like to PeriodDtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _raise_on_incompatible ( left , right ) : # GH#24283 error message format depends on whether right is scalar if isinstance ( right , np . ndarray ) : other_freq = None elif isinstance ( right , ( ABCPeriodIndex , PeriodArray , Period , DateOffset ) ) : other_freq = right . freqstr else : other_freq = _delta_to_tick ( Timedelta ( right ) ) . freqstr msg = DIFFERENT_FREQ . format ( cls = type ( left ) . __name__ , own_freq = left . freqstr , other_freq = other_freq ) raise IncompatibleFrequency ( msg )\n\n\n        Summary:", "input": "", "output": "Helper function to render a consistent error message when raising IncompatibleFrequency ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def period_array ( data : Sequence [ Optional [ Period ] ] , freq : Optional [ Tick ] = None , copy : bool = False , ) -> PeriodArray : if is_datetime64_dtype ( data ) : return PeriodArray . _from_datetime64 ( data , freq ) if isinstance ( data , ( ABCPeriodIndex , ABCSeries , PeriodArray ) ) : return PeriodArray ( data , freq ) # other iterable of some kind if not isinstance ( data , ( np . ndarray , list , tuple ) ) : data = list ( data ) data = np . asarray ( data ) if freq : dtype = PeriodDtype ( freq ) else : dtype = None if is_float_dtype ( data ) and len ( data ) > 0 : raise TypeError ( \"PeriodIndex does not allow \" \"floating point in construction\" ) data = ensure_object ( data ) return PeriodArray . _from_sequence ( data , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Construct a new PeriodArray from a sequence of Period scalars ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_dtype_freq ( dtype , freq ) : if freq is not None : freq = frequencies . to_offset ( freq ) if dtype is not None : dtype = pandas_dtype ( dtype ) if not is_period_dtype ( dtype ) : raise ValueError ( 'dtype must be PeriodDtype' ) if freq is None : freq = dtype . freq elif freq != dtype . freq : raise IncompatibleFrequency ( 'specified freq and dtype ' 'are different' ) return freq\n\n\n        Summary:", "input": "", "output": "If both a dtype and a freq are available ensure they match . If only dtype is available extract the implied freq ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dt64arr_to_periodarr ( data , freq , tz = None ) : if data . dtype != np . dtype ( 'M8[ns]' ) : raise ValueError ( 'Wrong dtype: {dtype}' . format ( dtype = data . dtype ) ) if freq is None : if isinstance ( data , ABCIndexClass ) : data , freq = data . _values , data . freq elif isinstance ( data , ABCSeries ) : data , freq = data . _values , data . dt . freq freq = Period . _maybe_convert_freq ( freq ) if isinstance ( data , ( ABCIndexClass , ABCSeries ) ) : data = data . _values base , mult = libfrequencies . get_freq_code ( freq ) return libperiod . dt64arr_to_periodarr ( data . view ( 'i8' ) , base , tz ) , freq\n\n\n        Summary:", "input": "", "output": "Convert an datetime - like array to values Period ordinals ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _from_datetime64 ( cls , data , freq , tz = None ) : data , freq = dt64arr_to_periodarr ( data , freq , tz ) return cls ( data , freq = freq )\n\n\n        Summary:", "input": "", "output": "Construct a PeriodArray from a datetime64 array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_timestamp ( self , freq = None , how = 'start' ) : from pandas . core . arrays import DatetimeArray how = libperiod . _validate_end_alias ( how ) end = how == 'E' if end : if freq == 'B' : # roll forward to ensure we land on B date adjust = Timedelta ( 1 , 'D' ) - Timedelta ( 1 , 'ns' ) return self . to_timestamp ( how = 'start' ) + adjust else : adjust = Timedelta ( 1 , 'ns' ) return ( self + self . freq ) . to_timestamp ( how = 'start' ) - adjust if freq is None : base , mult = libfrequencies . get_freq_code ( self . freq ) freq = libfrequencies . get_to_timestamp_base ( base ) else : freq = Period . _maybe_convert_freq ( freq ) base , mult = libfrequencies . get_freq_code ( freq ) new_data = self . asfreq ( freq , how = how ) new_data = libperiod . periodarr_to_dt64arr ( new_data . asi8 , base ) return DatetimeArray . _from_sequence ( new_data , freq = 'infer' )\n\n\n        Summary:", "input": "", "output": "Cast to DatetimeArray / Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _time_shift ( self , periods , freq = None ) : if freq is not None : raise TypeError ( \"`freq` argument is not supported for \" \"{cls}._time_shift\" . format ( cls = type ( self ) . __name__ ) ) values = self . asi8 + periods * self . freq . n if self . _hasnans : values [ self . _isnan ] = iNaT return type ( self ) ( values , freq = self . freq )\n\n\n        Summary:", "input": "", "output": "Shift each value by periods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asfreq ( self , freq = None , how = 'E' ) : how = libperiod . _validate_end_alias ( how ) freq = Period . _maybe_convert_freq ( freq ) base1 , mult1 = libfrequencies . get_freq_code ( self . freq ) base2 , mult2 = libfrequencies . get_freq_code ( freq ) asi8 = self . asi8 # mult1 can't be negative or 0 end = how == 'E' if end : ordinal = asi8 + mult1 - 1 else : ordinal = asi8 new_data = period_asfreq_arr ( ordinal , base1 , base2 , end ) if self . _hasnans : new_data [ self . _isnan ] = iNaT return type ( self ) ( new_data , freq = freq )\n\n\n        Summary:", "input": "", "output": "Convert the Period Array / Index to the specified frequency freq ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_native_types ( self , na_rep = 'NaT' , date_format = None , * * kwargs ) : values = self . astype ( object ) if date_format : formatter = lambda dt : dt . strftime ( date_format ) else : formatter = lambda dt : '%s' % dt if self . _hasnans : mask = self . _isnan values [ mask ] = na_rep imask = ~ mask values [ imask ] = np . array ( [ formatter ( dt ) for dt in values [ imask ] ] ) else : values = np . array ( [ formatter ( dt ) for dt in values ] ) return values\n\n\n        Summary:", "input": "", "output": "actually format my specific types"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_timedeltalike_scalar ( self , other ) : assert isinstance ( self . freq , Tick ) # checked by calling function assert isinstance ( other , ( timedelta , np . timedelta64 , Tick ) ) if notna ( other ) : # special handling for np.timedelta64(\"NaT\"), avoid calling # _check_timedeltalike_freq_compat as that would raise TypeError other = self . _check_timedeltalike_freq_compat ( other ) # Note: when calling parent class's _add_timedeltalike_scalar, # it will call delta_to_nanoseconds(delta). Because delta here # is an integer, delta_to_nanoseconds will return it unchanged. ordinals = super ( ) . _add_timedeltalike_scalar ( other ) return ordinals\n\n\n        Summary:", "input": "", "output": "Parameters ---------- other : timedelta Tick np . timedelta64"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_delta_tdi ( self , other ) : assert isinstance ( self . freq , Tick ) # checked by calling function delta = self . _check_timedeltalike_freq_compat ( other ) return self . _addsub_int_array ( delta , operator . add ) . asi8\n\n\n        Summary:", "input": "", "output": "Parameters ---------- other : TimedeltaArray or ndarray [ timedelta64 ]"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_delta ( self , other ) : if not isinstance ( self . freq , Tick ) : # We cannot add timedelta-like to non-tick PeriodArray _raise_on_incompatible ( self , other ) new_ordinals = super ( ) . _add_delta ( other ) return type ( self ) ( new_ordinals , freq = self . freq )\n\n\n        Summary:", "input": "", "output": "Add a timedelta - like Tick or TimedeltaIndex - like object to self yielding a new PeriodArray"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_timedeltalike_freq_compat ( self , other ) : assert isinstance ( self . freq , Tick ) # checked by calling function own_offset = frequencies . to_offset ( self . freq . rule_code ) base_nanos = delta_to_nanoseconds ( own_offset ) if isinstance ( other , ( timedelta , np . timedelta64 , Tick ) ) : nanos = delta_to_nanoseconds ( other ) elif isinstance ( other , np . ndarray ) : # numpy timedelta64 array; all entries must be compatible assert other . dtype . kind == 'm' if other . dtype != _TD_DTYPE : # i.e. non-nano unit # TODO: disallow unit-less timedelta64 other = other . astype ( _TD_DTYPE ) nanos = other . view ( 'i8' ) else : # TimedeltaArray/Index nanos = other . asi8 if np . all ( nanos % base_nanos == 0 ) : # nanos being added is an integer multiple of the # base-frequency to self.freq delta = nanos // base_nanos # delta is the integer (or integer-array) number of periods # by which will be added to self. return delta _raise_on_incompatible ( self , other )\n\n\n        Summary:", "input": "", "output": "Arithmetic operations with timedelta - like scalars or array other are only valid if other is an integer multiple of self . freq . If the operation is valid find that integer multiple . Otherwise raise because the operation is invalid ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _isna_old ( obj ) : if is_scalar ( obj ) : return libmissing . checknull_old ( obj ) # hack (for now) because MI registers as ndarray elif isinstance ( obj , ABCMultiIndex ) : raise NotImplementedError ( \"isna is not defined for MultiIndex\" ) elif isinstance ( obj , ( ABCSeries , np . ndarray , ABCIndexClass ) ) : return _isna_ndarraylike_old ( obj ) elif isinstance ( obj , ABCGeneric ) : return obj . _constructor ( obj . _data . isna ( func = _isna_old ) ) elif isinstance ( obj , list ) : return _isna_ndarraylike_old ( np . asarray ( obj , dtype = object ) ) elif hasattr ( obj , '__array__' ) : return _isna_ndarraylike_old ( np . asarray ( obj ) ) else : return obj is None\n\n\n        Summary:", "input": "", "output": "Detect missing values . Treat None NaN INF - INF as null ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _use_inf_as_na ( key ) : from pandas . _config import get_option flag = get_option ( key ) if flag : globals ( ) [ '_isna' ] = _isna_old else : globals ( ) [ '_isna' ] = _isna_new\n\n\n        Summary:", "input": "", "output": "Option change callback for na / inf behaviour Choose which replacement for numpy . isnan / - numpy . isfinite is used ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _isna_compat ( arr , fill_value = np . nan ) : dtype = arr . dtype if isna ( fill_value ) : return not ( is_bool_dtype ( dtype ) or is_integer_dtype ( dtype ) ) return True\n\n\n        Summary:", "input": "", "output": "Parameters ---------- arr : a numpy array fill_value : fill value default to np . nan"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array_equivalent ( left , right , strict_nan = False ) : left , right = np . asarray ( left ) , np . asarray ( right ) # shape compat if left . shape != right . shape : return False # Object arrays can contain None, NaN and NaT. # string dtypes must be come to this path for NumPy 1.7.1 compat if is_string_dtype ( left ) or is_string_dtype ( right ) : if not strict_nan : # isna considers NaN and None to be equivalent. return lib . array_equivalent_object ( ensure_object ( left . ravel ( ) ) , ensure_object ( right . ravel ( ) ) ) for left_value , right_value in zip ( left , right ) : if left_value is NaT and right_value is not NaT : return False elif isinstance ( left_value , float ) and np . isnan ( left_value ) : if ( not isinstance ( right_value , float ) or not np . isnan ( right_value ) ) : return False else : if left_value != right_value : return False return True # NaNs can occur in float and complex arrays. if is_float_dtype ( left ) or is_complex_dtype ( left ) : # empty if not ( np . prod ( left . shape ) and np . prod ( right . shape ) ) : return True return ( ( left == right ) | ( isna ( left ) & isna ( right ) ) ) . all ( ) # numpy will will not allow this type of datetimelike vs integer comparison elif is_datetimelike_v_numeric ( left , right ) : return False # M8/m8 elif needs_i8_conversion ( left ) and needs_i8_conversion ( right ) : if not is_dtype_equal ( left . dtype , right . dtype ) : return False left = left . view ( 'i8' ) right = right . view ( 'i8' ) # if we have structured dtypes, compare first if ( left . dtype . type is np . void or right . dtype . type is np . void ) : if left . dtype != right . dtype : return False return np . array_equal ( left , right )\n\n\n        Summary:", "input": "", "output": "True if two arrays left and right have equal non - NaN elements and NaNs in corresponding locations . False otherwise . It is assumed that left and right are NumPy arrays of the same dtype . The behavior of this function ( particularly with respect to NaNs ) is not defined if the dtypes are different ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _infer_fill_value ( val ) : if not is_list_like ( val ) : val = [ val ] val = np . array ( val , copy = False ) if is_datetimelike ( val ) : return np . array ( 'NaT' , dtype = val . dtype ) elif is_object_dtype ( val . dtype ) : dtype = lib . infer_dtype ( ensure_object ( val ) , skipna = False ) if dtype in [ 'datetime' , 'datetime64' ] : return np . array ( 'NaT' , dtype = _NS_DTYPE ) elif dtype in [ 'timedelta' , 'timedelta64' ] : return np . array ( 'NaT' , dtype = _TD_DTYPE ) return np . nan\n\n\n        Summary:", "input": "", "output": "infer the fill value for the nan / NaT from the provided scalar / ndarray / list - like if we are a NaT return the correct dtyped element to provide proper block construction"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_fill ( arr , fill_value = np . nan ) : if _isna_compat ( arr , fill_value ) : arr . fill ( fill_value ) return arr\n\n\n        Summary:", "input": "", "output": "if we have a compatible fill_value and arr dtype then fill"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def na_value_for_dtype ( dtype , compat = True ) : dtype = pandas_dtype ( dtype ) if is_extension_array_dtype ( dtype ) : return dtype . na_value if ( is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) or is_timedelta64_dtype ( dtype ) or is_period_dtype ( dtype ) ) : return NaT elif is_float_dtype ( dtype ) : return np . nan elif is_integer_dtype ( dtype ) : if compat : return 0 return np . nan elif is_bool_dtype ( dtype ) : return False return np . nan\n\n\n        Summary:", "input": "", "output": "Return a dtype compat na value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def remove_na_arraylike ( arr ) : if is_extension_array_dtype ( arr ) : return arr [ notna ( arr ) ] else : return arr [ notna ( lib . values_from_object ( arr ) ) ]\n\n\n        Summary:", "input": "", "output": "Return array - like containing only true / non - NaN values possibly empty ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def table ( ax , data , rowLabels = None , colLabels = None , * * kwargs ) : if isinstance ( data , ABCSeries ) : data = data . to_frame ( ) elif isinstance ( data , ABCDataFrame ) : pass else : raise ValueError ( 'Input data must be DataFrame or Series' ) if rowLabels is None : rowLabels = data . index if colLabels is None : colLabels = data . columns cellText = data . values import matplotlib . table table = matplotlib . table . table ( ax , cellText = cellText , rowLabels = rowLabels , colLabels = colLabels , * * kwargs ) return table\n\n\n        Summary:", "input": "", "output": "Helper function to convert DataFrame and Series to matplotlib . table"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _subplots ( naxes = None , sharex = False , sharey = False , squeeze = True , subplot_kw = None , ax = None , layout = None , layout_type = 'box' , * * fig_kw ) : import matplotlib . pyplot as plt if subplot_kw is None : subplot_kw = { } if ax is None : fig = plt . figure ( * * fig_kw ) else : if is_list_like ( ax ) : ax = _flatten ( ax ) if layout is not None : warnings . warn ( \"When passing multiple axes, layout keyword is \" \"ignored\" , UserWarning ) if sharex or sharey : warnings . warn ( \"When passing multiple axes, sharex and sharey \" \"are ignored. These settings must be specified \" \"when creating axes\" , UserWarning , stacklevel = 4 ) if len ( ax ) == naxes : fig = ax [ 0 ] . get_figure ( ) return fig , ax else : raise ValueError ( \"The number of passed axes must be {0}, the \" \"same as the output plot\" . format ( naxes ) ) fig = ax . get_figure ( ) # if ax is passed and a number of subplots is 1, return ax as it is if naxes == 1 : if squeeze : return fig , ax else : return fig , _flatten ( ax ) else : warnings . warn ( \"To output multiple subplots, the figure containing \" \"the passed axes is being cleared\" , UserWarning , stacklevel = 4 ) fig . clear ( ) nrows , ncols = _get_layout ( naxes , layout = layout , layout_type = layout_type ) nplots = nrows * ncols # Create empty object array to hold all axes. It's easiest to make it 1-d # so we can just append subplots upon creation, and then axarr = np . empty ( nplots , dtype = object ) # Create first subplot separately, so we can share it if requested ax0 = fig . add_subplot ( nrows , ncols , 1 , * * subplot_kw ) if sharex : subplot_kw [ 'sharex' ] = ax0 if sharey : subplot_kw [ 'sharey' ] = ax0 axarr [ 0 ] = ax0 # Note off-by-one counting because add_subplot uses the MATLAB 1-based # convention. for i in range ( 1 , nplots ) : kwds = subplot_kw . copy ( ) # Set sharex and sharey to None for blank/dummy axes, these can # interfere with proper axis limits on the visible axes if # they share axes e.g. issue #7528 if i >= naxes : kwds [ 'sharex' ] = None kwds [ 'sharey' ] = None ax = fig . add_subplot ( nrows , ncols , i + 1 , * * kwds ) axarr [ i ] = ax if naxes != nplots : for ax in axarr [ naxes : ] : ax . set_visible ( False ) _handle_shared_axes ( axarr , nplots , naxes , nrows , ncols , sharex , sharey ) if squeeze : # Reshape the array to have the final desired dimension (nrow,ncol), # though discarding unneeded dimensions that equal 1. If we only have # one subplot, just return it instead of a 1-element array. if nplots == 1 : axes = axarr [ 0 ] else : axes = axarr . reshape ( nrows , ncols ) . squeeze ( ) else : # returned axis array will be always 2-d, even if nrows=ncols=1 axes = axarr . reshape ( nrows , ncols ) return fig , axes\n\n\n        Summary:", "input": "", "output": "Create a figure with a set of subplots already made ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_cythonize ( extensions , * args , * * kwargs ) : if len ( sys . argv ) > 1 and 'clean' in sys . argv : # Avoid running cythonize on `python setup.py clean` # See https://github.com/cython/cython/issues/1495 return extensions if not cython : # Avoid trying to look up numpy when installing from sdist # https://github.com/pandas-dev/pandas/issues/25193 # TODO: See if this can be removed after pyproject.toml added. return extensions numpy_incl = pkg_resources . resource_filename ( 'numpy' , 'core/include' ) # TODO: Is this really necessary here? for ext in extensions : if ( hasattr ( ext , 'include_dirs' ) and numpy_incl not in ext . include_dirs ) : ext . include_dirs . append ( numpy_incl ) build_ext . render_templates ( _pxifiles ) return cythonize ( extensions , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Render tempita templates before calling cythonize"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _transform_fast ( self , result , obj , func_nm ) : # if there were groups with no observations (Categorical only?) # try casting data to original dtype cast = self . _transform_should_cast ( func_nm ) # for each col, reshape to to size of original frame # by take operation ids , _ , ngroup = self . grouper . group_info output = [ ] for i , _ in enumerate ( result . columns ) : res = algorithms . take_1d ( result . iloc [ : , i ] . values , ids ) if cast : res = self . _try_cast ( res , obj . iloc [ : , i ] ) output . append ( res ) return DataFrame . _from_arrays ( output , columns = result . columns , index = obj . index )\n\n\n        Summary:", "input": "", "output": "Fast transform path for aggregations"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def filter ( self , func , dropna = True , * args , * * kwargs ) : # noqa indices = [ ] obj = self . _selected_obj gen = self . grouper . get_iterator ( obj , axis = self . axis ) for name , group in gen : object . __setattr__ ( group , 'name' , name ) res = func ( group , * args , * * kwargs ) try : res = res . squeeze ( ) except AttributeError : # allow e.g., scalars and frames to pass pass # interpret the result of the filter if is_bool ( res ) or ( is_scalar ( res ) and isna ( res ) ) : if res and notna ( res ) : indices . append ( self . _get_index ( name ) ) else : # non scalars aren't allowed raise TypeError ( \"filter function returned a %s, \" \"but expected a scalar bool\" % type ( res ) . __name__ ) return self . _apply_filter ( indices , dropna )\n\n\n        Summary:", "input": "", "output": "Return a copy of a DataFrame excluding elements from groups that do not satisfy the boolean criterion specified by func ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_output ( self , output , index , names = None ) : output = output [ self . _selection_name ] if names is not None : return DataFrame ( output , index = index , columns = names ) else : name = self . _selection_name if name is None : name = self . _selected_obj . name return Series ( output , index = index , name = name )\n\n\n        Summary:", "input": "", "output": "common agg / transform wrapping logic"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _transform_fast ( self , func , func_nm ) : if isinstance ( func , str ) : func = getattr ( self , func ) ids , _ , ngroup = self . grouper . group_info cast = self . _transform_should_cast ( func_nm ) out = algorithms . take_1d ( func ( ) . _values , ids ) if cast : out = self . _try_cast ( out , self . obj ) return Series ( out , index = self . obj . index , name = self . obj . name )\n\n\n        Summary:", "input": "", "output": "fast version of transform only applicable to builtin / cythonizable functions"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def filter ( self , func , dropna = True , * args , * * kwargs ) : # noqa if isinstance ( func , str ) : wrapper = lambda x : getattr ( x , func ) ( * args , * * kwargs ) else : wrapper = lambda x : func ( x , * args , * * kwargs ) # Interpret np.nan as False. def true_and_notna ( x , * args , * * kwargs ) : b = wrapper ( x , * args , * * kwargs ) return b and notna ( b ) try : indices = [ self . _get_index ( name ) for name , group in self if true_and_notna ( group ) ] except ValueError : raise TypeError ( \"the filter must return a boolean result\" ) except TypeError : raise TypeError ( \"the filter must return a boolean result\" ) filtered = self . _apply_filter ( indices , dropna ) return filtered\n\n\n        Summary:", "input": "", "output": "Return a copy of a Series excluding elements from groups that do not satisfy the boolean criterion specified by func ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nunique ( self , dropna = True ) : ids , _ , _ = self . grouper . group_info val = self . obj . get_values ( ) try : sorter = np . lexsort ( ( val , ids ) ) except TypeError : # catches object dtypes msg = 'val.dtype must be object, got {}' . format ( val . dtype ) assert val . dtype == object , msg val , _ = algorithms . factorize ( val , sort = False ) sorter = np . lexsort ( ( val , ids ) ) _isna = lambda a : a == - 1 else : _isna = isna ids , val = ids [ sorter ] , val [ sorter ] # group boundaries are where group ids change # unique observations are where sorted values change idx = np . r_ [ 0 , 1 + np . nonzero ( ids [ 1 : ] != ids [ : - 1 ] ) [ 0 ] ] inc = np . r_ [ 1 , val [ 1 : ] != val [ : - 1 ] ] # 1st item of each group is a new unique observation mask = _isna ( val ) if dropna : inc [ idx ] = 1 inc [ mask ] = 0 else : inc [ mask & np . r_ [ False , mask [ : - 1 ] ] ] = 0 inc [ idx ] = 1 out = np . add . reduceat ( inc , idx ) . astype ( 'int64' , copy = False ) if len ( ids ) : # NaN/NaT group exists if the head of ids is -1, # so remove it from res and exclude its index from idx if ids [ 0 ] == - 1 : res = out [ 1 : ] idx = idx [ np . flatnonzero ( idx ) ] else : res = out else : res = out [ 1 : ] ri = self . grouper . result_index # we might have duplications among the bins if len ( res ) != len ( ri ) : res , out = np . zeros ( len ( ri ) , dtype = out . dtype ) , res res [ ids [ idx ] ] = out return Series ( res , index = ri , name = self . _selection_name )\n\n\n        Summary:", "input": "", "output": "Return number of unique elements in the group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def count ( self ) : ids , _ , ngroups = self . grouper . group_info val = self . obj . get_values ( ) mask = ( ids != - 1 ) & ~ isna ( val ) ids = ensure_platform_int ( ids ) minlength = ngroups or 0 out = np . bincount ( ids [ mask ] , minlength = minlength ) return Series ( out , index = self . grouper . result_index , name = self . _selection_name , dtype = 'int64' )\n\n\n        Summary:", "input": "", "output": "Compute count of group excluding missing values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pct_change ( self , periods = 1 , fill_method = 'pad' , limit = None , freq = None ) : # TODO: Remove this conditional when #23918 is fixed if freq : return self . apply ( lambda x : x . pct_change ( periods = periods , fill_method = fill_method , limit = limit , freq = freq ) ) filled = getattr ( self , fill_method ) ( limit = limit ) fill_grp = filled . groupby ( self . grouper . labels ) shifted = fill_grp . shift ( periods = periods , freq = freq ) return ( filled / shifted ) - 1\n\n\n        Summary:", "input": "", "output": "Calcuate pct_change of each value to previous entry in group"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _gotitem ( self , key , ndim , subset = None ) : if ndim == 2 : if subset is None : subset = self . obj return DataFrameGroupBy ( subset , self . grouper , selection = key , grouper = self . grouper , exclusions = self . exclusions , as_index = self . as_index , observed = self . observed ) elif ndim == 1 : if subset is None : subset = self . obj [ key ] return SeriesGroupBy ( subset , selection = key , grouper = self . grouper ) raise AssertionError ( \"invalid ndim for _gotitem\" )\n\n\n        Summary:", "input": "", "output": "sub - classes to define return a sliced object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reindex_output ( self , result ) : # we need to re-expand the output space to accomodate all values # whether observed or not in the cartesian product of our groupes groupings = self . grouper . groupings if groupings is None : return result elif len ( groupings ) == 1 : return result # if we only care about the observed values # we are done elif self . observed : return result # reindexing only applies to a Categorical grouper elif not any ( isinstance ( ping . grouper , ( Categorical , CategoricalIndex ) ) for ping in groupings ) : return result levels_list = [ ping . group_index for ping in groupings ] index , _ = MultiIndex . from_product ( levels_list , names = self . grouper . names ) . sortlevel ( ) if self . as_index : d = { self . obj . _get_axis_name ( self . axis ) : index , 'copy' : False } return result . reindex ( * * d ) # GH 13204 # Here, the categorical in-axis groupers, which need to be fully # expanded, are columns in `result`. An idea is to do: # result = result.set_index(self.grouper.names) # .reindex(index).reset_index() # but special care has to be taken because of possible not-in-axis # groupers. # So, we manually select and drop the in-axis grouper columns, # reindex `result`, and then reset the in-axis grouper columns. # Select in-axis groupers in_axis_grps = ( ( i , ping . name ) for ( i , ping ) in enumerate ( groupings ) if ping . in_axis ) g_nums , g_names = zip ( * in_axis_grps ) result = result . drop ( labels = list ( g_names ) , axis = 1 ) # Set a temp index and reindex (possibly expanding) result = result . set_index ( self . grouper . result_index ) . reindex ( index , copy = False ) # Reset in-axis grouper columns # (using level numbers `g_nums` because level names may not be unique) result = result . reset_index ( level = g_nums ) return result . reset_index ( drop = True )\n\n\n        Summary:", "input": "", "output": "If we have categorical groupers then we want to make sure that we have a fully reindex - output to the levels . These may have not participated in the groupings ( e . g . may have all been nan groups ) ;"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _fill ( self , direction , limit = None ) : res = super ( ) . _fill ( direction , limit = limit ) output = OrderedDict ( ( grp . name , grp . grouper ) for grp in self . grouper . groupings ) from pandas import concat return concat ( ( self . _wrap_transformed_output ( output ) , res ) , axis = 1 )\n\n\n        Summary:", "input": "", "output": "Overridden method to join grouped columns in output"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def count ( self ) : from pandas . core . dtypes . missing import _isna_ndarraylike as _isna data , _ = self . _get_data_to_aggregate ( ) ids , _ , ngroups = self . grouper . group_info mask = ids != - 1 val = ( ( mask & ~ _isna ( np . atleast_2d ( blk . get_values ( ) ) ) ) for blk in data . blocks ) loc = ( blk . mgr_locs for blk in data . blocks ) counter = partial ( lib . count_level_2d , labels = ids , max_bin = ngroups , axis = 1 ) blk = map ( make_block , map ( counter , val ) , loc ) return self . _wrap_agged_blocks ( data . items , list ( blk ) )\n\n\n        Summary:", "input": "", "output": "Compute count of group excluding missing values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nunique ( self , dropna = True ) : obj = self . _selected_obj def groupby_series ( obj , col = None ) : return SeriesGroupBy ( obj , selection = col , grouper = self . grouper ) . nunique ( dropna = dropna ) if isinstance ( obj , Series ) : results = groupby_series ( obj ) else : from pandas . core . reshape . concat import concat results = [ groupby_series ( obj [ col ] , col ) for col in obj . columns ] results = concat ( results , axis = 1 ) results . columns . names = obj . columns . names if not self . as_index : results . index = ibase . default_index ( len ( results ) ) return results\n\n\n        Summary:", "input": "", "output": "Return DataFrame with number of distinct observations per group for each column ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def extract_array ( obj , extract_numpy = False ) : if isinstance ( obj , ( ABCIndexClass , ABCSeries ) ) : obj = obj . array if extract_numpy and isinstance ( obj , ABCPandasArray ) : obj = obj . to_numpy ( ) return obj\n\n\n        Summary:", "input": "", "output": "Extract the ndarray or ExtensionArray from a Series or Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def flatten ( l ) : for el in l : if _iterable_not_string ( el ) : for s in flatten ( el ) : yield s else : yield el\n\n\n        Summary:", "input": "", "output": "Flatten an arbitrarily nested sequence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_bool_indexer ( key : Any ) -> bool : na_msg = 'cannot index with vector containing NA / NaN values' if ( isinstance ( key , ( ABCSeries , np . ndarray , ABCIndex ) ) or ( is_array_like ( key ) and is_extension_array_dtype ( key . dtype ) ) ) : if key . dtype == np . object_ : key = np . asarray ( values_from_object ( key ) ) if not lib . is_bool_array ( key ) : if isna ( key ) . any ( ) : raise ValueError ( na_msg ) return False return True elif is_bool_dtype ( key . dtype ) : # an ndarray with bool-dtype by definition has no missing values. # So we only need to check for NAs in ExtensionArrays if is_extension_array_dtype ( key . dtype ) : if np . any ( key . isna ( ) ) : raise ValueError ( na_msg ) return True elif isinstance ( key , list ) : try : arr = np . asarray ( key ) return arr . dtype == np . bool_ and len ( arr ) == len ( key ) except TypeError : # pragma: no cover return False return False\n\n\n        Summary:", "input": "", "output": "Check whether key is a valid boolean indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cast_scalar_indexer ( val ) : # assumes lib.is_scalar(val) if lib . is_float ( val ) and val == int ( val ) : return int ( val ) return val\n\n\n        Summary:", "input": "", "output": "To avoid numpy DeprecationWarnings cast float to integer where valid ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def index_labels_to_array ( labels , dtype = None ) : if isinstance ( labels , ( str , tuple ) ) : labels = [ labels ] if not isinstance ( labels , ( list , np . ndarray ) ) : try : labels = list ( labels ) except TypeError : # non-iterable labels = [ labels ] labels = asarray_tuplesafe ( labels , dtype = dtype ) return labels\n\n\n        Summary:", "input": "", "output": "Transform label or iterable of labels to array for use in Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_null_slice ( obj ) : return ( isinstance ( obj , slice ) and obj . start is None and obj . stop is None and obj . step is None )\n\n\n        Summary:", "input": "", "output": "We have a null slice ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_full_slice ( obj , l ) : return ( isinstance ( obj , slice ) and obj . start == 0 and obj . stop == l and obj . step is None )\n\n\n        Summary:", "input": "", "output": "We have a full length slice ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply_if_callable ( maybe_callable , obj , * * kwargs ) : if callable ( maybe_callable ) : return maybe_callable ( obj , * * kwargs ) return maybe_callable\n\n\n        Summary:", "input": "", "output": "Evaluate possibly callable input using obj and kwargs if it is callable otherwise return as it is ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def standardize_mapping ( into ) : if not inspect . isclass ( into ) : if isinstance ( into , collections . defaultdict ) : return partial ( collections . defaultdict , into . default_factory ) into = type ( into ) if not issubclass ( into , abc . Mapping ) : raise TypeError ( 'unsupported type: {into}' . format ( into = into ) ) elif into == collections . defaultdict : raise TypeError ( 'to_dict() only accepts initialized defaultdicts' ) return into\n\n\n        Summary:", "input": "", "output": "Helper function to standardize a supplied mapping ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def random_state ( state = None ) : if is_integer ( state ) : return np . random . RandomState ( state ) elif isinstance ( state , np . random . RandomState ) : return state elif state is None : return np . random else : raise ValueError ( \"random_state must be an integer, a numpy \" \"RandomState, or None\" )\n\n\n        Summary:", "input": "", "output": "Helper function for processing random_state arguments ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _pipe ( obj , func , * args , * * kwargs ) : if isinstance ( func , tuple ) : func , target = func if target in kwargs : msg = '%s is both the pipe target and a keyword argument' % target raise ValueError ( msg ) kwargs [ target ] = obj return func ( * args , * * kwargs ) else : return func ( obj , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Apply a function func to object obj either by passing obj as the first argument to the function or in the case that the func is a tuple interpret the first element of the tuple as a function and pass the obj to that function as a keyword argument whose key is the value of the second element of the tuple ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_rename_function ( mapper ) : if isinstance ( mapper , ( abc . Mapping , ABCSeries ) ) : def f ( x ) : if x in mapper : return mapper [ x ] else : return x else : f = mapper return f\n\n\n        Summary:", "input": "", "output": "Returns a function that will map names / labels dependent if mapper is a dict Series or just a function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_fill_value ( dtype , fill_value = None , fill_value_typ = None ) : if fill_value is not None : return fill_value if _na_ok_dtype ( dtype ) : if fill_value_typ is None : return np . nan else : if fill_value_typ == '+inf' : return np . inf else : return - np . inf else : if fill_value_typ is None : return tslibs . iNaT else : if fill_value_typ == '+inf' : # need the max int here return _int64_max else : return tslibs . iNaT\n\n\n        Summary:", "input": "", "output": "return the correct fill value for the dtype of the values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_values ( values , skipna , fill_value = None , fill_value_typ = None , isfinite = False , copy = True , mask = None ) : if is_datetime64tz_dtype ( values ) : # com.values_from_object returns M8[ns] dtype instead of tz-aware, # so this case must be handled separately from the rest dtype = values . dtype values = getattr ( values , \"_values\" , values ) else : values = com . values_from_object ( values ) dtype = values . dtype if mask is None : if isfinite : mask = _isfinite ( values ) else : mask = isna ( values ) if is_datetime_or_timedelta_dtype ( values ) or is_datetime64tz_dtype ( values ) : # changing timedelta64/datetime64 to int64 needs to happen after # finding `mask` above values = getattr ( values , \"asi8\" , values ) values = values . view ( np . int64 ) dtype_ok = _na_ok_dtype ( dtype ) # get our fill value (in case we need to provide an alternative # dtype for it) fill_value = _get_fill_value ( dtype , fill_value = fill_value , fill_value_typ = fill_value_typ ) if skipna : if copy : values = values . copy ( ) if dtype_ok : np . putmask ( values , mask , fill_value ) # promote if needed else : values , changed = maybe_upcast_putmask ( values , mask , fill_value ) elif copy : values = values . copy ( ) # return a platform independent precision dtype dtype_max = dtype if is_integer_dtype ( dtype ) or is_bool_dtype ( dtype ) : dtype_max = np . int64 elif is_float_dtype ( dtype ) : dtype_max = np . float64 return values , mask , dtype , dtype_max , fill_value\n\n\n        Summary:", "input": "", "output": "utility to get the values view mask dtype if necessary copy and mask using the specified fill_value copy = True will force the copy"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_results ( result , dtype , fill_value = None ) : if is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) : if fill_value is None : # GH#24293 fill_value = iNaT if not isinstance ( result , np . ndarray ) : tz = getattr ( dtype , 'tz' , None ) assert not isna ( fill_value ) , \"Expected non-null fill_value\" if result == fill_value : result = np . nan result = tslibs . Timestamp ( result , tz = tz ) else : result = result . view ( dtype ) elif is_timedelta64_dtype ( dtype ) : if not isinstance ( result , np . ndarray ) : if result == fill_value : result = np . nan # raise if we have a timedelta64[ns] which is too large if np . fabs ( result ) > _int64_max : raise ValueError ( \"overflow in timedelta operation\" ) result = tslibs . Timedelta ( result , unit = 'ns' ) else : result = result . astype ( 'i8' ) . view ( dtype ) return result\n\n\n        Summary:", "input": "", "output": "wrap our results if needed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _na_for_min_count ( values , axis ) : # we either return np.nan or pd.NaT if is_numeric_dtype ( values ) : values = values . astype ( 'float64' ) fill_value = na_value_for_dtype ( values . dtype ) if values . ndim == 1 : return fill_value else : result_shape = ( values . shape [ : axis ] + values . shape [ axis + 1 : ] ) result = np . empty ( result_shape , dtype = values . dtype ) result . fill ( fill_value ) return result\n\n\n        Summary:", "input": "", "output": "Return the missing value for values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanany ( values , axis = None , skipna = True , mask = None ) : values , mask , dtype , _ , _ = _get_values ( values , skipna , False , copy = skipna , mask = mask ) return values . any ( axis )\n\n\n        Summary:", "input": "", "output": "Check if any elements along an axis evaluate to True ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanall ( values , axis = None , skipna = True , mask = None ) : values , mask , dtype , _ , _ = _get_values ( values , skipna , True , copy = skipna , mask = mask ) return values . all ( axis )\n\n\n        Summary:", "input": "", "output": "Check if all elements along an axis evaluate to True ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nansum ( values , axis = None , skipna = True , min_count = 0 , mask = None ) : values , mask , dtype , dtype_max , _ = _get_values ( values , skipna , 0 , mask = mask ) dtype_sum = dtype_max if is_float_dtype ( dtype ) : dtype_sum = dtype elif is_timedelta64_dtype ( dtype ) : dtype_sum = np . float64 the_sum = values . sum ( axis , dtype = dtype_sum ) the_sum = _maybe_null_out ( the_sum , axis , mask , min_count = min_count ) return _wrap_results ( the_sum , dtype )\n\n\n        Summary:", "input": "", "output": "Sum the elements along an axis ignoring NaNs"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanmean ( values , axis = None , skipna = True , mask = None ) : values , mask , dtype , dtype_max , _ = _get_values ( values , skipna , 0 , mask = mask ) dtype_sum = dtype_max dtype_count = np . float64 if ( is_integer_dtype ( dtype ) or is_timedelta64_dtype ( dtype ) or is_datetime64_dtype ( dtype ) or is_datetime64tz_dtype ( dtype ) ) : dtype_sum = np . float64 elif is_float_dtype ( dtype ) : dtype_sum = dtype dtype_count = dtype count = _get_counts ( mask , axis , dtype = dtype_count ) the_sum = _ensure_numeric ( values . sum ( axis , dtype = dtype_sum ) ) if axis is not None and getattr ( the_sum , 'ndim' , False ) : with np . errstate ( all = \"ignore\" ) : # suppress division by zero warnings the_mean = the_sum / count ct_mask = count == 0 if ct_mask . any ( ) : the_mean [ ct_mask ] = np . nan else : the_mean = the_sum / count if count > 0 else np . nan return _wrap_results ( the_mean , dtype )\n\n\n        Summary:", "input": "", "output": "Compute the mean of the element along an axis ignoring NaNs"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanmedian ( values , axis = None , skipna = True , mask = None ) : def get_median ( x ) : mask = notna ( x ) if not skipna and not mask . all ( ) : return np . nan return np . nanmedian ( x [ mask ] ) values , mask , dtype , dtype_max , _ = _get_values ( values , skipna , mask = mask ) if not is_float_dtype ( values ) : values = values . astype ( 'f8' ) values [ mask ] = np . nan if axis is None : values = values . ravel ( ) notempty = values . size # an array from a frame if values . ndim > 1 : # there's a non-empty array to apply over otherwise numpy raises if notempty : if not skipna : return _wrap_results ( np . apply_along_axis ( get_median , axis , values ) , dtype ) # fastpath for the skipna case return _wrap_results ( np . nanmedian ( values , axis ) , dtype ) # must return the correct shape, but median is not defined for the # empty set so return nans of shape \"everything but the passed axis\" # since \"axis\" is where the reduction would occur if we had a nonempty # array shp = np . array ( values . shape ) dims = np . arange ( values . ndim ) ret = np . empty ( shp [ dims != axis ] ) ret . fill ( np . nan ) return _wrap_results ( ret , dtype ) # otherwise return a scalar value return _wrap_results ( get_median ( values ) if notempty else np . nan , dtype )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- values : ndarray axis : int optional skipna : bool default True mask : ndarray [ bool ] optional nan - mask if known"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanstd ( values , axis = None , skipna = True , ddof = 1 , mask = None ) : result = np . sqrt ( nanvar ( values , axis = axis , skipna = skipna , ddof = ddof , mask = mask ) ) return _wrap_results ( result , values . dtype )\n\n\n        Summary:", "input": "", "output": "Compute the standard deviation along given axis while ignoring NaNs"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanvar ( values , axis = None , skipna = True , ddof = 1 , mask = None ) : values = com . values_from_object ( values ) dtype = values . dtype if mask is None : mask = isna ( values ) if is_any_int_dtype ( values ) : values = values . astype ( 'f8' ) values [ mask ] = np . nan if is_float_dtype ( values ) : count , d = _get_counts_nanvar ( mask , axis , ddof , values . dtype ) else : count , d = _get_counts_nanvar ( mask , axis , ddof ) if skipna : values = values . copy ( ) np . putmask ( values , mask , 0 ) # xref GH10242 # Compute variance via two-pass algorithm, which is stable against # cancellation errors and relatively accurate for small numbers of # observations. # # See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance avg = _ensure_numeric ( values . sum ( axis = axis , dtype = np . float64 ) ) / count if axis is not None : avg = np . expand_dims ( avg , axis ) sqr = _ensure_numeric ( ( avg - values ) ** 2 ) np . putmask ( sqr , mask , 0 ) result = sqr . sum ( axis = axis , dtype = np . float64 ) / d # Return variance as np.float64 (the datatype used in the accumulator), # unless we were dealing with a float array, in which case use the same # precision as the original values array. if is_float_dtype ( dtype ) : result = result . astype ( dtype ) return _wrap_results ( result , values . dtype )\n\n\n        Summary:", "input": "", "output": "Compute the variance along given axis while ignoring NaNs"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nansem ( values , axis = None , skipna = True , ddof = 1 , mask = None ) : # This checks if non-numeric-like data is passed with numeric_only=False # and raises a TypeError otherwise nanvar ( values , axis , skipna , ddof = ddof , mask = mask ) if mask is None : mask = isna ( values ) if not is_float_dtype ( values . dtype ) : values = values . astype ( 'f8' ) count , _ = _get_counts_nanvar ( mask , axis , ddof , values . dtype ) var = nanvar ( values , axis , skipna , ddof = ddof ) return np . sqrt ( var ) / np . sqrt ( count )\n\n\n        Summary:", "input": "", "output": "Compute the standard error in the mean along given axis while ignoring NaNs"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanargmax ( values , axis = None , skipna = True , mask = None ) : values , mask , dtype , _ , _ = _get_values ( values , skipna , fill_value_typ = '-inf' , mask = mask ) result = values . argmax ( axis ) result = _maybe_arg_null_out ( result , axis , mask , skipna ) return result\n\n\n        Summary:", "input": "", "output": "Parameters ---------- values : ndarray axis : int optional skipna : bool default True mask : ndarray [ bool ] optional nan - mask if known"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanargmin ( values , axis = None , skipna = True , mask = None ) : values , mask , dtype , _ , _ = _get_values ( values , skipna , fill_value_typ = '+inf' , mask = mask ) result = values . argmin ( axis ) result = _maybe_arg_null_out ( result , axis , mask , skipna ) return result\n\n\n        Summary:", "input": "", "output": "Parameters ---------- values : ndarray axis : int optional skipna : bool default True mask : ndarray [ bool ] optional nan - mask if known"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanskew ( values , axis = None , skipna = True , mask = None ) : values = com . values_from_object ( values ) if mask is None : mask = isna ( values ) if not is_float_dtype ( values . dtype ) : values = values . astype ( 'f8' ) count = _get_counts ( mask , axis ) else : count = _get_counts ( mask , axis , dtype = values . dtype ) if skipna : values = values . copy ( ) np . putmask ( values , mask , 0 ) mean = values . sum ( axis , dtype = np . float64 ) / count if axis is not None : mean = np . expand_dims ( mean , axis ) adjusted = values - mean if skipna : np . putmask ( adjusted , mask , 0 ) adjusted2 = adjusted ** 2 adjusted3 = adjusted2 * adjusted m2 = adjusted2 . sum ( axis , dtype = np . float64 ) m3 = adjusted3 . sum ( axis , dtype = np . float64 ) # floating point error # # #18044 in _libs/windows.pyx calc_skew follow this behavior # to fix the fperr to treat m2 <1e-14 as zero m2 = _zero_out_fperr ( m2 ) m3 = _zero_out_fperr ( m3 ) with np . errstate ( invalid = 'ignore' , divide = 'ignore' ) : result = ( count * ( count - 1 ) ** 0.5 / ( count - 2 ) ) * ( m3 / m2 ** 1.5 ) dtype = values . dtype if is_float_dtype ( dtype ) : result = result . astype ( dtype ) if isinstance ( result , np . ndarray ) : result = np . where ( m2 == 0 , 0 , result ) result [ count < 3 ] = np . nan return result else : result = 0 if m2 == 0 else result if count < 3 : return np . nan return result\n\n\n        Summary:", "input": "", "output": "Compute the sample skewness ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nankurt ( values , axis = None , skipna = True , mask = None ) : values = com . values_from_object ( values ) if mask is None : mask = isna ( values ) if not is_float_dtype ( values . dtype ) : values = values . astype ( 'f8' ) count = _get_counts ( mask , axis ) else : count = _get_counts ( mask , axis , dtype = values . dtype ) if skipna : values = values . copy ( ) np . putmask ( values , mask , 0 ) mean = values . sum ( axis , dtype = np . float64 ) / count if axis is not None : mean = np . expand_dims ( mean , axis ) adjusted = values - mean if skipna : np . putmask ( adjusted , mask , 0 ) adjusted2 = adjusted ** 2 adjusted4 = adjusted2 ** 2 m2 = adjusted2 . sum ( axis , dtype = np . float64 ) m4 = adjusted4 . sum ( axis , dtype = np . float64 ) with np . errstate ( invalid = 'ignore' , divide = 'ignore' ) : adj = 3 * ( count - 1 ) ** 2 / ( ( count - 2 ) * ( count - 3 ) ) numer = count * ( count + 1 ) * ( count - 1 ) * m4 denom = ( count - 2 ) * ( count - 3 ) * m2 ** 2 # floating point error # # #18044 in _libs/windows.pyx calc_kurt follow this behavior # to fix the fperr to treat denom <1e-14 as zero numer = _zero_out_fperr ( numer ) denom = _zero_out_fperr ( denom ) if not isinstance ( denom , np . ndarray ) : # if ``denom`` is a scalar, check these corner cases first before # doing division if count < 4 : return np . nan if denom == 0 : return 0 with np . errstate ( invalid = 'ignore' , divide = 'ignore' ) : result = numer / denom - adj dtype = values . dtype if is_float_dtype ( dtype ) : result = result . astype ( dtype ) if isinstance ( result , np . ndarray ) : result = np . where ( denom == 0 , 0 , result ) result [ count < 4 ] = np . nan return result\n\n\n        Summary:", "input": "", "output": "Compute the sample excess kurtosis"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanprod ( values , axis = None , skipna = True , min_count = 0 , mask = None ) : if mask is None : mask = isna ( values ) if skipna and not is_any_int_dtype ( values ) : values = values . copy ( ) values [ mask ] = 1 result = values . prod ( axis ) return _maybe_null_out ( result , axis , mask , min_count = min_count )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- values : ndarray [ dtype ] axis : int optional skipna : bool default True min_count : int default 0 mask : ndarray [ bool ] optional nan - mask if known"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nancorr ( a , b , method = 'pearson' , min_periods = None ) : if len ( a ) != len ( b ) : raise AssertionError ( 'Operands to nancorr must have same size' ) if min_periods is None : min_periods = 1 valid = notna ( a ) & notna ( b ) if not valid . all ( ) : a = a [ valid ] b = b [ valid ] if len ( a ) < min_periods : return np . nan f = get_corr_func ( method ) return f ( a , b )\n\n\n        Summary:", "input": "", "output": "a b : ndarrays"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _nanpercentile_1d ( values , mask , q , na_value , interpolation ) : # mask is Union[ExtensionArray, ndarray] values = values [ ~ mask ] if len ( values ) == 0 : if lib . is_scalar ( q ) : return na_value else : return np . array ( [ na_value ] * len ( q ) , dtype = values . dtype ) return np . percentile ( values , q , interpolation = interpolation )\n\n\n        Summary:", "input": "", "output": "Wraper for np . percentile that skips missing values specialized to 1 - dimensional case ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nanpercentile ( values , q , axis , na_value , mask , ndim , interpolation ) : if not lib . is_scalar ( mask ) and mask . any ( ) : if ndim == 1 : return _nanpercentile_1d ( values , mask , q , na_value , interpolation = interpolation ) else : # for nonconsolidatable blocks mask is 1D, but values 2D if mask . ndim < values . ndim : mask = mask . reshape ( values . shape ) if axis == 0 : values = values . T mask = mask . T result = [ _nanpercentile_1d ( val , m , q , na_value , interpolation = interpolation ) for ( val , m ) in zip ( list ( values ) , list ( mask ) ) ] result = np . array ( result , dtype = values . dtype , copy = False ) . T return result else : return np . percentile ( values , q , axis = axis , interpolation = interpolation )\n\n\n        Summary:", "input": "", "output": "Wraper for np . percentile that skips missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_th ( self , s , header = False , indent = 0 , tags = None ) : if header and self . fmt . col_space is not None : tags = ( tags or \"\" ) tags += ( 'style=\"min-width: {colspace};\"' . format ( colspace = self . fmt . col_space ) ) return self . _write_cell ( s , kind = 'th' , indent = indent , tags = tags )\n\n\n        Summary:", "input": "", "output": "Method for writting a formatted <th > cell ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_clipboard ( sep = r'\\s+' , * * kwargs ) : # pragma: no cover encoding = kwargs . pop ( 'encoding' , 'utf-8' ) # only utf-8 is valid for passed value because that's what clipboard # supports if encoding is not None and encoding . lower ( ) . replace ( '-' , '' ) != 'utf8' : raise NotImplementedError ( 'reading from clipboard only supports utf-8 encoding' ) from pandas . io . clipboard import clipboard_get from pandas . io . parsers import read_csv text = clipboard_get ( ) # Try to decode (if needed, as \"text\" might already be a string here). try : text = text . decode ( kwargs . get ( 'encoding' ) or get_option ( 'display.encoding' ) ) except AttributeError : pass # Excel copies into clipboard with \\t separation # inspect no more then the 10 first lines, if they # all contain an equal number (>0) of tabs, infer # that this came from excel and set 'sep' accordingly lines = text [ : 10000 ] . split ( '\\n' ) [ : - 1 ] [ : 10 ] # Need to remove leading white space, since read_csv # accepts: # a b # 0 1 2 # 1 3 4 counts = { x . lstrip ( ) . count ( '\\t' ) for x in lines } if len ( lines ) > 1 and len ( counts ) == 1 and counts . pop ( ) != 0 : sep = '\\t' # Edge case where sep is specified to be None, return to default if sep is None and kwargs . get ( 'delim_whitespace' ) is None : sep = r'\\s+' # Regex separator currently only works with python engine. # Default to python if separator is multi-character (regex) if len ( sep ) > 1 and kwargs . get ( 'engine' ) is None : kwargs [ 'engine' ] = 'python' elif len ( sep ) > 1 and kwargs . get ( 'engine' ) == 'c' : warnings . warn ( 'read_clipboard with regex separator does not work' ' properly with c engine' ) return read_csv ( StringIO ( text ) , sep = sep , * * kwargs )\n\n\n        Summary:", "input": "", "output": "r Read text from clipboard and pass to read_csv . See read_csv for the full argument list"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_clipboard ( obj , excel = True , sep = None , * * kwargs ) : # pragma: no cover encoding = kwargs . pop ( 'encoding' , 'utf-8' ) # testing if an invalid encoding is passed to clipboard if encoding is not None and encoding . lower ( ) . replace ( '-' , '' ) != 'utf8' : raise ValueError ( 'clipboard only supports utf-8 encoding' ) from pandas . io . clipboard import clipboard_set if excel is None : excel = True if excel : try : if sep is None : sep = '\\t' buf = StringIO ( ) # clipboard_set (pyperclip) expects unicode obj . to_csv ( buf , sep = sep , encoding = 'utf-8' , * * kwargs ) text = buf . getvalue ( ) clipboard_set ( text ) return except TypeError : warnings . warn ( 'to_clipboard in excel mode requires a single ' 'character separator.' ) elif sep is not None : warnings . warn ( 'to_clipboard with excel=False ignores the sep argument' ) if isinstance ( obj , ABCDataFrame ) : # str(df) has various unhelpful defaults, like truncation with option_context ( 'display.max_colwidth' , 999999 ) : objstr = obj . to_string ( * * kwargs ) else : objstr = str ( obj ) clipboard_set ( objstr )\n\n\n        Summary:", "input": "", "output": "Attempt to write text representation of object to the system clipboard The clipboard can be then pasted into Excel for example ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_skiprows ( skiprows ) : if isinstance ( skiprows , slice ) : return lrange ( skiprows . start or 0 , skiprows . stop , skiprows . step or 1 ) elif isinstance ( skiprows , numbers . Integral ) or is_list_like ( skiprows ) : return skiprows elif skiprows is None : return 0 raise TypeError ( '%r is not a valid type for skipping rows' % type ( skiprows ) . __name__ )\n\n\n        Summary:", "input": "", "output": "Get an iterator given an integer slice or container ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _read ( obj ) : if _is_url ( obj ) : with urlopen ( obj ) as url : text = url . read ( ) elif hasattr ( obj , 'read' ) : text = obj . read ( ) elif isinstance ( obj , ( str , bytes ) ) : text = obj try : if os . path . isfile ( text ) : with open ( text , 'rb' ) as f : return f . read ( ) except ( TypeError , ValueError ) : pass else : raise TypeError ( \"Cannot read object of type %r\" % type ( obj ) . __name__ ) return text\n\n\n        Summary:", "input": "", "output": "Try to read from a url file or string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _build_xpath_expr ( attrs ) : # give class attribute as class_ because class is a python keyword if 'class_' in attrs : attrs [ 'class' ] = attrs . pop ( 'class_' ) s = [ \"@{key}={val!r}\" . format ( key = k , val = v ) for k , v in attrs . items ( ) ] return '[{expr}]' . format ( expr = ' and ' . join ( s ) )\n\n\n        Summary:", "input": "", "output": "Build an xpath expression to simulate bs4 s ability to pass in kwargs to search for attributes when using the lxml parser ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parser_dispatch ( flavor ) : valid_parsers = list ( _valid_parsers . keys ( ) ) if flavor not in valid_parsers : raise ValueError ( '{invalid!r} is not a valid flavor, valid flavors ' 'are {valid}' . format ( invalid = flavor , valid = valid_parsers ) ) if flavor in ( 'bs4' , 'html5lib' ) : if not _HAS_HTML5LIB : raise ImportError ( \"html5lib not found, please install it\" ) if not _HAS_BS4 : raise ImportError ( \"BeautifulSoup4 (bs4) not found, please install it\" ) import bs4 if LooseVersion ( bs4 . __version__ ) <= LooseVersion ( '4.2.0' ) : raise ValueError ( \"A minimum version of BeautifulSoup 4.2.1 \" \"is required\" ) else : if not _HAS_LXML : raise ImportError ( \"lxml not found, please install it\" ) return _valid_parsers [ flavor ]\n\n\n        Summary:", "input": "", "output": "Choose the parser based on the input flavor ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_html ( io , match = '.+' , flavor = None , header = None , index_col = None , skiprows = None , attrs = None , parse_dates = False , tupleize_cols = None , thousands = ',' , encoding = None , decimal = '.' , converters = None , na_values = None , keep_default_na = True , displayed_only = True ) : _importers ( ) # Type check here. We don't want to parse only to fail because of an # invalid value of an integer skiprows. if isinstance ( skiprows , numbers . Integral ) and skiprows < 0 : raise ValueError ( 'cannot skip rows starting from the end of the ' 'data (you passed a negative value)' ) _validate_header_arg ( header ) return _parse ( flavor = flavor , io = io , match = match , header = header , index_col = index_col , skiprows = skiprows , parse_dates = parse_dates , tupleize_cols = tupleize_cols , thousands = thousands , attrs = attrs , encoding = encoding , decimal = decimal , converters = converters , na_values = na_values , keep_default_na = keep_default_na , displayed_only = displayed_only )\n\n\n        Summary:", "input": "", "output": "r Read HTML tables into a list of DataFrame objects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def parse_tables ( self ) : tables = self . _parse_tables ( self . _build_doc ( ) , self . match , self . attrs ) return ( self . _parse_thead_tbody_tfoot ( table ) for table in tables )\n\n\n        Summary:", "input": "", "output": "Parse and return all tables from the DOM ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _parse_thead_tbody_tfoot ( self , table_html ) : header_rows = self . _parse_thead_tr ( table_html ) body_rows = self . _parse_tbody_tr ( table_html ) footer_rows = self . _parse_tfoot_tr ( table_html ) def row_is_all_th ( row ) : return all ( self . _equals_tag ( t , 'th' ) for t in self . _parse_td ( row ) ) if not header_rows : # The table has no <thead>. Move the top all-<th> rows from # body_rows to header_rows. (This is a common case because many # tables in the wild have no <thead> or <tfoot> while body_rows and row_is_all_th ( body_rows [ 0 ] ) : header_rows . append ( body_rows . pop ( 0 ) ) header = self . _expand_colspan_rowspan ( header_rows ) body = self . _expand_colspan_rowspan ( body_rows ) footer = self . _expand_colspan_rowspan ( footer_rows ) return header , body , footer\n\n\n        Summary:", "input": "", "output": "Given a table return parsed header body and foot ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _expand_colspan_rowspan ( self , rows ) : all_texts = [ ] # list of rows, each a list of str remainder = [ ] # list of (index, text, nrows) for tr in rows : texts = [ ] # the output for this row next_remainder = [ ] index = 0 tds = self . _parse_td ( tr ) for td in tds : # Append texts from previous rows with rowspan>1 that come # before this <td> while remainder and remainder [ 0 ] [ 0 ] <= index : prev_i , prev_text , prev_rowspan = remainder . pop ( 0 ) texts . append ( prev_text ) if prev_rowspan > 1 : next_remainder . append ( ( prev_i , prev_text , prev_rowspan - 1 ) ) index += 1 # Append the text from this <td>, colspan times text = _remove_whitespace ( self . _text_getter ( td ) ) rowspan = int ( self . _attr_getter ( td , 'rowspan' ) or 1 ) colspan = int ( self . _attr_getter ( td , 'colspan' ) or 1 ) for _ in range ( colspan ) : texts . append ( text ) if rowspan > 1 : next_remainder . append ( ( index , text , rowspan - 1 ) ) index += 1 # Append texts from previous rows at the final position for prev_i , prev_text , prev_rowspan in remainder : texts . append ( prev_text ) if prev_rowspan > 1 : next_remainder . append ( ( prev_i , prev_text , prev_rowspan - 1 ) ) all_texts . append ( texts ) remainder = next_remainder # Append rows that only appear because the previous row had non-1 # rowspan while remainder : next_remainder = [ ] texts = [ ] for prev_i , prev_text , prev_rowspan in remainder : texts . append ( prev_text ) if prev_rowspan > 1 : next_remainder . append ( ( prev_i , prev_text , prev_rowspan - 1 ) ) all_texts . append ( texts ) remainder = next_remainder return all_texts\n\n\n        Summary:", "input": "", "output": "Given a list of <tr > s return a list of text rows ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _handle_hidden_tables ( self , tbl_list , attr_name ) : if not self . displayed_only : return tbl_list return [ x for x in tbl_list if \"display:none\" not in getattr ( x , attr_name ) . get ( 'style' , '' ) . replace ( \" \" , \"\" ) ]\n\n\n        Summary:", "input": "", "output": "Return list of tables potentially removing hidden elements"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _build_doc ( self ) : from lxml . html import parse , fromstring , HTMLParser from lxml . etree import XMLSyntaxError parser = HTMLParser ( recover = True , encoding = self . encoding ) try : if _is_url ( self . io ) : with urlopen ( self . io ) as f : r = parse ( f , parser = parser ) else : # try to parse the input in the simplest way r = parse ( self . io , parser = parser ) try : r = r . getroot ( ) except AttributeError : pass except ( UnicodeDecodeError , IOError ) as e : # if the input is a blob of html goop if not _is_url ( self . io ) : r = fromstring ( self . io , parser = parser ) try : r = r . getroot ( ) except AttributeError : pass else : raise e else : if not hasattr ( r , 'text_content' ) : raise XMLSyntaxError ( \"no text parsed from document\" , 0 , 0 , 0 ) return r\n\n\n        Summary:", "input": "", "output": "Raises ------ ValueError * If a URL that lxml cannot parse is passed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_dtype_kinds ( l ) : typs = set ( ) for arr in l : dtype = arr . dtype if is_categorical_dtype ( dtype ) : typ = 'category' elif is_sparse ( arr ) : typ = 'sparse' elif isinstance ( arr , ABCRangeIndex ) : typ = 'range' elif is_datetime64tz_dtype ( arr ) : # if to_concat contains different tz, # the result must be object dtype typ = str ( arr . dtype ) elif is_datetime64_dtype ( dtype ) : typ = 'datetime' elif is_timedelta64_dtype ( dtype ) : typ = 'timedelta' elif is_object_dtype ( dtype ) : typ = 'object' elif is_bool_dtype ( dtype ) : typ = 'bool' elif is_extension_array_dtype ( dtype ) : typ = str ( arr . dtype ) else : typ = dtype . kind typs . add ( typ ) return typs\n\n\n        Summary:", "input": "", "output": "Parameters ---------- l : list of arrays"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_series_result_type ( result , objs = None ) : from pandas import SparseSeries , SparseDataFrame , DataFrame # concat Series with axis 1 if isinstance ( result , dict ) : # concat Series with axis 1 if all ( isinstance ( c , ( SparseSeries , SparseDataFrame ) ) for c in result . values ( ) ) : return SparseDataFrame else : return DataFrame # otherwise it is a SingleBlockManager (axis = 0) if result . _block . is_sparse : return SparseSeries else : return objs [ 0 ] . _constructor\n\n\n        Summary:", "input": "", "output": "return appropriate class of Series concat input is either dict or array - like"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_frame_result_type ( result , objs ) : if ( result . blocks and ( any ( isinstance ( obj , ABCSparseDataFrame ) for obj in objs ) ) ) : from pandas . core . sparse . api import SparseDataFrame return SparseDataFrame else : return next ( obj for obj in objs if not isinstance ( obj , ABCSparseDataFrame ) )\n\n\n        Summary:", "input": "", "output": "return appropriate class of DataFrame - like concat if all blocks are sparse return SparseDataFrame otherwise return 1st obj"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_compat ( to_concat , axis = 0 ) : # filter empty arrays # 1-d dtypes always are included here def is_nonempty ( x ) : try : return x . shape [ axis ] > 0 except Exception : return True # If all arrays are empty, there's nothing to convert, just short-cut to # the concatenation, #3121. # # Creating an empty array directly is tempting, but the winnings would be # marginal given that it would still require shape & dtype calculation and # np.concatenate which has them both implemented is compiled. typs = get_dtype_kinds ( to_concat ) _contains_datetime = any ( typ . startswith ( 'datetime' ) for typ in typs ) _contains_period = any ( typ . startswith ( 'period' ) for typ in typs ) if 'category' in typs : # this must be priort to _concat_datetime, # to support Categorical + datetime-like return _concat_categorical ( to_concat , axis = axis ) elif _contains_datetime or 'timedelta' in typs or _contains_period : return _concat_datetime ( to_concat , axis = axis , typs = typs ) # these are mandated to handle empties as well elif 'sparse' in typs : return _concat_sparse ( to_concat , axis = axis , typs = typs ) all_empty = all ( not is_nonempty ( x ) for x in to_concat ) if any ( is_extension_array_dtype ( x ) for x in to_concat ) and axis == 1 : to_concat = [ np . atleast_2d ( x . astype ( 'object' ) ) for x in to_concat ] if all_empty : # we have all empties, but may need to coerce the result dtype to # object if we have non-numeric type operands (numpy would otherwise # cast this to float) typs = get_dtype_kinds ( to_concat ) if len ( typs ) != 1 : if ( not len ( typs - { 'i' , 'u' , 'f' } ) or not len ( typs - { 'bool' , 'i' , 'u' } ) ) : # let numpy coerce pass else : # coerce to object to_concat = [ x . astype ( 'object' ) for x in to_concat ] return np . concatenate ( to_concat , axis = axis )\n\n\n        Summary:", "input": "", "output": "provide concatenation of an array of arrays each of which is a single normalized dtypes ( in that for example if it s object then it is a non - datetimelike and provide a combined dtype for the resulting array that preserves the overall dtype if possible )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_categorical ( to_concat , axis = 0 ) : # we could have object blocks and categoricals here # if we only have a single categoricals then combine everything # else its a non-compat categorical categoricals = [ x for x in to_concat if is_categorical_dtype ( x . dtype ) ] # validate the categories if len ( categoricals ) != len ( to_concat ) : pass else : # when all categories are identical first = to_concat [ 0 ] if all ( first . is_dtype_equal ( other ) for other in to_concat [ 1 : ] ) : return union_categoricals ( categoricals ) # extract the categoricals & coerce to object if needed to_concat = [ x . get_values ( ) if is_categorical_dtype ( x . dtype ) else np . asarray ( x ) . ravel ( ) if not is_datetime64tz_dtype ( x ) else np . asarray ( x . astype ( object ) ) for x in to_concat ] result = _concat_compat ( to_concat ) if axis == 1 : result = result . reshape ( 1 , len ( result ) ) return result\n\n\n        Summary:", "input": "", "output": "Concatenate an object / categorical array of arrays each of which is a single dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union_categoricals ( to_union , sort_categories = False , ignore_order = False ) : from pandas import Index , Categorical , CategoricalIndex , Series from pandas . core . arrays . categorical import _recode_for_categories if len ( to_union ) == 0 : raise ValueError ( 'No Categoricals to union' ) def _maybe_unwrap ( x ) : if isinstance ( x , ( CategoricalIndex , Series ) ) : return x . values elif isinstance ( x , Categorical ) : return x else : raise TypeError ( \"all components to combine must be Categorical\" ) to_union = [ _maybe_unwrap ( x ) for x in to_union ] first = to_union [ 0 ] if not all ( is_dtype_equal ( other . categories . dtype , first . categories . dtype ) for other in to_union [ 1 : ] ) : raise TypeError ( \"dtype of categories must be the same\" ) ordered = False if all ( first . is_dtype_equal ( other ) for other in to_union [ 1 : ] ) : # identical categories - fastpath categories = first . categories ordered = first . ordered if all ( first . categories . equals ( other . categories ) for other in to_union [ 1 : ] ) : new_codes = np . concatenate ( [ c . codes for c in to_union ] ) else : codes = [ first . codes ] + [ _recode_for_categories ( other . codes , other . categories , first . categories ) for other in to_union [ 1 : ] ] new_codes = np . concatenate ( codes ) if sort_categories and not ignore_order and ordered : raise TypeError ( \"Cannot use sort_categories=True with \" \"ordered Categoricals\" ) if sort_categories and not categories . is_monotonic_increasing : categories = categories . sort_values ( ) indexer = categories . get_indexer ( first . categories ) from pandas . core . algorithms import take_1d new_codes = take_1d ( indexer , new_codes , fill_value = - 1 ) elif ignore_order or all ( not c . ordered for c in to_union ) : # different categories - union and recode cats = first . categories . append ( [ c . categories for c in to_union [ 1 : ] ] ) categories = Index ( cats . unique ( ) ) if sort_categories : categories = categories . sort_values ( ) new_codes = [ _recode_for_categories ( c . codes , c . categories , categories ) for c in to_union ] new_codes = np . concatenate ( new_codes ) else : # ordered - to show a proper error message if all ( c . ordered for c in to_union ) : msg = ( \"to union ordered Categoricals, \" \"all categories must be the same\" ) raise TypeError ( msg ) else : raise TypeError ( 'Categorical.ordered must be the same' ) if ignore_order : ordered = False return Categorical ( new_codes , categories = categories , ordered = ordered , fastpath = True )\n\n\n        Summary:", "input": "", "output": "Combine list - like of Categorical - like unioning categories . All categories must have the same dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_datetime ( to_concat , axis = 0 , typs = None ) : if typs is None : typs = get_dtype_kinds ( to_concat ) # multiple types, need to coerce to object if len ( typs ) != 1 : return _concatenate_2d ( [ _convert_datetimelike_to_object ( x ) for x in to_concat ] , axis = axis ) # must be single dtype if any ( typ . startswith ( 'datetime' ) for typ in typs ) : if 'datetime' in typs : to_concat = [ x . astype ( np . int64 , copy = False ) for x in to_concat ] return _concatenate_2d ( to_concat , axis = axis ) . view ( _NS_DTYPE ) else : # when to_concat has different tz, len(typs) > 1. # thus no need to care return _concat_datetimetz ( to_concat ) elif 'timedelta' in typs : return _concatenate_2d ( [ x . view ( np . int64 ) for x in to_concat ] , axis = axis ) . view ( _TD_DTYPE ) elif any ( typ . startswith ( 'period' ) for typ in typs ) : assert len ( typs ) == 1 cls = to_concat [ 0 ] new_values = cls . _concat_same_type ( to_concat ) return new_values\n\n\n        Summary:", "input": "", "output": "provide concatenation of an datetimelike array of arrays each of which is a single M8 [ ns ] datetimet64 [ ns tz ] or m8 [ ns ] dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_datetimetz ( to_concat , name = None ) : # Right now, internals will pass a List[DatetimeArray] here # for reductions like quantile. I would like to disentangle # all this before we get here. sample = to_concat [ 0 ] if isinstance ( sample , ABCIndexClass ) : return sample . _concat_same_dtype ( to_concat , name = name ) elif isinstance ( sample , ABCDatetimeArray ) : return sample . _concat_same_type ( to_concat )\n\n\n        Summary:", "input": "", "output": "concat DatetimeIndex with the same tz all inputs must be DatetimeIndex it is used in DatetimeIndex . append also"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_index_asobject ( to_concat , name = None ) : from pandas import Index from pandas . core . arrays import ExtensionArray klasses = ( ABCDatetimeIndex , ABCTimedeltaIndex , ABCPeriodIndex , ExtensionArray ) to_concat = [ x . astype ( object ) if isinstance ( x , klasses ) else x for x in to_concat ] self = to_concat [ 0 ] attribs = self . _get_attributes_dict ( ) attribs [ 'name' ] = name to_concat = [ x . _values if isinstance ( x , Index ) else x for x in to_concat ] return self . _shallow_copy_with_infer ( np . concatenate ( to_concat ) , * * attribs )\n\n\n        Summary:", "input": "", "output": "concat all inputs as object . DatetimeIndex TimedeltaIndex and PeriodIndex are converted to object dtype before concatenation"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_sparse ( to_concat , axis = 0 , typs = None ) : from pandas . core . arrays import SparseArray fill_values = [ x . fill_value for x in to_concat if isinstance ( x , SparseArray ) ] fill_value = fill_values [ 0 ] # TODO: Fix join unit generation so we aren't passed this. to_concat = [ x if isinstance ( x , SparseArray ) else SparseArray ( x . squeeze ( ) , fill_value = fill_value ) for x in to_concat ] return SparseArray . _concat_same_type ( to_concat )\n\n\n        Summary:", "input": "", "output": "provide concatenation of an sparse / dense array of arrays each of which is a single dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_rangeindex_same_dtype ( indexes ) : from pandas import Int64Index , RangeIndex start = step = next = None # Filter the empty indexes non_empty_indexes = [ obj for obj in indexes if len ( obj ) ] for obj in non_empty_indexes : if start is None : # This is set by the first non-empty index start = obj . _start if step is None and len ( obj ) > 1 : step = obj . _step elif step is None : # First non-empty index had only one element if obj . _start == start : return _concat_index_same_dtype ( indexes , klass = Int64Index ) step = obj . _start - start non_consecutive = ( ( step != obj . _step and len ( obj ) > 1 ) or ( next is not None and obj . _start != next ) ) if non_consecutive : return _concat_index_same_dtype ( indexes , klass = Int64Index ) if step is not None : next = obj [ - 1 ] + step if non_empty_indexes : # Get the stop value from \"next\" or alternatively # from the last non-empty index stop = non_empty_indexes [ - 1 ] . _stop if next is None else next return RangeIndex ( start , stop , step ) # Here all \"indexes\" had 0 length, i.e. were empty. # In this case return an empty range index. return RangeIndex ( 0 , 0 )\n\n\n        Summary:", "input": "", "output": "Concatenates multiple RangeIndex instances . All members of indexes must be of type RangeIndex ; result will be RangeIndex if possible Int64Index otherwise . E . g . : indexes = [ RangeIndex ( 3 ) RangeIndex ( 3 6 ) ] - > RangeIndex ( 6 ) indexes = [ RangeIndex ( 3 ) RangeIndex ( 4 6 ) ] - > Int64Index ( [ 0 1 2 4 5 ] )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rewrite_exception ( old_name , new_name ) : try : yield except Exception as e : msg = e . args [ 0 ] msg = msg . replace ( old_name , new_name ) args = ( msg , ) if len ( e . args ) > 1 : args = args + e . args [ 1 : ] e . args = args raise\n\n\n        Summary:", "input": "", "output": "Rewrite the message of an exception ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_level_lengths ( index , hidden_elements = None ) : sentinel = object ( ) levels = index . format ( sparsify = sentinel , adjoin = False , names = False ) if hidden_elements is None : hidden_elements = [ ] lengths = { } if index . nlevels == 1 : for i , value in enumerate ( levels ) : if ( i not in hidden_elements ) : lengths [ ( 0 , i ) ] = 1 return lengths for i , lvl in enumerate ( levels ) : for j , row in enumerate ( lvl ) : if not get_option ( 'display.multi_sparse' ) : lengths [ ( i , j ) ] = 1 elif ( row != sentinel ) and ( j not in hidden_elements ) : last_label = j lengths [ ( i , last_label ) ] = 1 elif ( row != sentinel ) : # even if its hidden, keep track of it in case # length >1 and later elements are visible last_label = j lengths [ ( i , last_label ) ] = 0 elif ( j not in hidden_elements ) : lengths [ ( i , last_label ) ] += 1 non_zero_lengths = { element : length for element , length in lengths . items ( ) if length >= 1 } return non_zero_lengths\n\n\n        Summary:", "input": "", "output": "Given an index find the level length for each element ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _translate ( self ) : table_styles = self . table_styles or [ ] caption = self . caption ctx = self . ctx precision = self . precision hidden_index = self . hidden_index hidden_columns = self . hidden_columns uuid = self . uuid or str ( uuid1 ( ) ) . replace ( \"-\" , \"_\" ) ROW_HEADING_CLASS = \"row_heading\" COL_HEADING_CLASS = \"col_heading\" INDEX_NAME_CLASS = \"index_name\" DATA_CLASS = \"data\" BLANK_CLASS = \"blank\" BLANK_VALUE = \"\" def format_attr ( pair ) : return \"{key}={value}\" . format ( * * pair ) # for sparsifying a MultiIndex idx_lengths = _get_level_lengths ( self . index ) col_lengths = _get_level_lengths ( self . columns , hidden_columns ) cell_context = dict ( ) n_rlvls = self . data . index . nlevels n_clvls = self . data . columns . nlevels rlabels = self . data . index . tolist ( ) clabels = self . data . columns . tolist ( ) if n_rlvls == 1 : rlabels = [ [ x ] for x in rlabels ] if n_clvls == 1 : clabels = [ [ x ] for x in clabels ] clabels = list ( zip ( * clabels ) ) cellstyle = [ ] head = [ ] for r in range ( n_clvls ) : # Blank for Index columns... row_es = [ { \"type\" : \"th\" , \"value\" : BLANK_VALUE , \"display_value\" : BLANK_VALUE , \"is_visible\" : not hidden_index , \"class\" : \" \" . join ( [ BLANK_CLASS ] ) } ] * ( n_rlvls - 1 ) # ... except maybe the last for columns.names name = self . data . columns . names [ r ] cs = [ BLANK_CLASS if name is None else INDEX_NAME_CLASS , \"level{lvl}\" . format ( lvl = r ) ] name = BLANK_VALUE if name is None else name row_es . append ( { \"type\" : \"th\" , \"value\" : name , \"display_value\" : name , \"class\" : \" \" . join ( cs ) , \"is_visible\" : not hidden_index } ) if clabels : for c , value in enumerate ( clabels [ r ] ) : cs = [ COL_HEADING_CLASS , \"level{lvl}\" . format ( lvl = r ) , \"col{col}\" . format ( col = c ) ] cs . extend ( cell_context . get ( \"col_headings\" , { } ) . get ( r , { } ) . get ( c , [ ] ) ) es = { \"type\" : \"th\" , \"value\" : value , \"display_value\" : value , \"class\" : \" \" . join ( cs ) , \"is_visible\" : _is_visible ( c , r , col_lengths ) , } colspan = col_lengths . get ( ( r , c ) , 0 ) if colspan > 1 : es [ \"attributes\" ] = [ format_attr ( { \"key\" : \"colspan\" , \"value\" : colspan } ) ] row_es . append ( es ) head . append ( row_es ) if ( self . data . index . names and com . _any_not_none ( * self . data . index . names ) and not hidden_index ) : index_header_row = [ ] for c , name in enumerate ( self . data . index . names ) : cs = [ INDEX_NAME_CLASS , \"level{lvl}\" . format ( lvl = c ) ] name = '' if name is None else name index_header_row . append ( { \"type\" : \"th\" , \"value\" : name , \"class\" : \" \" . join ( cs ) } ) index_header_row . extend ( [ { \"type\" : \"th\" , \"value\" : BLANK_VALUE , \"class\" : \" \" . join ( [ BLANK_CLASS ] ) } ] * ( len ( clabels [ 0 ] ) - len ( hidden_columns ) ) ) head . append ( index_header_row ) body = [ ] for r , idx in enumerate ( self . data . index ) : row_es = [ ] for c , value in enumerate ( rlabels [ r ] ) : rid = [ ROW_HEADING_CLASS , \"level{lvl}\" . format ( lvl = c ) , \"row{row}\" . format ( row = r ) ] es = { \"type\" : \"th\" , \"is_visible\" : ( _is_visible ( r , c , idx_lengths ) and not hidden_index ) , \"value\" : value , \"display_value\" : value , \"id\" : \"_\" . join ( rid [ 1 : ] ) , \"class\" : \" \" . join ( rid ) } rowspan = idx_lengths . get ( ( c , r ) , 0 ) if rowspan > 1 : es [ \"attributes\" ] = [ format_attr ( { \"key\" : \"rowspan\" , \"value\" : rowspan } ) ] row_es . append ( es ) for c , col in enumerate ( self . data . columns ) : cs = [ DATA_CLASS , \"row{row}\" . format ( row = r ) , \"col{col}\" . format ( col = c ) ] cs . extend ( cell_context . get ( \"data\" , { } ) . get ( r , { } ) . get ( c , [ ] ) ) formatter = self . _display_funcs [ ( r , c ) ] value = self . data . iloc [ r , c ] row_dict = { \"type\" : \"td\" , \"value\" : value , \"class\" : \" \" . join ( cs ) , \"display_value\" : formatter ( value ) , \"is_visible\" : ( c not in hidden_columns ) } # only add an id if the cell has a style if ( self . cell_ids or not ( len ( ctx [ r , c ] ) == 1 and ctx [ r , c ] [ 0 ] == '' ) ) : row_dict [ \"id\" ] = \"_\" . join ( cs [ 1 : ] ) row_es . append ( row_dict ) props = [ ] for x in ctx [ r , c ] : # have to handle empty styles like [''] if x . count ( \":\" ) : props . append ( x . split ( \":\" ) ) else : props . append ( [ '' , '' ] ) cellstyle . append ( { 'props' : props , 'selector' : \"row{row}_col{col}\" . format ( row = r , col = c ) } ) body . append ( row_es ) table_attr = self . table_attributes use_mathjax = get_option ( \"display.html.use_mathjax\" ) if not use_mathjax : table_attr = table_attr or '' if 'class=\"' in table_attr : table_attr = table_attr . replace ( 'class=\"' , 'class=\"tex2jax_ignore ' ) else : table_attr += ' class=\"tex2jax_ignore\"' return dict ( head = head , cellstyle = cellstyle , body = body , uuid = uuid , precision = precision , table_styles = table_styles , caption = caption , table_attributes = table_attr )\n\n\n        Summary:", "input": "", "output": "Convert the DataFrame in self . data and the attrs from _build_styles into a dictionary of { head body uuid cellstyle } ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def format ( self , formatter , subset = None ) : if subset is None : row_locs = range ( len ( self . data ) ) col_locs = range ( len ( self . data . columns ) ) else : subset = _non_reducing_slice ( subset ) if len ( subset ) == 1 : subset = subset , self . data . columns sub_df = self . data . loc [ subset ] row_locs = self . data . index . get_indexer_for ( sub_df . index ) col_locs = self . data . columns . get_indexer_for ( sub_df . columns ) if is_dict_like ( formatter ) : for col , col_formatter in formatter . items ( ) : # formatter must be callable, so '{}' are converted to lambdas col_formatter = _maybe_wrap_formatter ( col_formatter ) col_num = self . data . columns . get_indexer_for ( [ col ] ) [ 0 ] for row_num in row_locs : self . _display_funcs [ ( row_num , col_num ) ] = col_formatter else : # single scalar to format all cells with locs = product ( * ( row_locs , col_locs ) ) for i , j in locs : formatter = _maybe_wrap_formatter ( formatter ) self . _display_funcs [ ( i , j ) ] = formatter return self\n\n\n        Summary:", "input": "", "output": "Format the text display value of cells ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def render ( self , * * kwargs ) : self . _compute ( ) # TODO: namespace all the pandas keys d = self . _translate ( ) # filter out empty styles, every cell will have a class # but the list of props may just be [['', '']]. # so we have the neested anys below trimmed = [ x for x in d [ 'cellstyle' ] if any ( any ( y ) for y in x [ 'props' ] ) ] d [ 'cellstyle' ] = trimmed d . update ( kwargs ) return self . template . render ( * * d )\n\n\n        Summary:", "input": "", "output": "Render the built up styles to HTML ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _update_ctx ( self , attrs ) : for row_label , v in attrs . iterrows ( ) : for col_label , col in v . iteritems ( ) : i = self . index . get_indexer ( [ row_label ] ) [ 0 ] j = self . columns . get_indexer ( [ col_label ] ) [ 0 ] for pair in col . rstrip ( \";\" ) . split ( \";\" ) : self . ctx [ ( i , j ) ] . append ( pair )\n\n\n        Summary:", "input": "", "output": "Update the state of the Styler ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _compute ( self ) : r = self for func , args , kwargs in self . _todo : r = func ( self ) ( * args , * * kwargs ) return r\n\n\n        Summary:", "input": "", "output": "Execute the style functions built up in self . _todo ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , func , axis = 0 , subset = None , * * kwargs ) : self . _todo . append ( ( lambda instance : getattr ( instance , '_apply' ) , ( func , axis , subset ) , kwargs ) ) return self\n\n\n        Summary:", "input": "", "output": "Apply a function column - wise row - wise or table - wise updating the HTML representation with the result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def applymap ( self , func , subset = None , * * kwargs ) : self . _todo . append ( ( lambda instance : getattr ( instance , '_applymap' ) , ( func , subset ) , kwargs ) ) return self\n\n\n        Summary:", "input": "", "output": "Apply a function elementwise updating the HTML representation with the result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def where ( self , cond , value , other = None , subset = None , * * kwargs ) : if other is None : other = '' return self . applymap ( lambda val : value if cond ( val ) else other , subset = subset , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Apply a function elementwise updating the HTML representation with a style which is selected in accordance with the return value of a function ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def hide_columns ( self , subset ) : subset = _non_reducing_slice ( subset ) hidden_df = self . data . loc [ subset ] self . hidden_columns = self . columns . get_indexer_for ( hidden_df . columns ) return self\n\n\n        Summary:", "input": "", "output": "Hide columns from rendering ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def highlight_null ( self , null_color = 'red' ) : self . applymap ( self . _highlight_null , null_color = null_color ) return self\n\n\n        Summary:", "input": "", "output": "Shade the background null_color for missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def background_gradient ( self , cmap = 'PuBu' , low = 0 , high = 0 , axis = 0 , subset = None , text_color_threshold = 0.408 ) : subset = _maybe_numeric_slice ( self . data , subset ) subset = _non_reducing_slice ( subset ) self . apply ( self . _background_gradient , cmap = cmap , subset = subset , axis = axis , low = low , high = high , text_color_threshold = text_color_threshold ) return self\n\n\n        Summary:", "input": "", "output": "Color the background in a gradient according to the data in each column ( optionally row ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _background_gradient ( s , cmap = 'PuBu' , low = 0 , high = 0 , text_color_threshold = 0.408 ) : if ( not isinstance ( text_color_threshold , ( float , int ) ) or not 0 <= text_color_threshold <= 1 ) : msg = \"`text_color_threshold` must be a value from 0 to 1.\" raise ValueError ( msg ) with _mpl ( Styler . background_gradient ) as ( plt , colors ) : smin = s . values . min ( ) smax = s . values . max ( ) rng = smax - smin # extend lower / upper bounds, compresses color range norm = colors . Normalize ( smin - ( rng * low ) , smax + ( rng * high ) ) # matplotlib colors.Normalize modifies inplace? # https://github.com/matplotlib/matplotlib/issues/5427 rgbas = plt . cm . get_cmap ( cmap ) ( norm ( s . values ) ) def relative_luminance ( rgba ) : \"\"\" Calculate relative luminance of a color. The calculation adheres to the W3C standards (https://www.w3.org/WAI/GL/wiki/Relative_luminance) Parameters ---------- color : rgb or rgba tuple Returns ------- float The relative luminance as a value from 0 to 1 \"\"\" r , g , b = ( x / 12.92 if x <= 0.03928 else ( ( x + 0.055 ) / 1.055 ** 2.4 ) for x in rgba [ : 3 ] ) return 0.2126 * r + 0.7152 * g + 0.0722 * b def css ( rgba ) : dark = relative_luminance ( rgba ) < text_color_threshold text_color = '#f1f1f1' if dark else '#000000' return 'background-color: {b};color: {c};' . format ( b = colors . rgb2hex ( rgba ) , c = text_color ) if s . ndim == 1 : return [ css ( rgba ) for rgba in rgbas ] else : return pd . DataFrame ( [ [ css ( rgba ) for rgba in row ] for row in rgbas ] , index = s . index , columns = s . columns )\n\n\n        Summary:", "input": "", "output": "Color background in a range according to the data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_properties ( self , subset = None , * * kwargs ) : values = ';' . join ( '{p}: {v}' . format ( p = p , v = v ) for p , v in kwargs . items ( ) ) f = lambda x : values return self . applymap ( f , subset = subset )\n\n\n        Summary:", "input": "", "output": "Convenience method for setting one or more non - data dependent properties or each cell ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _bar ( s , align , colors , width = 100 , vmin = None , vmax = None ) : # Get input value range. smin = s . min ( ) if vmin is None else vmin if isinstance ( smin , ABCSeries ) : smin = smin . min ( ) smax = s . max ( ) if vmax is None else vmax if isinstance ( smax , ABCSeries ) : smax = smax . max ( ) if align == 'mid' : smin = min ( 0 , smin ) smax = max ( 0 , smax ) elif align == 'zero' : # For \"zero\" mode, we want the range to be symmetrical around zero. smax = max ( abs ( smin ) , abs ( smax ) ) smin = - smax # Transform to percent-range of linear-gradient normed = width * ( s . values - smin ) / ( smax - smin + 1e-12 ) zero = - width * smin / ( smax - smin + 1e-12 ) def css_bar ( start , end , color ) : \"\"\" Generate CSS code to draw a bar from start to end. \"\"\" css = 'width: 10em; height: 80%;' if end > start : css += 'background: linear-gradient(90deg,' if start > 0 : css += ' transparent {s:.1f}%, {c} {s:.1f}%, ' . format ( s = start , c = color ) css += '{c} {e:.1f}%, transparent {e:.1f}%)' . format ( e = min ( end , width ) , c = color , ) return css def css ( x ) : if pd . isna ( x ) : return '' # avoid deprecated indexing `colors[x > zero]` color = colors [ 1 ] if x > zero else colors [ 0 ] if align == 'left' : return css_bar ( 0 , x , color ) else : return css_bar ( min ( x , zero ) , max ( x , zero ) , color ) if s . ndim == 1 : return [ css ( x ) for x in normed ] else : return pd . DataFrame ( [ [ css ( x ) for x in row ] for row in normed ] , index = s . index , columns = s . columns )\n\n\n        Summary:", "input": "", "output": "Draw bar chart in dataframe cells ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def bar ( self , subset = None , axis = 0 , color = '#d65f5f' , width = 100 , align = 'left' , vmin = None , vmax = None ) : if align not in ( 'left' , 'zero' , 'mid' ) : raise ValueError ( \"`align` must be one of {'left', 'zero',' mid'}\" ) if not ( is_list_like ( color ) ) : color = [ color , color ] elif len ( color ) == 1 : color = [ color [ 0 ] , color [ 0 ] ] elif len ( color ) > 2 : raise ValueError ( \"`color` must be string or a list-like\" \" of length 2: [`color_neg`, `color_pos`]\" \" (eg: color=['#d65f5f', '#5fba7d'])\" ) subset = _maybe_numeric_slice ( self . data , subset ) subset = _non_reducing_slice ( subset ) self . apply ( self . _bar , subset = subset , axis = axis , align = align , colors = color , width = width , vmin = vmin , vmax = vmax ) return self\n\n\n        Summary:", "input": "", "output": "Draw bar chart in the cell backgrounds ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def highlight_max ( self , subset = None , color = 'yellow' , axis = 0 ) : return self . _highlight_handler ( subset = subset , color = color , axis = axis , max_ = True )\n\n\n        Summary:", "input": "", "output": "Highlight the maximum by shading the background ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def highlight_min ( self , subset = None , color = 'yellow' , axis = 0 ) : return self . _highlight_handler ( subset = subset , color = color , axis = axis , max_ = False )\n\n\n        Summary:", "input": "", "output": "Highlight the minimum by shading the background ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _highlight_extrema ( data , color = 'yellow' , max_ = True ) : attr = 'background-color: {0}' . format ( color ) if data . ndim == 1 : # Series from .apply if max_ : extrema = data == data . max ( ) else : extrema = data == data . min ( ) return [ attr if v else '' for v in extrema ] else : # DataFrame from .tee if max_ : extrema = data == data . max ( ) . max ( ) else : extrema = data == data . min ( ) . min ( ) return pd . DataFrame ( np . where ( extrema , attr , '' ) , index = data . index , columns = data . columns )\n\n\n        Summary:", "input": "", "output": "Highlight the min or max in a Series or DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_custom_template ( cls , searchpath , name ) : loader = ChoiceLoader ( [ FileSystemLoader ( searchpath ) , cls . loader , ] ) class MyStyler ( cls ) : env = Environment ( loader = loader ) template = env . get_template ( name ) return MyStyler\n\n\n        Summary:", "input": "", "output": "Factory function for creating a subclass of Styler with a custom template and Jinja environment ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def register ( self , dtype ) : if not issubclass ( dtype , ( PandasExtensionDtype , ExtensionDtype ) ) : raise ValueError ( \"can only register pandas extension dtypes\" ) self . dtypes . append ( dtype )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- dtype : ExtensionDtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def find ( self , dtype ) : if not isinstance ( dtype , str ) : dtype_type = dtype if not isinstance ( dtype , type ) : dtype_type = type ( dtype ) if issubclass ( dtype_type , ExtensionDtype ) : return dtype return None for dtype_type in self . dtypes : try : return dtype_type . construct_from_string ( dtype ) except TypeError : pass return None\n\n\n        Summary:", "input": "", "output": "Parameters ---------- dtype : PandasExtensionDtype or string"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def np_datetime64_compat ( s , * args , * * kwargs ) : s = tz_replacer ( s ) return np . datetime64 ( s , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "provide compat for construction of strings to numpy datetime64 s with tz - changes in 1 . 11 that make 2015 - 01 - 01 09 : 00 : 00Z show a deprecation warning when need to pass 2015 - 01 - 01 09 : 00 : 00"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def np_array_datetime64_compat ( arr , * args , * * kwargs ) : # is_list_like if ( hasattr ( arr , '__iter__' ) and not isinstance ( arr , ( str , bytes ) ) ) : arr = [ tz_replacer ( s ) for s in arr ] else : arr = tz_replacer ( arr ) return np . array ( arr , * args , * * kwargs )\n\n\n        Summary:", "input": "", "output": "provide compat for construction of an array of strings to a np . array ( ... dtype = np . datetime64 ( .. )) tz - changes in 1 . 11 that make 2015 - 01 - 01 09 : 00 : 00Z show a deprecation warning when need to pass 2015 - 01 - 01 09 : 00 : 00"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _assert_safe_casting ( cls , data , subarr ) : if not issubclass ( data . dtype . type , np . signedinteger ) : if not np . array_equal ( data , subarr ) : raise TypeError ( 'Unsafe NumPy casting, you must ' 'explicitly cast' )\n\n\n        Summary:", "input": "", "output": "Ensure incoming data can be represented as ints ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_value ( self , series , key ) : if not is_scalar ( key ) : raise InvalidIndexError k = com . values_from_object ( key ) loc = self . get_loc ( k ) new_values = com . values_from_object ( series ) [ loc ] return new_values\n\n\n        Summary:", "input": "", "output": "we always want to get an index value never a value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def equals ( self , other ) : if self is other : return True if not isinstance ( other , Index ) : return False # need to compare nans locations and make sure that they are the same # since nans don't compare equal this is a bit tricky try : if not isinstance ( other , Float64Index ) : other = self . _constructor ( other ) if ( not is_dtype_equal ( self . dtype , other . dtype ) or self . shape != other . shape ) : return False left , right = self . _ndarray_values , other . _ndarray_values return ( ( left == right ) | ( self . _isnan & other . _isnan ) ) . all ( ) except ( TypeError , ValueError ) : return False\n\n\n        Summary:", "input": "", "output": "Determines if two Index objects contain the same elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_decoded ( s ) : if isinstance ( s , np . bytes_ ) : s = s . decode ( 'UTF-8' ) return s\n\n\n        Summary:", "input": "", "output": "if we have bytes decode them to unicode"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_term ( where , scope_level ) : # only consider list/tuple here as an ndarray is automatically a coordinate # list level = scope_level + 1 if isinstance ( where , ( list , tuple ) ) : wlist = [ ] for w in filter ( lambda x : x is not None , where ) : if not maybe_expression ( w ) : wlist . append ( w ) else : wlist . append ( Term ( w , scope_level = level ) ) where = wlist elif maybe_expression ( where ) : where = Term ( where , scope_level = level ) return where\n\n\n        Summary:", "input": "", "output": "ensure that the where is a Term or a list of Term this makes sure that we are capturing the scope of variables that are passed create the terms here with a frame_level = 2 ( we are 2 levels down )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_hdf ( path_or_buf , key , value , mode = None , complevel = None , complib = None , append = None , * * kwargs ) : if append : f = lambda store : store . append ( key , value , * * kwargs ) else : f = lambda store : store . put ( key , value , * * kwargs ) path_or_buf = _stringify_path ( path_or_buf ) if isinstance ( path_or_buf , str ) : with HDFStore ( path_or_buf , mode = mode , complevel = complevel , complib = complib ) as store : f ( store ) else : f ( path_or_buf )\n\n\n        Summary:", "input": "", "output": "store this object close it if we opened it"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_hdf ( path_or_buf , key = None , mode = 'r' , * * kwargs ) : if mode not in [ 'r' , 'r+' , 'a' ] : raise ValueError ( 'mode {0} is not allowed while performing a read. ' 'Allowed modes are r, r+ and a.' . format ( mode ) ) # grab the scope if 'where' in kwargs : kwargs [ 'where' ] = _ensure_term ( kwargs [ 'where' ] , scope_level = 1 ) if isinstance ( path_or_buf , HDFStore ) : if not path_or_buf . is_open : raise IOError ( 'The HDFStore must be open for reading.' ) store = path_or_buf auto_close = False else : path_or_buf = _stringify_path ( path_or_buf ) if not isinstance ( path_or_buf , str ) : raise NotImplementedError ( 'Support for generic buffers has not ' 'been implemented.' ) try : exists = os . path . exists ( path_or_buf ) # if filepath is too long except ( TypeError , ValueError ) : exists = False if not exists : raise FileNotFoundError ( 'File {path} does not exist' . format ( path = path_or_buf ) ) store = HDFStore ( path_or_buf , mode = mode , * * kwargs ) # can't auto open/close if we are using an iterator # so delegate to the iterator auto_close = True try : if key is None : groups = store . groups ( ) if len ( groups ) == 0 : raise ValueError ( 'No dataset in HDF5 file.' ) candidate_only_group = groups [ 0 ] # For the HDF file to have only one dataset, all other groups # should then be metadata groups for that candidate group. (This # assumes that the groups() method enumerates parent groups # before their children.) for group_to_check in groups [ 1 : ] : if not _is_metadata_of ( group_to_check , candidate_only_group ) : raise ValueError ( 'key must be provided when HDF5 file ' 'contains multiple datasets.' ) key = candidate_only_group . _v_pathname return store . select ( key , auto_close = auto_close , * * kwargs ) except ( ValueError , TypeError , KeyError ) : # if there is an error, close the store try : store . close ( ) except AttributeError : pass raise\n\n\n        Summary:", "input": "", "output": "Read from the store close it if we opened it ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_metadata_of ( group , parent_group ) : if group . _v_depth <= parent_group . _v_depth : return False current = group while current . _v_depth > 1 : parent = current . _v_parent if parent == parent_group and current . _v_name == 'meta' : return True current = current . _v_parent return False\n\n\n        Summary:", "input": "", "output": "Check if a given group is a metadata group for a given parent_group ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_info ( info , name ) : try : idx = info [ name ] except KeyError : idx = info [ name ] = dict ( ) return idx\n\n\n        Summary:", "input": "", "output": "get / create the info for this name"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_tz ( tz ) : zone = timezones . get_timezone ( tz ) if zone is None : zone = tz . utcoffset ( ) . total_seconds ( ) return zone\n\n\n        Summary:", "input": "", "output": "for a tz - aware type return an encoded zone"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_tz ( values , tz , preserve_UTC = False , coerce = False ) : if tz is not None : name = getattr ( values , 'name' , None ) values = values . ravel ( ) tz = timezones . get_timezone ( _ensure_decoded ( tz ) ) values = DatetimeIndex ( values , name = name ) if values . tz is None : values = values . tz_localize ( 'UTC' ) . tz_convert ( tz ) if preserve_UTC : if tz == 'UTC' : values = list ( values ) elif coerce : values = np . asarray ( values , dtype = 'M8[ns]' ) return values\n\n\n        Summary:", "input": "", "output": "coerce the values to a DatetimeIndex if tz is set preserve the input shape if possible"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_string_array ( data , encoding , errors , itemsize = None ) : # encode if needed if encoding is not None and len ( data ) : data = Series ( data . ravel ( ) ) . str . encode ( encoding , errors ) . values . reshape ( data . shape ) # create the sized dtype if itemsize is None : ensured = ensure_object ( data . ravel ( ) ) itemsize = max ( 1 , libwriters . max_len_string_array ( ensured ) ) data = np . asarray ( data , dtype = \"S{size}\" . format ( size = itemsize ) ) return data\n\n\n        Summary:", "input": "", "output": "we take a string - like that is object dtype and coerce to a fixed size string type"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _unconvert_string_array ( data , nan_rep = None , encoding = None , errors = 'strict' ) : shape = data . shape data = np . asarray ( data . ravel ( ) , dtype = object ) # guard against a None encoding (because of a legacy # where the passed encoding is actually None) encoding = _ensure_encoding ( encoding ) if encoding is not None and len ( data ) : itemsize = libwriters . max_len_string_array ( ensure_object ( data ) ) dtype = \"U{0}\" . format ( itemsize ) if isinstance ( data [ 0 ] , bytes ) : data = Series ( data ) . str . decode ( encoding , errors = errors ) . values else : data = data . astype ( dtype , copy = False ) . astype ( object , copy = False ) if nan_rep is None : nan_rep = 'nan' data = libwriters . string_array_replace_from_nan_rep ( data , nan_rep ) return data . reshape ( shape )\n\n\n        Summary:", "input": "", "output": "inverse of _convert_string_array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def open ( self , mode = 'a' , * * kwargs ) : tables = _tables ( ) if self . _mode != mode : # if we are changing a write mode to read, ok if self . _mode in [ 'a' , 'w' ] and mode in [ 'r' , 'r+' ] : pass elif mode in [ 'w' ] : # this would truncate, raise here if self . is_open : raise PossibleDataLossError ( \"Re-opening the file [{0}] with mode [{1}] \" \"will delete the current file!\" . format ( self . _path , self . _mode ) ) self . _mode = mode # close and reopen the handle if self . is_open : self . close ( ) if self . _complevel and self . _complevel > 0 : self . _filters = _tables ( ) . Filters ( self . _complevel , self . _complib , fletcher32 = self . _fletcher32 ) try : self . _handle = tables . open_file ( self . _path , self . _mode , * * kwargs ) except ( IOError ) as e : # pragma: no cover if 'can not be written' in str ( e ) : print ( 'Opening {path} in read-only mode' . format ( path = self . _path ) ) self . _handle = tables . open_file ( self . _path , 'r' , * * kwargs ) else : raise except ( ValueError ) as e : # trap PyTables >= 3.1 FILE_OPEN_POLICY exception # to provide an updated message if 'FILE_OPEN_POLICY' in str ( e ) : e = ValueError ( \"PyTables [{version}] no longer supports opening multiple \" \"files\\n\" \"even in read-only mode on this HDF5 version \" \"[{hdf_version}]. You can accept this\\n\" \"and not open the same file multiple times at once,\\n\" \"upgrade the HDF5 version, or downgrade to PyTables 3.0.0 \" \"which allows\\n\" \"files to be opened multiple times at once\\n\" . format ( version = tables . __version__ , hdf_version = tables . get_hdf5_version ( ) ) ) raise e except ( Exception ) as e : # trying to read from a non-existent file causes an error which # is not part of IOError, make it one if self . _mode == 'r' and 'Unable to open/create file' in str ( e ) : raise IOError ( str ( e ) ) raise\n\n\n        Summary:", "input": "", "output": "Open the file in the specified mode"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def flush ( self , fsync = False ) : if self . _handle is not None : self . _handle . flush ( ) if fsync : try : os . fsync ( self . _handle . fileno ( ) ) except OSError : pass\n\n\n        Summary:", "input": "", "output": "Force all buffered modifications to be written to disk ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( self , key ) : group = self . get_node ( key ) if group is None : raise KeyError ( 'No object named {key} in the file' . format ( key = key ) ) return self . _read_group ( group )\n\n\n        Summary:", "input": "", "output": "Retrieve pandas object stored in file"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select ( self , key , where = None , start = None , stop = None , columns = None , iterator = False , chunksize = None , auto_close = False , * * kwargs ) : group = self . get_node ( key ) if group is None : raise KeyError ( 'No object named {key} in the file' . format ( key = key ) ) # create the storer and axes where = _ensure_term ( where , scope_level = 1 ) s = self . _create_storer ( group ) s . infer_axes ( ) # function to call on iteration def func ( _start , _stop , _where ) : return s . read ( start = _start , stop = _stop , where = _where , columns = columns ) # create the iterator it = TableIterator ( self , s , func , where = where , nrows = s . nrows , start = start , stop = stop , iterator = iterator , chunksize = chunksize , auto_close = auto_close ) return it . get_result ( )\n\n\n        Summary:", "input": "", "output": "Retrieve pandas object stored in file optionally based on where criteria"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select_as_coordinates ( self , key , where = None , start = None , stop = None , * * kwargs ) : where = _ensure_term ( where , scope_level = 1 ) return self . get_storer ( key ) . read_coordinates ( where = where , start = start , stop = stop , * * kwargs )\n\n\n        Summary:", "input": "", "output": "return the selection as an Index"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select_column ( self , key , column , * * kwargs ) : return self . get_storer ( key ) . read_column ( column = column , * * kwargs )\n\n\n        Summary:", "input": "", "output": "return a single column from the table . This is generally only useful to select an indexable"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select_as_multiple ( self , keys , where = None , selector = None , columns = None , start = None , stop = None , iterator = False , chunksize = None , auto_close = False , * * kwargs ) : # default to single select where = _ensure_term ( where , scope_level = 1 ) if isinstance ( keys , ( list , tuple ) ) and len ( keys ) == 1 : keys = keys [ 0 ] if isinstance ( keys , str ) : return self . select ( key = keys , where = where , columns = columns , start = start , stop = stop , iterator = iterator , chunksize = chunksize , * * kwargs ) if not isinstance ( keys , ( list , tuple ) ) : raise TypeError ( \"keys must be a list/tuple\" ) if not len ( keys ) : raise ValueError ( \"keys must have a non-zero length\" ) if selector is None : selector = keys [ 0 ] # collect the tables tbls = [ self . get_storer ( k ) for k in keys ] s = self . get_storer ( selector ) # validate rows nrows = None for t , k in itertools . chain ( [ ( s , selector ) ] , zip ( tbls , keys ) ) : if t is None : raise KeyError ( \"Invalid table [{key}]\" . format ( key = k ) ) if not t . is_table : raise TypeError ( \"object [{obj}] is not a table, and cannot be used in all \" \"select as multiple\" . format ( obj = t . pathname ) ) if nrows is None : nrows = t . nrows elif t . nrows != nrows : raise ValueError ( \"all tables must have exactly the same nrows!\" ) # axis is the concentation axes axis = list ( { t . non_index_axes [ 0 ] [ 0 ] for t in tbls } ) [ 0 ] def func ( _start , _stop , _where ) : # retrieve the objs, _where is always passed as a set of # coordinates here objs = [ t . read ( where = _where , columns = columns , start = _start , stop = _stop , * * kwargs ) for t in tbls ] # concat and return return concat ( objs , axis = axis , verify_integrity = False ) . _consolidate ( ) # create the iterator it = TableIterator ( self , s , func , where = where , nrows = nrows , start = start , stop = stop , iterator = iterator , chunksize = chunksize , auto_close = auto_close ) return it . get_result ( coordinates = True )\n\n\n        Summary:", "input": "", "output": "Retrieve pandas objects from multiple tables"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def put ( self , key , value , format = None , append = False , * * kwargs ) : if format is None : format = get_option ( \"io.hdf.default_format\" ) or 'fixed' kwargs = self . _validate_format ( format , kwargs ) self . _write_to_group ( key , value , append = append , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Store object in HDFStore"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def remove ( self , key , where = None , start = None , stop = None ) : where = _ensure_term ( where , scope_level = 1 ) try : s = self . get_storer ( key ) except KeyError : # the key is not a valid store, re-raising KeyError raise except Exception : if where is not None : raise ValueError ( \"trying to remove a node with a non-None where clause!\" ) # we are actually trying to remove a node (with children) s = self . get_node ( key ) if s is not None : s . _f_remove ( recursive = True ) return None # remove the node if com . _all_none ( where , start , stop ) : s . group . _f_remove ( recursive = True ) # delete from the table else : if not s . is_table : raise ValueError ( 'can only remove with where on objects written as tables' ) return s . delete ( where = where , start = start , stop = stop )\n\n\n        Summary:", "input": "", "output": "Remove pandas object partially by specifying the where condition"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def append ( self , key , value , format = None , append = True , columns = None , dropna = None , * * kwargs ) : if columns is not None : raise TypeError ( \"columns is not a supported keyword in append, \" \"try data_columns\" ) if dropna is None : dropna = get_option ( \"io.hdf.dropna_table\" ) if format is None : format = get_option ( \"io.hdf.default_format\" ) or 'table' kwargs = self . _validate_format ( format , kwargs ) self . _write_to_group ( key , value , append = append , dropna = dropna , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Append to Table in file . Node must already exist and be Table format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def append_to_multiple ( self , d , value , selector , data_columns = None , axes = None , dropna = False , * * kwargs ) : if axes is not None : raise TypeError ( \"axes is currently not accepted as a parameter to\" \" append_to_multiple; you can create the \" \"tables independently instead\" ) if not isinstance ( d , dict ) : raise ValueError ( \"append_to_multiple must have a dictionary specified as the \" \"way to split the value\" ) if selector not in d : raise ValueError ( \"append_to_multiple requires a selector that is in passed dict\" ) # figure out the splitting axis (the non_index_axis) axis = list ( set ( range ( value . ndim ) ) - set ( _AXES_MAP [ type ( value ) ] ) ) [ 0 ] # figure out how to split the value remain_key = None remain_values = [ ] for k , v in d . items ( ) : if v is None : if remain_key is not None : raise ValueError ( \"append_to_multiple can only have one value in d that \" \"is None\" ) remain_key = k else : remain_values . extend ( v ) if remain_key is not None : ordered = value . axes [ axis ] ordd = ordered . difference ( Index ( remain_values ) ) ordd = sorted ( ordered . get_indexer ( ordd ) ) d [ remain_key ] = ordered . take ( ordd ) # data_columns if data_columns is None : data_columns = d [ selector ] # ensure rows are synchronized across the tables if dropna : idxs = ( value [ cols ] . dropna ( how = 'all' ) . index for cols in d . values ( ) ) valid_index = next ( idxs ) for index in idxs : valid_index = valid_index . intersection ( index ) value = value . loc [ valid_index ] # append for k , v in d . items ( ) : dc = data_columns if k == selector else None # compute the val val = value . reindex ( v , axis = axis ) self . append ( k , val , data_columns = dc , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Append to multiple tables"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_table_index ( self , key , * * kwargs ) : # version requirements _tables ( ) s = self . get_storer ( key ) if s is None : return if not s . is_table : raise TypeError ( \"cannot create table index on a Fixed format store\" ) s . create_index ( * * kwargs )\n\n\n        Summary:", "input": "", "output": "Create a pytables index on the table Parameters ---------- key : object ( the node to index )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groups ( self ) : _tables ( ) self . _check_if_open ( ) return [ g for g in self . _handle . walk_groups ( ) if ( not isinstance ( g , _table_mod . link . Link ) and ( getattr ( g . _v_attrs , 'pandas_type' , None ) or getattr ( g , 'table' , None ) or ( isinstance ( g , _table_mod . table . Table ) and g . _v_name != 'table' ) ) ) ]\n\n\n        Summary:", "input": "", "output": "return a list of all the top - level nodes ( that are not themselves a pandas storage object )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def walk ( self , where = \"/\" ) : _tables ( ) self . _check_if_open ( ) for g in self . _handle . walk_groups ( where ) : if getattr ( g . _v_attrs , 'pandas_type' , None ) is not None : continue groups = [ ] leaves = [ ] for child in g . _v_children . values ( ) : pandas_type = getattr ( child . _v_attrs , 'pandas_type' , None ) if pandas_type is None : if isinstance ( child , _table_mod . group . Group ) : groups . append ( child . _v_name ) else : leaves . append ( child . _v_name ) yield ( g . _v_pathname . rstrip ( '/' ) , groups , leaves )\n\n\n        Summary:", "input": "", "output": "Walk the pytables group hierarchy for pandas objects"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_node ( self , key ) : self . _check_if_open ( ) try : if not key . startswith ( '/' ) : key = '/' + key return self . _handle . get_node ( self . root , key ) except _table_mod . exceptions . NoSuchNodeError : return None\n\n\n        Summary:", "input": "", "output": "return the node with the key or None if it does not exist"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_storer ( self , key ) : group = self . get_node ( key ) if group is None : raise KeyError ( 'No object named {key} in the file' . format ( key = key ) ) s = self . _create_storer ( group ) s . infer_axes ( ) return s\n\n\n        Summary:", "input": "", "output": "return the storer object for a key raise if not in the file"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , file , mode = 'w' , propindexes = True , keys = None , complib = None , complevel = None , fletcher32 = False , overwrite = True ) : new_store = HDFStore ( file , mode = mode , complib = complib , complevel = complevel , fletcher32 = fletcher32 ) if keys is None : keys = list ( self . keys ( ) ) if not isinstance ( keys , ( tuple , list ) ) : keys = [ keys ] for k in keys : s = self . get_storer ( k ) if s is not None : if k in new_store : if overwrite : new_store . remove ( k ) data = self . select ( k ) if s . is_table : index = False if propindexes : index = [ a . name for a in s . axes if a . is_indexed ] new_store . append ( k , data , index = index , data_columns = getattr ( s , 'data_columns' , None ) , encoding = s . encoding ) else : new_store . put ( k , data , encoding = s . encoding ) return new_store\n\n\n        Summary:", "input": "", "output": "copy the existing store to a new file upgrading in place"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def info ( self ) : output = '{type}\\nFile path: {path}\\n' . format ( type = type ( self ) , path = pprint_thing ( self . _path ) ) if self . is_open : lkeys = sorted ( list ( self . keys ( ) ) ) if len ( lkeys ) : keys = [ ] values = [ ] for k in lkeys : try : s = self . get_storer ( k ) if s is not None : keys . append ( pprint_thing ( s . pathname or k ) ) values . append ( pprint_thing ( s or 'invalid_HDFStore node' ) ) except Exception as detail : keys . append ( k ) values . append ( \"[invalid_HDFStore node: {detail}]\" . format ( detail = pprint_thing ( detail ) ) ) output += adjoin ( 12 , keys , values ) else : output += 'Empty' else : output += \"File is CLOSED\" return output\n\n\n        Summary:", "input": "", "output": "Print detailed information on the store ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_format ( self , format , kwargs ) : kwargs = kwargs . copy ( ) # validate try : kwargs [ 'format' ] = _FORMAT_MAP [ format . lower ( ) ] except KeyError : raise TypeError ( \"invalid HDFStore format specified [{0}]\" . format ( format ) ) return kwargs\n\n\n        Summary:", "input": "", "output": "validate / deprecate formats ; return the new kwargs"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_storer ( self , group , format = None , value = None , append = False , * * kwargs ) : def error ( t ) : raise TypeError ( \"cannot properly create the storer for: [{t}] [group->\" \"{group},value->{value},format->{format},append->{append},\" \"kwargs->{kwargs}]\" . format ( t = t , group = group , value = type ( value ) , format = format , append = append , kwargs = kwargs ) ) pt = _ensure_decoded ( getattr ( group . _v_attrs , 'pandas_type' , None ) ) tt = _ensure_decoded ( getattr ( group . _v_attrs , 'table_type' , None ) ) # infer the pt from the passed value if pt is None : if value is None : _tables ( ) if ( getattr ( group , 'table' , None ) or isinstance ( group , _table_mod . table . Table ) ) : pt = 'frame_table' tt = 'generic_table' else : raise TypeError ( \"cannot create a storer if the object is not existing \" \"nor a value are passed\" ) else : try : pt = _TYPE_MAP [ type ( value ) ] except KeyError : error ( '_TYPE_MAP' ) # we are actually a table if format == 'table' : pt += '_table' # a storer node if 'table' not in pt : try : return globals ( ) [ _STORER_MAP [ pt ] ] ( self , group , * * kwargs ) except KeyError : error ( '_STORER_MAP' ) # existing node (and must be a table) if tt is None : # if we are a writer, determine the tt if value is not None : if pt == 'series_table' : index = getattr ( value , 'index' , None ) if index is not None : if index . nlevels == 1 : tt = 'appendable_series' elif index . nlevels > 1 : tt = 'appendable_multiseries' elif pt == 'frame_table' : index = getattr ( value , 'index' , None ) if index is not None : if index . nlevels == 1 : tt = 'appendable_frame' elif index . nlevels > 1 : tt = 'appendable_multiframe' elif pt == 'wide_table' : tt = 'appendable_panel' elif pt == 'ndim_table' : tt = 'appendable_ndim' else : # distiguish between a frame/table tt = 'legacy_panel' try : fields = group . table . _v_attrs . fields if len ( fields ) == 1 and fields [ 0 ] == 'value' : tt = 'legacy_frame' except IndexError : pass try : return globals ( ) [ _TABLE_MAP [ tt ] ] ( self , group , * * kwargs ) except KeyError : error ( '_TABLE_MAP' )\n\n\n        Summary:", "input": "", "output": "return a suitable class to operate"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_name ( self , name , kind_attr = None ) : self . name = name self . kind_attr = kind_attr or \"{name}_kind\" . format ( name = name ) if self . cname is None : self . cname = name return self\n\n\n        Summary:", "input": "", "output": "set the name of this indexer"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_pos ( self , pos ) : self . pos = pos if pos is not None and self . typ is not None : self . typ . _v_pos = pos return self\n\n\n        Summary:", "input": "", "output": "set the position of this column in the Table"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_indexed ( self ) : try : return getattr ( self . table . cols , self . cname ) . is_indexed except AttributeError : False\n\n\n        Summary:", "input": "", "output": "return whether I am an indexed column"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer ( self , handler ) : table = handler . table new_self = self . copy ( ) new_self . set_table ( table ) new_self . get_attr ( ) new_self . read_metadata ( handler ) return new_self\n\n\n        Summary:", "input": "", "output": "infer this column from the table : create and return a new object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert ( self , values , nan_rep , encoding , errors ) : # values is a recarray if values . dtype . fields is not None : values = values [ self . cname ] values = _maybe_convert ( values , self . kind , encoding , errors ) kwargs = dict ( ) if self . freq is not None : kwargs [ 'freq' ] = _ensure_decoded ( self . freq ) if self . index_name is not None : kwargs [ 'name' ] = _ensure_decoded ( self . index_name ) # making an Index instance could throw a number of different errors try : self . values = Index ( values , * * kwargs ) except Exception : # noqa: E722 # if the output freq is different that what we recorded, # it should be None (see also 'doc example part 2') if 'freq' in kwargs : kwargs [ 'freq' ] = None self . values = Index ( values , * * kwargs ) self . values = _set_tz ( self . values , self . tz ) return self\n\n\n        Summary:", "input": "", "output": "set the values from this selection : take = take ownership"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def maybe_set_size ( self , min_itemsize = None ) : if _ensure_decoded ( self . kind ) == 'string' : if isinstance ( min_itemsize , dict ) : min_itemsize = min_itemsize . get ( self . name ) if min_itemsize is not None and self . typ . itemsize < min_itemsize : self . typ = _tables ( ) . StringCol ( itemsize = min_itemsize , pos = self . pos )\n\n\n        Summary:", "input": "", "output": "maybe set a string col itemsize : min_itemsize can be an integer or a dict with this columns name with an integer size"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_col ( self , itemsize = None ) : # validate this column for string truncation (or reset to the max size) if _ensure_decoded ( self . kind ) == 'string' : c = self . col if c is not None : if itemsize is None : itemsize = self . itemsize if c . itemsize < itemsize : raise ValueError ( \"Trying to store a string with len [{itemsize}] in \" \"[{cname}] column but\\nthis column has a limit of \" \"[{c_itemsize}]!\\nConsider using min_itemsize to \" \"preset the sizes on these columns\" . format ( itemsize = itemsize , cname = self . cname , c_itemsize = c . itemsize ) ) return c . itemsize return None\n\n\n        Summary:", "input": "", "output": "validate this column : return the compared against itemsize"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update_info ( self , info ) : for key in self . _info_fields : value = getattr ( self , key , None ) idx = _get_info ( info , self . name ) existing_value = idx . get ( key ) if key in idx and value is not None and existing_value != value : # frequency/name just warn if key in [ 'freq' , 'index_name' ] : ws = attribute_conflict_doc % ( key , existing_value , value ) warnings . warn ( ws , AttributeConflictWarning , stacklevel = 6 ) # reset idx [ key ] = None setattr ( self , key , None ) else : raise ValueError ( \"invalid info for [{name}] for [{key}], \" \"existing_value [{existing_value}] conflicts with \" \"new value [{value}]\" . format ( name = self . name , key = key , existing_value = existing_value , value = value ) ) else : if value is not None or existing_value is not None : idx [ key ] = value return self\n\n\n        Summary:", "input": "", "output": "set / update the info for this indexable with the key / value if there is a conflict raise / warn as needed"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_info ( self , info ) : idx = info . get ( self . name ) if idx is not None : self . __dict__ . update ( idx )\n\n\n        Summary:", "input": "", "output": "set my state from the passed info"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_metadata ( self , handler ) : if self . meta == 'category' : new_metadata = self . metadata cur_metadata = handler . read_metadata ( self . cname ) if ( new_metadata is not None and cur_metadata is not None and not array_equivalent ( new_metadata , cur_metadata ) ) : raise ValueError ( \"cannot append a categorical with \" \"different categories to the existing\" )\n\n\n        Summary:", "input": "", "output": "validate that kind = category does not change the categories"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_metadata ( self , handler ) : if self . metadata is not None : handler . write_metadata ( self . cname , self . metadata )\n\n\n        Summary:", "input": "", "output": "set the meta data"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert ( self , values , nan_rep , encoding , errors ) : self . values = Int64Index ( np . arange ( self . table . nrows ) ) return self\n\n\n        Summary:", "input": "", "output": "set the values from this selection : take = take ownership"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_for_block ( cls , i = None , name = None , cname = None , version = None , * * kwargs ) : if cname is None : cname = name or 'values_block_{idx}' . format ( idx = i ) if name is None : name = cname # prior to 0.10.1, we named values blocks like: values_block_0 an the # name values_0 try : if version [ 0 ] == 0 and version [ 1 ] <= 10 and version [ 2 ] == 0 : m = re . search ( r\"values_block_(\\d+)\" , name ) if m : name = \"values_{group}\" . format ( group = m . groups ( ) [ 0 ] ) except IndexError : pass return cls ( name = name , cname = cname , * * kwargs )\n\n\n        Summary:", "input": "", "output": "return a new datacol with the block i"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_metadata ( self , metadata ) : if metadata is not None : metadata = np . array ( metadata , copy = False ) . ravel ( ) self . metadata = metadata\n\n\n        Summary:", "input": "", "output": "record the metadata"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_atom ( self , block , block_items , existing_col , min_itemsize , nan_rep , info , encoding = None , errors = 'strict' ) : self . values = list ( block_items ) # short-cut certain block types if block . is_categorical : return self . set_atom_categorical ( block , items = block_items , info = info ) elif block . is_datetimetz : return self . set_atom_datetime64tz ( block , info = info ) elif block . is_datetime : return self . set_atom_datetime64 ( block ) elif block . is_timedelta : return self . set_atom_timedelta64 ( block ) elif block . is_complex : return self . set_atom_complex ( block ) dtype = block . dtype . name inferred_type = lib . infer_dtype ( block . values , skipna = False ) if inferred_type == 'date' : raise TypeError ( \"[date] is not implemented as a table column\" ) elif inferred_type == 'datetime' : # after 8260 # this only would be hit for a mutli-timezone dtype # which is an error raise TypeError ( \"too many timezones in this block, create separate \" \"data columns\" ) elif inferred_type == 'unicode' : raise TypeError ( \"[unicode] is not implemented as a table column\" ) # this is basically a catchall; if say a datetime64 has nans then will # end up here ### elif inferred_type == 'string' or dtype == 'object' : self . set_atom_string ( block , block_items , existing_col , min_itemsize , nan_rep , encoding , errors ) # set as a data block else : self . set_atom_data ( block )\n\n\n        Summary:", "input": "", "output": "create and setup my atom from the block b"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_atom_coltype ( self , kind = None ) : if kind is None : kind = self . kind if self . kind . startswith ( 'uint' ) : col_name = \"UInt{name}Col\" . format ( name = kind [ 4 : ] ) else : col_name = \"{name}Col\" . format ( name = kind . capitalize ( ) ) return getattr ( _tables ( ) , col_name )\n\n\n        Summary:", "input": "", "output": "return the PyTables column class for this column"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_attr ( self , append ) : if append : existing_fields = getattr ( self . attrs , self . kind_attr , None ) if ( existing_fields is not None and existing_fields != list ( self . values ) ) : raise ValueError ( \"appended items do not match existing items\" \" in table!\" ) existing_dtype = getattr ( self . attrs , self . dtype_attr , None ) if ( existing_dtype is not None and existing_dtype != self . dtype ) : raise ValueError ( \"appended items dtype do not match existing \" \"items dtype in table!\" )\n\n\n        Summary:", "input": "", "output": "validate that we have the same order as the existing & same dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert ( self , values , nan_rep , encoding , errors ) : # values is a recarray if values . dtype . fields is not None : values = values [ self . cname ] self . set_data ( values ) # use the meta if needed meta = _ensure_decoded ( self . meta ) # convert to the correct dtype if self . dtype is not None : dtype = _ensure_decoded ( self . dtype ) # reverse converts if dtype == 'datetime64' : # recreate with tz if indicated self . data = _set_tz ( self . data , self . tz , coerce = True ) elif dtype == 'timedelta64' : self . data = np . asarray ( self . data , dtype = 'm8[ns]' ) elif dtype == 'date' : try : self . data = np . asarray ( [ date . fromordinal ( v ) for v in self . data ] , dtype = object ) except ValueError : self . data = np . asarray ( [ date . fromtimestamp ( v ) for v in self . data ] , dtype = object ) elif dtype == 'datetime' : self . data = np . asarray ( [ datetime . fromtimestamp ( v ) for v in self . data ] , dtype = object ) elif meta == 'category' : # we have a categorical categories = self . metadata codes = self . data . ravel ( ) # if we have stored a NaN in the categories # then strip it; in theory we could have BOTH # -1s in the codes and nulls :< if categories is None : # Handle case of NaN-only categorical columns in which case # the categories are an empty array; when this is stored, # pytables cannot write a zero-len array, so on readback # the categories would be None and `read_hdf()` would fail. categories = Index ( [ ] , dtype = np . float64 ) else : mask = isna ( categories ) if mask . any ( ) : categories = categories [ ~ mask ] codes [ codes != - 1 ] -= mask . astype ( int ) . cumsum ( ) . values self . data = Categorical . from_codes ( codes , categories = categories , ordered = self . ordered ) else : try : self . data = self . data . astype ( dtype , copy = False ) except TypeError : self . data = self . data . astype ( 'O' , copy = False ) # convert nans / decode if _ensure_decoded ( self . kind ) == 'string' : self . data = _unconvert_string_array ( self . data , nan_rep = nan_rep , encoding = encoding , errors = errors ) return self\n\n\n        Summary:", "input": "", "output": "set the data from this selection ( and convert to the correct dtype if we can )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_attr ( self ) : self . values = getattr ( self . attrs , self . kind_attr , None ) self . dtype = getattr ( self . attrs , self . dtype_attr , None ) self . meta = getattr ( self . attrs , self . meta_attr , None ) self . set_kind ( )\n\n\n        Summary:", "input": "", "output": "get the data for this column"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_attr ( self ) : setattr ( self . attrs , self . kind_attr , self . values ) setattr ( self . attrs , self . meta_attr , self . meta ) if self . dtype is not None : setattr ( self . attrs , self . dtype_attr , self . dtype )\n\n\n        Summary:", "input": "", "output": "set the data for this column"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_version ( self ) : version = _ensure_decoded ( getattr ( self . group . _v_attrs , 'pandas_version' , None ) ) try : self . version = tuple ( int ( x ) for x in version . split ( '.' ) ) if len ( self . version ) == 2 : self . version = self . version + ( 0 , ) except AttributeError : self . version = ( 0 , 0 , 0 )\n\n\n        Summary:", "input": "", "output": "compute and set our version"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_object_info ( self ) : self . attrs . pandas_type = str ( self . pandas_kind ) self . attrs . pandas_version = str ( _version ) self . set_version ( )\n\n\n        Summary:", "input": "", "output": "set my pandas type & version"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer_axes ( self ) : s = self . storable if s is None : return False self . get_attrs ( ) return True\n\n\n        Summary:", "input": "", "output": "infer the axes of my storer return a boolean indicating if we have a valid storer or not"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def delete ( self , where = None , start = None , stop = None , * * kwargs ) : if com . _all_none ( where , start , stop ) : self . _handle . remove_node ( self . group , recursive = True ) return None raise TypeError ( \"cannot delete on an abstract storer\" )\n\n\n        Summary:", "input": "", "output": "support fully deleting the node in its entirety ( only ) - where specification must be None"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_read ( self , kwargs ) : kwargs = copy . copy ( kwargs ) columns = kwargs . pop ( 'columns' , None ) if columns is not None : raise TypeError ( \"cannot pass a column specification when reading \" \"a Fixed format store. this store must be \" \"selected in its entirety\" ) where = kwargs . pop ( 'where' , None ) if where is not None : raise TypeError ( \"cannot pass a where specification when reading \" \"from a Fixed format store. this store must be \" \"selected in its entirety\" ) return kwargs\n\n\n        Summary:", "input": "", "output": "remove table keywords from kwargs and return raise if any keywords are passed which are not - None"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_attrs ( self ) : self . attrs . encoding = self . encoding self . attrs . errors = self . errors\n\n\n        Summary:", "input": "", "output": "set our object attributes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_attrs ( self ) : self . encoding = _ensure_encoding ( getattr ( self . attrs , 'encoding' , None ) ) self . errors = _ensure_decoded ( getattr ( self . attrs , 'errors' , 'strict' ) ) for n in self . attributes : setattr ( self , n , _ensure_decoded ( getattr ( self . attrs , n , None ) ) )\n\n\n        Summary:", "input": "", "output": "retrieve our attributes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_array ( self , key , start = None , stop = None ) : import tables node = getattr ( self . group , key ) attrs = node . _v_attrs transposed = getattr ( attrs , 'transposed' , False ) if isinstance ( node , tables . VLArray ) : ret = node [ 0 ] [ start : stop ] else : dtype = getattr ( attrs , 'value_type' , None ) shape = getattr ( attrs , 'shape' , None ) if shape is not None : # length 0 axis ret = np . empty ( shape , dtype = dtype ) else : ret = node [ start : stop ] if dtype == 'datetime64' : # reconstruct a timezone if indicated ret = _set_tz ( ret , getattr ( attrs , 'tz' , None ) , coerce = True ) elif dtype == 'timedelta64' : ret = np . asarray ( ret , dtype = 'm8[ns]' ) if transposed : return ret . T else : return ret\n\n\n        Summary:", "input": "", "output": "read an array for the specified node ( off of group"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_array_empty ( self , key , value ) : # ugly hack for length 0 axes arr = np . empty ( ( 1 , ) * value . ndim ) self . _handle . create_array ( self . group , key , arr ) getattr ( self . group , key ) . _v_attrs . value_type = str ( value . dtype ) getattr ( self . group , key ) . _v_attrs . shape = value . shape\n\n\n        Summary:", "input": "", "output": "write a 0 - len array"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_read ( self , kwargs ) : kwargs = super ( ) . validate_read ( kwargs ) if 'start' in kwargs or 'stop' in kwargs : raise NotImplementedError ( \"start and/or stop are not supported \" \"in fixed Sparse reading\" ) return kwargs\n\n\n        Summary:", "input": "", "output": "we don t support start stop kwds in Sparse"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write ( self , obj , * * kwargs ) : super ( ) . write ( obj , * * kwargs ) for name , ss in obj . items ( ) : key = 'sparse_series_{name}' . format ( name = name ) if key not in self . group . _v_children : node = self . _handle . create_group ( self . group , key ) else : node = getattr ( self . group , key ) s = SparseSeriesFixed ( self . parent , node ) s . write ( ss ) self . attrs . default_fill_value = obj . default_fill_value self . attrs . default_kind = obj . default_kind self . write_index ( 'columns' , obj . columns )\n\n\n        Summary:", "input": "", "output": "write it as a collection of individual sparse series"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate ( self , other ) : if other is None : return if other . table_type != self . table_type : raise TypeError ( \"incompatible table_type with existing \" \"[{other} - {self}]\" . format ( other = other . table_type , self = self . table_type ) ) for c in [ 'index_axes' , 'non_index_axes' , 'values_axes' ] : sv = getattr ( self , c , None ) ov = getattr ( other , c , None ) if sv != ov : # show the error for the specific axes for i , sax in enumerate ( sv ) : oax = ov [ i ] if sax != oax : raise ValueError ( \"invalid combinate of [{c}] on appending data \" \"[{sax}] vs current table [{oax}]\" . format ( c = c , sax = sax , oax = oax ) ) # should never get here raise Exception ( \"invalid combinate of [{c}] on appending data [{sv}] vs \" \"current table [{ov}]\" . format ( c = c , sv = sv , ov = ov ) )\n\n\n        Summary:", "input": "", "output": "validate against an existing table"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_metadata ( self , existing ) : self . metadata = [ c . name for c in self . values_axes if c . metadata is not None ]\n\n\n        Summary:", "input": "", "output": "create / validate metadata"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_multiindex ( self , obj ) : levels = [ l if l is not None else \"level_{0}\" . format ( i ) for i , l in enumerate ( obj . index . names ) ] try : return obj . reset_index ( ) , levels except ValueError : raise ValueError ( \"duplicate names/columns in the multi-index when \" \"storing as a table\" )\n\n\n        Summary:", "input": "", "output": "validate that we can store the multi - index ; reset and return the new object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nrows_expected ( self ) : return np . prod ( [ i . cvalues . shape [ 0 ] for i in self . index_axes ] )\n\n\n        Summary:", "input": "", "output": "based on our axes compute the expected nrows"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def data_orientation ( self ) : return tuple ( itertools . chain ( [ int ( a [ 0 ] ) for a in self . non_index_axes ] , [ int ( a . axis ) for a in self . index_axes ] ) )\n\n\n        Summary:", "input": "", "output": "return a tuple of my permutated axes non_indexable at the front"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def queryables ( self ) : # compute the values_axes queryables return dict ( [ ( a . cname , a ) for a in self . index_axes ] + [ ( self . storage_obj_type . _AXIS_NAMES [ axis ] , None ) for axis , values in self . non_index_axes ] + [ ( v . cname , v ) for v in self . values_axes if v . name in set ( self . data_columns ) ] )\n\n\n        Summary:", "input": "", "output": "return a dict of the kinds allowable columns for this object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_metadata_path ( self , key ) : return \"{group}/meta/{key}/meta\" . format ( group = self . group . _v_pathname , key = key )\n\n\n        Summary:", "input": "", "output": "return the metadata pathname for this key"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_metadata ( self , key , values ) : values = Series ( values ) self . parent . put ( self . _get_metadata_path ( key ) , values , format = 'table' , encoding = self . encoding , errors = self . errors , nan_rep = self . nan_rep )\n\n\n        Summary:", "input": "", "output": "write out a meta data array to the key as a fixed - format Series"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_metadata ( self , key ) : if getattr ( getattr ( self . group , 'meta' , None ) , key , None ) is not None : return self . parent . select ( self . _get_metadata_path ( key ) ) return None\n\n\n        Summary:", "input": "", "output": "return the meta data array for this key"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_attrs ( self ) : self . attrs . table_type = str ( self . table_type ) self . attrs . index_cols = self . index_cols ( ) self . attrs . values_cols = self . values_cols ( ) self . attrs . non_index_axes = self . non_index_axes self . attrs . data_columns = self . data_columns self . attrs . nan_rep = self . nan_rep self . attrs . encoding = self . encoding self . attrs . errors = self . errors self . attrs . levels = self . levels self . attrs . metadata = self . metadata self . set_info ( )\n\n\n        Summary:", "input": "", "output": "set our table type & indexables"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_attrs ( self ) : self . non_index_axes = getattr ( self . attrs , 'non_index_axes' , None ) or [ ] self . data_columns = getattr ( self . attrs , 'data_columns' , None ) or [ ] self . info = getattr ( self . attrs , 'info' , None ) or dict ( ) self . nan_rep = getattr ( self . attrs , 'nan_rep' , None ) self . encoding = _ensure_encoding ( getattr ( self . attrs , 'encoding' , None ) ) self . errors = _ensure_decoded ( getattr ( self . attrs , 'errors' , 'strict' ) ) self . levels = getattr ( self . attrs , 'levels' , None ) or [ ] self . index_axes = [ a . infer ( self ) for a in self . indexables if a . is_an_indexable ] self . values_axes = [ a . infer ( self ) for a in self . indexables if not a . is_an_indexable ] self . metadata = getattr ( self . attrs , 'metadata' , None ) or [ ]\n\n\n        Summary:", "input": "", "output": "retrieve our attributes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_version ( self , where = None ) : if where is not None : if ( self . version [ 0 ] <= 0 and self . version [ 1 ] <= 10 and self . version [ 2 ] < 1 ) : ws = incompatibility_doc % '.' . join ( [ str ( x ) for x in self . version ] ) warnings . warn ( ws , IncompatibilityWarning )\n\n\n        Summary:", "input": "", "output": "are we trying to operate on an old version?"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_min_itemsize ( self , min_itemsize ) : if min_itemsize is None : return if not isinstance ( min_itemsize , dict ) : return q = self . queryables ( ) for k , v in min_itemsize . items ( ) : # ok, apply generally if k == 'values' : continue if k not in q : raise ValueError ( \"min_itemsize has the key [{key}] which is not an axis or \" \"data_column\" . format ( key = k ) )\n\n\n        Summary:", "input": "", "output": "validate the min_itemisze doesn t contain items that are not in the axes this needs data_columns to be defined"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def indexables ( self ) : if self . _indexables is None : self . _indexables = [ ] # index columns self . _indexables . extend ( [ IndexCol ( name = name , axis = axis , pos = i ) for i , ( axis , name ) in enumerate ( self . attrs . index_cols ) ] ) # values columns dc = set ( self . data_columns ) base_pos = len ( self . _indexables ) def f ( i , c ) : klass = DataCol if c in dc : klass = DataIndexableCol return klass . create_for_block ( i = i , name = c , pos = base_pos + i , version = self . version ) self . _indexables . extend ( [ f ( i , c ) for i , c in enumerate ( self . attrs . values_cols ) ] ) return self . _indexables\n\n\n        Summary:", "input": "", "output": "create / cache the indexables if they don t exist"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_index ( self , columns = None , optlevel = None , kind = None ) : if not self . infer_axes ( ) : return if columns is False : return # index all indexables and data_columns if columns is None or columns is True : columns = [ a . cname for a in self . axes if a . is_data_indexable ] if not isinstance ( columns , ( tuple , list ) ) : columns = [ columns ] kw = dict ( ) if optlevel is not None : kw [ 'optlevel' ] = optlevel if kind is not None : kw [ 'kind' ] = kind table = self . table for c in columns : v = getattr ( table . cols , c , None ) if v is not None : # remove the index if the kind/optlevel have changed if v . is_indexed : index = v . index cur_optlevel = index . optlevel cur_kind = index . kind if kind is not None and cur_kind != kind : v . remove_index ( ) else : kw [ 'kind' ] = cur_kind if optlevel is not None and cur_optlevel != optlevel : v . remove_index ( ) else : kw [ 'optlevel' ] = cur_optlevel # create the index if not v . is_indexed : if v . type . startswith ( 'complex' ) : raise TypeError ( 'Columns containing complex values can be stored ' 'but cannot' ' be indexed when using table format. Either use ' 'fixed format, set index=False, or do not include ' 'the columns containing complex values to ' 'data_columns when initializing the table.' ) v . create_index ( * * kw )\n\n\n        Summary:", "input": "", "output": "Create a pytables index on the specified columns note : cannot index Time64Col () or ComplexCol currently ; PyTables must be > = 3 . 0"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_axes ( self , where , * * kwargs ) : # validate the version self . validate_version ( where ) # infer the data kind if not self . infer_axes ( ) : return False # create the selection self . selection = Selection ( self , where = where , * * kwargs ) values = self . selection . select ( ) # convert the data for a in self . axes : a . set_info ( self . info ) a . convert ( values , nan_rep = self . nan_rep , encoding = self . encoding , errors = self . errors ) return True\n\n\n        Summary:", "input": "", "output": "create and return the axes sniffed from the table : return boolean for success"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def validate_data_columns ( self , data_columns , min_itemsize ) : if not len ( self . non_index_axes ) : return [ ] axis , axis_labels = self . non_index_axes [ 0 ] info = self . info . get ( axis , dict ( ) ) if info . get ( 'type' ) == 'MultiIndex' and data_columns : raise ValueError ( \"cannot use a multi-index on axis [{0}] with \" \"data_columns {1}\" . format ( axis , data_columns ) ) # evaluate the passed data_columns, True == use all columns # take only valide axis labels if data_columns is True : data_columns = list ( axis_labels ) elif data_columns is None : data_columns = [ ] # if min_itemsize is a dict, add the keys (exclude 'values') if isinstance ( min_itemsize , dict ) : existing_data_columns = set ( data_columns ) data_columns . extend ( [ k for k in min_itemsize . keys ( ) if k != 'values' and k not in existing_data_columns ] ) # return valid columns in the order of our axis return [ c for c in data_columns if c in axis_labels ]\n\n\n        Summary:", "input": "", "output": "take the input data_columns and min_itemize and create a data columns spec"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_axes ( self , axes , obj , validate = True , nan_rep = None , data_columns = None , min_itemsize = None , * * kwargs ) : # set the default axes if needed if axes is None : try : axes = _AXES_MAP [ type ( obj ) ] except KeyError : raise TypeError ( \"cannot properly create the storer for: [group->{group},\" \"value->{value}]\" . format ( group = self . group . _v_name , value = type ( obj ) ) ) # map axes to numbers axes = [ obj . _get_axis_number ( a ) for a in axes ] # do we have an existing table (if so, use its axes & data_columns) if self . infer_axes ( ) : existing_table = self . copy ( ) existing_table . infer_axes ( ) axes = [ a . axis for a in existing_table . index_axes ] data_columns = existing_table . data_columns nan_rep = existing_table . nan_rep self . encoding = existing_table . encoding self . errors = existing_table . errors self . info = copy . copy ( existing_table . info ) else : existing_table = None # currently support on ndim-1 axes if len ( axes ) != self . ndim - 1 : raise ValueError ( \"currently only support ndim-1 indexers in an AppendableTable\" ) # create according to the new data self . non_index_axes = [ ] self . data_columns = [ ] # nan_representation if nan_rep is None : nan_rep = 'nan' self . nan_rep = nan_rep # create axes to index and non_index index_axes_map = dict ( ) for i , a in enumerate ( obj . axes ) : if i in axes : name = obj . _AXIS_NAMES [ i ] index_axes_map [ i ] = _convert_index ( a , self . encoding , self . errors , self . format_type ) . set_name ( name ) . set_axis ( i ) else : # we might be able to change the axes on the appending data if # necessary append_axis = list ( a ) if existing_table is not None : indexer = len ( self . non_index_axes ) exist_axis = existing_table . non_index_axes [ indexer ] [ 1 ] if not array_equivalent ( np . array ( append_axis ) , np . array ( exist_axis ) ) : # ahah! -> reindex if array_equivalent ( np . array ( sorted ( append_axis ) ) , np . array ( sorted ( exist_axis ) ) ) : append_axis = exist_axis # the non_index_axes info info = _get_info ( self . info , i ) info [ 'names' ] = list ( a . names ) info [ 'type' ] = a . __class__ . __name__ self . non_index_axes . append ( ( i , append_axis ) ) # set axis positions (based on the axes) self . index_axes = [ index_axes_map [ a ] . set_pos ( j ) . update_info ( self . info ) for j , a in enumerate ( axes ) ] j = len ( self . index_axes ) # check for column conflicts for a in self . axes : a . maybe_set_size ( min_itemsize = min_itemsize ) # reindex by our non_index_axes & compute data_columns for a in self . non_index_axes : obj = _reindex_axis ( obj , a [ 0 ] , a [ 1 ] ) def get_blk_items ( mgr , blocks ) : return [ mgr . items . take ( blk . mgr_locs ) for blk in blocks ] # figure out data_columns and get out blocks block_obj = self . get_object ( obj ) . _consolidate ( ) blocks = block_obj . _data . blocks blk_items = get_blk_items ( block_obj . _data , blocks ) if len ( self . non_index_axes ) : axis , axis_labels = self . non_index_axes [ 0 ] data_columns = self . validate_data_columns ( data_columns , min_itemsize ) if len ( data_columns ) : mgr = block_obj . reindex ( Index ( axis_labels ) . difference ( Index ( data_columns ) ) , axis = axis ) . _data blocks = list ( mgr . blocks ) blk_items = get_blk_items ( mgr , blocks ) for c in data_columns : mgr = block_obj . reindex ( [ c ] , axis = axis ) . _data blocks . extend ( mgr . blocks ) blk_items . extend ( get_blk_items ( mgr , mgr . blocks ) ) # reorder the blocks in the same order as the existing_table if we can if existing_table is not None : by_items = { tuple ( b_items . tolist ( ) ) : ( b , b_items ) for b , b_items in zip ( blocks , blk_items ) } new_blocks = [ ] new_blk_items = [ ] for ea in existing_table . values_axes : items = tuple ( ea . values ) try : b , b_items = by_items . pop ( items ) new_blocks . append ( b ) new_blk_items . append ( b_items ) except ( IndexError , KeyError ) : raise ValueError ( \"cannot match existing table structure for [{items}] \" \"on appending data\" . format ( items = ( ',' . join ( pprint_thing ( item ) for item in items ) ) ) ) blocks = new_blocks blk_items = new_blk_items # add my values self . values_axes = [ ] for i , ( b , b_items ) in enumerate ( zip ( blocks , blk_items ) ) : # shape of the data column are the indexable axes klass = DataCol name = None # we have a data_column if ( data_columns and len ( b_items ) == 1 and b_items [ 0 ] in data_columns ) : klass = DataIndexableCol name = b_items [ 0 ] self . data_columns . append ( name ) # make sure that we match up the existing columns # if we have an existing table if existing_table is not None and validate : try : existing_col = existing_table . values_axes [ i ] except ( IndexError , KeyError ) : raise ValueError ( \"Incompatible appended table [{blocks}]\" \"with existing table [{table}]\" . format ( blocks = blocks , table = existing_table . values_axes ) ) else : existing_col = None try : col = klass . create_for_block ( i = i , name = name , version = self . version ) col . set_atom ( block = b , block_items = b_items , existing_col = existing_col , min_itemsize = min_itemsize , nan_rep = nan_rep , encoding = self . encoding , errors = self . errors , info = self . info ) col . set_pos ( j ) self . values_axes . append ( col ) except ( NotImplementedError , ValueError , TypeError ) as e : raise e except Exception as detail : raise Exception ( \"cannot find the correct atom type -> \" \"[dtype->{name},items->{items}] {detail!s}\" . format ( name = b . dtype . name , items = b_items , detail = detail ) ) j += 1 # validate our min_itemsize self . validate_min_itemsize ( min_itemsize ) # validate our metadata self . validate_metadata ( existing_table ) # validate the axes if we have an existing table if validate : self . validate ( existing_table )\n\n\n        Summary:", "input": "", "output": "create and return the axes leagcy tables create an indexable column indexable index non - indexable fields"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def process_axes ( self , obj , columns = None ) : # make a copy to avoid side effects if columns is not None : columns = list ( columns ) # make sure to include levels if we have them if columns is not None and self . is_multi_index : for n in self . levels : if n not in columns : columns . insert ( 0 , n ) # reorder by any non_index_axes & limit to the select columns for axis , labels in self . non_index_axes : obj = _reindex_axis ( obj , axis , labels , columns ) # apply the selection filters (but keep in the same order) if self . selection . filter is not None : for field , op , filt in self . selection . filter . format ( ) : def process_filter ( field , filt ) : for axis_name in obj . _AXIS_NAMES . values ( ) : axis_number = obj . _get_axis_number ( axis_name ) axis_values = obj . _get_axis ( axis_name ) # see if the field is the name of an axis if field == axis_name : # if we have a multi-index, then need to include # the levels if self . is_multi_index : filt = filt . union ( Index ( self . levels ) ) takers = op ( axis_values , filt ) return obj . loc . _getitem_axis ( takers , axis = axis_number ) # this might be the name of a file IN an axis elif field in axis_values : # we need to filter on this dimension values = ensure_index ( getattr ( obj , field ) . values ) filt = ensure_index ( filt ) # hack until we support reversed dim flags if isinstance ( obj , DataFrame ) : axis_number = 1 - axis_number takers = op ( values , filt ) return obj . loc . _getitem_axis ( takers , axis = axis_number ) raise ValueError ( \"cannot find the field [{field}] for \" \"filtering!\" . format ( field = field ) ) obj = process_filter ( field , filt ) return obj\n\n\n        Summary:", "input": "", "output": "process axes filters"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def create_description ( self , complib = None , complevel = None , fletcher32 = False , expectedrows = None ) : # provided expected rows if its passed if expectedrows is None : expectedrows = max ( self . nrows_expected , 10000 ) d = dict ( name = 'table' , expectedrows = expectedrows ) # description from the axes & values d [ 'description' ] = { a . cname : a . typ for a in self . axes } if complib : if complevel is None : complevel = self . _complevel or 9 filters = _tables ( ) . Filters ( complevel = complevel , complib = complib , fletcher32 = fletcher32 or self . _fletcher32 ) d [ 'filters' ] = filters elif self . _filters is not None : d [ 'filters' ] = self . _filters return d\n\n\n        Summary:", "input": "", "output": "create the description of the table from the axes & values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_coordinates ( self , where = None , start = None , stop = None , * * kwargs ) : # validate the version self . validate_version ( where ) # infer the data kind if not self . infer_axes ( ) : return False # create the selection self . selection = Selection ( self , where = where , start = start , stop = stop , * * kwargs ) coords = self . selection . select_coords ( ) if self . selection . filter is not None : for field , op , filt in self . selection . filter . format ( ) : data = self . read_column ( field , start = coords . min ( ) , stop = coords . max ( ) + 1 ) coords = coords [ op ( data . iloc [ coords - coords . min ( ) ] , filt ) . values ] return Index ( coords )\n\n\n        Summary:", "input": "", "output": "select coordinates ( row numbers ) from a table ; return the coordinates object"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_column ( self , column , where = None , start = None , stop = None ) : # validate the version self . validate_version ( ) # infer the data kind if not self . infer_axes ( ) : return False if where is not None : raise TypeError ( \"read_column does not currently accept a where \" \"clause\" ) # find the axes for a in self . axes : if column == a . name : if not a . is_data_indexable : raise ValueError ( \"column [{column}] can not be extracted individually; \" \"it is not data indexable\" . format ( column = column ) ) # column must be an indexable or a data column c = getattr ( self . table . cols , column ) a . set_info ( self . info ) return Series ( _set_tz ( a . convert ( c [ start : stop ] , nan_rep = self . nan_rep , encoding = self . encoding , errors = self . errors ) . take_data ( ) , a . tz , True ) , name = column ) raise KeyError ( \"column [{column}] not found in the table\" . format ( column = column ) )\n\n\n        Summary:", "input": "", "output": "return a single column from the table generally only indexables are interesting"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read ( self , where = None , columns = None , * * kwargs ) : if not self . read_axes ( where = where , * * kwargs ) : return None raise NotImplementedError ( \"Panel is removed in pandas 0.25.0\" )\n\n\n        Summary:", "input": "", "output": "we have n indexable columns with an arbitrary number of data axes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_data ( self , chunksize , dropna = False ) : names = self . dtype . names nrows = self . nrows_expected # if dropna==True, then drop ALL nan rows masks = [ ] if dropna : for a in self . values_axes : # figure the mask: only do if we can successfully process this # column, otherwise ignore the mask mask = isna ( a . data ) . all ( axis = 0 ) if isinstance ( mask , np . ndarray ) : masks . append ( mask . astype ( 'u1' , copy = False ) ) # consolidate masks if len ( masks ) : mask = masks [ 0 ] for m in masks [ 1 : ] : mask = mask & m mask = mask . ravel ( ) else : mask = None # broadcast the indexes if needed indexes = [ a . cvalues for a in self . index_axes ] nindexes = len ( indexes ) bindexes = [ ] for i , idx in enumerate ( indexes ) : # broadcast to all other indexes except myself if i > 0 and i < nindexes : repeater = np . prod ( [ indexes [ bi ] . shape [ 0 ] for bi in range ( 0 , i ) ] ) idx = np . tile ( idx , repeater ) if i < nindexes - 1 : repeater = np . prod ( [ indexes [ bi ] . shape [ 0 ] for bi in range ( i + 1 , nindexes ) ] ) idx = np . repeat ( idx , repeater ) bindexes . append ( idx ) # transpose the values so first dimension is last # reshape the values if needed values = [ a . take_data ( ) for a in self . values_axes ] values = [ v . transpose ( np . roll ( np . arange ( v . ndim ) , v . ndim - 1 ) ) for v in values ] bvalues = [ ] for i , v in enumerate ( values ) : new_shape = ( nrows , ) + self . dtype [ names [ nindexes + i ] ] . shape bvalues . append ( values [ i ] . reshape ( new_shape ) ) # write the chunks if chunksize is None : chunksize = 100000 rows = np . empty ( min ( chunksize , nrows ) , dtype = self . dtype ) chunks = int ( nrows / chunksize ) + 1 for i in range ( chunks ) : start_i = i * chunksize end_i = min ( ( i + 1 ) * chunksize , nrows ) if start_i >= end_i : break self . write_data_chunk ( rows , indexes = [ a [ start_i : end_i ] for a in bindexes ] , mask = mask [ start_i : end_i ] if mask is not None else None , values = [ v [ start_i : end_i ] for v in bvalues ] )\n\n\n        Summary:", "input": "", "output": "we form the data into a 2 - d including indexes values mask write chunk - by - chunk"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write_data_chunk ( self , rows , indexes , mask , values ) : # 0 len for v in values : if not np . prod ( v . shape ) : return try : nrows = indexes [ 0 ] . shape [ 0 ] if nrows != len ( rows ) : rows = np . empty ( nrows , dtype = self . dtype ) names = self . dtype . names nindexes = len ( indexes ) # indexes for i , idx in enumerate ( indexes ) : rows [ names [ i ] ] = idx # values for i , v in enumerate ( values ) : rows [ names [ i + nindexes ] ] = v # mask if mask is not None : m = ~ mask . ravel ( ) . astype ( bool , copy = False ) if not m . all ( ) : rows = rows [ m ] except Exception as detail : raise Exception ( \"cannot create row-data -> {detail}\" . format ( detail = detail ) ) try : if len ( rows ) : self . table . append ( rows ) self . table . flush ( ) except Exception as detail : raise TypeError ( \"tables cannot write this data -> {detail}\" . format ( detail = detail ) )\n\n\n        Summary:", "input": "", "output": "Parameters ---------- rows : an empty memory space where we are putting the chunk indexes : an array of the indexes mask : an array of the masks values : an array of the values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write ( self , obj , data_columns = None , * * kwargs ) : if not isinstance ( obj , DataFrame ) : name = obj . name or 'values' obj = DataFrame ( { name : obj } , index = obj . index ) obj . columns = [ name ] return super ( ) . write ( obj = obj , data_columns = obj . columns . tolist ( ) , * * kwargs )\n\n\n        Summary:", "input": "", "output": "we are going to write this as a frame table"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def write ( self , obj , * * kwargs ) : name = obj . name or 'values' obj , self . levels = self . validate_multiindex ( obj ) cols = list ( self . levels ) cols . append ( name ) obj . columns = cols return super ( ) . write ( obj = obj , * * kwargs )\n\n\n        Summary:", "input": "", "output": "we are going to write this as a frame table"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_attrs ( self ) : self . non_index_axes = [ ] self . nan_rep = None self . levels = [ ] self . index_axes = [ a . infer ( self ) for a in self . indexables if a . is_an_indexable ] self . values_axes = [ a . infer ( self ) for a in self . indexables if not a . is_an_indexable ] self . data_columns = [ a . name for a in self . values_axes ]\n\n\n        Summary:", "input": "", "output": "retrieve our attributes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def indexables ( self ) : if self . _indexables is None : d = self . description # the index columns is just a simple index self . _indexables = [ GenericIndexCol ( name = 'index' , axis = 0 ) ] for i , n in enumerate ( d . _v_names ) : dc = GenericDataIndexableCol ( name = n , pos = i , values = [ n ] , version = self . version ) self . _indexables . append ( dc ) return self . _indexables\n\n\n        Summary:", "input": "", "output": "create the indexables from the table description"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def generate ( self , where ) : if where is None : return None q = self . table . queryables ( ) try : return Expr ( where , queryables = q , encoding = self . table . encoding ) except NameError : # raise a nice message, suggesting that the user should use # data_columns raise ValueError ( \"The passed where expression: {0}\\n\" \" contains an invalid variable reference\\n\" \" all of the variable references must be a \" \"reference to\\n\" \" an axis (e.g. 'index' or 'columns'), or a \" \"data_column\\n\" \" The currently defined references are: {1}\\n\" . format ( where , ',' . join ( q . keys ( ) ) ) )\n\n\n        Summary:", "input": "", "output": "where can be a : dict list tuple string"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select ( self ) : if self . condition is not None : return self . table . table . read_where ( self . condition . format ( ) , start = self . start , stop = self . stop ) elif self . coordinates is not None : return self . table . table . read_coordinates ( self . coordinates ) return self . table . table . read ( start = self . start , stop = self . stop )\n\n\n        Summary:", "input": "", "output": "generate the selection"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select_coords ( self ) : start , stop = self . start , self . stop nrows = self . table . nrows if start is None : start = 0 elif start < 0 : start += nrows if self . stop is None : stop = nrows elif stop < 0 : stop += nrows if self . condition is not None : return self . table . table . get_where_list ( self . condition . format ( ) , start = start , stop = stop , sort = True ) elif self . coordinates is not None : return self . coordinates return np . arange ( start , stop )\n\n\n        Summary:", "input": "", "output": "generate the selection"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def astype ( self , dtype , copy = True ) : return np . array ( self , dtype = dtype , copy = copy )\n\n\n        Summary:", "input": "", "output": "Cast to a NumPy array with dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argsort ( self , ascending = True , kind = 'quicksort' , * args , * * kwargs ) : # Implementor note: You have two places to override the behavior of # argsort. # 1. _values_for_argsort : construct the values passed to np.argsort # 2. argsort : total control over sorting. ascending = nv . validate_argsort_with_ascending ( ascending , args , kwargs ) values = self . _values_for_argsort ( ) result = np . argsort ( values , kind = kind , * * kwargs ) if not ascending : result = result [ : : - 1 ] return result\n\n\n        Summary:", "input": "", "output": "Return the indices that would sort this array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fillna ( self , value = None , method = None , limit = None ) : from pandas . api . types import is_array_like from pandas . util . _validators import validate_fillna_kwargs from pandas . core . missing import pad_1d , backfill_1d value , method = validate_fillna_kwargs ( value , method ) mask = self . isna ( ) if is_array_like ( value ) : if len ( value ) != len ( self ) : raise ValueError ( \"Length of 'value' does not match. Got ({}) \" \" expected {}\" . format ( len ( value ) , len ( self ) ) ) value = value [ mask ] if mask . any ( ) : if method is not None : func = pad_1d if method == 'pad' else backfill_1d new_values = func ( self . astype ( object ) , limit = limit , mask = mask ) new_values = self . _from_sequence ( new_values , dtype = self . dtype ) else : # fill with value new_values = self . copy ( ) new_values [ mask ] = value else : new_values = self . copy ( ) return new_values\n\n\n        Summary:", "input": "", "output": "Fill NA / NaN values using the specified method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods : int = 1 , fill_value : object = None , ) -> ABCExtensionArray : # Note: this implementation assumes that `self.dtype.na_value` can be # stored in an instance of your ExtensionArray with `self.dtype`. if not len ( self ) or periods == 0 : return self . copy ( ) if isna ( fill_value ) : fill_value = self . dtype . na_value empty = self . _from_sequence ( [ fill_value ] * min ( abs ( periods ) , len ( self ) ) , dtype = self . dtype ) if periods > 0 : a = empty b = self [ : - periods ] else : a = self [ abs ( periods ) : ] b = empty return self . _concat_same_type ( [ a , b ] )\n\n\n        Summary:", "input": "", "output": "Shift values by desired number ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def unique ( self ) : from pandas import unique uniques = unique ( self . astype ( object ) ) return self . _from_sequence ( uniques , dtype = self . dtype )\n\n\n        Summary:", "input": "", "output": "Compute the ExtensionArray of unique values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def searchsorted ( self , value , side = \"left\" , sorter = None ) : # Note: the base tests provided by pandas only test the basics. # We do not test # 1. Values outside the range of the `data_for_sorting` fixture # 2. Values between the values in the `data_for_sorting` fixture # 3. Missing values. arr = self . astype ( object ) return arr . searchsorted ( value , side = side , sorter = sorter )\n\n\n        Summary:", "input": "", "output": "Find indices where elements should be inserted to maintain order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _values_for_factorize ( self ) -> Tuple [ np . ndarray , Any ] : return self . astype ( object ) , np . nan\n\n\n        Summary:", "input": "", "output": "Return an array and missing value suitable for factorization ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def factorize ( self , na_sentinel : int = - 1 , ) -> Tuple [ np . ndarray , ABCExtensionArray ] : # Impelmentor note: There are two ways to override the behavior of # pandas.factorize # 1. _values_for_factorize and _from_factorize. # Specify the values passed to pandas' internal factorization # routines, and how to convert from those values back to the # original ExtensionArray. # 2. ExtensionArray.factorize. # Complete control over factorization. from pandas . core . algorithms import _factorize_array arr , na_value = self . _values_for_factorize ( ) labels , uniques = _factorize_array ( arr , na_sentinel = na_sentinel , na_value = na_value ) uniques = self . _from_factorized ( uniques , self ) return labels , uniques\n\n\n        Summary:", "input": "", "output": "Encode the extension array as an enumerated type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take ( self , indices : Sequence [ int ] , allow_fill : bool = False , fill_value : Any = None ) -> ABCExtensionArray : # Implementer note: The `fill_value` parameter should be a user-facing # value, an instance of self.dtype.type. When passed `fill_value=None`, # the default of `self.dtype.na_value` should be used. # This may differ from the physical storage type your ExtensionArray # uses. In this case, your implementation is responsible for casting # the user-facing type to the storage type, before using # pandas.api.extensions.take raise AbstractMethodError ( self )\n\n\n        Summary:", "input": "", "output": "Take elements from an array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _formatter ( self , boxed : bool = False , ) -> Callable [ [ Any ] , Optional [ str ] ] : if boxed : return str return repr\n\n\n        Summary:", "input": "", "output": "Formatting function for scalar values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reduce ( self , name , skipna = True , * * kwargs ) : raise TypeError ( \"cannot perform {name} with type {dtype}\" . format ( name = name , dtype = self . dtype ) )\n\n\n        Summary:", "input": "", "output": "Return a scalar result of performing the reduction operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_method ( cls , op , coerce_to_dtype = True ) : def _binop ( self , other ) : def convert_values ( param ) : if isinstance ( param , ExtensionArray ) or is_list_like ( param ) : ovalues = param else : # Assume its an object ovalues = [ param ] * len ( self ) return ovalues if isinstance ( other , ( ABCSeries , ABCIndexClass ) ) : # rely on pandas to unbox and dispatch to us return NotImplemented lvalues = self rvalues = convert_values ( other ) # If the operator is not defined for the underlying objects, # a TypeError should be raised res = [ op ( a , b ) for ( a , b ) in zip ( lvalues , rvalues ) ] def _maybe_convert ( arr ) : if coerce_to_dtype : # https://github.com/pandas-dev/pandas/issues/22850 # We catch all regular exceptions here, and fall back # to an ndarray. try : res = self . _from_sequence ( arr ) except Exception : res = np . asarray ( arr ) else : res = np . asarray ( arr ) return res if op . __name__ in { 'divmod' , 'rdivmod' } : a , b = zip ( * res ) res = _maybe_convert ( a ) , _maybe_convert ( b ) else : res = _maybe_convert ( res ) return res op_name = ops . _get_op_name ( op , True ) return set_function_name ( _binop , op_name , cls )\n\n\n        Summary:", "input": "", "output": "A class method that returns a method that will correspond to an operator for an ExtensionArray subclass by dispatching to the relevant operator defined on the individual elements of the ExtensionArray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ea_passthrough ( array_method ) : def method ( self , * args , * * kwargs ) : return array_method ( self . _data , * args , * * kwargs ) method . __name__ = array_method . __name__ method . __doc__ = array_method . __doc__ return method\n\n\n        Summary:", "input": "", "output": "Make an alias for a method of the underlying ExtensionArray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_comparison_method ( cls , op ) : def wrapper ( self , other ) : if isinstance ( other , ABCSeries ) : # the arrays defer to Series for comparison ops but the indexes # don't, so we have to unwrap here. other = other . _values result = op ( self . _data , maybe_unwrap_index ( other ) ) return result wrapper . __doc__ = op . __doc__ wrapper . __name__ = '__{}__' . format ( op . __name__ ) return wrapper\n\n\n        Summary:", "input": "", "output": "Create a comparison method that dispatches to cls . values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def equals ( self , other ) : if self . is_ ( other ) : return True if not isinstance ( other , ABCIndexClass ) : return False elif not isinstance ( other , type ( self ) ) : try : other = type ( self ) ( other ) except Exception : return False if not is_dtype_equal ( self . dtype , other . dtype ) : # have different timezone return False elif is_period_dtype ( self ) : if not is_period_dtype ( other ) : return False if self . freq != other . freq : return False return np . array_equal ( self . asi8 , other . asi8 )\n\n\n        Summary:", "input": "", "output": "Determines if two Index objects contain the same elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _join_i8_wrapper ( joinf , dtype , with_indexers = True ) : from pandas . core . arrays . datetimelike import DatetimeLikeArrayMixin @ staticmethod def wrapper ( left , right ) : if isinstance ( left , ( np . ndarray , ABCIndex , ABCSeries , DatetimeLikeArrayMixin ) ) : left = left . view ( 'i8' ) if isinstance ( right , ( np . ndarray , ABCIndex , ABCSeries , DatetimeLikeArrayMixin ) ) : right = right . view ( 'i8' ) results = joinf ( left , right ) if with_indexers : join_index , left_indexer , right_indexer = results join_index = join_index . view ( dtype ) return join_index , left_indexer , right_indexer return results return wrapper\n\n\n        Summary:", "input": "", "output": "Create the join wrapper methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_values ( self , return_indexer = False , ascending = True ) : if return_indexer : _as = self . argsort ( ) if not ascending : _as = _as [ : : - 1 ] sorted_index = self . take ( _as ) return sorted_index , _as else : sorted_values = np . sort ( self . _ndarray_values ) attribs = self . _get_attributes_dict ( ) freq = attribs [ 'freq' ] if freq is not None and not is_period_dtype ( self ) : if freq . n > 0 and not ascending : freq = freq * - 1 elif freq . n < 0 and ascending : freq = freq * - 1 attribs [ 'freq' ] = freq if not ascending : sorted_values = sorted_values [ : : - 1 ] return self . _simple_new ( sorted_values , * * attribs )\n\n\n        Summary:", "input": "", "output": "Return sorted copy of Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def min ( self , axis = None , skipna = True , * args , * * kwargs ) : nv . validate_min ( args , kwargs ) nv . validate_minmax_axis ( axis ) if not len ( self ) : return self . _na_value i8 = self . asi8 try : # quick check if len ( i8 ) and self . is_monotonic : if i8 [ 0 ] != iNaT : return self . _box_func ( i8 [ 0 ] ) if self . hasnans : if skipna : min_stamp = self [ ~ self . _isnan ] . asi8 . min ( ) else : return self . _na_value else : min_stamp = i8 . min ( ) return self . _box_func ( min_stamp ) except ValueError : return self . _na_value\n\n\n        Summary:", "input": "", "output": "Return the minimum value of the Index or minimum along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argmin ( self , axis = None , skipna = True , * args , * * kwargs ) : nv . validate_argmin ( args , kwargs ) nv . validate_minmax_axis ( axis ) i8 = self . asi8 if self . hasnans : mask = self . _isnan if mask . all ( ) or not skipna : return - 1 i8 = i8 . copy ( ) i8 [ mask ] = np . iinfo ( 'int64' ) . max return i8 . argmin ( )\n\n\n        Summary:", "input": "", "output": "Returns the indices of the minimum values along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def max ( self , axis = None , skipna = True , * args , * * kwargs ) : nv . validate_max ( args , kwargs ) nv . validate_minmax_axis ( axis ) if not len ( self ) : return self . _na_value i8 = self . asi8 try : # quick check if len ( i8 ) and self . is_monotonic : if i8 [ - 1 ] != iNaT : return self . _box_func ( i8 [ - 1 ] ) if self . hasnans : if skipna : max_stamp = self [ ~ self . _isnan ] . asi8 . max ( ) else : return self . _na_value else : max_stamp = i8 . max ( ) return self . _box_func ( max_stamp ) except ValueError : return self . _na_value\n\n\n        Summary:", "input": "", "output": "Return the maximum value of the Index or maximum along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argmax ( self , axis = None , skipna = True , * args , * * kwargs ) : nv . validate_argmax ( args , kwargs ) nv . validate_minmax_axis ( axis ) i8 = self . asi8 if self . hasnans : mask = self . _isnan if mask . all ( ) or not skipna : return - 1 i8 = i8 . copy ( ) i8 [ mask ] = 0 return i8 . argmax ( )\n\n\n        Summary:", "input": "", "output": "Returns the indices of the maximum values along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_attrs ( self ) : attrs = super ( ) . _format_attrs ( ) for attrib in self . _attributes : if attrib == 'freq' : freq = self . freqstr if freq is not None : freq = \"'%s'\" % freq attrs . append ( ( 'freq' , freq ) ) return attrs\n\n\n        Summary:", "input": "", "output": "Return a list of tuples of the ( attr formatted_value ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert_scalar_indexer ( self , key , kind = None ) : assert kind in [ 'ix' , 'loc' , 'getitem' , 'iloc' , None ] # we don't allow integer/float indexing for loc # we don't allow float indexing for ix/getitem if is_scalar ( key ) : is_int = is_integer ( key ) is_flt = is_float ( key ) if kind in [ 'loc' ] and ( is_int or is_flt ) : self . _invalid_indexer ( 'index' , key ) elif kind in [ 'ix' , 'getitem' ] and is_flt : self . _invalid_indexer ( 'index' , key ) return super ( ) . _convert_scalar_indexer ( key , kind = kind )\n\n\n        Summary:", "input": "", "output": "We don t allow integer or float indexing on datetime - like when using loc ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_datetimelike_methods ( cls ) : def __add__ ( self , other ) : # dispatch to ExtensionArray implementation result = self . _data . __add__ ( maybe_unwrap_index ( other ) ) return wrap_arithmetic_op ( self , other , result ) cls . __add__ = __add__ def __radd__ ( self , other ) : # alias for __add__ return self . __add__ ( other ) cls . __radd__ = __radd__ def __sub__ ( self , other ) : # dispatch to ExtensionArray implementation result = self . _data . __sub__ ( maybe_unwrap_index ( other ) ) return wrap_arithmetic_op ( self , other , result ) cls . __sub__ = __sub__ def __rsub__ ( self , other ) : result = self . _data . __rsub__ ( maybe_unwrap_index ( other ) ) return wrap_arithmetic_op ( self , other , result ) cls . __rsub__ = __rsub__\n\n\n        Summary:", "input": "", "output": "Add in the datetimelike methods ( as we may have to override the superclass ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isin ( self , values ) : if not isinstance ( values , type ( self ) ) : try : values = type ( self ) ( values ) except ValueError : return self . astype ( object ) . isin ( values ) return algorithms . isin ( self . asi8 , values . asi8 )\n\n\n        Summary:", "input": "", "output": "Compute boolean array of whether each index value is found in the passed set of values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _summary ( self , name = None ) : formatter = self . _formatter_func if len ( self ) > 0 : index_summary = ', %s to %s' % ( formatter ( self [ 0 ] ) , formatter ( self [ - 1 ] ) ) else : index_summary = '' if name is None : name = type ( self ) . __name__ result = '%s: %s entries%s' % ( printing . pprint_thing ( name ) , len ( self ) , index_summary ) if self . freq : result += '\\nFreq: %s' % self . freqstr # display as values, not quoted result = result . replace ( \"'\" , \"\" ) return result\n\n\n        Summary:", "input": "", "output": "Return a summarized representation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _concat_same_dtype ( self , to_concat , name ) : attribs = self . _get_attributes_dict ( ) attribs [ 'name' ] = name # do not pass tz to set because tzlocal cannot be hashed if len ( { str ( x . dtype ) for x in to_concat } ) != 1 : raise ValueError ( 'to_concat must have the same tz' ) new_data = type ( self . _values ) . _concat_same_type ( to_concat ) . asi8 # GH 3232: If the concat result is evenly spaced, we can retain the # original frequency is_diff_evenly_spaced = len ( unique_deltas ( new_data ) ) == 1 if not is_period_dtype ( self ) and not is_diff_evenly_spaced : # reset freq attribs [ 'freq' ] = None return self . _simple_new ( new_data , * * attribs )\n\n\n        Summary:", "input": "", "output": "Concatenate to_concat which has the same class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shift ( self , periods , freq = None ) : result = self . _data . _time_shift ( periods , freq = freq ) return type ( self ) ( result , name = self . name )\n\n\n        Summary:", "input": "", "output": "Shift index by desired number of time frequency increments ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _single_replace ( self , to_replace , method , inplace , limit ) : if self . ndim != 1 : raise TypeError ( 'cannot replace {0} with method {1} on a {2}' . format ( to_replace , method , type ( self ) . __name__ ) ) orig_dtype = self . dtype result = self if inplace else self . copy ( ) fill_f = missing . get_fill_func ( method ) mask = missing . mask_missing ( result . values , to_replace ) values = fill_f ( result . values , limit = limit , mask = mask ) if values . dtype == orig_dtype and inplace : return result = pd . Series ( values , index = self . index , dtype = self . dtype ) . __finalize__ ( self ) if inplace : self . _update_inplace ( result . _data ) return return result\n\n\n        Summary:", "input": "", "output": "Replaces values in a Series using the fill method specified when no replacement value is given in the replace method"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _doc_parms ( cls ) : axis_descr = \"{%s}\" % ', ' . join ( \"{0} ({1})\" . format ( a , i ) for i , a in enumerate ( cls . _AXIS_ORDERS ) ) name = ( cls . _constructor_sliced . __name__ if cls . _AXIS_LEN > 1 else 'scalar' ) name2 = cls . __name__ return axis_descr , name , name2\n\n\n        Summary:", "input": "", "output": "Return a tuple of the doc parms ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _init_mgr ( self , mgr , axes = None , dtype = None , copy = False ) : for a , axe in axes . items ( ) : if axe is not None : mgr = mgr . reindex_axis ( axe , axis = self . _get_block_manager_axis ( a ) , copy = False ) # make a copy if explicitly requested if copy : mgr = mgr . copy ( ) if dtype is not None : # avoid further copies if we can if len ( mgr . blocks ) > 1 or mgr . blocks [ 0 ] . values . dtype != dtype : mgr = mgr . astype ( dtype = dtype ) return mgr\n\n\n        Summary:", "input": "", "output": "passed a manager and a axes dict"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _validate_dtype ( self , dtype ) : if dtype is not None : dtype = pandas_dtype ( dtype ) # a compound dtype if dtype . kind == 'V' : raise NotImplementedError ( \"compound dtypes are not implemented\" \" in the {0} constructor\" . format ( self . __class__ . __name__ ) ) return dtype\n\n\n        Summary:", "input": "", "output": "validate the passed dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _setup_axes ( cls , axes , info_axis = None , stat_axis = None , aliases = None , slicers = None , axes_are_reversed = False , build_axes = True , ns = None , docs = None ) : cls . _AXIS_ORDERS = axes cls . _AXIS_NUMBERS = { a : i for i , a in enumerate ( axes ) } cls . _AXIS_LEN = len ( axes ) cls . _AXIS_ALIASES = aliases or dict ( ) cls . _AXIS_IALIASES = { v : k for k , v in cls . _AXIS_ALIASES . items ( ) } cls . _AXIS_NAMES = dict ( enumerate ( axes ) ) cls . _AXIS_SLICEMAP = slicers or None cls . _AXIS_REVERSED = axes_are_reversed # typ setattr ( cls , '_typ' , cls . __name__ . lower ( ) ) # indexing support cls . _ix = None if info_axis is not None : cls . _info_axis_number = info_axis cls . _info_axis_name = axes [ info_axis ] if stat_axis is not None : cls . _stat_axis_number = stat_axis cls . _stat_axis_name = axes [ stat_axis ] # setup the actual axis if build_axes : def set_axis ( a , i ) : setattr ( cls , a , properties . AxisProperty ( i , docs . get ( a , a ) ) ) cls . _internal_names_set . add ( a ) if axes_are_reversed : m = cls . _AXIS_LEN - 1 for i , a in cls . _AXIS_NAMES . items ( ) : set_axis ( a , m - i ) else : for i , a in cls . _AXIS_NAMES . items ( ) : set_axis ( a , i ) assert not isinstance ( ns , dict )\n\n\n        Summary:", "input": "", "output": "Provide axes setup for the major PandasObjects ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_axes_dict ( self , axes = None , * * kwargs ) : d = { a : self . _get_axis ( a ) for a in ( axes or self . _AXIS_ORDERS ) } d . update ( kwargs ) return d\n\n\n        Summary:", "input": "", "output": "Return an axes dictionary for myself ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_axes_dict_from ( self , axes , * * kwargs ) : d = { a : ax for a , ax in zip ( self . _AXIS_ORDERS , axes ) } d . update ( kwargs ) return d\n\n\n        Summary:", "input": "", "output": "Return an axes dictionary for the passed axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_axes_dict_for_slice ( self , axes = None , * * kwargs ) : d = { self . _AXIS_SLICEMAP [ a ] : self . _get_axis ( a ) for a in ( axes or self . _AXIS_ORDERS ) } d . update ( kwargs ) return d\n\n\n        Summary:", "input": "", "output": "Return an axes dictionary for myself ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _construct_axes_from_arguments ( self , args , kwargs , require_all = False , sentinel = None ) : # construct the args args = list ( args ) for a in self . _AXIS_ORDERS : # if we have an alias for this axis alias = self . _AXIS_IALIASES . get ( a ) if alias is not None : if a in kwargs : if alias in kwargs : raise TypeError ( \"arguments are mutually exclusive \" \"for [%s,%s]\" % ( a , alias ) ) continue if alias in kwargs : kwargs [ a ] = kwargs . pop ( alias ) continue # look for a argument by position if a not in kwargs : try : kwargs [ a ] = args . pop ( 0 ) except IndexError : if require_all : raise TypeError ( \"not enough/duplicate arguments \" \"specified!\" ) axes = { a : kwargs . pop ( a , sentinel ) for a in self . _AXIS_ORDERS } return axes , kwargs\n\n\n        Summary:", "input": "", "output": "Construct and returns axes if supplied in args / kwargs ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_block_manager_axis ( cls , axis ) : axis = cls . _get_axis_number ( axis ) if cls . _AXIS_REVERSED : m = cls . _AXIS_LEN - 1 return m - axis return axis\n\n\n        Summary:", "input": "", "output": "Map the axis to the block_manager axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_space_character_free_column_resolvers ( self ) : from pandas . core . computation . common import _remove_spaces_column_name return { _remove_spaces_column_name ( k ) : v for k , v in self . iteritems ( ) }\n\n\n        Summary:", "input": "", "output": "Return the space character free column resolvers of a dataframe ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def shape ( self ) : return tuple ( len ( self . _get_axis ( a ) ) for a in self . _AXIS_ORDERS )\n\n\n        Summary:", "input": "", "output": "Return a tuple of axis dimensions"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def transpose ( self , * args , * * kwargs ) : # construct the args axes , kwargs = self . _construct_axes_from_arguments ( args , kwargs , require_all = True ) axes_names = tuple ( self . _get_axis_name ( axes [ a ] ) for a in self . _AXIS_ORDERS ) axes_numbers = tuple ( self . _get_axis_number ( axes [ a ] ) for a in self . _AXIS_ORDERS ) # we must have unique axes if len ( axes ) != len ( set ( axes ) ) : raise ValueError ( 'Must specify %s unique axes' % self . _AXIS_LEN ) new_axes = self . _construct_axes_dict_from ( self , [ self . _get_axis ( x ) for x in axes_names ] ) new_values = self . values . transpose ( axes_numbers ) if kwargs . pop ( 'copy' , None ) or ( len ( args ) and args [ - 1 ] ) : new_values = new_values . copy ( ) nv . validate_transpose_for_generic ( self , kwargs ) return self . _constructor ( new_values , * * new_axes ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Permute the dimensions of the % ( klass ) s"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def swapaxes ( self , axis1 , axis2 , copy = True ) : i = self . _get_axis_number ( axis1 ) j = self . _get_axis_number ( axis2 ) if i == j : if copy : return self . copy ( ) return self mapping = { i : j , j : i } new_axes = ( self . _get_axis ( mapping . get ( k , k ) ) for k in range ( self . _AXIS_LEN ) ) new_values = self . values . swapaxes ( i , j ) if copy : new_values = new_values . copy ( ) return self . _constructor ( new_values , * new_axes ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Interchange axes and swap values axes appropriately ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def droplevel ( self , level , axis = 0 ) : labels = self . _get_axis ( axis ) new_labels = labels . droplevel ( level ) result = self . set_axis ( new_labels , axis = axis , inplace = False ) return result\n\n\n        Summary:", "input": "", "output": "Return DataFrame with requested index / column level ( s ) removed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def pop ( self , item ) : result = self [ item ] del self [ item ] try : result . _reset_cacher ( ) except AttributeError : pass return result\n\n\n        Summary:", "input": "", "output": "Return item and drop from frame . Raise KeyError if not found ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def squeeze ( self , axis = None ) : axis = ( self . _AXIS_NAMES if axis is None else ( self . _get_axis_number ( axis ) , ) ) try : return self . iloc [ tuple ( 0 if i in axis and len ( a ) == 1 else slice ( None ) for i , a in enumerate ( self . axes ) ) ] except Exception : return self\n\n\n        Summary:", "input": "", "output": "Squeeze 1 dimensional axis objects into scalars ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def swaplevel ( self , i = - 2 , j = - 1 , axis = 0 ) : axis = self . _get_axis_number ( axis ) result = self . copy ( ) labels = result . _data . axes [ axis ] result . _data . set_axis ( axis , labels . swaplevel ( i , j ) ) return result\n\n\n        Summary:", "input": "", "output": "Swap levels i and j in a MultiIndex on a particular axis"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename ( self , * args , * * kwargs ) : axes , kwargs = self . _construct_axes_from_arguments ( args , kwargs ) copy = kwargs . pop ( 'copy' , True ) inplace = kwargs . pop ( 'inplace' , False ) level = kwargs . pop ( 'level' , None ) axis = kwargs . pop ( 'axis' , None ) errors = kwargs . pop ( 'errors' , 'ignore' ) if axis is not None : # Validate the axis self . _get_axis_number ( axis ) if kwargs : raise TypeError ( 'rename() got an unexpected keyword ' 'argument \"{0}\"' . format ( list ( kwargs . keys ( ) ) [ 0 ] ) ) if com . count_not_none ( * axes . values ( ) ) == 0 : raise TypeError ( 'must pass an index to rename' ) self . _consolidate_inplace ( ) result = self if inplace else self . copy ( deep = copy ) # start in the axis order to eliminate too many copies for axis in lrange ( self . _AXIS_LEN ) : v = axes . get ( self . _AXIS_NAMES [ axis ] ) if v is None : continue f = com . _get_rename_function ( v ) baxis = self . _get_block_manager_axis ( axis ) if level is not None : level = self . axes [ axis ] . _get_level_number ( level ) # GH 13473 if not callable ( v ) : indexer = self . axes [ axis ] . get_indexer_for ( v ) if errors == 'raise' and len ( indexer [ indexer == - 1 ] ) : missing_labels = [ label for index , label in enumerate ( v ) if indexer [ index ] == - 1 ] raise KeyError ( '{} not found in axis' . format ( missing_labels ) ) result . _data = result . _data . rename_axis ( f , axis = baxis , copy = copy , level = level ) result . _clear_item_cache ( ) if inplace : self . _update_inplace ( result . _data ) else : return result . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Alter axes input function or functions . Function / dict values must be unique ( 1 - to - 1 ) . Labels not contained in a dict / Series will be left as - is . Extra labels listed don t throw an error . Alternatively change Series . name with a scalar value ( Series only ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename_axis ( self , mapper = sentinel , * * kwargs ) : axes , kwargs = self . _construct_axes_from_arguments ( ( ) , kwargs , sentinel = sentinel ) copy = kwargs . pop ( 'copy' , True ) inplace = kwargs . pop ( 'inplace' , False ) axis = kwargs . pop ( 'axis' , 0 ) if axis is not None : axis = self . _get_axis_number ( axis ) if kwargs : raise TypeError ( 'rename_axis() got an unexpected keyword ' 'argument \"{0}\"' . format ( list ( kwargs . keys ( ) ) [ 0 ] ) ) inplace = validate_bool_kwarg ( inplace , 'inplace' ) if ( mapper is not sentinel ) : # Use v0.23 behavior if a scalar or list non_mapper = is_scalar ( mapper ) or ( is_list_like ( mapper ) and not is_dict_like ( mapper ) ) if non_mapper : return self . _set_axis_name ( mapper , axis = axis , inplace = inplace ) else : # Deprecated (v0.21) behavior is if mapper is specified, # and not a list or scalar, then call rename msg = ( \"Using 'rename_axis' to alter labels is deprecated. \" \"Use '.rename' instead\" ) warnings . warn ( msg , FutureWarning , stacklevel = 3 ) axis = self . _get_axis_name ( axis ) d = { 'copy' : copy , 'inplace' : inplace } d [ axis ] = mapper return self . rename ( * * d ) else : # Use new behavior. Means that index and/or columns # is specified result = self if inplace else self . copy ( deep = copy ) for axis in lrange ( self . _AXIS_LEN ) : v = axes . get ( self . _AXIS_NAMES [ axis ] ) if v is sentinel : continue non_mapper = is_scalar ( v ) or ( is_list_like ( v ) and not is_dict_like ( v ) ) if non_mapper : newnames = v else : f = com . _get_rename_function ( v ) curnames = self . _get_axis ( axis ) . names newnames = [ f ( name ) for name in curnames ] result . _set_axis_name ( newnames , axis = axis , inplace = True ) if not inplace : return result\n\n\n        Summary:", "input": "", "output": "Set the name of the axis for the index or columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_axis_name ( self , name , axis = 0 , inplace = False ) : axis = self . _get_axis_number ( axis ) idx = self . _get_axis ( axis ) . set_names ( name ) inplace = validate_bool_kwarg ( inplace , 'inplace' ) renamed = self if inplace else self . copy ( ) renamed . set_axis ( idx , axis = axis , inplace = True ) if not inplace : return renamed\n\n\n        Summary:", "input": "", "output": "Set the name ( s ) of the axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def equals ( self , other ) : if not isinstance ( other , self . _constructor ) : return False return self . _data . equals ( other . _data )\n\n\n        Summary:", "input": "", "output": "Test whether two objects contain the same elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def bool ( self ) : v = self . squeeze ( ) if isinstance ( v , ( bool , np . bool_ ) ) : return bool ( v ) elif is_scalar ( v ) : raise ValueError ( \"bool cannot act on a non-boolean single element \" \"{0}\" . format ( self . __class__ . __name__ ) ) self . __nonzero__ ( )\n\n\n        Summary:", "input": "", "output": "Return the bool of a single element PandasObject ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_level_reference ( self , key , axis = 0 ) : axis = self . _get_axis_number ( axis ) if self . ndim > 2 : raise NotImplementedError ( \"_is_level_reference is not implemented for {type}\" . format ( type = type ( self ) ) ) return ( key is not None and is_hashable ( key ) and key in self . axes [ axis ] . names and not self . _is_label_reference ( key , axis = axis ) )\n\n\n        Summary:", "input": "", "output": "Test whether a key is a level reference for a given axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_label_reference ( self , key , axis = 0 ) : if self . ndim > 2 : raise NotImplementedError ( \"_is_label_reference is not implemented for {type}\" . format ( type = type ( self ) ) ) axis = self . _get_axis_number ( axis ) other_axes = ( ax for ax in range ( self . _AXIS_LEN ) if ax != axis ) return ( key is not None and is_hashable ( key ) and any ( key in self . axes [ ax ] for ax in other_axes ) )\n\n\n        Summary:", "input": "", "output": "Test whether a key is a label reference for a given axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_label_or_level_reference ( self , key , axis = 0 ) : if self . ndim > 2 : raise NotImplementedError ( \"_is_label_or_level_reference is not implemented for {type}\" . format ( type = type ( self ) ) ) return ( self . _is_level_reference ( key , axis = axis ) or self . _is_label_reference ( key , axis = axis ) )\n\n\n        Summary:", "input": "", "output": "Test whether a key is a label or level reference for a given axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_label_or_level_ambiguity ( self , key , axis = 0 ) : if self . ndim > 2 : raise NotImplementedError ( \"_check_label_or_level_ambiguity is not implemented for {type}\" . format ( type = type ( self ) ) ) axis = self . _get_axis_number ( axis ) other_axes = ( ax for ax in range ( self . _AXIS_LEN ) if ax != axis ) if ( key is not None and is_hashable ( key ) and key in self . axes [ axis ] . names and any ( key in self . axes [ ax ] for ax in other_axes ) ) : # Build an informative and grammatical warning level_article , level_type = ( ( 'an' , 'index' ) if axis == 0 else ( 'a' , 'column' ) ) label_article , label_type = ( ( 'a' , 'column' ) if axis == 0 else ( 'an' , 'index' ) ) msg = ( \"'{key}' is both {level_article} {level_type} level and \" \"{label_article} {label_type} label, which is ambiguous.\" ) . format ( key = key , level_article = level_article , level_type = level_type , label_article = label_article , label_type = label_type ) raise ValueError ( msg )\n\n\n        Summary:", "input": "", "output": "Check whether key is ambiguous ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_label_or_level_values ( self , key , axis = 0 ) : if self . ndim > 2 : raise NotImplementedError ( \"_get_label_or_level_values is not implemented for {type}\" . format ( type = type ( self ) ) ) axis = self . _get_axis_number ( axis ) other_axes = [ ax for ax in range ( self . _AXIS_LEN ) if ax != axis ] if self . _is_label_reference ( key , axis = axis ) : self . _check_label_or_level_ambiguity ( key , axis = axis ) values = self . xs ( key , axis = other_axes [ 0 ] ) . _values elif self . _is_level_reference ( key , axis = axis ) : values = self . axes [ axis ] . get_level_values ( key ) . _values else : raise KeyError ( key ) # Check for duplicates if values . ndim > 1 : if other_axes and isinstance ( self . _get_axis ( other_axes [ 0 ] ) , MultiIndex ) : multi_message = ( '\\n' 'For a multi-index, the label must be a ' 'tuple with elements corresponding to ' 'each level.' ) else : multi_message = '' label_axis_name = 'column' if axis == 0 else 'index' raise ValueError ( ( \"The {label_axis_name} label '{key}' \" \"is not unique.{multi_message}\" ) . format ( key = key , label_axis_name = label_axis_name , multi_message = multi_message ) ) return values\n\n\n        Summary:", "input": "", "output": "Return a 1 - D array of values associated with key a label or level from the given axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _drop_labels_or_levels ( self , keys , axis = 0 ) : if self . ndim > 2 : raise NotImplementedError ( \"_drop_labels_or_levels is not implemented for {type}\" . format ( type = type ( self ) ) ) axis = self . _get_axis_number ( axis ) # Validate keys keys = com . maybe_make_list ( keys ) invalid_keys = [ k for k in keys if not self . _is_label_or_level_reference ( k , axis = axis ) ] if invalid_keys : raise ValueError ( ( \"The following keys are not valid labels or \" \"levels for axis {axis}: {invalid_keys}\" ) . format ( axis = axis , invalid_keys = invalid_keys ) ) # Compute levels and labels to drop levels_to_drop = [ k for k in keys if self . _is_level_reference ( k , axis = axis ) ] labels_to_drop = [ k for k in keys if not self . _is_level_reference ( k , axis = axis ) ] # Perform copy upfront and then use inplace operations below. # This ensures that we always perform exactly one copy. # ``copy`` and/or ``inplace`` options could be added in the future. dropped = self . copy ( ) if axis == 0 : # Handle dropping index levels if levels_to_drop : dropped . reset_index ( levels_to_drop , drop = True , inplace = True ) # Handle dropping columns labels if labels_to_drop : dropped . drop ( labels_to_drop , axis = 1 , inplace = True ) else : # Handle dropping column levels if levels_to_drop : if isinstance ( dropped . columns , MultiIndex ) : # Drop the specified levels from the MultiIndex dropped . columns = dropped . columns . droplevel ( levels_to_drop ) else : # Drop the last level of Index by replacing with # a RangeIndex dropped . columns = RangeIndex ( dropped . columns . size ) # Handle dropping index labels if labels_to_drop : dropped . drop ( labels_to_drop , axis = 0 , inplace = True ) return dropped\n\n\n        Summary:", "input": "", "output": "Drop labels and / or levels for the given axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def empty ( self ) : return any ( len ( self . _get_axis ( a ) ) == 0 for a in self . _AXIS_ORDERS )\n\n\n        Summary:", "input": "", "output": "Indicator whether DataFrame is empty ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _repr_data_resource_ ( self ) : if config . get_option ( \"display.html.table_schema\" ) : data = self . head ( config . get_option ( 'display.max_rows' ) ) payload = json . loads ( data . to_json ( orient = 'table' ) , object_pairs_hook = collections . OrderedDict ) return payload\n\n\n        Summary:", "input": "", "output": "Not a real Jupyter special repr method but we use the same naming convention ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_json ( self , path_or_buf = None , orient = None , date_format = None , double_precision = 10 , force_ascii = True , date_unit = 'ms' , default_handler = None , lines = False , compression = 'infer' , index = True ) : from pandas . io import json if date_format is None and orient == 'table' : date_format = 'iso' elif date_format is None : date_format = 'epoch' return json . to_json ( path_or_buf = path_or_buf , obj = self , orient = orient , date_format = date_format , double_precision = double_precision , force_ascii = force_ascii , date_unit = date_unit , default_handler = default_handler , lines = lines , compression = compression , index = index )\n\n\n        Summary:", "input": "", "output": "Convert the object to a JSON string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_hdf ( self , path_or_buf , key , * * kwargs ) : from pandas . io import pytables return pytables . to_hdf ( path_or_buf , key , self , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Write the contained data to an HDF5 file using HDFStore ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_msgpack ( self , path_or_buf = None , encoding = 'utf-8' , * * kwargs ) : from pandas . io import packers return packers . to_msgpack ( path_or_buf , self , encoding = encoding , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Serialize object to input file path using msgpack format ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_sql ( self , name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None ) : from pandas . io import sql sql . to_sql ( self , name , con , schema = schema , if_exists = if_exists , index = index , index_label = index_label , chunksize = chunksize , dtype = dtype , method = method )\n\n\n        Summary:", "input": "", "output": "Write records stored in a DataFrame to a SQL database ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_pickle ( self , path , compression = 'infer' , protocol = pickle . HIGHEST_PROTOCOL ) : from pandas . io . pickle import to_pickle return to_pickle ( self , path , compression = compression , protocol = protocol )\n\n\n        Summary:", "input": "", "output": "Pickle ( serialize ) object to file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_clipboard ( self , excel = True , sep = None , * * kwargs ) : from pandas . io import clipboards clipboards . to_clipboard ( self , excel = excel , sep = sep , * * kwargs )\n\n\n        Summary:", "input": "", "output": "r Copy object to the system clipboard ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_xarray ( self ) : try : import xarray except ImportError : # Give a nice error message raise ImportError ( \"the xarray library is not installed\\n\" \"you can install via conda\\n\" \"conda install xarray\\n\" \"or via pip\\n\" \"pip install xarray\\n\" ) if self . ndim == 1 : return xarray . DataArray . from_series ( self ) elif self . ndim == 2 : return xarray . Dataset . from_dataframe ( self ) # > 2 dims coords = [ ( a , self . _get_axis ( a ) ) for a in self . _AXIS_ORDERS ] return xarray . DataArray ( self , coords = coords , )\n\n\n        Summary:", "input": "", "output": "Return an xarray object from the pandas object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_latex ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , bold_rows = False , column_format = None , longtable = None , escape = None , encoding = None , decimal = '.' , multicolumn = None , multicolumn_format = None , multirow = None ) : # Get defaults from the pandas config if self . ndim == 1 : self = self . to_frame ( ) if longtable is None : longtable = config . get_option ( \"display.latex.longtable\" ) if escape is None : escape = config . get_option ( \"display.latex.escape\" ) if multicolumn is None : multicolumn = config . get_option ( \"display.latex.multicolumn\" ) if multicolumn_format is None : multicolumn_format = config . get_option ( \"display.latex.multicolumn_format\" ) if multirow is None : multirow = config . get_option ( \"display.latex.multirow\" ) formatter = DataFrameFormatter ( self , buf = buf , columns = columns , col_space = col_space , na_rep = na_rep , header = header , index = index , formatters = formatters , float_format = float_format , bold_rows = bold_rows , sparsify = sparsify , index_names = index_names , escape = escape , decimal = decimal ) formatter . to_latex ( column_format = column_format , longtable = longtable , encoding = encoding , multicolumn = multicolumn , multicolumn_format = multicolumn_format , multirow = multirow ) if buf is None : return formatter . buf . getvalue ( )\n\n\n        Summary:", "input": "", "output": "r Render an object to a LaTeX tabular environment table ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_csv ( self , path_or_buf = None , sep = \",\" , na_rep = '' , float_format = None , columns = None , header = True , index = True , index_label = None , mode = 'w' , encoding = None , compression = 'infer' , quoting = None , quotechar = '\"' , line_terminator = None , chunksize = None , tupleize_cols = None , date_format = None , doublequote = True , escapechar = None , decimal = '.' ) : df = self if isinstance ( self , ABCDataFrame ) else self . to_frame ( ) if tupleize_cols is not None : warnings . warn ( \"The 'tupleize_cols' parameter is deprecated and \" \"will be removed in a future version\" , FutureWarning , stacklevel = 2 ) else : tupleize_cols = False from pandas . io . formats . csvs import CSVFormatter formatter = CSVFormatter ( df , path_or_buf , line_terminator = line_terminator , sep = sep , encoding = encoding , compression = compression , quoting = quoting , na_rep = na_rep , float_format = float_format , cols = columns , header = header , index = index , index_label = index_label , mode = mode , chunksize = chunksize , quotechar = quotechar , tupleize_cols = tupleize_cols , date_format = date_format , doublequote = doublequote , escapechar = escapechar , decimal = decimal ) formatter . save ( ) if path_or_buf is None : return formatter . path_or_buf . getvalue ( )\n\n\n        Summary:", "input": "", "output": "r Write object to a comma - separated values ( csv ) file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _create_indexer ( cls , name , indexer ) : if getattr ( cls , name , None ) is None : _indexer = functools . partial ( indexer , name ) setattr ( cls , name , property ( _indexer , doc = indexer . __doc__ ) )\n\n\n        Summary:", "input": "", "output": "Create an indexer like _name in the class ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get ( self , key , default = None ) : try : return self [ key ] except ( KeyError , ValueError , IndexError ) : return default\n\n\n        Summary:", "input": "", "output": "Get item from object for given key ( DataFrame column Panel slice etc . ) . Returns default value if not found ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_item_cache ( self , item ) : cache = self . _item_cache res = cache . get ( item ) if res is None : values = self . _data . get ( item ) res = self . _box_item_values ( item , values ) cache [ item ] = res res . _set_as_cached ( item , self ) # for a chain res . _is_copy = self . _is_copy return res\n\n\n        Summary:", "input": "", "output": "Return the cached item item represents a label indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_as_cached ( self , item , cacher ) : self . _cacher = ( item , weakref . ref ( cacher ) )\n\n\n        Summary:", "input": "", "output": "Set the _cacher attribute on the calling object with a weakref to cacher ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _iget_item_cache ( self , item ) : ax = self . _info_axis if ax . is_unique : lower = self . _get_item_cache ( ax [ item ] ) else : lower = self . _take ( item , axis = self . _info_axis_number ) return lower\n\n\n        Summary:", "input": "", "output": "Return the cached item item represents a positional indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_update_cacher ( self , clear = False , verify_is_copy = True ) : cacher = getattr ( self , '_cacher' , None ) if cacher is not None : ref = cacher [ 1 ] ( ) # we are trying to reference a dead referant, hence # a copy if ref is None : del self . _cacher else : try : ref . _maybe_cache_changed ( cacher [ 0 ] , self ) except Exception : pass if verify_is_copy : self . _check_setitem_copy ( stacklevel = 5 , t = 'referant' ) if clear : self . _clear_item_cache ( )\n\n\n        Summary:", "input": "", "output": "See if we need to update our parent cacher if clear then clear our cache ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _slice ( self , slobj , axis = 0 , kind = None ) : axis = self . _get_block_manager_axis ( axis ) result = self . _constructor ( self . _data . get_slice ( slobj , axis = axis ) ) result = result . __finalize__ ( self ) # this could be a view # but only in a single-dtyped view slicable case is_copy = axis != 0 or result . _is_view result . _set_is_copy ( self , copy = is_copy ) return result\n\n\n        Summary:", "input": "", "output": "Construct a slice of this container ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_is_chained_assignment_possible ( self ) : if self . _is_view and self . _is_cached : ref = self . _get_cacher ( ) if ref is not None and ref . _is_mixed_type : self . _check_setitem_copy ( stacklevel = 4 , t = 'referant' , force = True ) return True elif self . _is_copy : self . _check_setitem_copy ( stacklevel = 4 , t = 'referant' ) return False\n\n\n        Summary:", "input": "", "output": "Check if we are a view have a cacher and are of mixed type . If so then force a setitem_copy check ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _take ( self , indices , axis = 0 , is_copy = True ) : self . _consolidate_inplace ( ) new_data = self . _data . take ( indices , axis = self . _get_block_manager_axis ( axis ) , verify = True ) result = self . _constructor ( new_data ) . __finalize__ ( self ) # Maybe set copy if we didn't actually change the index. if is_copy : if not result . _get_axis ( axis ) . equals ( self . _get_axis ( axis ) ) : result . _set_is_copy ( self ) return result\n\n\n        Summary:", "input": "", "output": "Return the elements in the given * positional * indices along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def take ( self , indices , axis = 0 , convert = None , is_copy = True , * * kwargs ) : if convert is not None : msg = ( \"The 'convert' parameter is deprecated \" \"and will be removed in a future version.\" ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) nv . validate_take ( tuple ( ) , kwargs ) return self . _take ( indices , axis = axis , is_copy = is_copy )\n\n\n        Summary:", "input": "", "output": "Return the elements in the given * positional * indices along an axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def xs ( self , key , axis = 0 , level = None , drop_level = True ) : axis = self . _get_axis_number ( axis ) labels = self . _get_axis ( axis ) if level is not None : loc , new_ax = labels . get_loc_level ( key , level = level , drop_level = drop_level ) # create the tuple of the indexer indexer = [ slice ( None ) ] * self . ndim indexer [ axis ] = loc indexer = tuple ( indexer ) result = self . iloc [ indexer ] setattr ( result , result . _get_axis_name ( axis ) , new_ax ) return result if axis == 1 : return self [ key ] self . _consolidate_inplace ( ) index = self . index if isinstance ( index , MultiIndex ) : loc , new_index = self . index . get_loc_level ( key , drop_level = drop_level ) else : loc = self . index . get_loc ( key ) if isinstance ( loc , np . ndarray ) : if loc . dtype == np . bool_ : inds , = loc . nonzero ( ) return self . _take ( inds , axis = axis ) else : return self . _take ( loc , axis = axis ) if not is_scalar ( loc ) : new_index = self . index [ loc ] if is_scalar ( loc ) : new_values = self . _data . fast_xs ( loc ) # may need to box a datelike-scalar # # if we encounter an array-like and we only have 1 dim # that means that their are list/ndarrays inside the Series! # so just return them (GH 6394) if not is_list_like ( new_values ) or self . ndim == 1 : return com . maybe_box_datetimelike ( new_values ) result = self . _constructor_sliced ( new_values , index = self . columns , name = self . index [ loc ] , dtype = new_values . dtype ) else : result = self . iloc [ loc ] result . index = new_index # this could be a view # but only in a single-dtyped view slicable case result . _set_is_copy ( self , copy = not result . _is_view ) return result\n\n\n        Summary:", "input": "", "output": "Return cross - section from the Series / DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def select ( self , crit , axis = 0 ) : warnings . warn ( \"'select' is deprecated and will be removed in a \" \"future release. You can use \" \".loc[labels.map(crit)] as a replacement\" , FutureWarning , stacklevel = 2 ) axis = self . _get_axis_number ( axis ) axis_name = self . _get_axis_name ( axis ) axis_values = self . _get_axis ( axis ) if len ( axis_values ) > 0 : new_axis = axis_values [ np . asarray ( [ bool ( crit ( label ) ) for label in axis_values ] ) ] else : new_axis = axis_values return self . reindex ( * * { axis_name : new_axis } )\n\n\n        Summary:", "input": "", "output": "Return data corresponding to axis labels matching criteria ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reindex_like ( self , other , method = None , copy = True , limit = None , tolerance = None ) : d = other . _construct_axes_dict ( axes = self . _AXIS_ORDERS , method = method , copy = copy , limit = limit , tolerance = tolerance ) return self . reindex ( * * d )\n\n\n        Summary:", "input": "", "output": "Return an object with matching indices as other object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _drop_axis ( self , labels , axis , level = None , errors = 'raise' ) : axis = self . _get_axis_number ( axis ) axis_name = self . _get_axis_name ( axis ) axis = self . _get_axis ( axis ) if axis . is_unique : if level is not None : if not isinstance ( axis , MultiIndex ) : raise AssertionError ( 'axis must be a MultiIndex' ) new_axis = axis . drop ( labels , level = level , errors = errors ) else : new_axis = axis . drop ( labels , errors = errors ) result = self . reindex ( * * { axis_name : new_axis } ) # Case for non-unique axis else : labels = ensure_object ( com . index_labels_to_array ( labels ) ) if level is not None : if not isinstance ( axis , MultiIndex ) : raise AssertionError ( 'axis must be a MultiIndex' ) indexer = ~ axis . get_level_values ( level ) . isin ( labels ) # GH 18561 MultiIndex.drop should raise if label is absent if errors == 'raise' and indexer . all ( ) : raise KeyError ( '{} not found in axis' . format ( labels ) ) else : indexer = ~ axis . isin ( labels ) # Check if label doesn't exist along axis labels_missing = ( axis . get_indexer_for ( labels ) == - 1 ) . any ( ) if errors == 'raise' and labels_missing : raise KeyError ( '{} not found in axis' . format ( labels ) ) slicer = [ slice ( None ) ] * self . ndim slicer [ self . _get_axis_number ( axis_name ) ] = indexer result = self . loc [ tuple ( slicer ) ] return result\n\n\n        Summary:", "input": "", "output": "Drop labels from specified axis . Used in the drop method internally ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _update_inplace ( self , result , verify_is_copy = True ) : # NOTE: This does *not* call __finalize__ and that's an explicit # decision that we may revisit in the future. self . _reset_cache ( ) self . _clear_item_cache ( ) self . _data = getattr ( result , '_data' , result ) self . _maybe_update_cacher ( verify_is_copy = verify_is_copy )\n\n\n        Summary:", "input": "", "output": "Replace self internals with result ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_prefix ( self , prefix ) : f = functools . partial ( '{prefix}{}' . format , prefix = prefix ) mapper = { self . _info_axis_name : f } return self . rename ( * * mapper )\n\n\n        Summary:", "input": "", "output": "Prefix labels with string prefix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_suffix ( self , suffix ) : f = functools . partial ( '{}{suffix}' . format , suffix = suffix ) mapper = { self . _info_axis_name : f } return self . rename ( * * mapper )\n\n\n        Summary:", "input": "", "output": "Suffix labels with string suffix ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_values ( self , by = None , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) : raise NotImplementedError ( \"sort_values has not been implemented \" \"on Panel or Panel4D objects.\" )\n\n\n        Summary:", "input": "", "output": "Sort by the values along either axis ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_index ( self , axis = 0 , level = None , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , sort_remaining = True ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) axis = self . _get_axis_number ( axis ) axis_name = self . _get_axis_name ( axis ) labels = self . _get_axis ( axis ) if level is not None : raise NotImplementedError ( \"level is not implemented\" ) if inplace : raise NotImplementedError ( \"inplace is not implemented\" ) sort_index = labels . argsort ( ) if not ascending : sort_index = sort_index [ : : - 1 ] new_axis = labels . take ( sort_index ) return self . reindex ( * * { axis_name : new_axis } )\n\n\n        Summary:", "input": "", "output": "Sort object by labels ( along an axis ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reindex ( self , * args , * * kwargs ) : # TODO: Decide if we care about having different examples for different # kinds # construct the args axes , kwargs = self . _construct_axes_from_arguments ( args , kwargs ) method = missing . clean_reindex_fill_method ( kwargs . pop ( 'method' , None ) ) level = kwargs . pop ( 'level' , None ) copy = kwargs . pop ( 'copy' , True ) limit = kwargs . pop ( 'limit' , None ) tolerance = kwargs . pop ( 'tolerance' , None ) fill_value = kwargs . pop ( 'fill_value' , None ) # Series.reindex doesn't use / need the axis kwarg # We pop and ignore it here, to make writing Series/Frame generic code # easier kwargs . pop ( \"axis\" , None ) if kwargs : raise TypeError ( 'reindex() got an unexpected keyword ' 'argument \"{0}\"' . format ( list ( kwargs . keys ( ) ) [ 0 ] ) ) self . _consolidate_inplace ( ) # if all axes that are requested to reindex are equal, then only copy # if indicated must have index names equal here as well as values if all ( self . _get_axis ( axis ) . identical ( ax ) for axis , ax in axes . items ( ) if ax is not None ) : if copy : return self . copy ( ) return self # check if we are a multi reindex if self . _needs_reindex_multi ( axes , method , level ) : try : return self . _reindex_multi ( axes , copy , fill_value ) except Exception : pass # perform the reindex on the axes return self . _reindex_axes ( axes , level , limit , tolerance , method , fill_value , copy ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Conform % ( klass ) s to new index with optional filling logic placing NA / NaN in locations having no value in the previous index . A new object is produced unless the new index is equivalent to the current one and copy = False ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reindex_axes ( self , axes , level , limit , tolerance , method , fill_value , copy ) : obj = self for a in self . _AXIS_ORDERS : labels = axes [ a ] if labels is None : continue ax = self . _get_axis ( a ) new_index , indexer = ax . reindex ( labels , level = level , limit = limit , tolerance = tolerance , method = method ) axis = self . _get_axis_number ( a ) obj = obj . _reindex_with_indexers ( { axis : [ new_index , indexer ] } , fill_value = fill_value , copy = copy , allow_dups = False ) return obj\n\n\n        Summary:", "input": "", "output": "Perform the reindex for all the axes ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _needs_reindex_multi ( self , axes , method , level ) : return ( ( com . count_not_none ( * axes . values ( ) ) == self . _AXIS_LEN ) and method is None and level is None and not self . _is_mixed_type )\n\n\n        Summary:", "input": "", "output": "Check if we do need a multi reindex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reindex_with_indexers ( self , reindexers , fill_value = None , copy = False , allow_dups = False ) : # reindex doing multiple operations on different axes if indicated new_data = self . _data for axis in sorted ( reindexers . keys ( ) ) : index , indexer = reindexers [ axis ] baxis = self . _get_block_manager_axis ( axis ) if index is None : continue index = ensure_index ( index ) if indexer is not None : indexer = ensure_int64 ( indexer ) # TODO: speed up on homogeneous DataFrame objects new_data = new_data . reindex_indexer ( index , indexer , axis = baxis , fill_value = fill_value , allow_dups = allow_dups , copy = copy ) if copy and new_data is self . _data : new_data = new_data . copy ( ) return self . _constructor ( new_data ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "allow_dups indicates an internal call here"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def filter ( self , items = None , like = None , regex = None , axis = None ) : import re nkw = com . count_not_none ( items , like , regex ) if nkw > 1 : raise TypeError ( 'Keyword arguments `items`, `like`, or `regex` ' 'are mutually exclusive' ) if axis is None : axis = self . _info_axis_name labels = self . _get_axis ( axis ) if items is not None : name = self . _get_axis_name ( axis ) return self . reindex ( * * { name : [ r for r in items if r in labels ] } ) elif like : def f ( x ) : return like in to_str ( x ) values = labels . map ( f ) return self . loc ( axis = axis ) [ values ] elif regex : def f ( x ) : return matcher . search ( to_str ( x ) ) is not None matcher = re . compile ( regex ) values = labels . map ( f ) return self . loc ( axis = axis ) [ values ] else : raise TypeError ( 'Must pass either `items`, `like`, or `regex`' )\n\n\n        Summary:", "input": "", "output": "Subset rows or columns of dataframe according to labels in the specified index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sample ( self , n = None , frac = None , replace = False , weights = None , random_state = None , axis = None ) : if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) axis_length = self . shape [ axis ] # Process random_state argument rs = com . random_state ( random_state ) # Check weights for compliance if weights is not None : # If a series, align with frame if isinstance ( weights , pd . Series ) : weights = weights . reindex ( self . axes [ axis ] ) # Strings acceptable if a dataframe and axis = 0 if isinstance ( weights , str ) : if isinstance ( self , pd . DataFrame ) : if axis == 0 : try : weights = self [ weights ] except KeyError : raise KeyError ( \"String passed to weights not a \" \"valid column\" ) else : raise ValueError ( \"Strings can only be passed to \" \"weights when sampling from rows on \" \"a DataFrame\" ) else : raise ValueError ( \"Strings cannot be passed as weights \" \"when sampling from a Series or Panel.\" ) weights = pd . Series ( weights , dtype = 'float64' ) if len ( weights ) != axis_length : raise ValueError ( \"Weights and axis to be sampled must be of \" \"same length\" ) if ( weights == np . inf ) . any ( ) or ( weights == - np . inf ) . any ( ) : raise ValueError ( \"weight vector may not include `inf` values\" ) if ( weights < 0 ) . any ( ) : raise ValueError ( \"weight vector many not include negative \" \"values\" ) # If has nan, set to zero. weights = weights . fillna ( 0 ) # Renormalize if don't sum to 1 if weights . sum ( ) != 1 : if weights . sum ( ) != 0 : weights = weights / weights . sum ( ) else : raise ValueError ( \"Invalid weights: weights sum to zero\" ) weights = weights . values # If no frac or n, default to n=1. if n is None and frac is None : n = 1 elif n is not None and frac is None and n % 1 != 0 : raise ValueError ( \"Only integers accepted as `n` values\" ) elif n is None and frac is not None : n = int ( round ( frac * axis_length ) ) elif n is not None and frac is not None : raise ValueError ( 'Please enter a value for `frac` OR `n`, not ' 'both' ) # Check for negative sizes if n < 0 : raise ValueError ( \"A negative number of rows requested. Please \" \"provide positive value.\" ) locs = rs . choice ( axis_length , size = n , replace = replace , p = weights ) return self . take ( locs , axis = axis , is_copy = False )\n\n\n        Summary:", "input": "", "output": "Return a random sample of items from an axis of object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _dir_additions ( self ) : additions = { c for c in self . _info_axis . unique ( level = 0 ) [ : 100 ] if isinstance ( c , str ) and c . isidentifier ( ) } return super ( ) . _dir_additions ( ) . union ( additions )\n\n\n        Summary:", "input": "", "output": "add the string - like attributes from the info_axis . If info_axis is a MultiIndex it s first level values are used ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _protect_consolidate ( self , f ) : blocks_before = len ( self . _data . blocks ) result = f ( ) if len ( self . _data . blocks ) != blocks_before : self . _clear_item_cache ( ) return result\n\n\n        Summary:", "input": "", "output": "Consolidate _data -- if the blocks have changed then clear the cache"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _consolidate_inplace ( self ) : def f ( ) : self . _data = self . _data . consolidate ( ) self . _protect_consolidate ( f )\n\n\n        Summary:", "input": "", "output": "Consolidate data in place and return None"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _consolidate ( self , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if inplace : self . _consolidate_inplace ( ) else : f = lambda : self . _data . consolidate ( ) cons_data = self . _protect_consolidate ( f ) return self . _constructor ( cons_data ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Compute NDFrame with consolidated internals ( data of each dtype grouped together in a single ndarray ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_inplace_setting ( self , value ) : if self . _is_mixed_type : if not self . _is_numeric_mixed_type : # allow an actual np.nan thru try : if np . isnan ( value ) : return True except Exception : pass raise TypeError ( 'Cannot do inplace boolean setting on ' 'mixed-types with a non np.nan value' ) return True\n\n\n        Summary:", "input": "", "output": "check whether we allow in - place setting with this type of value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def as_matrix ( self , columns = None ) : warnings . warn ( \"Method .as_matrix will be removed in a future version. \" \"Use .values instead.\" , FutureWarning , stacklevel = 2 ) self . _consolidate_inplace ( ) return self . _data . as_array ( transpose = self . _AXIS_REVERSED , items = columns )\n\n\n        Summary:", "input": "", "output": "Convert the frame to its Numpy - array representation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def values ( self ) : self . _consolidate_inplace ( ) return self . _data . as_array ( transpose = self . _AXIS_REVERSED )\n\n\n        Summary:", "input": "", "output": "Return a Numpy representation of the DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_ftype_counts ( self ) : warnings . warn ( \"get_ftype_counts is deprecated and will \" \"be removed in a future version\" , FutureWarning , stacklevel = 2 ) from pandas import Series return Series ( self . _data . get_ftype_counts ( ) )\n\n\n        Summary:", "input": "", "output": "Return counts of unique ftypes in this object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dtypes ( self ) : from pandas import Series return Series ( self . _data . get_dtypes ( ) , index = self . _info_axis , dtype = np . object_ )\n\n\n        Summary:", "input": "", "output": "Return the dtypes in the DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ftypes ( self ) : from pandas import Series return Series ( self . _data . get_ftypes ( ) , index = self . _info_axis , dtype = np . object_ )\n\n\n        Summary:", "input": "", "output": "Return the ftypes ( indication of sparse / dense and dtype ) in DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def as_blocks ( self , copy = True ) : warnings . warn ( \"as_blocks is deprecated and will \" \"be removed in a future version\" , FutureWarning , stacklevel = 2 ) return self . _to_dict_of_blocks ( copy = copy )\n\n\n        Summary:", "input": "", "output": "Convert the frame to a dict of dtype - > Constructor Types that each has a homogeneous dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _to_dict_of_blocks ( self , copy = True ) : return { k : self . _constructor ( v ) . __finalize__ ( self ) for k , v , in self . _data . to_dict ( copy = copy ) . items ( ) }\n\n\n        Summary:", "input": "", "output": "Return a dict of dtype - > Constructor Types that each is a homogeneous dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def astype ( self , dtype , copy = True , errors = 'raise' , * * kwargs ) : if is_dict_like ( dtype ) : if self . ndim == 1 : # i.e. Series if len ( dtype ) > 1 or self . name not in dtype : raise KeyError ( 'Only the Series name can be used for ' 'the key in Series dtype mappings.' ) new_type = dtype [ self . name ] return self . astype ( new_type , copy , errors , * * kwargs ) elif self . ndim > 2 : raise NotImplementedError ( 'astype() only accepts a dtype arg of type dict when ' 'invoked on Series and DataFrames. A single dtype must be ' 'specified when invoked on a Panel.' ) for col_name in dtype . keys ( ) : if col_name not in self : raise KeyError ( 'Only a column name can be used for the ' 'key in a dtype mappings argument.' ) results = [ ] for col_name , col in self . iteritems ( ) : if col_name in dtype : results . append ( col . astype ( dtype = dtype [ col_name ] , copy = copy , errors = errors , * * kwargs ) ) else : results . append ( results . append ( col . copy ( ) if copy else col ) ) elif is_extension_array_dtype ( dtype ) and self . ndim > 1 : # GH 18099/22869: columnwise conversion to extension dtype # GH 24704: use iloc to handle duplicate column names results = ( self . iloc [ : , i ] . astype ( dtype , copy = copy ) for i in range ( len ( self . columns ) ) ) else : # else, only a single dtype is given new_data = self . _data . astype ( dtype = dtype , copy = copy , errors = errors , * * kwargs ) return self . _constructor ( new_data ) . __finalize__ ( self ) # GH 19920: retain column metadata after concat result = pd . concat ( results , axis = 1 , copy = False ) result . columns = self . columns return result\n\n\n        Summary:", "input": "", "output": "Cast a pandas object to a specified dtype dtype ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , deep = True ) : data = self . _data . copy ( deep = deep ) return self . _constructor ( data ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Make a copy of this object s indices and data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _convert ( self , datetime = False , numeric = False , timedelta = False , coerce = False , copy = True ) : return self . _constructor ( self . _data . convert ( datetime = datetime , numeric = numeric , timedelta = timedelta , coerce = coerce , copy = copy ) ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Attempt to infer better dtype for object columns"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def convert_objects ( self , convert_dates = True , convert_numeric = False , convert_timedeltas = True , copy = True ) : msg = ( \"convert_objects is deprecated. To re-infer data dtypes for \" \"object columns, use {klass}.infer_objects()\\nFor all \" \"other conversions use the data-type specific converters \" \"pd.to_datetime, pd.to_timedelta and pd.to_numeric.\" ) . format ( klass = self . __class__ . __name__ ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) return self . _constructor ( self . _data . convert ( convert_dates = convert_dates , convert_numeric = convert_numeric , convert_timedeltas = convert_timedeltas , copy = copy ) ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Attempt to infer better dtype for object columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def infer_objects ( self ) : # numeric=False necessary to only soft convert; # python objects will still be converted to # native numpy numeric types return self . _constructor ( self . _data . convert ( datetime = True , numeric = False , timedelta = True , coerce = False , copy = True ) ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Attempt to infer better dtypes for object columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fillna ( self , value = None , method = None , axis = None , inplace = False , limit = None , downcast = None ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) value , method = validate_fillna_kwargs ( value , method ) self . _consolidate_inplace ( ) # set the default here, so functions examining the signaure # can detect if something was set (e.g. in groupby) (GH9221) if axis is None : axis = 0 axis = self . _get_axis_number ( axis ) from pandas import DataFrame if value is None : if self . _is_mixed_type and axis == 1 : if inplace : raise NotImplementedError ( ) result = self . T . fillna ( method = method , limit = limit ) . T # need to downcast here because of all of the transposes result . _data = result . _data . downcast ( ) return result # > 3d if self . ndim > 3 : raise NotImplementedError ( 'Cannot fillna with a method for > ' '3dims' ) # 3d elif self . ndim == 3 : # fill in 2d chunks result = { col : s . fillna ( method = method , value = value ) for col , s in self . iteritems ( ) } prelim_obj = self . _constructor . from_dict ( result ) new_obj = prelim_obj . __finalize__ ( self ) new_data = new_obj . _data else : # 2d or less new_data = self . _data . interpolate ( method = method , axis = axis , limit = limit , inplace = inplace , coerce = True , downcast = downcast ) else : if len ( self . _get_axis ( axis ) ) == 0 : return self if self . ndim == 1 : if isinstance ( value , ( dict , ABCSeries ) ) : from pandas import Series value = Series ( value ) elif not is_list_like ( value ) : pass else : raise TypeError ( '\"value\" parameter must be a scalar, dict ' 'or Series, but you passed a ' '\"{0}\"' . format ( type ( value ) . __name__ ) ) new_data = self . _data . fillna ( value = value , limit = limit , inplace = inplace , downcast = downcast ) elif isinstance ( value , ( dict , ABCSeries ) ) : if axis == 1 : raise NotImplementedError ( 'Currently only can fill ' 'with dict/Series column ' 'by column' ) result = self if inplace else self . copy ( ) for k , v in value . items ( ) : if k not in result : continue obj = result [ k ] obj . fillna ( v , limit = limit , inplace = True , downcast = downcast ) return result if not inplace else None elif not is_list_like ( value ) : new_data = self . _data . fillna ( value = value , limit = limit , inplace = inplace , downcast = downcast ) elif isinstance ( value , DataFrame ) and self . ndim == 2 : new_data = self . where ( self . notna ( ) , value ) else : raise ValueError ( \"invalid fill value with a %s\" % type ( value ) ) if inplace : self . _update_inplace ( new_data ) else : return self . _constructor ( new_data ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Fill NA / NaN values using the specified method ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def interpolate ( self , method = 'linear' , axis = 0 , limit = None , inplace = False , limit_direction = 'forward' , limit_area = None , downcast = None , * * kwargs ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if self . ndim > 2 : raise NotImplementedError ( \"Interpolate has not been implemented \" \"on Panel and Panel 4D objects.\" ) if axis == 0 : ax = self . _info_axis_name _maybe_transposed_self = self elif axis == 1 : _maybe_transposed_self = self . T ax = 1 else : _maybe_transposed_self = self ax = _maybe_transposed_self . _get_axis_number ( ax ) if _maybe_transposed_self . ndim == 2 : alt_ax = 1 - ax else : alt_ax = ax if ( isinstance ( _maybe_transposed_self . index , MultiIndex ) and method != 'linear' ) : raise ValueError ( \"Only `method=linear` interpolation is supported \" \"on MultiIndexes.\" ) if _maybe_transposed_self . _data . get_dtype_counts ( ) . get ( 'object' ) == len ( _maybe_transposed_self . T ) : raise TypeError ( \"Cannot interpolate with all object-dtype columns \" \"in the DataFrame. Try setting at least one \" \"column to a numeric dtype.\" ) # create/use the index if method == 'linear' : # prior default index = np . arange ( len ( _maybe_transposed_self . _get_axis ( alt_ax ) ) ) else : index = _maybe_transposed_self . _get_axis ( alt_ax ) methods = { \"index\" , \"values\" , \"nearest\" , \"time\" } is_numeric_or_datetime = ( is_numeric_dtype ( index ) or is_datetime64_dtype ( index ) or is_timedelta64_dtype ( index ) ) if method not in methods and not is_numeric_or_datetime : raise ValueError ( \"Index column must be numeric or datetime type when \" \"using {method} method other than linear. \" \"Try setting a numeric or datetime index column before \" \"interpolating.\" . format ( method = method ) ) if isna ( index ) . any ( ) : raise NotImplementedError ( \"Interpolation with NaNs in the index \" \"has not been implemented. Try filling \" \"those NaNs before interpolating.\" ) data = _maybe_transposed_self . _data new_data = data . interpolate ( method = method , axis = ax , index = index , values = _maybe_transposed_self , limit = limit , limit_direction = limit_direction , limit_area = limit_area , inplace = inplace , downcast = downcast , * * kwargs ) if inplace : if axis == 1 : new_data = self . _constructor ( new_data ) . T . _data self . _update_inplace ( new_data ) else : res = self . _constructor ( new_data ) . __finalize__ ( self ) if axis == 1 : res = res . T return res\n\n\n        Summary:", "input": "", "output": "Interpolate values according to different methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asof ( self , where , subset = None ) : if isinstance ( where , str ) : from pandas import to_datetime where = to_datetime ( where ) if not self . index . is_monotonic : raise ValueError ( \"asof requires a sorted index\" ) is_series = isinstance ( self , ABCSeries ) if is_series : if subset is not None : raise ValueError ( \"subset is not valid for Series\" ) elif self . ndim > 2 : raise NotImplementedError ( \"asof is not implemented \" \"for {type}\" . format ( type = type ( self ) ) ) else : if subset is None : subset = self . columns if not is_list_like ( subset ) : subset = [ subset ] is_list = is_list_like ( where ) if not is_list : start = self . index [ 0 ] if isinstance ( self . index , PeriodIndex ) : where = Period ( where , freq = self . index . freq ) . ordinal start = start . ordinal if where < start : if not is_series : from pandas import Series return Series ( index = self . columns , name = where ) return np . nan # It's always much faster to use a *while* loop here for # Series than pre-computing all the NAs. However a # *while* loop is extremely expensive for DataFrame # so we later pre-compute all the NAs and use the same # code path whether *where* is a scalar or list. # See PR: https://github.com/pandas-dev/pandas/pull/14476 if is_series : loc = self . index . searchsorted ( where , side = 'right' ) if loc > 0 : loc -= 1 values = self . _values while loc > 0 and isna ( values [ loc ] ) : loc -= 1 return values [ loc ] if not isinstance ( where , Index ) : where = Index ( where ) if is_list else Index ( [ where ] ) nulls = self . isna ( ) if is_series else self [ subset ] . isna ( ) . any ( 1 ) if nulls . all ( ) : if is_series : return self . _constructor ( np . nan , index = where , name = self . name ) elif is_list : from pandas import DataFrame return DataFrame ( np . nan , index = where , columns = self . columns ) else : from pandas import Series return Series ( np . nan , index = self . columns , name = where [ 0 ] ) locs = self . index . asof_locs ( where , ~ ( nulls . values ) ) # mask the missing missing = locs == - 1 data = self . take ( locs , is_copy = False ) data . index = where data . loc [ missing ] = np . nan return data if is_list else data . iloc [ - 1 ]\n\n\n        Summary:", "input": "", "output": "Return the last row ( s ) without any NaNs before where ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def clip ( self , lower = None , upper = None , axis = None , inplace = False , * args , * * kwargs ) : if isinstance ( self , ABCPanel ) : raise NotImplementedError ( \"clip is not supported yet for panels\" ) inplace = validate_bool_kwarg ( inplace , 'inplace' ) axis = nv . validate_clip_with_axis ( axis , args , kwargs ) if axis is not None : axis = self . _get_axis_number ( axis ) # GH 17276 # numpy doesn't like NaN as a clip value # so ignore # GH 19992 # numpy doesn't drop a list-like bound containing NaN if not is_list_like ( lower ) and np . any ( pd . isnull ( lower ) ) : lower = None if not is_list_like ( upper ) and np . any ( pd . isnull ( upper ) ) : upper = None # GH 2747 (arguments were reversed) if lower is not None and upper is not None : if is_scalar ( lower ) and is_scalar ( upper ) : lower , upper = min ( lower , upper ) , max ( lower , upper ) # fast-path for scalars if ( ( lower is None or ( is_scalar ( lower ) and is_number ( lower ) ) ) and ( upper is None or ( is_scalar ( upper ) and is_number ( upper ) ) ) ) : return self . _clip_with_scalar ( lower , upper , inplace = inplace ) result = self if lower is not None : result = result . _clip_with_one_bound ( lower , method = self . ge , axis = axis , inplace = inplace ) if upper is not None : if inplace : result = self result = result . _clip_with_one_bound ( upper , method = self . le , axis = axis , inplace = inplace ) return result\n\n\n        Summary:", "input": "", "output": "Trim values at input threshold ( s ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def clip_upper ( self , threshold , axis = None , inplace = False ) : warnings . warn ( 'clip_upper(threshold) is deprecated, ' 'use clip(upper=threshold) instead' , FutureWarning , stacklevel = 2 ) return self . _clip_with_one_bound ( threshold , method = self . le , axis = axis , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Trim values above a given threshold ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def clip_lower ( self , threshold , axis = None , inplace = False ) : warnings . warn ( 'clip_lower(threshold) is deprecated, ' 'use clip(lower=threshold) instead' , FutureWarning , stacklevel = 2 ) return self . _clip_with_one_bound ( threshold , method = self . ge , axis = axis , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Trim values below a given threshold ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def groupby ( self , by = None , axis = 0 , level = None , as_index = True , sort = True , group_keys = True , squeeze = False , observed = False , * * kwargs ) : from pandas . core . groupby . groupby import groupby if level is None and by is None : raise TypeError ( \"You have to supply one of 'by' and 'level'\" ) axis = self . _get_axis_number ( axis ) return groupby ( self , by = by , axis = axis , level = level , as_index = as_index , sort = sort , group_keys = group_keys , squeeze = squeeze , observed = observed , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Group DataFrame or Series using a mapper or by a Series of columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asfreq ( self , freq , method = None , how = None , normalize = False , fill_value = None ) : from pandas . core . resample import asfreq return asfreq ( self , freq , method = method , how = how , normalize = normalize , fill_value = fill_value )\n\n\n        Summary:", "input": "", "output": "Convert TimeSeries to specified frequency ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def at_time ( self , time , asof = False , axis = None ) : if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) index = self . _get_axis ( axis ) try : indexer = index . indexer_at_time ( time , asof = asof ) except AttributeError : raise TypeError ( 'Index must be DatetimeIndex' ) return self . _take ( indexer , axis = axis )\n\n\n        Summary:", "input": "", "output": "Select values at particular time of day ( e . g . 9 : 30AM ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def between_time ( self , start_time , end_time , include_start = True , include_end = True , axis = None ) : if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) index = self . _get_axis ( axis ) try : indexer = index . indexer_between_time ( start_time , end_time , include_start = include_start , include_end = include_end ) except AttributeError : raise TypeError ( 'Index must be DatetimeIndex' ) return self . _take ( indexer , axis = axis )\n\n\n        Summary:", "input": "", "output": "Select values between particular times of the day ( e . g . 9 : 00 - 9 : 30 AM ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def resample ( self , rule , how = None , axis = 0 , fill_method = None , closed = None , label = None , convention = 'start' , kind = None , loffset = None , limit = None , base = 0 , on = None , level = None ) : from pandas . core . resample import ( resample , _maybe_process_deprecations ) axis = self . _get_axis_number ( axis ) r = resample ( self , freq = rule , label = label , closed = closed , axis = axis , kind = kind , loffset = loffset , convention = convention , base = base , key = on , level = level ) return _maybe_process_deprecations ( r , how = how , fill_method = fill_method , limit = limit )\n\n\n        Summary:", "input": "", "output": "Resample time - series data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def first ( self , offset ) : if not isinstance ( self . index , DatetimeIndex ) : raise TypeError ( \"'first' only supports a DatetimeIndex index\" ) if len ( self . index ) == 0 : return self offset = to_offset ( offset ) end_date = end = self . index [ 0 ] + offset # Tick-like, e.g. 3 weeks if not offset . isAnchored ( ) and hasattr ( offset , '_inc' ) : if end_date in self . index : end = self . index . searchsorted ( end_date , side = 'left' ) return self . iloc [ : end ] return self . loc [ : end ]\n\n\n        Summary:", "input": "", "output": "Convenience method for subsetting initial periods of time series data based on a date offset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def last ( self , offset ) : if not isinstance ( self . index , DatetimeIndex ) : raise TypeError ( \"'last' only supports a DatetimeIndex index\" ) if len ( self . index ) == 0 : return self offset = to_offset ( offset ) start_date = self . index [ - 1 ] - offset start = self . index . searchsorted ( start_date , side = 'right' ) return self . iloc [ start : ]\n\n\n        Summary:", "input": "", "output": "Convenience method for subsetting final periods of time series data based on a date offset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rank ( self , axis = 0 , method = 'average' , numeric_only = None , na_option = 'keep' , ascending = True , pct = False ) : axis = self . _get_axis_number ( axis ) if self . ndim > 2 : msg = \"rank does not make sense when ndim > 2\" raise NotImplementedError ( msg ) if na_option not in { 'keep' , 'top' , 'bottom' } : msg = \"na_option must be one of 'keep', 'top', or 'bottom'\" raise ValueError ( msg ) def ranker ( data ) : ranks = algos . rank ( data . values , axis = axis , method = method , ascending = ascending , na_option = na_option , pct = pct ) ranks = self . _constructor ( ranks , * * data . _construct_axes_dict ( ) ) return ranks . __finalize__ ( self ) # if numeric_only is None, and we can't get anything, we try with # numeric_only=True if numeric_only is None : try : return ranker ( self ) except TypeError : numeric_only = True if numeric_only : data = self . _get_numeric_data ( ) else : data = self return ranker ( data )\n\n\n        Summary:", "input": "", "output": "Compute numerical data ranks ( 1 through n ) along axis . Equal values are assigned a rank that is the average of the ranks of those values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _where ( self , cond , other = np . nan , inplace = False , axis = None , level = None , errors = 'raise' , try_cast = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) # align the cond to same shape as myself cond = com . apply_if_callable ( cond , self ) if isinstance ( cond , NDFrame ) : cond , _ = cond . align ( self , join = 'right' , broadcast_axis = 1 ) else : if not hasattr ( cond , 'shape' ) : cond = np . asanyarray ( cond ) if cond . shape != self . shape : raise ValueError ( 'Array conditional must be same shape as ' 'self' ) cond = self . _constructor ( cond , * * self . _construct_axes_dict ( ) ) # make sure we are boolean fill_value = bool ( inplace ) cond = cond . fillna ( fill_value ) msg = \"Boolean array expected for the condition, not {dtype}\" if not isinstance ( cond , pd . DataFrame ) : # This is a single-dimensional object. if not is_bool_dtype ( cond ) : raise ValueError ( msg . format ( dtype = cond . dtype ) ) elif not cond . empty : for dt in cond . dtypes : if not is_bool_dtype ( dt ) : raise ValueError ( msg . format ( dtype = dt ) ) cond = - cond if inplace else cond # try to align with other try_quick = True if hasattr ( other , 'align' ) : # align with me if other . ndim <= self . ndim : _ , other = self . align ( other , join = 'left' , axis = axis , level = level , fill_value = np . nan ) # if we are NOT aligned, raise as we cannot where index if ( axis is None and not all ( other . _get_axis ( i ) . equals ( ax ) for i , ax in enumerate ( self . axes ) ) ) : raise InvalidIndexError # slice me out of the other else : raise NotImplementedError ( \"cannot align with a higher \" \"dimensional NDFrame\" ) if isinstance ( other , np . ndarray ) : if other . shape != self . shape : if self . ndim == 1 : icond = cond . values # GH 2745 / GH 4192 # treat like a scalar if len ( other ) == 1 : other = np . array ( other [ 0 ] ) # GH 3235 # match True cond to other elif len ( cond [ icond ] ) == len ( other ) : # try to not change dtype at first (if try_quick) if try_quick : try : new_other = com . values_from_object ( self ) new_other = new_other . copy ( ) new_other [ icond ] = other other = new_other except Exception : try_quick = False # let's create a new (if we failed at the above # or not try_quick if not try_quick : dtype , fill_value = maybe_promote ( other . dtype ) new_other = np . empty ( len ( icond ) , dtype = dtype ) new_other . fill ( fill_value ) maybe_upcast_putmask ( new_other , icond , other ) other = new_other else : raise ValueError ( 'Length of replacements must equal ' 'series length' ) else : raise ValueError ( 'other must be the same shape as self ' 'when an ndarray' ) # we are the same shape, so create an actual object for alignment else : other = self . _constructor ( other , * * self . _construct_axes_dict ( ) ) if axis is None : axis = 0 if self . ndim == getattr ( other , 'ndim' , 0 ) : align = True else : align = ( self . _get_axis_number ( axis ) == 1 ) block_axis = self . _get_block_manager_axis ( axis ) if inplace : # we may have different type blocks come out of putmask, so # reconstruct the block manager self . _check_inplace_setting ( other ) new_data = self . _data . putmask ( mask = cond , new = other , align = align , inplace = True , axis = block_axis , transpose = self . _AXIS_REVERSED ) self . _update_inplace ( new_data ) else : new_data = self . _data . where ( other = other , cond = cond , align = align , errors = errors , try_cast = try_cast , axis = block_axis , transpose = self . _AXIS_REVERSED ) return self . _constructor ( new_data ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Equivalent to public method where except that other is not applied as a function even if callable . Used in __setitem__ ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def slice_shift ( self , periods = 1 , axis = 0 ) : if periods == 0 : return self if periods > 0 : vslicer = slice ( None , - periods ) islicer = slice ( periods , None ) else : vslicer = slice ( - periods , None ) islicer = slice ( None , periods ) new_obj = self . _slice ( vslicer , axis = axis ) shifted_axis = self . _get_axis ( axis ) [ islicer ] new_obj . set_axis ( shifted_axis , axis = axis , inplace = True ) return new_obj . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Equivalent to shift without copying data . The shifted data will not include the dropped periods and the shifted axis will be smaller than the original ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tshift ( self , periods = 1 , freq = None , axis = 0 ) : index = self . _get_axis ( axis ) if freq is None : freq = getattr ( index , 'freq' , None ) if freq is None : freq = getattr ( index , 'inferred_freq' , None ) if freq is None : msg = 'Freq was not given and was not set in the index' raise ValueError ( msg ) if periods == 0 : return self if isinstance ( freq , str ) : freq = to_offset ( freq ) block_axis = self . _get_block_manager_axis ( axis ) if isinstance ( index , PeriodIndex ) : orig_freq = to_offset ( index . freq ) if freq == orig_freq : new_data = self . _data . copy ( ) new_data . axes [ block_axis ] = index . shift ( periods ) else : msg = ( 'Given freq %s does not match PeriodIndex freq %s' % ( freq . rule_code , orig_freq . rule_code ) ) raise ValueError ( msg ) else : new_data = self . _data . copy ( ) new_data . axes [ block_axis ] = index . shift ( periods , freq ) return self . _constructor ( new_data ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Shift the time index using the index s frequency if available ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def truncate ( self , before = None , after = None , axis = None , copy = True ) : if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) # GH 17935 # Check that index is sorted if not ax . is_monotonic_increasing and not ax . is_monotonic_decreasing : raise ValueError ( \"truncate requires a sorted index\" ) # if we have a date index, convert to dates, otherwise # treat like a slice if ax . is_all_dates : from pandas . core . tools . datetimes import to_datetime before = to_datetime ( before ) after = to_datetime ( after ) if before is not None and after is not None : if before > after : raise ValueError ( 'Truncate: %s must be after %s' % ( after , before ) ) slicer = [ slice ( None , None ) ] * self . _AXIS_LEN slicer [ axis ] = slice ( before , after ) result = self . loc [ tuple ( slicer ) ] if isinstance ( ax , MultiIndex ) : setattr ( result , self . _get_axis_name ( axis ) , ax . truncate ( before , after ) ) if copy : result = result . copy ( ) return result\n\n\n        Summary:", "input": "", "output": "Truncate a Series or DataFrame before and after some index value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tz_convert ( self , tz , axis = 0 , level = None , copy = True ) : axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) def _tz_convert ( ax , tz ) : if not hasattr ( ax , 'tz_convert' ) : if len ( ax ) > 0 : ax_name = self . _get_axis_name ( axis ) raise TypeError ( '%s is not a valid DatetimeIndex or ' 'PeriodIndex' % ax_name ) else : ax = DatetimeIndex ( [ ] , tz = tz ) else : ax = ax . tz_convert ( tz ) return ax # if a level is given it must be a MultiIndex level or # equivalent to the axis name if isinstance ( ax , MultiIndex ) : level = ax . _get_level_number ( level ) new_level = _tz_convert ( ax . levels [ level ] , tz ) ax = ax . set_levels ( new_level , level = level ) else : if level not in ( None , 0 , ax . name ) : raise ValueError ( \"The level {0} is not valid\" . format ( level ) ) ax = _tz_convert ( ax , tz ) result = self . _constructor ( self . _data , copy = copy ) result = result . set_axis ( ax , axis = axis , inplace = False ) return result . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Convert tz - aware axis to target time zone ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tz_localize ( self , tz , axis = 0 , level = None , copy = True , ambiguous = 'raise' , nonexistent = 'raise' ) : nonexistent_options = ( 'raise' , 'NaT' , 'shift_forward' , 'shift_backward' ) if nonexistent not in nonexistent_options and not isinstance ( nonexistent , timedelta ) : raise ValueError ( \"The nonexistent argument must be one of 'raise',\" \" 'NaT', 'shift_forward', 'shift_backward' or\" \" a timedelta object\" ) axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) def _tz_localize ( ax , tz , ambiguous , nonexistent ) : if not hasattr ( ax , 'tz_localize' ) : if len ( ax ) > 0 : ax_name = self . _get_axis_name ( axis ) raise TypeError ( '%s is not a valid DatetimeIndex or ' 'PeriodIndex' % ax_name ) else : ax = DatetimeIndex ( [ ] , tz = tz ) else : ax = ax . tz_localize ( tz , ambiguous = ambiguous , nonexistent = nonexistent ) return ax # if a level is given it must be a MultiIndex level or # equivalent to the axis name if isinstance ( ax , MultiIndex ) : level = ax . _get_level_number ( level ) new_level = _tz_localize ( ax . levels [ level ] , tz , ambiguous , nonexistent ) ax = ax . set_levels ( new_level , level = level ) else : if level not in ( None , 0 , ax . name ) : raise ValueError ( \"The level {0} is not valid\" . format ( level ) ) ax = _tz_localize ( ax , tz , ambiguous , nonexistent ) result = self . _constructor ( self . _data , copy = copy ) result = result . set_axis ( ax , axis = axis , inplace = False ) return result . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Localize tz - naive index of a Series or DataFrame to target time zone ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def describe ( self , percentiles = None , include = None , exclude = None ) : if self . ndim >= 3 : msg = \"describe is not implemented on Panel objects.\" raise NotImplementedError ( msg ) elif self . ndim == 2 and self . columns . size == 0 : raise ValueError ( \"Cannot describe a DataFrame without columns\" ) if percentiles is not None : # explicit conversion of `percentiles` to list percentiles = list ( percentiles ) # get them all to be in [0, 1] self . _check_percentile ( percentiles ) # median should always be included if 0.5 not in percentiles : percentiles . append ( 0.5 ) percentiles = np . asarray ( percentiles ) else : percentiles = np . array ( [ 0.25 , 0.5 , 0.75 ] ) # sort and check for duplicates unique_pcts = np . unique ( percentiles ) if len ( unique_pcts ) < len ( percentiles ) : raise ValueError ( \"percentiles cannot contain duplicates\" ) percentiles = unique_pcts formatted_percentiles = format_percentiles ( percentiles ) def describe_numeric_1d ( series ) : stat_index = ( [ 'count' , 'mean' , 'std' , 'min' ] + formatted_percentiles + [ 'max' ] ) d = ( [ series . count ( ) , series . mean ( ) , series . std ( ) , series . min ( ) ] + series . quantile ( percentiles ) . tolist ( ) + [ series . max ( ) ] ) return pd . Series ( d , index = stat_index , name = series . name ) def describe_categorical_1d ( data ) : names = [ 'count' , 'unique' ] objcounts = data . value_counts ( ) count_unique = len ( objcounts [ objcounts != 0 ] ) result = [ data . count ( ) , count_unique ] if result [ 1 ] > 0 : top , freq = objcounts . index [ 0 ] , objcounts . iloc [ 0 ] if is_datetime64_any_dtype ( data ) : tz = data . dt . tz asint = data . dropna ( ) . values . view ( 'i8' ) top = Timestamp ( top ) if top . tzinfo is not None and tz is not None : # Don't tz_localize(None) if key is already tz-aware top = top . tz_convert ( tz ) else : top = top . tz_localize ( tz ) names += [ 'top' , 'freq' , 'first' , 'last' ] result += [ top , freq , Timestamp ( asint . min ( ) , tz = tz ) , Timestamp ( asint . max ( ) , tz = tz ) ] else : names += [ 'top' , 'freq' ] result += [ top , freq ] return pd . Series ( result , index = names , name = data . name ) def describe_1d ( data ) : if is_bool_dtype ( data ) : return describe_categorical_1d ( data ) elif is_numeric_dtype ( data ) : return describe_numeric_1d ( data ) elif is_timedelta64_dtype ( data ) : return describe_numeric_1d ( data ) else : return describe_categorical_1d ( data ) if self . ndim == 1 : return describe_1d ( self ) elif ( include is None ) and ( exclude is None ) : # when some numerics are found, keep only numerics data = self . select_dtypes ( include = [ np . number ] ) if len ( data . columns ) == 0 : data = self elif include == 'all' : if exclude is not None : msg = \"exclude must be None when include is 'all'\" raise ValueError ( msg ) data = self else : data = self . select_dtypes ( include = include , exclude = exclude ) ldesc = [ describe_1d ( s ) for _ , s in data . iteritems ( ) ] # set a convenient order for rows names = [ ] ldesc_indexes = sorted ( ( x . index for x in ldesc ) , key = len ) for idxnames in ldesc_indexes : for name in idxnames : if name not in names : names . append ( name ) d = pd . concat ( ldesc , join_axes = pd . Index ( [ names ] ) , axis = 1 ) d . columns = data . columns . copy ( ) return d\n\n\n        Summary:", "input": "", "output": "Generate descriptive statistics that summarize the central tendency dispersion and shape of a dataset s distribution excluding NaN values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_percentile ( self , q ) : msg = ( \"percentiles should all be in the interval [0, 1]. \" \"Try {0} instead.\" ) q = np . asarray ( q ) if q . ndim == 0 : if not 0 <= q <= 1 : raise ValueError ( msg . format ( q / 100.0 ) ) else : if not all ( 0 <= qs <= 1 for qs in q ) : raise ValueError ( msg . format ( q / 100.0 ) ) return q\n\n\n        Summary:", "input": "", "output": "Validate percentiles ( used by describe and quantile ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_numeric_operations ( cls ) : axis_descr , name , name2 = _doc_parms ( cls ) cls . any = _make_logical_function ( cls , 'any' , name , name2 , axis_descr , _any_desc , nanops . nanany , _any_see_also , _any_examples , empty_value = False ) cls . all = _make_logical_function ( cls , 'all' , name , name2 , axis_descr , _all_desc , nanops . nanall , _all_see_also , _all_examples , empty_value = True ) @ Substitution ( desc = \"Return the mean absolute deviation of the values \" \"for the requested axis.\" , name1 = name , name2 = name2 , axis_descr = axis_descr , min_count = '' , see_also = '' , examples = '' ) @ Appender ( _num_doc ) def mad ( self , axis = None , skipna = None , level = None ) : if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( 'mad' , axis = axis , level = level , skipna = skipna ) data = self . _get_numeric_data ( ) if axis == 0 : demeaned = data - data . mean ( axis = 0 ) else : demeaned = data . sub ( data . mean ( axis = 1 ) , axis = 0 ) return np . abs ( demeaned ) . mean ( axis = axis , skipna = skipna ) cls . mad = mad cls . sem = _make_stat_function_ddof ( cls , 'sem' , name , name2 , axis_descr , \"Return unbiased standard error of the mean over requested \" \"axis.\\n\\nNormalized by N-1 by default. This can be changed \" \"using the ddof argument\" , nanops . nansem ) cls . var = _make_stat_function_ddof ( cls , 'var' , name , name2 , axis_descr , \"Return unbiased variance over requested axis.\\n\\nNormalized by \" \"N-1 by default. This can be changed using the ddof argument\" , nanops . nanvar ) cls . std = _make_stat_function_ddof ( cls , 'std' , name , name2 , axis_descr , \"Return sample standard deviation over requested axis.\" \"\\n\\nNormalized by N-1 by default. This can be changed using the \" \"ddof argument\" , nanops . nanstd ) @ Substitution ( desc = \"Return the compound percentage of the values for \" \"the requested axis.\" , name1 = name , name2 = name2 , axis_descr = axis_descr , min_count = '' , see_also = '' , examples = '' ) @ Appender ( _num_doc ) def compound ( self , axis = None , skipna = None , level = None ) : if skipna is None : skipna = True return ( 1 + self ) . prod ( axis = axis , skipna = skipna , level = level ) - 1 cls . compound = compound cls . cummin = _make_cum_function ( cls , 'cummin' , name , name2 , axis_descr , \"minimum\" , lambda y , axis : np . minimum . accumulate ( y , axis ) , \"min\" , np . inf , np . nan , _cummin_examples ) cls . cumsum = _make_cum_function ( cls , 'cumsum' , name , name2 , axis_descr , \"sum\" , lambda y , axis : y . cumsum ( axis ) , \"sum\" , 0. , np . nan , _cumsum_examples ) cls . cumprod = _make_cum_function ( cls , 'cumprod' , name , name2 , axis_descr , \"product\" , lambda y , axis : y . cumprod ( axis ) , \"prod\" , 1. , np . nan , _cumprod_examples ) cls . cummax = _make_cum_function ( cls , 'cummax' , name , name2 , axis_descr , \"maximum\" , lambda y , axis : np . maximum . accumulate ( y , axis ) , \"max\" , - np . inf , np . nan , _cummax_examples ) cls . sum = _make_min_count_stat_function ( cls , 'sum' , name , name2 , axis_descr , \"\"\"Return the sum of the values for the requested axis.\\n This is equivalent to the method ``numpy.sum``.\"\"\" , nanops . nansum , _stat_func_see_also , _sum_examples ) cls . mean = _make_stat_function ( cls , 'mean' , name , name2 , axis_descr , 'Return the mean of the values for the requested axis.' , nanops . nanmean ) cls . skew = _make_stat_function ( cls , 'skew' , name , name2 , axis_descr , 'Return unbiased skew over requested axis\\nNormalized by N-1.' , nanops . nanskew ) cls . kurt = _make_stat_function ( cls , 'kurt' , name , name2 , axis_descr , \"Return unbiased kurtosis over requested axis using Fisher's \" \"definition of\\nkurtosis (kurtosis of normal == 0.0). Normalized \" \"by N-1.\" , nanops . nankurt ) cls . kurtosis = cls . kurt cls . prod = _make_min_count_stat_function ( cls , 'prod' , name , name2 , axis_descr , 'Return the product of the values for the requested axis.' , nanops . nanprod , examples = _prod_examples ) cls . product = cls . prod cls . median = _make_stat_function ( cls , 'median' , name , name2 , axis_descr , 'Return the median of the values for the requested axis.' , nanops . nanmedian ) cls . max = _make_stat_function ( cls , 'max' , name , name2 , axis_descr , \"\"\"Return the maximum of the values for the requested axis.\\n If you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.\"\"\" , nanops . nanmax , _stat_func_see_also , _max_examples ) cls . min = _make_stat_function ( cls , 'min' , name , name2 , axis_descr , \"\"\"Return the minimum of the values for the requested axis.\\n If you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.\"\"\" , nanops . nanmin , _stat_func_see_also , _min_examples )\n\n\n        Summary:", "input": "", "output": "Add the operations to the cls ; evaluate the doc strings again"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_series_only_operations ( cls ) : axis_descr , name , name2 = _doc_parms ( cls ) def nanptp ( values , axis = 0 , skipna = True ) : nmax = nanops . nanmax ( values , axis , skipna ) nmin = nanops . nanmin ( values , axis , skipna ) warnings . warn ( \"Method .ptp is deprecated and will be removed \" \"in a future version. Use numpy.ptp instead.\" , FutureWarning , stacklevel = 4 ) return nmax - nmin cls . ptp = _make_stat_function ( cls , 'ptp' , name , name2 , axis_descr , \"\"\"Return the difference between the maximum value and the minimum value in the object. This is the equivalent of the ``numpy.ndarray`` method ``ptp``.\\n\\n.. deprecated:: 0.24.0 Use numpy.ptp instead\"\"\" , nanptp )\n\n\n        Summary:", "input": "", "output": "Add the series only operations to the cls ; evaluate the doc strings again ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_series_or_dataframe_operations ( cls ) : from pandas . core import window as rwindow @ Appender ( rwindow . rolling . __doc__ ) def rolling ( self , window , min_periods = None , center = False , win_type = None , on = None , axis = 0 , closed = None ) : axis = self . _get_axis_number ( axis ) return rwindow . rolling ( self , window = window , min_periods = min_periods , center = center , win_type = win_type , on = on , axis = axis , closed = closed ) cls . rolling = rolling @ Appender ( rwindow . expanding . __doc__ ) def expanding ( self , min_periods = 1 , center = False , axis = 0 ) : axis = self . _get_axis_number ( axis ) return rwindow . expanding ( self , min_periods = min_periods , center = center , axis = axis ) cls . expanding = expanding @ Appender ( rwindow . ewm . __doc__ ) def ewm ( self , com = None , span = None , halflife = None , alpha = None , min_periods = 0 , adjust = True , ignore_na = False , axis = 0 ) : axis = self . _get_axis_number ( axis ) return rwindow . ewm ( self , com = com , span = span , halflife = halflife , alpha = alpha , min_periods = min_periods , adjust = adjust , ignore_na = ignore_na , axis = axis ) cls . ewm = ewm\n\n\n        Summary:", "input": "", "output": "Add the series or dataframe only operations to the cls ; evaluate the doc strings again ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _find_valid_index ( self , how ) : assert how in [ 'first' , 'last' ] if len ( self ) == 0 : # early stop return None is_valid = ~ self . isna ( ) if self . ndim == 2 : is_valid = is_valid . any ( 1 ) # reduce axis 1 if how == 'first' : idxpos = is_valid . values [ : : ] . argmax ( ) if how == 'last' : idxpos = len ( self ) - 1 - is_valid . values [ : : - 1 ] . argmax ( ) chk_notna = is_valid . iat [ idxpos ] idx = self . index [ idxpos ] if not chk_notna : return None return idx\n\n\n        Summary:", "input": "", "output": "Retrieves the index of the first valid value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reset_cache ( self , key = None ) : if getattr ( self , '_cache' , None ) is None : return if key is None : self . _cache . clear ( ) else : self . _cache . pop ( key , None )\n\n\n        Summary:", "input": "", "output": "Reset cached properties . If key is passed only clears that key ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _try_aggregate_string_function ( self , arg , * args , * * kwargs ) : assert isinstance ( arg , str ) f = getattr ( self , arg , None ) if f is not None : if callable ( f ) : return f ( * args , * * kwargs ) # people may try to aggregate on a non-callable attribute # but don't let them think they can pass args to it assert len ( args ) == 0 assert len ( [ kwarg for kwarg in kwargs if kwarg not in [ 'axis' , '_level' ] ] ) == 0 return f f = getattr ( np , arg , None ) if f is not None : return f ( self , * args , * * kwargs ) raise ValueError ( \"{arg} is an unknown string function\" . format ( arg = arg ) )\n\n\n        Summary:", "input": "", "output": "if arg is a string then try to operate on it : - try to find a function ( or attribute ) on ourselves - try to find a numpy function - raise"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _aggregate ( self , arg , * args , * * kwargs ) : is_aggregator = lambda x : isinstance ( x , ( list , tuple , dict ) ) is_nested_renamer = False _axis = kwargs . pop ( '_axis' , None ) if _axis is None : _axis = getattr ( self , 'axis' , 0 ) _level = kwargs . pop ( '_level' , None ) if isinstance ( arg , str ) : return self . _try_aggregate_string_function ( arg , * args , * * kwargs ) , None if isinstance ( arg , dict ) : # aggregate based on the passed dict if _axis != 0 : # pragma: no cover raise ValueError ( 'Can only pass dict with axis=0' ) obj = self . _selected_obj def nested_renaming_depr ( level = 4 ) : # deprecation of nested renaming # GH 15931 warnings . warn ( ( \"using a dict with renaming \" \"is deprecated and will be removed in a future \" \"version\" ) , FutureWarning , stacklevel = level ) # if we have a dict of any non-scalars # eg. {'A' : ['mean']}, normalize all to # be list-likes if any ( is_aggregator ( x ) for x in arg . values ( ) ) : new_arg = OrderedDict ( ) for k , v in arg . items ( ) : if not isinstance ( v , ( tuple , list , dict ) ) : new_arg [ k ] = [ v ] else : new_arg [ k ] = v # the keys must be in the columns # for ndim=2, or renamers for ndim=1 # ok for now, but deprecated # {'A': { 'ra': 'mean' }} # {'A': { 'ra': ['mean'] }} # {'ra': ['mean']} # not ok # {'ra' : { 'A' : 'mean' }} if isinstance ( v , dict ) : is_nested_renamer = True if k not in obj . columns : msg = ( 'cannot perform renaming for {key} with a ' 'nested dictionary' ) . format ( key = k ) raise SpecificationError ( msg ) nested_renaming_depr ( 4 + ( _level or 0 ) ) elif isinstance ( obj , ABCSeries ) : nested_renaming_depr ( ) elif ( isinstance ( obj , ABCDataFrame ) and k not in obj . columns ) : raise KeyError ( \"Column '{col}' does not exist!\" . format ( col = k ) ) arg = new_arg else : # deprecation of renaming keys # GH 15931 keys = list ( arg . keys ( ) ) if ( isinstance ( obj , ABCDataFrame ) and len ( obj . columns . intersection ( keys ) ) != len ( keys ) ) : nested_renaming_depr ( ) from pandas . core . reshape . concat import concat def _agg_1dim ( name , how , subset = None ) : \"\"\" aggregate a 1-dim with how \"\"\" colg = self . _gotitem ( name , ndim = 1 , subset = subset ) if colg . ndim != 1 : raise SpecificationError ( \"nested dictionary is ambiguous \" \"in aggregation\" ) return colg . aggregate ( how , _level = ( _level or 0 ) + 1 ) def _agg_2dim ( name , how ) : \"\"\" aggregate a 2-dim with how \"\"\" colg = self . _gotitem ( self . _selection , ndim = 2 , subset = obj ) return colg . aggregate ( how , _level = None ) def _agg ( arg , func ) : \"\"\" run the aggregations over the arg with func return an OrderedDict \"\"\" result = OrderedDict ( ) for fname , agg_how in arg . items ( ) : result [ fname ] = func ( fname , agg_how ) return result # set the final keys keys = list ( arg . keys ( ) ) result = OrderedDict ( ) # nested renamer if is_nested_renamer : result = list ( _agg ( arg , _agg_1dim ) . values ( ) ) if all ( isinstance ( r , dict ) for r in result ) : result , results = OrderedDict ( ) , result for r in results : result . update ( r ) keys = list ( result . keys ( ) ) else : if self . _selection is not None : keys = None # some selection on the object elif self . _selection is not None : sl = set ( self . _selection_list ) # we are a Series like object, # but may have multiple aggregations if len ( sl ) == 1 : result = _agg ( arg , lambda fname , agg_how : _agg_1dim ( self . _selection , agg_how ) ) # we are selecting the same set as we are aggregating elif not len ( sl - set ( keys ) ) : result = _agg ( arg , _agg_1dim ) # we are a DataFrame, with possibly multiple aggregations else : result = _agg ( arg , _agg_2dim ) # no selection else : try : result = _agg ( arg , _agg_1dim ) except SpecificationError : # we are aggregating expecting all 1d-returns # but we have 2d result = _agg ( arg , _agg_2dim ) # combine results def is_any_series ( ) : # return a boolean if we have *any* nested series return any ( isinstance ( r , ABCSeries ) for r in result . values ( ) ) def is_any_frame ( ) : # return a boolean if we have *any* nested series return any ( isinstance ( r , ABCDataFrame ) for r in result . values ( ) ) if isinstance ( result , list ) : return concat ( result , keys = keys , axis = 1 , sort = True ) , True elif is_any_frame ( ) : # we have a dict of DataFrames # return a MI DataFrame return concat ( [ result [ k ] for k in keys ] , keys = keys , axis = 1 ) , True elif isinstance ( self , ABCSeries ) and is_any_series ( ) : # we have a dict of Series # return a MI Series try : result = concat ( result ) except TypeError : # we want to give a nice error here if # we have non-same sized objects, so # we don't automatically broadcast raise ValueError ( \"cannot perform both aggregation \" \"and transformation operations \" \"simultaneously\" ) return result , True # fall thru from pandas import DataFrame , Series try : result = DataFrame ( result ) except ValueError : # we have a dict of scalars result = Series ( result , name = getattr ( self , 'name' , None ) ) return result , True elif is_list_like ( arg ) : # we require a list, but not an 'str' return self . _aggregate_multiple_funcs ( arg , _level = _level , _axis = _axis ) , None else : result = None f = self . _is_cython_func ( arg ) if f and not args and not kwargs : return getattr ( self , f ) ( ) , None # caller can react return result , True\n\n\n        Summary:", "input": "", "output": "provide an implementation for the aggregators"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _shallow_copy ( self , obj = None , obj_type = None , * * kwargs ) : if obj is None : obj = self . _selected_obj . copy ( ) if obj_type is None : obj_type = self . _constructor if isinstance ( obj , obj_type ) : obj = obj . obj for attr in self . _attributes : if attr not in kwargs : kwargs [ attr ] = getattr ( self , attr ) return obj_type ( obj , * * kwargs )\n\n\n        Summary:", "input": "", "output": "return a new object with the replacement attributes"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def itemsize ( self ) : warnings . warn ( \"{obj}.itemsize is deprecated and will be removed \" \"in a future version\" . format ( obj = type ( self ) . __name__ ) , FutureWarning , stacklevel = 2 ) return self . _ndarray_values . itemsize\n\n\n        Summary:", "input": "", "output": "Return the size of the dtype of the item of the underlying data ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def base ( self ) : warnings . warn ( \"{obj}.base is deprecated and will be removed \" \"in a future version\" . format ( obj = type ( self ) . __name__ ) , FutureWarning , stacklevel = 2 ) return self . values . base\n\n\n        Summary:", "input": "", "output": "Return the base object if the memory of the underlying data is shared ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def array ( self ) -> ExtensionArray : result = self . _values if is_datetime64_ns_dtype ( result . dtype ) : from pandas . arrays import DatetimeArray result = DatetimeArray ( result ) elif is_timedelta64_ns_dtype ( result . dtype ) : from pandas . arrays import TimedeltaArray result = TimedeltaArray ( result ) elif not is_extension_array_dtype ( result . dtype ) : from pandas . core . arrays . numpy_ import PandasArray result = PandasArray ( result ) return result\n\n\n        Summary:", "input": "", "output": "The ExtensionArray of the data backing this Series or Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_numpy ( self , dtype = None , copy = False ) : if is_datetime64tz_dtype ( self . dtype ) and dtype is None : # note: this is going to change very soon. # I have a WIP PR making this unnecessary, but it's # a bit out of scope for the DatetimeArray PR. dtype = \"object\" result = np . asarray ( self . _values , dtype = dtype ) # TODO(GH-24345): Avoid potential double copy if copy : result = result . copy ( ) return result\n\n\n        Summary:", "input": "", "output": "A NumPy ndarray representing the values in this Series or Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ndarray_values ( self ) -> np . ndarray : if is_extension_array_dtype ( self ) : return self . array . _ndarray_values return self . values\n\n\n        Summary:", "input": "", "output": "The data as an ndarray possibly losing information ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def max ( self , axis = None , skipna = True ) : nv . validate_minmax_axis ( axis ) return nanops . nanmax ( self . _values , skipna = skipna )\n\n\n        Summary:", "input": "", "output": "Return the maximum value of the Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argmax ( self , axis = None , skipna = True ) : nv . validate_minmax_axis ( axis ) return nanops . nanargmax ( self . _values , skipna = skipna )\n\n\n        Summary:", "input": "", "output": "Return an ndarray of the maximum argument indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def min ( self , axis = None , skipna = True ) : nv . validate_minmax_axis ( axis ) return nanops . nanmin ( self . _values , skipna = skipna )\n\n\n        Summary:", "input": "", "output": "Return the minimum value of the Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argmin ( self , axis = None , skipna = True ) : nv . validate_minmax_axis ( axis ) return nanops . nanargmin ( self . _values , skipna = skipna )\n\n\n        Summary:", "input": "", "output": "Return a ndarray of the minimum argument indexer ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tolist ( self ) : if is_datetimelike ( self . _values ) : return [ com . maybe_box_datetimelike ( x ) for x in self . _values ] elif is_extension_array_dtype ( self . _values ) : return list ( self . _values ) else : return self . _values . tolist ( )\n\n\n        Summary:", "input": "", "output": "Return a list of the values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reduce ( self , op , name , axis = 0 , skipna = True , numeric_only = None , filter_type = None , * * kwds ) : func = getattr ( self , name , None ) if func is None : raise TypeError ( \"{klass} cannot perform the operation {op}\" . format ( klass = self . __class__ . __name__ , op = name ) ) return func ( skipna = skipna , * * kwds )\n\n\n        Summary:", "input": "", "output": "perform the reduction type operation if we can"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _map_values ( self , mapper , na_action = None ) : # we can fastpath dict/Series to an efficient map # as we know that we are not going to have to yield # python types if isinstance ( mapper , dict ) : if hasattr ( mapper , '__missing__' ) : # If a dictionary subclass defines a default value method, # convert mapper to a lookup function (GH #15999). dict_with_default = mapper mapper = lambda x : dict_with_default [ x ] else : # Dictionary does not have a default. Thus it's safe to # convert to an Series for efficiency. # we specify the keys here to handle the # possibility that they are tuples from pandas import Series mapper = Series ( mapper ) if isinstance ( mapper , ABCSeries ) : # Since values were input this means we came from either # a dict or a series and mapper should be an index if is_extension_type ( self . dtype ) : values = self . _values else : values = self . values indexer = mapper . index . get_indexer ( values ) new_values = algorithms . take_1d ( mapper . _values , indexer ) return new_values # we must convert to python types if is_extension_type ( self . dtype ) : values = self . _values if na_action is not None : raise NotImplementedError map_f = lambda values , f : values . map ( f ) else : values = self . astype ( object ) values = getattr ( values , 'values' , values ) if na_action == 'ignore' : def map_f ( values , f ) : return lib . map_infer_mask ( values , f , isna ( values ) . view ( np . uint8 ) ) else : map_f = lib . map_infer # mapper is a function new_values = map_f ( values , mapper ) return new_values\n\n\n        Summary:", "input": "", "output": "An internal function that maps values using the input correspondence ( which can be a dict Series or function ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def value_counts ( self , normalize = False , sort = True , ascending = False , bins = None , dropna = True ) : from pandas . core . algorithms import value_counts result = value_counts ( self , sort = sort , ascending = ascending , normalize = normalize , bins = bins , dropna = dropna ) return result\n\n\n        Summary:", "input": "", "output": "Return a Series containing counts of unique values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nunique ( self , dropna = True ) : uniqs = self . unique ( ) n = len ( uniqs ) if dropna and isna ( uniqs ) . any ( ) : n -= 1 return n\n\n\n        Summary:", "input": "", "output": "Return number of unique elements in the object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def memory_usage ( self , deep = False ) : if hasattr ( self . array , 'memory_usage' ) : return self . array . memory_usage ( deep = deep ) v = self . array . nbytes if deep and is_object_dtype ( self ) and not PYPY : v += lib . memory_usage_of_objects ( self . array ) return v\n\n\n        Summary:", "input": "", "output": "Memory usage of the values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _expand_user ( filepath_or_buffer ) : if isinstance ( filepath_or_buffer , str ) : return os . path . expanduser ( filepath_or_buffer ) return filepath_or_buffer\n\n\n        Summary:", "input": "", "output": "Return the argument with an initial component of ~ or ~user replaced by that user s home directory ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _stringify_path ( filepath_or_buffer ) : try : import pathlib _PATHLIB_INSTALLED = True except ImportError : _PATHLIB_INSTALLED = False try : from py . path import local as LocalPath _PY_PATH_INSTALLED = True except ImportError : _PY_PATH_INSTALLED = False if hasattr ( filepath_or_buffer , '__fspath__' ) : return filepath_or_buffer . __fspath__ ( ) if _PATHLIB_INSTALLED and isinstance ( filepath_or_buffer , pathlib . Path ) : return str ( filepath_or_buffer ) if _PY_PATH_INSTALLED and isinstance ( filepath_or_buffer , LocalPath ) : return filepath_or_buffer . strpath return _expand_user ( filepath_or_buffer )\n\n\n        Summary:", "input": "", "output": "Attempt to convert a path - like object to a string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_filepath_or_buffer ( filepath_or_buffer , encoding = None , compression = None , mode = None ) : filepath_or_buffer = _stringify_path ( filepath_or_buffer ) if _is_url ( filepath_or_buffer ) : req = urlopen ( filepath_or_buffer ) content_encoding = req . headers . get ( 'Content-Encoding' , None ) if content_encoding == 'gzip' : # Override compression based on Content-Encoding header compression = 'gzip' reader = BytesIO ( req . read ( ) ) req . close ( ) return reader , encoding , compression , True if is_s3_url ( filepath_or_buffer ) : from pandas . io import s3 return s3 . get_filepath_or_buffer ( filepath_or_buffer , encoding = encoding , compression = compression , mode = mode ) if is_gcs_url ( filepath_or_buffer ) : from pandas . io import gcs return gcs . get_filepath_or_buffer ( filepath_or_buffer , encoding = encoding , compression = compression , mode = mode ) if isinstance ( filepath_or_buffer , ( str , bytes , mmap . mmap ) ) : return _expand_user ( filepath_or_buffer ) , None , compression , False if not is_file_like ( filepath_or_buffer ) : msg = \"Invalid file path or buffer object type: {_type}\" raise ValueError ( msg . format ( _type = type ( filepath_or_buffer ) ) ) return filepath_or_buffer , None , compression , False\n\n\n        Summary:", "input": "", "output": "If the filepath_or_buffer is a url translate and return the buffer . Otherwise passthrough ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _infer_compression ( filepath_or_buffer , compression ) : # No compression has been explicitly specified if compression is None : return None # Infer compression if compression == 'infer' : # Convert all path types (e.g. pathlib.Path) to strings filepath_or_buffer = _stringify_path ( filepath_or_buffer ) if not isinstance ( filepath_or_buffer , str ) : # Cannot infer compression of a buffer, assume no compression return None # Infer compression from the filename/URL extension for compression , extension in _compression_to_extension . items ( ) : if filepath_or_buffer . endswith ( extension ) : return compression return None # Compression has been specified. Check that it's valid if compression in _compression_to_extension : return compression msg = 'Unrecognized compression type: {}' . format ( compression ) valid = [ 'infer' , None ] + sorted ( _compression_to_extension ) msg += '\\nValid compression types are {}' . format ( valid ) raise ValueError ( msg )\n\n\n        Summary:", "input": "", "output": "Get the compression method for filepath_or_buffer . If compression = infer the inferred compression method is returned . Otherwise the input compression method is returned unchanged unless it s invalid in which case an error is raised ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_handle ( path_or_buf , mode , encoding = None , compression = None , memory_map = False , is_text = True ) : try : from s3fs import S3File need_text_wrapping = ( BytesIO , S3File ) except ImportError : need_text_wrapping = ( BytesIO , ) handles = list ( ) f = path_or_buf # Convert pathlib.Path/py.path.local or string path_or_buf = _stringify_path ( path_or_buf ) is_path = isinstance ( path_or_buf , str ) if is_path : compression = _infer_compression ( path_or_buf , compression ) if compression : # GZ Compression if compression == 'gzip' : if is_path : f = gzip . open ( path_or_buf , mode ) else : f = gzip . GzipFile ( fileobj = path_or_buf ) # BZ Compression elif compression == 'bz2' : if is_path : f = bz2 . BZ2File ( path_or_buf , mode ) else : f = bz2 . BZ2File ( path_or_buf ) # ZIP Compression elif compression == 'zip' : zf = BytesZipFile ( path_or_buf , mode ) # Ensure the container is closed as well. handles . append ( zf ) if zf . mode == 'w' : f = zf elif zf . mode == 'r' : zip_names = zf . namelist ( ) if len ( zip_names ) == 1 : f = zf . open ( zip_names . pop ( ) ) elif len ( zip_names ) == 0 : raise ValueError ( 'Zero files found in ZIP file {}' . format ( path_or_buf ) ) else : raise ValueError ( 'Multiple files found in ZIP file.' ' Only one file per ZIP: {}' . format ( zip_names ) ) # XZ Compression elif compression == 'xz' : f = lzma . LZMAFile ( path_or_buf , mode ) # Unrecognized Compression else : msg = 'Unrecognized compression type: {}' . format ( compression ) raise ValueError ( msg ) handles . append ( f ) elif is_path : if encoding : # Encoding f = open ( path_or_buf , mode , encoding = encoding , newline = \"\" ) elif is_text : # No explicit encoding f = open ( path_or_buf , mode , errors = 'replace' , newline = \"\" ) else : # Binary mode f = open ( path_or_buf , mode ) handles . append ( f ) # Convert BytesIO or file objects passed with an encoding if is_text and ( compression or isinstance ( f , need_text_wrapping ) ) : from io import TextIOWrapper f = TextIOWrapper ( f , encoding = encoding , newline = '' ) handles . append ( f ) if memory_map and hasattr ( f , 'fileno' ) : try : g = MMapWrapper ( f ) f . close ( ) f = g except Exception : # we catch any errors that may have occurred # because that is consistent with the lower-level # functionality of the C engine (pd.read_csv), so # leave the file handler as is then pass return f , handles\n\n\n        Summary:", "input": "", "output": "Get file handle for given path / buffer and mode ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _td_array_cmp ( cls , op ) : opname = '__{name}__' . format ( name = op . __name__ ) nat_result = opname == '__ne__' def wrapper ( self , other ) : if isinstance ( other , ( ABCDataFrame , ABCSeries , ABCIndexClass ) ) : return NotImplemented if _is_convertible_to_td ( other ) or other is NaT : try : other = Timedelta ( other ) except ValueError : # failed to parse as timedelta return ops . invalid_comparison ( self , other , op ) result = op ( self . view ( 'i8' ) , other . value ) if isna ( other ) : result . fill ( nat_result ) elif not is_list_like ( other ) : return ops . invalid_comparison ( self , other , op ) elif len ( other ) != len ( self ) : raise ValueError ( \"Lengths must match\" ) else : try : other = type ( self ) . _from_sequence ( other ) . _data except ( ValueError , TypeError ) : return ops . invalid_comparison ( self , other , op ) result = op ( self . view ( 'i8' ) , other . view ( 'i8' ) ) result = com . values_from_object ( result ) o_mask = np . array ( isna ( other ) ) if o_mask . any ( ) : result [ o_mask ] = nat_result if self . _hasnans : result [ self . _isnan ] = nat_result return result return compat . set_function_name ( wrapper , opname , cls )\n\n\n        Summary:", "input": "", "output": "Wrap comparison operations to convert timedelta - like to timedelta64"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sequence_to_td64ns ( data , copy = False , unit = \"ns\" , errors = \"raise\" ) : inferred_freq = None unit = parse_timedelta_unit ( unit ) # Unwrap whatever we have into a np.ndarray if not hasattr ( data , 'dtype' ) : # e.g. list, tuple if np . ndim ( data ) == 0 : # i.e. generator data = list ( data ) data = np . array ( data , copy = False ) elif isinstance ( data , ABCSeries ) : data = data . _values elif isinstance ( data , ( ABCTimedeltaIndex , TimedeltaArray ) ) : inferred_freq = data . freq data = data . _data # Convert whatever we have into timedelta64[ns] dtype if is_object_dtype ( data . dtype ) or is_string_dtype ( data . dtype ) : # no need to make a copy, need to convert if string-dtyped data = objects_to_td64ns ( data , unit = unit , errors = errors ) copy = False elif is_integer_dtype ( data . dtype ) : # treat as multiples of the given unit data , copy_made = ints_to_td64ns ( data , unit = unit ) copy = copy and not copy_made elif is_float_dtype ( data . dtype ) : # cast the unit, multiply base/frace separately # to avoid precision issues from float -> int mask = np . isnan ( data ) m , p = precision_from_unit ( unit ) base = data . astype ( np . int64 ) frac = data - base if p : frac = np . round ( frac , p ) data = ( base * m + ( frac * m ) . astype ( np . int64 ) ) . view ( 'timedelta64[ns]' ) data [ mask ] = iNaT copy = False elif is_timedelta64_dtype ( data . dtype ) : if data . dtype != _TD_DTYPE : # non-nano unit # TODO: watch out for overflows data = data . astype ( _TD_DTYPE ) copy = False elif is_datetime64_dtype ( data ) : # GH#23539 warnings . warn ( \"Passing datetime64-dtype data to TimedeltaIndex is \" \"deprecated, will raise a TypeError in a future \" \"version\" , FutureWarning , stacklevel = 4 ) data = ensure_int64 ( data ) . view ( _TD_DTYPE ) else : raise TypeError ( \"dtype {dtype} cannot be converted to timedelta64[ns]\" . format ( dtype = data . dtype ) ) data = np . array ( data , copy = copy ) if data . ndim != 1 : raise ValueError ( \"Only 1-dimensional input arrays are supported.\" ) assert data . dtype == 'm8[ns]' , data return data , inferred_freq\n\n\n        Summary:", "input": "", "output": "Parameters ---------- array : list - like copy : bool default False unit : str default ns The timedelta unit to treat integers as multiples of . errors : { raise coerce ignore } default raise How to handle elements that cannot be converted to timedelta64 [ ns ] . See pandas . to_timedelta for details ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def ints_to_td64ns ( data , unit = \"ns\" ) : copy_made = False unit = unit if unit is not None else \"ns\" if data . dtype != np . int64 : # converting to int64 makes a copy, so we can avoid # re-copying later data = data . astype ( np . int64 ) copy_made = True if unit != \"ns\" : dtype_str = \"timedelta64[{unit}]\" . format ( unit = unit ) data = data . view ( dtype_str ) # TODO: watch out for overflows when converting from lower-resolution data = data . astype ( \"timedelta64[ns]\" ) # the astype conversion makes a copy, so we can avoid re-copying later copy_made = True else : data = data . view ( \"timedelta64[ns]\" ) return data , copy_made\n\n\n        Summary:", "input": "", "output": "Convert an ndarray with integer - dtype to timedelta64 [ ns ] dtype treating the integers as multiples of the given timedelta unit ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def objects_to_td64ns ( data , unit = \"ns\" , errors = \"raise\" ) : # coerce Index to np.ndarray, converting string-dtype if necessary values = np . array ( data , dtype = np . object_ , copy = False ) result = array_to_timedelta64 ( values , unit = unit , errors = errors ) return result . view ( 'timedelta64[ns]' )\n\n\n        Summary:", "input": "", "output": "Convert a object - dtyped or string - dtyped array into an timedelta64 [ ns ] - dtyped array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _add_datetime_arraylike ( self , other ) : if isinstance ( other , np . ndarray ) : # At this point we have already checked that dtype is datetime64 from pandas . core . arrays import DatetimeArray other = DatetimeArray ( other ) # defer to implementation in DatetimeArray return other + self\n\n\n        Summary:", "input": "", "output": "Add DatetimeArray / Index or ndarray [ datetime64 ] to TimedeltaArray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def components ( self ) : from pandas import DataFrame columns = [ 'days' , 'hours' , 'minutes' , 'seconds' , 'milliseconds' , 'microseconds' , 'nanoseconds' ] hasnans = self . _hasnans if hasnans : def f ( x ) : if isna ( x ) : return [ np . nan ] * len ( columns ) return x . components else : def f ( x ) : return x . components result = DataFrame ( [ f ( x ) for x in self ] , columns = columns ) if not hasnans : result = result . astype ( 'int64' ) return result\n\n\n        Summary:", "input": "", "output": "Return a dataframe of the components ( days hours minutes seconds milliseconds microseconds nanoseconds ) of the Timedeltas ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def register_writer ( klass ) : if not callable ( klass ) : raise ValueError ( \"Can only register callables as engines\" ) engine_name = klass . engine _writers [ engine_name ] = klass\n\n\n        Summary:", "input": "", "output": "Add engine to the excel writer registry . io . excel ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _excel2num ( x ) : index = 0 for c in x . upper ( ) . strip ( ) : cp = ord ( c ) if cp < ord ( \"A\" ) or cp > ord ( \"Z\" ) : raise ValueError ( \"Invalid column name: {x}\" . format ( x = x ) ) index = index * 26 + cp - ord ( \"A\" ) + 1 return index - 1\n\n\n        Summary:", "input": "", "output": "Convert Excel column name like AB to 0 - based column index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _range2cols ( areas ) : cols = [ ] for rng in areas . split ( \",\" ) : if \":\" in rng : rng = rng . split ( \":\" ) cols . extend ( lrange ( _excel2num ( rng [ 0 ] ) , _excel2num ( rng [ 1 ] ) + 1 ) ) else : cols . append ( _excel2num ( rng ) ) return cols\n\n\n        Summary:", "input": "", "output": "Convert comma separated list of column names and ranges to indices ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_convert_usecols ( usecols ) : if usecols is None : return usecols if is_integer ( usecols ) : warnings . warn ( ( \"Passing in an integer for `usecols` has been \" \"deprecated. Please pass in a list of int from \" \"0 to `usecols` inclusive instead.\" ) , FutureWarning , stacklevel = 2 ) return lrange ( usecols + 1 ) if isinstance ( usecols , str ) : return _range2cols ( usecols ) return usecols\n\n\n        Summary:", "input": "", "output": "Convert usecols into a compatible format for parsing in parsers . py ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _fill_mi_header ( row , control_row ) : last = row [ 0 ] for i in range ( 1 , len ( row ) ) : if not control_row [ i ] : last = row [ i ] if row [ i ] == '' or row [ i ] is None : row [ i ] = last else : control_row [ i ] = False last = row [ i ] return row , control_row\n\n\n        Summary:", "input": "", "output": "Forward fill blank entries in row but only inside the same parent index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _pop_header_name ( row , index_col ) : # Pop out header name and fill w/blank. i = index_col if not is_list_like ( index_col ) else max ( index_col ) header_name = row [ i ] header_name = None if header_name == \"\" else header_name return header_name , row [ : i ] + [ '' ] + row [ i + 1 : ]\n\n\n        Summary:", "input": "", "output": "Pop the header name for MultiIndex parsing ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ensure_scope ( level , global_dict = None , local_dict = None , resolvers = ( ) , target = None , * * kwargs ) : return Scope ( level + 1 , global_dict = global_dict , local_dict = local_dict , resolvers = resolvers , target = target )\n\n\n        Summary:", "input": "", "output": "Ensure that we are grabbing the correct scope ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replacer ( x ) : # get the hex repr of the binary char and remove 0x and pad by pad_size # zeros try : hexin = ord ( x ) except TypeError : # bytes literals masquerade as ints when iterating in py3 hexin = x return hex ( hexin )\n\n\n        Summary:", "input": "", "output": "Replace a number with its hexadecimal representation . Used to tag temporary variables with their calling scope s id ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _raw_hex_id ( obj ) : # interpret as a pointer since that's what really what id returns packed = struct . pack ( '@P' , id ( obj ) ) return '' . join ( map ( _replacer , packed ) )\n\n\n        Summary:", "input": "", "output": "Return the padded hexadecimal id of obj ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_pretty_string ( obj ) : sio = StringIO ( ) pprint . pprint ( obj , stream = sio ) return sio . getvalue ( )\n\n\n        Summary:", "input": "", "output": "Return a prettier version of obj"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def resolve ( self , key , is_local ) : try : # only look for locals in outer scope if is_local : return self . scope [ key ] # not a local variable so check in resolvers if we have them if self . has_resolvers : return self . resolvers [ key ] # if we're here that means that we have no locals and we also have # no resolvers assert not is_local and not self . has_resolvers return self . scope [ key ] except KeyError : try : # last ditch effort we look in temporaries # these are created when parsing indexing expressions # e.g., df[df > 0] return self . temps [ key ] except KeyError : raise compu . ops . UndefinedVariableError ( key , is_local )\n\n\n        Summary:", "input": "", "output": "Resolve a variable name in a possibly local context"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def swapkey ( self , old_key , new_key , new_value = None ) : if self . has_resolvers : maps = self . resolvers . maps + self . scope . maps else : maps = self . scope . maps maps . append ( self . temps ) for mapping in maps : if old_key in mapping : mapping [ new_key ] = new_value return\n\n\n        Summary:", "input": "", "output": "Replace a variable name with a potentially new value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_vars ( self , stack , scopes ) : variables = itertools . product ( scopes , stack ) for scope , ( frame , _ , _ , _ , _ , _ ) in variables : try : d = getattr ( frame , 'f_' + scope ) self . scope = self . scope . new_child ( d ) finally : # won't remove it, but DECREF it # in Py3 this probably isn't necessary since frame won't be # scope after the loop del frame\n\n\n        Summary:", "input": "", "output": "Get specifically scoped variables from a list of stack frames ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update ( self , level ) : sl = level + 1 # add sl frames to the scope starting with the # most distant and overwriting with more current # makes sure that we can capture variable scope stack = inspect . stack ( ) try : self . _get_vars ( stack [ : sl ] , scopes = [ 'locals' ] ) finally : del stack [ : ] , stack\n\n\n        Summary:", "input": "", "output": "Update the current scope by going back level levels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_tmp ( self , value ) : name = '{name}_{num}_{hex_id}' . format ( name = type ( value ) . __name__ , num = self . ntemps , hex_id = _raw_hex_id ( self ) ) # add to inner most scope assert name not in self . temps self . temps [ name ] = value assert name in self . temps # only increment if the variable gets put in the scope return name\n\n\n        Summary:", "input": "", "output": "Add a temporary variable to the scope ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def full_scope ( self ) : maps = [ self . temps ] + self . resolvers . maps + self . scope . maps return DeepChainMap ( * maps )\n\n\n        Summary:", "input": "", "output": "Return the full scope for use with passing to engines transparently as a mapping ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def read_sas ( filepath_or_buffer , format = None , index = None , encoding = None , chunksize = None , iterator = False ) : if format is None : buffer_error_msg = ( \"If this is a buffer object rather \" \"than a string name, you must specify \" \"a format string\" ) filepath_or_buffer = _stringify_path ( filepath_or_buffer ) if not isinstance ( filepath_or_buffer , str ) : raise ValueError ( buffer_error_msg ) fname = filepath_or_buffer . lower ( ) if fname . endswith ( \".xpt\" ) : format = \"xport\" elif fname . endswith ( \".sas7bdat\" ) : format = \"sas7bdat\" else : raise ValueError ( \"unable to infer format of SAS file\" ) if format . lower ( ) == 'xport' : from pandas . io . sas . sas_xport import XportReader reader = XportReader ( filepath_or_buffer , index = index , encoding = encoding , chunksize = chunksize ) elif format . lower ( ) == 'sas7bdat' : from pandas . io . sas . sas7bdat import SAS7BDATReader reader = SAS7BDATReader ( filepath_or_buffer , index = index , encoding = encoding , chunksize = chunksize ) else : raise ValueError ( 'unknown SAS format' ) if iterator or chunksize : return reader data = reader . read ( ) reader . close ( ) return data\n\n\n        Summary:", "input": "", "output": "Read SAS files stored as either XPORT or SAS7BDAT format files ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _coerce_method ( converter ) : def wrapper ( self ) : if len ( self ) == 1 : return converter ( self . iloc [ 0 ] ) raise TypeError ( \"cannot convert the series to \" \"{0}\" . format ( str ( converter ) ) ) wrapper . __name__ = \"__{name}__\" . format ( name = converter . __name__ ) return wrapper\n\n\n        Summary:", "input": "", "output": "Install the scalar coercion methods ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _init_dict ( self , data , index = None , dtype = None ) : # Looking for NaN in dict doesn't work ({np.nan : 1}[float('nan')] # raises KeyError), so we iterate the entire dict, and align if data : keys , values = zip ( * data . items ( ) ) values = list ( values ) elif index is not None : # fastpath for Series(data=None). Just use broadcasting a scalar # instead of reindexing. values = na_value_for_dtype ( dtype ) keys = index else : keys , values = [ ] , [ ] # Input is now list-like, so rely on \"standard\" construction: s = Series ( values , index = keys , dtype = dtype ) # Now we just make sure the order is respected, if any if data and index is not None : s = s . reindex ( index , copy = False ) elif not PY36 and not isinstance ( data , OrderedDict ) and data : # Need the `and data` to avoid sorting Series(None, index=[...]) # since that isn't really dict-like try : s = s . sort_index ( ) except TypeError : pass return s . _data , s . index\n\n\n        Summary:", "input": "", "output": "Derive the _data and index attributes of a new Series from a dictionary input ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_array ( cls , arr , index = None , name = None , dtype = None , copy = False , fastpath = False ) : warnings . warn ( \"'from_array' is deprecated and will be removed in a \" \"future version. Please use the pd.Series(..) \" \"constructor instead.\" , FutureWarning , stacklevel = 2 ) if isinstance ( arr , ABCSparseArray ) : from pandas . core . sparse . series import SparseSeries cls = SparseSeries return cls ( arr , index = index , name = name , dtype = dtype , copy = copy , fastpath = fastpath )\n\n\n        Summary:", "input": "", "output": "Construct Series from array ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_axis ( self , axis , labels , fastpath = False ) : if not fastpath : labels = ensure_index ( labels ) is_all_dates = labels . is_all_dates if is_all_dates : if not isinstance ( labels , ( DatetimeIndex , PeriodIndex , TimedeltaIndex ) ) : try : labels = DatetimeIndex ( labels ) # need to set here because we changed the index if fastpath : self . _data . set_axis ( axis , labels ) except ( tslibs . OutOfBoundsDatetime , ValueError ) : # labels may exceeds datetime bounds, # or not be a DatetimeIndex pass self . _set_subtyp ( is_all_dates ) object . __setattr__ ( self , '_index' , labels ) if not fastpath : self . _data . set_axis ( axis , labels )\n\n\n        Summary:", "input": "", "output": "Override generic we want to set the _typ here ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def asobject ( self ) : warnings . warn ( \"'asobject' is deprecated. Use 'astype(object)'\" \" instead\" , FutureWarning , stacklevel = 2 ) return self . astype ( object ) . values\n\n\n        Summary:", "input": "", "output": "Return object Series which contains boxed values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def compress ( self , condition , * args , * * kwargs ) : msg = ( \"Series.compress(condition) is deprecated. \" \"Use 'Series[condition]' or \" \"'np.asarray(series).compress(condition)' instead.\" ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) nv . validate_compress ( args , kwargs ) return self [ condition ]\n\n\n        Summary:", "input": "", "output": "Return selected slices of an array along given axis as a Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nonzero ( self ) : msg = ( \"Series.nonzero() is deprecated \" \"and will be removed in a future version.\" \"Use Series.to_numpy().nonzero() instead\" ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) return self . _values . nonzero ( )\n\n\n        Summary:", "input": "", "output": "Return the * integer * indices of the elements that are non - zero ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def view ( self , dtype = None ) : return self . _constructor ( self . _values . view ( dtype ) , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Create a new view of the Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _ixs ( self , i , axis = 0 ) : try : # dispatch to the values if we need values = self . _values if isinstance ( values , np . ndarray ) : return libindex . get_value_at ( values , i ) else : return values [ i ] except IndexError : raise except Exception : if isinstance ( i , slice ) : indexer = self . index . _convert_slice_indexer ( i , kind = 'iloc' ) return self . _get_values ( indexer ) else : label = self . index [ i ] if isinstance ( label , Index ) : return self . take ( i , axis = axis , convert = True ) else : return libindex . get_value_at ( self , i )\n\n\n        Summary:", "input": "", "output": "Return the i - th value or values in the Series by location ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def repeat ( self , repeats , axis = None ) : nv . validate_repeat ( tuple ( ) , dict ( axis = axis ) ) new_index = self . index . repeat ( repeats ) new_values = self . _values . repeat ( repeats ) return self . _constructor ( new_values , index = new_index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Repeat elements of a Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reset_index ( self , level = None , drop = False , name = None , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) if drop : new_index = ibase . default_index ( len ( self ) ) if level is not None : if not isinstance ( level , ( tuple , list ) ) : level = [ level ] level = [ self . index . _get_level_number ( lev ) for lev in level ] if len ( level ) < self . index . nlevels : new_index = self . index . droplevel ( level ) if inplace : self . index = new_index # set name if it was passed, otherwise, keep the previous name self . name = name or self . name else : return self . _constructor ( self . _values . copy ( ) , index = new_index ) . __finalize__ ( self ) elif inplace : raise TypeError ( 'Cannot reset_index inplace on a Series ' 'to create a DataFrame' ) else : df = self . to_frame ( name ) return df . reset_index ( level = level , drop = drop )\n\n\n        Summary:", "input": "", "output": "Generate a new DataFrame or Series with the index reset ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_string ( self , buf = None , na_rep = 'NaN' , float_format = None , header = True , index = True , length = False , dtype = False , name = False , max_rows = None ) : formatter = fmt . SeriesFormatter ( self , name = name , length = length , header = header , index = index , dtype = dtype , na_rep = na_rep , float_format = float_format , max_rows = max_rows ) result = formatter . to_string ( ) # catch contract violations if not isinstance ( result , str ) : raise AssertionError ( \"result must be of type unicode, type\" \" of result is {0!r}\" \"\" . format ( result . __class__ . __name__ ) ) if buf is None : return result else : try : buf . write ( result ) except AttributeError : with open ( buf , 'w' ) as f : f . write ( result )\n\n\n        Summary:", "input": "", "output": "Render a string representation of the Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_dict ( self , into = dict ) : # GH16122 into_c = com . standardize_mapping ( into ) return into_c ( self . items ( ) )\n\n\n        Summary:", "input": "", "output": "Convert Series to { label - > value } dict or dict - like object ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_frame ( self , name = None ) : if name is None : df = self . _constructor_expanddim ( self ) else : df = self . _constructor_expanddim ( { name : self } ) return df\n\n\n        Summary:", "input": "", "output": "Convert Series to DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_sparse ( self , kind = 'block' , fill_value = None ) : # TODO: deprecate from pandas . core . sparse . series import SparseSeries values = SparseArray ( self , kind = kind , fill_value = fill_value ) return SparseSeries ( values , index = self . index , name = self . name ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Convert Series to SparseSeries ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_name ( self , name , inplace = False ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) ser = self if inplace else self . copy ( ) ser . name = name return ser\n\n\n        Summary:", "input": "", "output": "Set the Series name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def count ( self , level = None ) : if level is None : return notna ( com . values_from_object ( self ) ) . sum ( ) if isinstance ( level , str ) : level = self . index . _get_level_number ( level ) lev = self . index . levels [ level ] level_codes = np . array ( self . index . codes [ level ] , subok = False , copy = True ) mask = level_codes == - 1 if mask . any ( ) : level_codes [ mask ] = cnt = len ( lev ) lev = lev . insert ( cnt , lev . _na_value ) obs = level_codes [ notna ( self . values ) ] out = np . bincount ( obs , minlength = len ( lev ) or None ) return self . _constructor ( out , index = lev , dtype = 'int64' ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Return number of non - NA / null observations in the Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def drop_duplicates ( self , keep = 'first' , inplace = False ) : return super ( ) . drop_duplicates ( keep = keep , inplace = inplace )\n\n\n        Summary:", "input": "", "output": "Return Series with duplicate values removed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def idxmin ( self , axis = 0 , skipna = True , * args , * * kwargs ) : skipna = nv . validate_argmin_with_skipna ( skipna , args , kwargs ) i = nanops . nanargmin ( com . values_from_object ( self ) , skipna = skipna ) if i == - 1 : return np . nan return self . index [ i ]\n\n\n        Summary:", "input": "", "output": "Return the row label of the minimum value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def idxmax ( self , axis = 0 , skipna = True , * args , * * kwargs ) : skipna = nv . validate_argmax_with_skipna ( skipna , args , kwargs ) i = nanops . nanargmax ( com . values_from_object ( self ) , skipna = skipna ) if i == - 1 : return np . nan return self . index [ i ]\n\n\n        Summary:", "input": "", "output": "Return the row label of the maximum value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def round ( self , decimals = 0 , * args , * * kwargs ) : nv . validate_round ( args , kwargs ) result = com . values_from_object ( self ) . round ( decimals ) result = self . _constructor ( result , index = self . index ) . __finalize__ ( self ) return result\n\n\n        Summary:", "input": "", "output": "Round each value in a Series to the given number of decimals ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def quantile ( self , q = 0.5 , interpolation = 'linear' ) : self . _check_percentile ( q ) # We dispatch to DataFrame so that core.internals only has to worry # about 2D cases. df = self . to_frame ( ) result = df . quantile ( q = q , interpolation = interpolation , numeric_only = False ) if result . ndim == 2 : result = result . iloc [ : , 0 ] if is_list_like ( q ) : result . name = self . name return self . _constructor ( result , index = Float64Index ( q ) , name = self . name ) else : # scalar return result . iloc [ 0 ]\n\n\n        Summary:", "input": "", "output": "Return value at the given quantile ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def corr ( self , other , method = 'pearson' , min_periods = None ) : this , other = self . align ( other , join = 'inner' , copy = False ) if len ( this ) == 0 : return np . nan if method in [ 'pearson' , 'spearman' , 'kendall' ] or callable ( method ) : return nanops . nancorr ( this . values , other . values , method = method , min_periods = min_periods ) raise ValueError ( \"method must be either 'pearson', \" \"'spearman', 'kendall', or a callable, \" \"'{method}' was supplied\" . format ( method = method ) )\n\n\n        Summary:", "input": "", "output": "Compute correlation with other Series excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cov ( self , other , min_periods = None ) : this , other = self . align ( other , join = 'inner' , copy = False ) if len ( this ) == 0 : return np . nan return nanops . nancov ( this . values , other . values , min_periods = min_periods )\n\n\n        Summary:", "input": "", "output": "Compute covariance with Series excluding missing values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def diff ( self , periods = 1 ) : result = algorithms . diff ( com . values_from_object ( self ) , periods ) return self . _constructor ( result , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "First discrete difference of element ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dot ( self , other ) : from pandas . core . frame import DataFrame if isinstance ( other , ( Series , DataFrame ) ) : common = self . index . union ( other . index ) if ( len ( common ) > len ( self . index ) or len ( common ) > len ( other . index ) ) : raise ValueError ( 'matrices are not aligned' ) left = self . reindex ( index = common , copy = False ) right = other . reindex ( index = common , copy = False ) lvals = left . values rvals = right . values else : lvals = self . values rvals = np . asarray ( other ) if lvals . shape [ 0 ] != rvals . shape [ 0 ] : raise Exception ( 'Dot product shape mismatch, %s vs %s' % ( lvals . shape , rvals . shape ) ) if isinstance ( other , DataFrame ) : return self . _constructor ( np . dot ( lvals , rvals ) , index = other . columns ) . __finalize__ ( self ) elif isinstance ( other , Series ) : return np . dot ( lvals , rvals ) elif isinstance ( rvals , np . ndarray ) : return np . dot ( lvals , rvals ) else : # pragma: no cover raise TypeError ( 'unsupported type: %s' % type ( other ) )\n\n\n        Summary:", "input": "", "output": "Compute the dot product between the Series and the columns of other ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def append ( self , to_append , ignore_index = False , verify_integrity = False ) : from pandas . core . reshape . concat import concat if isinstance ( to_append , ( list , tuple ) ) : to_concat = [ self ] + to_append else : to_concat = [ self , to_append ] return concat ( to_concat , ignore_index = ignore_index , verify_integrity = verify_integrity )\n\n\n        Summary:", "input": "", "output": "Concatenate two or more Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _binop ( self , other , func , level = None , fill_value = None ) : if not isinstance ( other , Series ) : raise AssertionError ( 'Other operand must be Series' ) new_index = self . index this = self if not self . index . equals ( other . index ) : this , other = self . align ( other , level = level , join = 'outer' , copy = False ) new_index = this . index this_vals , other_vals = ops . fill_binop ( this . values , other . values , fill_value ) with np . errstate ( all = 'ignore' ) : result = func ( this_vals , other_vals ) name = ops . get_op_result_name ( self , other ) if func . __name__ in [ 'divmod' , 'rdivmod' ] : ret = ops . _construct_divmod_result ( self , result , new_index , name ) else : ret = ops . _construct_result ( self , result , new_index , name ) return ret\n\n\n        Summary:", "input": "", "output": "Perform generic binary operation with optional fill value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine ( self , other , func , fill_value = None ) : if fill_value is None : fill_value = na_value_for_dtype ( self . dtype , compat = False ) if isinstance ( other , Series ) : # If other is a Series, result is based on union of Series, # so do this element by element new_index = self . index . union ( other . index ) new_name = ops . get_op_result_name ( self , other ) new_values = [ ] for idx in new_index : lv = self . get ( idx , fill_value ) rv = other . get ( idx , fill_value ) with np . errstate ( all = 'ignore' ) : new_values . append ( func ( lv , rv ) ) else : # Assume that other is a scalar, so apply the function for # each element in the Series new_index = self . index with np . errstate ( all = 'ignore' ) : new_values = [ func ( lv , other ) for lv in self . _values ] new_name = self . name if is_categorical_dtype ( self . values ) : pass elif is_extension_array_dtype ( self . values ) : # The function can return something of any type, so check # if the type is compatible with the calling EA. try : new_values = self . _values . _from_sequence ( new_values ) except Exception : # https://github.com/pandas-dev/pandas/issues/22850 # pandas has no control over what 3rd-party ExtensionArrays # do in _values_from_sequence. We still want ops to work # though, so we catch any regular Exception. pass return self . _constructor ( new_values , index = new_index , name = new_name )\n\n\n        Summary:", "input": "", "output": "Combine the Series with a Series or scalar according to func ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def combine_first ( self , other ) : new_index = self . index . union ( other . index ) this = self . reindex ( new_index , copy = False ) other = other . reindex ( new_index , copy = False ) if is_datetimelike ( this ) and not is_datetimelike ( other ) : other = to_datetime ( other ) return this . where ( notna ( this ) , other )\n\n\n        Summary:", "input": "", "output": "Combine Series values choosing the calling Series s values first ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def update ( self , other ) : other = other . reindex_like ( self ) mask = notna ( other ) self . _data = self . _data . putmask ( mask = mask , new = other , inplace = True ) self . _maybe_update_cacher ( )\n\n\n        Summary:", "input": "", "output": "Modify Series in place using non - NA values from passed Series . Aligns on index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_values ( self , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) # Validate the axis parameter self . _get_axis_number ( axis ) # GH 5856/5853 if inplace and self . _is_cached : raise ValueError ( \"This Series is a view of some other array, to \" \"sort in-place you must create a copy\" ) def _try_kind_sort ( arr ) : # easier to ask forgiveness than permission try : # if kind==mergesort, it can fail for object dtype return arr . argsort ( kind = kind ) except TypeError : # stable sort not available for object dtype # uses the argsort default quicksort return arr . argsort ( kind = 'quicksort' ) arr = self . _values sortedIdx = np . empty ( len ( self ) , dtype = np . int32 ) bad = isna ( arr ) good = ~ bad idx = ibase . default_index ( len ( self ) ) argsorted = _try_kind_sort ( arr [ good ] ) if is_list_like ( ascending ) : if len ( ascending ) != 1 : raise ValueError ( 'Length of ascending (%d) must be 1 ' 'for Series' % ( len ( ascending ) ) ) ascending = ascending [ 0 ] if not is_bool ( ascending ) : raise ValueError ( 'ascending must be boolean' ) if not ascending : argsorted = argsorted [ : : - 1 ] if na_position == 'last' : n = good . sum ( ) sortedIdx [ : n ] = idx [ good ] [ argsorted ] sortedIdx [ n : ] = idx [ bad ] elif na_position == 'first' : n = bad . sum ( ) sortedIdx [ n : ] = idx [ good ] [ argsorted ] sortedIdx [ : n ] = idx [ bad ] else : raise ValueError ( 'invalid na_position: {!r}' . format ( na_position ) ) result = self . _constructor ( arr [ sortedIdx ] , index = self . index [ sortedIdx ] ) if inplace : self . _update_inplace ( result ) else : return result . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Sort by the values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sort_index ( self , axis = 0 , level = None , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , sort_remaining = True ) : # TODO: this can be combined with DataFrame.sort_index impl as # almost identical inplace = validate_bool_kwarg ( inplace , 'inplace' ) # Validate the axis parameter self . _get_axis_number ( axis ) index = self . index if level is not None : new_index , indexer = index . sortlevel ( level , ascending = ascending , sort_remaining = sort_remaining ) elif isinstance ( index , MultiIndex ) : from pandas . core . sorting import lexsort_indexer labels = index . _sort_levels_monotonic ( ) indexer = lexsort_indexer ( labels . _get_codes_for_sorting ( ) , orders = ascending , na_position = na_position ) else : from pandas . core . sorting import nargsort # Check monotonic-ness before sort an index # GH11080 if ( ( ascending and index . is_monotonic_increasing ) or ( not ascending and index . is_monotonic_decreasing ) ) : if inplace : return else : return self . copy ( ) indexer = nargsort ( index , kind = kind , ascending = ascending , na_position = na_position ) indexer = ensure_platform_int ( indexer ) new_index = index . take ( indexer ) new_index = new_index . _sort_levels_monotonic ( ) new_values = self . _values . take ( indexer ) result = self . _constructor ( new_values , index = new_index ) if inplace : self . _update_inplace ( result ) else : return result . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Sort Series by index labels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def argsort ( self , axis = 0 , kind = 'quicksort' , order = None ) : values = self . _values mask = isna ( values ) if mask . any ( ) : result = Series ( - 1 , index = self . index , name = self . name , dtype = 'int64' ) notmask = ~ mask result [ notmask ] = np . argsort ( values [ notmask ] , kind = kind ) return self . _constructor ( result , index = self . index ) . __finalize__ ( self ) else : return self . _constructor ( np . argsort ( values , kind = kind ) , index = self . index , dtype = 'int64' ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Override ndarray . argsort . Argsorts the value omitting NA / null values and places the result in the same locations as the non - NA values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nlargest ( self , n = 5 , keep = 'first' ) : return algorithms . SelectNSeries ( self , n = n , keep = keep ) . nlargest ( )\n\n\n        Summary:", "input": "", "output": "Return the largest n elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def nsmallest ( self , n = 5 , keep = 'first' ) : return algorithms . SelectNSeries ( self , n = n , keep = keep ) . nsmallest ( )\n\n\n        Summary:", "input": "", "output": "Return the smallest n elements ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def swaplevel ( self , i = - 2 , j = - 1 , copy = True ) : new_index = self . index . swaplevel ( i , j ) return self . _constructor ( self . _values , index = new_index , copy = copy ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Swap levels i and j in a MultiIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reorder_levels ( self , order ) : if not isinstance ( self . index , MultiIndex ) : # pragma: no cover raise Exception ( 'Can only reorder levels on a hierarchical axis.' ) result = self . copy ( ) result . index = result . index . reorder_levels ( order ) return result\n\n\n        Summary:", "input": "", "output": "Rearrange index levels using input order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map ( self , arg , na_action = None ) : new_values = super ( ) . _map_values ( arg , na_action = na_action ) return self . _constructor ( new_values , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Map values of Series according to input correspondence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def apply ( self , func , convert_dtype = True , args = ( ) , * * kwds ) : if len ( self ) == 0 : return self . _constructor ( dtype = self . dtype , index = self . index ) . __finalize__ ( self ) # dispatch to agg if isinstance ( func , ( list , dict ) ) : return self . aggregate ( func , * args , * * kwds ) # if we are a string, try to dispatch if isinstance ( func , str ) : return self . _try_aggregate_string_function ( func , * args , * * kwds ) # handle ufuncs and lambdas if kwds or args and not isinstance ( func , np . ufunc ) : def f ( x ) : return func ( x , * args , * * kwds ) else : f = func with np . errstate ( all = 'ignore' ) : if isinstance ( f , np . ufunc ) : return f ( self ) # row-wise access if is_extension_type ( self . dtype ) : mapped = self . _values . map ( f ) else : values = self . astype ( object ) . values mapped = lib . map_infer ( values , f , convert = convert_dtype ) if len ( mapped ) and isinstance ( mapped [ 0 ] , Series ) : from pandas . core . frame import DataFrame return DataFrame ( mapped . tolist ( ) , index = self . index ) else : return self . _constructor ( mapped , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Invoke function on values of Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _reduce ( self , op , name , axis = 0 , skipna = True , numeric_only = None , filter_type = None , * * kwds ) : delegate = self . _values if axis is not None : self . _get_axis_number ( axis ) if isinstance ( delegate , Categorical ) : # TODO deprecate numeric_only argument for Categorical and use # skipna as well, see GH25303 return delegate . _reduce ( name , numeric_only = numeric_only , * * kwds ) elif isinstance ( delegate , ExtensionArray ) : # dispatch to ExtensionArray interface return delegate . _reduce ( name , skipna = skipna , * * kwds ) elif is_datetime64_dtype ( delegate ) : # use DatetimeIndex implementation to handle skipna correctly delegate = DatetimeIndex ( delegate ) # dispatch to numpy arrays elif isinstance ( delegate , np . ndarray ) : if numeric_only : raise NotImplementedError ( 'Series.{0} does not implement ' 'numeric_only.' . format ( name ) ) with np . errstate ( all = 'ignore' ) : return op ( delegate , skipna = skipna , * * kwds ) # TODO(EA) dispatch to Index # remove once all internals extension types are # moved to ExtensionArrays return delegate . _reduce ( op = op , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , filter_type = filter_type , * * kwds )\n\n\n        Summary:", "input": "", "output": "Perform a reduction operation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def rename ( self , index = None , * * kwargs ) : kwargs [ 'inplace' ] = validate_bool_kwarg ( kwargs . get ( 'inplace' , False ) , 'inplace' ) non_mapping = is_scalar ( index ) or ( is_list_like ( index ) and not is_dict_like ( index ) ) if non_mapping : return self . _set_name ( index , inplace = kwargs . get ( 'inplace' ) ) return super ( ) . rename ( index = index , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Alter Series index labels or name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reindex_axis ( self , labels , axis = 0 , * * kwargs ) : # for compatibility with higher dims if axis != 0 : raise ValueError ( \"cannot reindex series on non-zero axis!\" ) msg = ( \"'.reindex_axis' is deprecated and will be removed in a future \" \"version. Use '.reindex' instead.\" ) warnings . warn ( msg , FutureWarning , stacklevel = 2 ) return self . reindex ( index = labels , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Conform Series to new index with optional filling logic ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def memory_usage ( self , index = True , deep = False ) : v = super ( ) . memory_usage ( deep = deep ) if index : v += self . index . memory_usage ( deep = deep ) return v\n\n\n        Summary:", "input": "", "output": "Return the memory usage of the Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def isin ( self , values ) : result = algorithms . isin ( self , values ) return self . _constructor ( result , index = self . index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Check whether values are contained in Series ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def between ( self , left , right , inclusive = True ) : if inclusive : lmask = self >= left rmask = self <= right else : lmask = self > left rmask = self < right return lmask & rmask\n\n\n        Summary:", "input": "", "output": "Return boolean Series equivalent to left < = series < = right ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_csv ( cls , path , sep = ',' , parse_dates = True , header = None , index_col = 0 , encoding = None , infer_datetime_format = False ) : # We're calling `DataFrame.from_csv` in the implementation, # which will propagate a warning regarding `from_csv` deprecation. from pandas . core . frame import DataFrame df = DataFrame . from_csv ( path , header = header , index_col = index_col , sep = sep , parse_dates = parse_dates , encoding = encoding , infer_datetime_format = infer_datetime_format ) result = df . iloc [ : , 0 ] if header is None : result . index . name = result . name = None return result\n\n\n        Summary:", "input": "", "output": "Read CSV file ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def dropna ( self , axis = 0 , inplace = False , * * kwargs ) : inplace = validate_bool_kwarg ( inplace , 'inplace' ) kwargs . pop ( 'how' , None ) if kwargs : raise TypeError ( 'dropna() got an unexpected keyword ' 'argument \"{0}\"' . format ( list ( kwargs . keys ( ) ) [ 0 ] ) ) # Validate the axis parameter self . _get_axis_number ( axis or 0 ) if self . _can_hold_na : result = remove_na_arraylike ( self ) if inplace : self . _update_inplace ( result ) else : return result else : if inplace : # do nothing pass else : return self . copy ( )\n\n\n        Summary:", "input": "", "output": "Return a new Series with missing values removed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def valid ( self , inplace = False , * * kwargs ) : warnings . warn ( \"Method .valid will be removed in a future version. \" \"Use .dropna instead.\" , FutureWarning , stacklevel = 2 ) return self . dropna ( inplace = inplace , * * kwargs )\n\n\n        Summary:", "input": "", "output": "Return Series without null values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_timestamp ( self , freq = None , how = 'start' , copy = True ) : new_values = self . _values if copy : new_values = new_values . copy ( ) new_index = self . index . to_timestamp ( freq = freq , how = how ) return self . _constructor ( new_values , index = new_index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Cast to DatetimeIndex of Timestamps at * beginning * of period ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_period ( self , freq = None , copy = True ) : new_values = self . _values if copy : new_values = new_values . copy ( ) new_index = self . index . to_period ( freq = freq ) return self . _constructor ( new_values , index = new_index ) . __finalize__ ( self )\n\n\n        Summary:", "input": "", "output": "Convert Series from DatetimeIndex to PeriodIndex with desired frequency ( inferred from index if not passed ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_numeric ( arg , errors = 'raise' , downcast = None ) : if downcast not in ( None , 'integer' , 'signed' , 'unsigned' , 'float' ) : raise ValueError ( 'invalid downcasting method provided' ) is_series = False is_index = False is_scalars = False if isinstance ( arg , ABCSeries ) : is_series = True values = arg . values elif isinstance ( arg , ABCIndexClass ) : is_index = True values = arg . asi8 if values is None : values = arg . values elif isinstance ( arg , ( list , tuple ) ) : values = np . array ( arg , dtype = 'O' ) elif is_scalar ( arg ) : if is_decimal ( arg ) : return float ( arg ) if is_number ( arg ) : return arg is_scalars = True values = np . array ( [ arg ] , dtype = 'O' ) elif getattr ( arg , 'ndim' , 1 ) > 1 : raise TypeError ( 'arg must be a list, tuple, 1-d array, or Series' ) else : values = arg try : if is_numeric_dtype ( values ) : pass elif is_datetime_or_timedelta_dtype ( values ) : values = values . astype ( np . int64 ) else : values = ensure_object ( values ) coerce_numeric = errors not in ( 'ignore' , 'raise' ) values = lib . maybe_convert_numeric ( values , set ( ) , coerce_numeric = coerce_numeric ) except Exception : if errors == 'raise' : raise # attempt downcast only if the data has been successfully converted # to a numerical dtype and if a downcast method has been specified if downcast is not None and is_numeric_dtype ( values ) : typecodes = None if downcast in ( 'integer' , 'signed' ) : typecodes = np . typecodes [ 'Integer' ] elif downcast == 'unsigned' and np . min ( values ) >= 0 : typecodes = np . typecodes [ 'UnsignedInteger' ] elif downcast == 'float' : typecodes = np . typecodes [ 'Float' ] # pandas support goes only to np.float32, # as float dtypes smaller than that are # extremely rare and not well supported float_32_char = np . dtype ( np . float32 ) . char float_32_ind = typecodes . index ( float_32_char ) typecodes = typecodes [ float_32_ind : ] if typecodes is not None : # from smallest to largest for dtype in typecodes : if np . dtype ( dtype ) . itemsize <= values . dtype . itemsize : values = maybe_downcast_to_dtype ( values , dtype ) # successful conversion if values . dtype == dtype : break if is_series : return pd . Series ( values , index = arg . index , name = arg . name ) elif is_index : # because we want to coerce to numeric if possible, # do not use _shallow_copy_with_infer return pd . Index ( values , name = arg . name ) elif is_scalars : return values [ 0 ] else : return values\n\n\n        Summary:", "input": "", "output": "Convert argument to a numeric type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_fill ( arr : ABCSparseArray ) -> np . ndarray : try : return np . asarray ( arr . fill_value , dtype = arr . dtype . subtype ) except ValueError : return np . asarray ( arr . fill_value )\n\n\n        Summary:", "input": "", "output": "Create a 0 - dim ndarray containing the fill value"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sparse_array_op ( left : ABCSparseArray , right : ABCSparseArray , op : Callable , name : str ) -> Any : if name . startswith ( '__' ) : # For lookups in _libs.sparse we need non-dunder op name name = name [ 2 : - 2 ] # dtype used to find corresponding sparse method ltype = left . dtype . subtype rtype = right . dtype . subtype if not is_dtype_equal ( ltype , rtype ) : subtype = find_common_type ( [ ltype , rtype ] ) ltype = SparseDtype ( subtype , left . fill_value ) rtype = SparseDtype ( subtype , right . fill_value ) # TODO(GH-23092): pass copy=False. Need to fix astype_nansafe left = left . astype ( ltype ) right = right . astype ( rtype ) dtype = ltype . subtype else : dtype = ltype # dtype the result must have result_dtype = None if left . sp_index . ngaps == 0 or right . sp_index . ngaps == 0 : with np . errstate ( all = 'ignore' ) : result = op ( left . get_values ( ) , right . get_values ( ) ) fill = op ( _get_fill ( left ) , _get_fill ( right ) ) if left . sp_index . ngaps == 0 : index = left . sp_index else : index = right . sp_index elif left . sp_index . equals ( right . sp_index ) : with np . errstate ( all = 'ignore' ) : result = op ( left . sp_values , right . sp_values ) fill = op ( _get_fill ( left ) , _get_fill ( right ) ) index = left . sp_index else : if name [ 0 ] == 'r' : left , right = right , left name = name [ 1 : ] if name in ( 'and' , 'or' ) and dtype == 'bool' : opname = 'sparse_{name}_uint8' . format ( name = name ) # to make template simple, cast here left_sp_values = left . sp_values . view ( np . uint8 ) right_sp_values = right . sp_values . view ( np . uint8 ) result_dtype = np . bool else : opname = 'sparse_{name}_{dtype}' . format ( name = name , dtype = dtype ) left_sp_values = left . sp_values right_sp_values = right . sp_values sparse_op = getattr ( splib , opname ) with np . errstate ( all = 'ignore' ) : result , index , fill = sparse_op ( left_sp_values , left . sp_index , left . fill_value , right_sp_values , right . sp_index , right . fill_value ) if result_dtype is None : result_dtype = result . dtype return _wrap_result ( name , result , index , fill , dtype = result_dtype )\n\n\n        Summary:", "input": "", "output": "Perform a binary operation between two arrays ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _wrap_result ( name , data , sparse_index , fill_value , dtype = None ) : if name . startswith ( '__' ) : # e.g. __eq__ --> eq name = name [ 2 : - 2 ] if name in ( 'eq' , 'ne' , 'lt' , 'gt' , 'le' , 'ge' ) : dtype = np . bool fill_value = lib . item_from_zerodim ( fill_value ) if is_bool_dtype ( dtype ) : # fill_value may be np.bool_ fill_value = bool ( fill_value ) return SparseArray ( data , sparse_index = sparse_index , fill_value = fill_value , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "wrap op result to have correct dtype"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _maybe_to_sparse ( array ) : if isinstance ( array , ABCSparseSeries ) : array = array . values . copy ( ) return array\n\n\n        Summary:", "input": "", "output": "array must be SparseSeries or SparseArray"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sanitize_values ( arr ) : if hasattr ( arr , 'values' ) : arr = arr . values else : # scalar if is_scalar ( arr ) : arr = [ arr ] # ndarray if isinstance ( arr , np . ndarray ) : pass elif is_list_like ( arr ) and len ( arr ) > 0 : arr = maybe_convert_platform ( arr ) else : arr = np . asarray ( arr ) return arr\n\n\n        Summary:", "input": "", "output": "return an ndarray for our input in a platform independent manner"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def make_sparse ( arr , kind = 'block' , fill_value = None , dtype = None , copy = False ) : arr = _sanitize_values ( arr ) if arr . ndim > 1 : raise TypeError ( \"expected dimension <= 1 data\" ) if fill_value is None : fill_value = na_value_for_dtype ( arr . dtype ) if isna ( fill_value ) : mask = notna ( arr ) else : # cast to object comparison to be safe if is_string_dtype ( arr ) : arr = arr . astype ( object ) if is_object_dtype ( arr . dtype ) : # element-wise equality check method in numpy doesn't treat # each element type, eg. 0, 0.0, and False are treated as # same. So we have to check the both of its type and value. mask = splib . make_mask_object_ndarray ( arr , fill_value ) else : mask = arr != fill_value length = len ( arr ) if length != len ( mask ) : # the arr is a SparseArray indices = mask . sp_index . indices else : indices = mask . nonzero ( ) [ 0 ] . astype ( np . int32 ) index = _make_index ( length , indices , kind ) sparsified_values = arr [ mask ] if dtype is not None : sparsified_values = astype_nansafe ( sparsified_values , dtype = dtype ) # TODO: copy return sparsified_values , index , fill_value\n\n\n        Summary:", "input": "", "output": "Convert ndarray to sparse format"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def density ( self ) : r = float ( self . sp_index . npoints ) / float ( self . sp_index . length ) return r\n\n\n        Summary:", "input": "", "output": "The percent of non - fill_value points as decimal ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def fillna ( self , value = None , method = None , limit = None ) : if ( ( method is None and value is None ) or ( method is not None and value is not None ) ) : raise ValueError ( \"Must specify one of 'method' or 'value'.\" ) elif method is not None : msg = \"fillna with 'method' requires high memory usage.\" warnings . warn ( msg , PerformanceWarning ) filled = interpolate_2d ( np . asarray ( self ) , method = method , limit = limit ) return type ( self ) ( filled , fill_value = self . fill_value ) else : new_values = np . where ( isna ( self . sp_values ) , value , self . sp_values ) if self . _null_fill_value : # This is essentially just updating the dtype. new_dtype = SparseDtype ( self . dtype . subtype , fill_value = value ) else : new_dtype = self . dtype return self . _simple_new ( new_values , self . _sparse_index , new_dtype )\n\n\n        Summary:", "input": "", "output": "Fill missing values with value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _first_fill_value_loc ( self ) : if len ( self ) == 0 or self . sp_index . npoints == len ( self ) : return - 1 indices = self . sp_index . to_int_index ( ) . indices if not len ( indices ) or indices [ 0 ] > 0 : return 0 diff = indices [ 1 : ] - indices [ : - 1 ] return np . searchsorted ( diff , 2 ) + 1\n\n\n        Summary:", "input": "", "output": "Get the location of the first missing value ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def value_counts ( self , dropna = True ) : from pandas import Index , Series keys , counts = algos . _value_counts_arraylike ( self . sp_values , dropna = dropna ) fcounts = self . sp_index . ngaps if fcounts > 0 : if self . _null_fill_value and dropna : pass else : if self . _null_fill_value : mask = isna ( keys ) else : mask = keys == self . fill_value if mask . any ( ) : counts [ mask ] += fcounts else : keys = np . insert ( keys , 0 , self . fill_value ) counts = np . insert ( counts , 0 , fcounts ) if not isinstance ( keys , ABCIndexClass ) : keys = Index ( keys ) result = Series ( counts , index = keys ) return result\n\n\n        Summary:", "input": "", "output": "Returns a Series containing counts of unique values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def astype ( self , dtype = None , copy = True ) : dtype = self . dtype . update_dtype ( dtype ) subtype = dtype . _subtype_with_str sp_values = astype_nansafe ( self . sp_values , subtype , copy = copy ) if sp_values is self . sp_values and copy : sp_values = sp_values . copy ( ) return self . _simple_new ( sp_values , self . sp_index , dtype )\n\n\n        Summary:", "input": "", "output": "Change the dtype of a SparseArray ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def map ( self , mapper ) : # this is used in apply. # We get hit since we're an \"is_extension_type\" but regular extension # types are not hit. This may be worth adding to the interface. if isinstance ( mapper , ABCSeries ) : mapper = mapper . to_dict ( ) if isinstance ( mapper , abc . Mapping ) : fill_value = mapper . get ( self . fill_value , self . fill_value ) sp_values = [ mapper . get ( x , None ) for x in self . sp_values ] else : fill_value = mapper ( self . fill_value ) sp_values = [ mapper ( x ) for x in self . sp_values ] return type ( self ) ( sp_values , sparse_index = self . sp_index , fill_value = fill_value )\n\n\n        Summary:", "input": "", "output": "Map categories using input correspondence ( dict Series or function ) ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def all ( self , axis = None , * args , * * kwargs ) : nv . validate_all ( args , kwargs ) values = self . sp_values if len ( values ) != len ( self ) and not np . all ( self . fill_value ) : return False return values . all ( )\n\n\n        Summary:", "input": "", "output": "Tests whether all elements evaluate True"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def any ( self , axis = 0 , * args , * * kwargs ) : nv . validate_any ( args , kwargs ) values = self . sp_values if len ( values ) != len ( self ) and np . any ( self . fill_value ) : return True return values . any ( ) . item ( )\n\n\n        Summary:", "input": "", "output": "Tests whether at least one of elements evaluate True"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sum ( self , axis = 0 , * args , * * kwargs ) : nv . validate_sum ( args , kwargs ) valid_vals = self . _valid_sp_values sp_sum = valid_vals . sum ( ) if self . _null_fill_value : return sp_sum else : nsparse = self . sp_index . ngaps return sp_sum + self . fill_value * nsparse\n\n\n        Summary:", "input": "", "output": "Sum of non - NA / null values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def cumsum ( self , axis = 0 , * args , * * kwargs ) : nv . validate_cumsum ( args , kwargs ) if axis is not None and axis >= self . ndim : # Mimic ndarray behaviour. raise ValueError ( \"axis(={axis}) out of bounds\" . format ( axis = axis ) ) if not self . _null_fill_value : return SparseArray ( self . to_dense ( ) ) . cumsum ( ) return SparseArray ( self . sp_values . cumsum ( ) , sparse_index = self . sp_index , fill_value = self . fill_value )\n\n\n        Summary:", "input": "", "output": "Cumulative sum of non - NA / null values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def mean ( self , axis = 0 , * args , * * kwargs ) : nv . validate_mean ( args , kwargs ) valid_vals = self . _valid_sp_values sp_sum = valid_vals . sum ( ) ct = len ( valid_vals ) if self . _null_fill_value : return sp_sum / ct else : nsparse = self . sp_index . ngaps return ( sp_sum + self . fill_value * nsparse ) / ( ct + nsparse )\n\n\n        Summary:", "input": "", "output": "Mean of non - NA / null values"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def tokenize_string ( source ) : line_reader = StringIO ( source ) . readline token_generator = tokenize . generate_tokens ( line_reader ) # Loop over all tokens till a backtick (`) is found. # Then, take all tokens till the next backtick to form a backtick quoted # string. for toknum , tokval , _ , _ , _ in token_generator : if tokval == '`' : tokval = \" \" . join ( it . takewhile ( lambda tokval : tokval != '`' , map ( operator . itemgetter ( 1 ) , token_generator ) ) ) toknum = _BACKTICK_QUOTED_STRING yield toknum , tokval\n\n\n        Summary:", "input": "", "output": "Tokenize a Python source code string ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replace_booleans ( tok ) : toknum , tokval = tok if toknum == tokenize . OP : if tokval == '&' : return tokenize . NAME , 'and' elif tokval == '|' : return tokenize . NAME , 'or' return toknum , tokval return toknum , tokval\n\n\n        Summary:", "input": "", "output": "Replace & with and and | with or so that bitwise precedence is changed to boolean precedence ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _replace_locals ( tok ) : toknum , tokval = tok if toknum == tokenize . OP and tokval == '@' : return tokenize . OP , _LOCAL_TAG return toknum , tokval\n\n\n        Summary:", "input": "", "output": "Replace local variables with a syntactically valid name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _clean_spaces_backtick_quoted_names ( tok ) : toknum , tokval = tok if toknum == _BACKTICK_QUOTED_STRING : return tokenize . NAME , _remove_spaces_column_name ( tokval ) return toknum , tokval\n\n\n        Summary:", "input": "", "output": "Clean up a column name if surrounded by backticks ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _preparse ( source , f = _compose ( _replace_locals , _replace_booleans , _rewrite_assign , _clean_spaces_backtick_quoted_names ) ) : assert callable ( f ) , 'f must be callable' return tokenize . untokenize ( lmap ( f , tokenize_string ( source ) ) )\n\n\n        Summary:", "input": "", "output": "Compose a collection of tokenization functions"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _filter_nodes ( superclass , all_nodes = _all_nodes ) : node_names = ( node . __name__ for node in all_nodes if issubclass ( node , superclass ) ) return frozenset ( node_names )\n\n\n        Summary:", "input": "", "output": "Filter out AST nodes that are subclasses of superclass ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _node_not_implemented ( node_name , cls ) : def f ( self , * args , * * kwargs ) : raise NotImplementedError ( \"{name!r} nodes are not \" \"implemented\" . format ( name = node_name ) ) return f\n\n\n        Summary:", "input": "", "output": "Return a function that raises a NotImplementedError with a passed node name ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def disallow ( nodes ) : def disallowed ( cls ) : cls . unsupported_nodes = ( ) for node in nodes : new_method = _node_not_implemented ( node , cls ) name = 'visit_{node}' . format ( node = node ) cls . unsupported_nodes += ( name , ) setattr ( cls , name , new_method ) return cls return disallowed\n\n\n        Summary:", "input": "", "output": "Decorator to disallow certain nodes from parsing . Raises a NotImplementedError instead ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _op_maker ( op_class , op_symbol ) : def f ( self , node , * args , * * kwargs ) : \"\"\"Return a partial function with an Op subclass with an operator already passed. Returns ------- f : callable \"\"\" return partial ( op_class , op_symbol , * args , * * kwargs ) return f\n\n\n        Summary:", "input": "", "output": "Return a function to create an op class with its symbol already passed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def add_ops ( op_classes ) : def f ( cls ) : for op_attr_name , op_class in op_classes . items ( ) : ops = getattr ( cls , '{name}_ops' . format ( name = op_attr_name ) ) ops_map = getattr ( cls , '{name}_op_nodes_map' . format ( name = op_attr_name ) ) for op in ops : op_node = ops_map [ op ] if op_node is not None : made_op = _op_maker ( op_class , op ) setattr ( cls , 'visit_{node}' . format ( node = op_node ) , made_op ) return cls return f\n\n\n        Summary:", "input": "", "output": "Decorator to add default implementation of ops ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def names ( self ) : if is_term ( self . terms ) : return frozenset ( [ self . terms . name ] ) return frozenset ( term . name for term in com . flatten ( self . terms ) )\n\n\n        Summary:", "input": "", "output": "Get the names in an expression"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_convertible_to_index ( other ) : if isinstance ( other , TimedeltaIndex ) : return True elif ( len ( other ) > 0 and other . inferred_type not in ( 'floating' , 'mixed-integer' , 'integer' , 'mixed-integer-float' , 'mixed' ) ) : return True return False\n\n\n        Summary:", "input": "", "output": "return a boolean whether I can attempt conversion to a TimedeltaIndex"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def timedelta_range ( start = None , end = None , periods = None , freq = None , name = None , closed = None ) : if freq is None and com . _any_none ( periods , start , end ) : freq = 'D' freq , freq_infer = dtl . maybe_infer_freq ( freq ) tdarr = TimedeltaArray . _generate_range ( start , end , periods , freq , closed = closed ) return TimedeltaIndex . _simple_new ( tdarr . _data , freq = tdarr . freq , name = name )\n\n\n        Summary:", "input": "", "output": "Return a fixed frequency TimedeltaIndex with day as the default frequency"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def union ( self , other ) : if isinstance ( other , tuple ) : other = list ( other ) return type ( self ) ( super ( ) . __add__ ( other ) )\n\n\n        Summary:", "input": "", "output": "Returns a FrozenList with other concatenated to the end of self ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def difference ( self , other ) : other = set ( other ) temp = [ x for x in self if x not in other ] return type ( self ) ( temp )\n\n\n        Summary:", "input": "", "output": "Returns a FrozenList with elements from other removed from self ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def searchsorted ( self , value , side = \"left\" , sorter = None ) : # We are much more performant if the searched # indexer is the same type as the array. # # This doesn't matter for int64, but DOES # matter for smaller int dtypes. # # xref: https://github.com/numpy/numpy/issues/5370 try : value = self . dtype . type ( value ) except ValueError : pass return super ( ) . searchsorted ( value , side = side , sorter = sorter )\n\n\n        Summary:", "input": "", "output": "Find indices to insert value so as to maintain order ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def arrays_to_mgr ( arrays , arr_names , index , columns , dtype = None ) : # figure out the index, if necessary if index is None : index = extract_index ( arrays ) else : index = ensure_index ( index ) # don't force copy because getting jammed in an ndarray anyway arrays = _homogenize ( arrays , index , dtype ) # from BlockManager perspective axes = [ ensure_index ( columns ) , index ] return create_block_manager_from_arrays ( arrays , arr_names , axes )\n\n\n        Summary:", "input": "", "output": "Segregate Series based on type and coerce into matrices ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def masked_rec_array_to_mgr ( data , index , columns , dtype , copy ) : # essentially process a record array then fill it fill_value = data . fill_value fdata = ma . getdata ( data ) if index is None : index = get_names_from_index ( fdata ) if index is None : index = ibase . default_index ( len ( data ) ) index = ensure_index ( index ) if columns is not None : columns = ensure_index ( columns ) arrays , arr_columns = to_arrays ( fdata , columns ) # fill if needed new_arrays = [ ] for fv , arr , col in zip ( fill_value , arrays , arr_columns ) : mask = ma . getmaskarray ( data [ col ] ) if mask . any ( ) : arr , fv = maybe_upcast ( arr , fill_value = fv , copy = True ) arr [ mask ] = fv new_arrays . append ( arr ) # create the manager arrays , arr_columns = reorder_arrays ( new_arrays , arr_columns , columns ) if columns is None : columns = arr_columns mgr = arrays_to_mgr ( arrays , arr_columns , index , columns , dtype ) if copy : mgr = mgr . copy ( ) return mgr\n\n\n        Summary:", "input": "", "output": "Extract from a masked rec array and create the manager ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def init_dict ( data , index , columns , dtype = None ) : if columns is not None : from pandas . core . series import Series arrays = Series ( data , index = columns , dtype = object ) data_names = arrays . index missing = arrays . isnull ( ) if index is None : # GH10856 # raise ValueError if only scalars in dict index = extract_index ( arrays [ ~ missing ] ) else : index = ensure_index ( index ) # no obvious \"empty\" int column if missing . any ( ) and not is_integer_dtype ( dtype ) : if dtype is None or np . issubdtype ( dtype , np . flexible ) : # GH#1783 nan_dtype = object else : nan_dtype = dtype val = construct_1d_arraylike_from_scalar ( np . nan , len ( index ) , nan_dtype ) arrays . loc [ missing ] = [ val ] * missing . sum ( ) else : keys = com . dict_keys_to_ordered_list ( data ) columns = data_names = Index ( keys ) # GH#24096 need copy to be deep for datetime64tz case # TODO: See if we can avoid these copies arrays = [ data [ k ] if not is_datetime64tz_dtype ( data [ k ] ) else data [ k ] . copy ( deep = True ) for k in keys ] return arrays_to_mgr ( arrays , data_names , index , columns , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Segregate Series based on type and coerce into matrices . Needs to handle a lot of exceptional cases ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_arrays ( data , columns , coerce_float = False , dtype = None ) : if isinstance ( data , ABCDataFrame ) : if columns is not None : arrays = [ data . _ixs ( i , axis = 1 ) . values for i , col in enumerate ( data . columns ) if col in columns ] else : columns = data . columns arrays = [ data . _ixs ( i , axis = 1 ) . values for i in range ( len ( columns ) ) ] return arrays , columns if not len ( data ) : if isinstance ( data , np . ndarray ) : columns = data . dtype . names if columns is not None : return [ [ ] ] * len ( columns ) , columns return [ ] , [ ] # columns if columns is not None else [] if isinstance ( data [ 0 ] , ( list , tuple ) ) : return _list_to_arrays ( data , columns , coerce_float = coerce_float , dtype = dtype ) elif isinstance ( data [ 0 ] , abc . Mapping ) : return _list_of_dict_to_arrays ( data , columns , coerce_float = coerce_float , dtype = dtype ) elif isinstance ( data [ 0 ] , ABCSeries ) : return _list_of_series_to_arrays ( data , columns , coerce_float = coerce_float , dtype = dtype ) elif isinstance ( data [ 0 ] , Categorical ) : if columns is None : columns = ibase . default_index ( len ( data ) ) return data , columns elif ( isinstance ( data , ( np . ndarray , ABCSeries , Index ) ) and data . dtype . names is not None ) : columns = list ( data . dtype . names ) arrays = [ data [ k ] for k in columns ] return arrays , columns else : # last ditch effort data = lmap ( tuple , data ) return _list_to_arrays ( data , columns , coerce_float = coerce_float , dtype = dtype )\n\n\n        Summary:", "input": "", "output": "Return list of arrays columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sanitize_index ( data , index , copy = False ) : if index is None : return data if len ( data ) != len ( index ) : raise ValueError ( 'Length of values does not match length of index' ) if isinstance ( data , ABCIndexClass ) and not copy : pass elif isinstance ( data , ( ABCPeriodIndex , ABCDatetimeIndex ) ) : data = data . _values if copy : data = data . copy ( ) elif isinstance ( data , np . ndarray ) : # coerce datetimelike types if data . dtype . kind in [ 'M' , 'm' ] : data = sanitize_array ( data , index , copy = copy ) return data\n\n\n        Summary:", "input": "", "output": "Sanitize an index type to return an ndarray of the underlying pass through a non - Index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sanitize_array ( data , index , dtype = None , copy = False , raise_cast_failure = False ) : if dtype is not None : dtype = pandas_dtype ( dtype ) if isinstance ( data , ma . MaskedArray ) : mask = ma . getmaskarray ( data ) if mask . any ( ) : data , fill_value = maybe_upcast ( data , copy = True ) data . soften_mask ( ) # set hardmask False if it was True data [ mask ] = fill_value else : data = data . copy ( ) data = extract_array ( data , extract_numpy = True ) # GH#846 if isinstance ( data , np . ndarray ) : if dtype is not None : subarr = np . array ( data , copy = False ) # possibility of nan -> garbage if is_float_dtype ( data . dtype ) and is_integer_dtype ( dtype ) : try : subarr = _try_cast ( data , True , dtype , copy , True ) except ValueError : if copy : subarr = data . copy ( ) else : subarr = _try_cast ( data , True , dtype , copy , raise_cast_failure ) elif isinstance ( data , Index ) : # don't coerce Index types # e.g. indexes can have different conversions (so don't fast path # them) # GH#6140 subarr = sanitize_index ( data , index , copy = copy ) else : # we will try to copy be-definition here subarr = _try_cast ( data , True , dtype , copy , raise_cast_failure ) elif isinstance ( data , ExtensionArray ) : if isinstance ( data , ABCPandasArray ) : # We don't want to let people put our PandasArray wrapper # (the output of Series/Index.array), into a Series. So # we explicitly unwrap it here. subarr = data . to_numpy ( ) else : subarr = data # everything else in this block must also handle ndarray's, # becuase we've unwrapped PandasArray into an ndarray. if dtype is not None : subarr = data . astype ( dtype ) if copy : subarr = data . copy ( ) return subarr elif isinstance ( data , ( list , tuple ) ) and len ( data ) > 0 : if dtype is not None : try : subarr = _try_cast ( data , False , dtype , copy , raise_cast_failure ) except Exception : if raise_cast_failure : # pragma: no cover raise subarr = np . array ( data , dtype = object , copy = copy ) subarr = lib . maybe_convert_objects ( subarr ) else : subarr = maybe_convert_platform ( data ) subarr = maybe_cast_to_datetime ( subarr , dtype ) elif isinstance ( data , range ) : # GH#16804 arr = np . arange ( data . start , data . stop , data . step , dtype = 'int64' ) subarr = _try_cast ( arr , False , dtype , copy , raise_cast_failure ) else : subarr = _try_cast ( data , False , dtype , copy , raise_cast_failure ) # scalar like, GH if getattr ( subarr , 'ndim' , 0 ) == 0 : if isinstance ( data , list ) : # pragma: no cover subarr = np . array ( data , dtype = object ) elif index is not None : value = data # figure out the dtype from the value (upcast if necessary) if dtype is None : dtype , value = infer_dtype_from_scalar ( value ) else : # need to possibly convert the value here value = maybe_cast_to_datetime ( value , dtype ) subarr = construct_1d_arraylike_from_scalar ( value , len ( index ) , dtype ) else : return subarr . item ( ) # the result that we want elif subarr . ndim == 1 : if index is not None : # a 1-element ndarray if len ( subarr ) != len ( index ) and len ( subarr ) == 1 : subarr = construct_1d_arraylike_from_scalar ( subarr [ 0 ] , len ( index ) , subarr . dtype ) elif subarr . ndim > 1 : if isinstance ( data , np . ndarray ) : raise Exception ( 'Data must be 1-dimensional' ) else : subarr = com . asarray_tuplesafe ( data , dtype = dtype ) # This is to prevent mixed-type Series getting all casted to # NumPy string type, e.g. NaN --> '-1#IND'. if issubclass ( subarr . dtype . type , str ) : # GH#16605 # If not empty convert the data to dtype # GH#19853: If data is a scalar, subarr has already the result if not lib . is_scalar ( data ) : if not np . all ( isna ( data ) ) : data = np . array ( data , dtype = dtype , copy = False ) subarr = np . array ( data , dtype = object , copy = copy ) if is_object_dtype ( subarr . dtype ) and dtype != 'object' : inferred = lib . infer_dtype ( subarr , skipna = False ) if inferred == 'period' : try : subarr = period_array ( subarr ) except IncompatibleFrequency : pass return subarr\n\n\n        Summary:", "input": "", "output": "Sanitize input data to an ndarray copy if specified coerce to the dtype if specified ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_engine ( engine ) : from pandas . core . computation . check import _NUMEXPR_INSTALLED if engine is None : if _NUMEXPR_INSTALLED : engine = 'numexpr' else : engine = 'python' if engine not in _engines : valid = list ( _engines . keys ( ) ) raise KeyError ( 'Invalid engine {engine!r} passed, valid engines are' ' {valid}' . format ( engine = engine , valid = valid ) ) # TODO: validate this in a more general way (thinking of future engines # that won't necessarily be import-able) # Could potentially be done on engine instantiation if engine == 'numexpr' : if not _NUMEXPR_INSTALLED : raise ImportError ( \"'numexpr' is not installed or an \" \"unsupported version. Cannot use \" \"engine='numexpr' for query/eval \" \"if 'numexpr' is not installed\" ) return engine\n\n\n        Summary:", "input": "", "output": "Make sure a valid engine is passed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _check_parser ( parser ) : from pandas . core . computation . expr import _parsers if parser not in _parsers : raise KeyError ( 'Invalid parser {parser!r} passed, valid parsers are' ' {valid}' . format ( parser = parser , valid = _parsers . keys ( ) ) )\n\n\n        Summary:", "input": "", "output": "Make sure a valid parser is passed ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def eval ( expr , parser = 'pandas' , engine = None , truediv = True , local_dict = None , global_dict = None , resolvers = ( ) , level = 0 , target = None , inplace = False ) : from pandas . core . computation . expr import Expr inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if isinstance ( expr , str ) : _check_expression ( expr ) exprs = [ e . strip ( ) for e in expr . splitlines ( ) if e . strip ( ) != '' ] else : exprs = [ expr ] multi_line = len ( exprs ) > 1 if multi_line and target is None : raise ValueError ( \"multi-line expressions are only valid in the \" \"context of data, use DataFrame.eval\" ) ret = None first_expr = True target_modified = False for expr in exprs : expr = _convert_expression ( expr ) engine = _check_engine ( engine ) _check_parser ( parser ) _check_resolvers ( resolvers ) _check_for_locals ( expr , level , parser ) # get our (possibly passed-in) scope env = _ensure_scope ( level + 1 , global_dict = global_dict , local_dict = local_dict , resolvers = resolvers , target = target ) parsed_expr = Expr ( expr , engine = engine , parser = parser , env = env , truediv = truediv ) # construct the engine and evaluate the parsed expression eng = _engines [ engine ] eng_inst = eng ( parsed_expr ) ret = eng_inst . evaluate ( ) if parsed_expr . assigner is None : if multi_line : raise ValueError ( \"Multi-line expressions are only valid\" \" if all expressions contain an assignment\" ) elif inplace : raise ValueError ( \"Cannot operate inplace \" \"if there is no assignment\" ) # assign if needed assigner = parsed_expr . assigner if env . target is not None and assigner is not None : target_modified = True # if returning a copy, copy only on the first assignment if not inplace and first_expr : try : target = env . target . copy ( ) except AttributeError : raise ValueError ( \"Cannot return a copy of the target\" ) else : target = env . target # TypeError is most commonly raised (e.g. int, list), but you # get IndexError if you try to do this assignment on np.ndarray. # we will ignore numpy warnings here; e.g. if trying # to use a non-numeric indexer try : with warnings . catch_warnings ( record = True ) : # TODO: Filter the warnings we actually care about here. target [ assigner ] = ret except ( TypeError , IndexError ) : raise ValueError ( \"Cannot assign expression output to target\" ) if not resolvers : resolvers = ( { assigner : ret } , ) else : # existing resolver needs updated to handle # case of mutating existing column in copy for resolver in resolvers : if assigner in resolver : resolver [ assigner ] = ret break else : resolvers += ( { assigner : ret } , ) ret = None first_expr = False # We want to exclude `inplace=None` as being False. if inplace is False : return target if target_modified else ret\n\n\n        Summary:", "input": "", "output": "Evaluate a Python expression as a string using various backends ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _codes_to_ints ( self , codes ) : # Shift the representation of each level by the pre-calculated number # of bits: codes <<= self . offsets # Now sum and OR are in fact interchangeable. This is a simple # composition of the (disjunct) significant bits of each level (i.e. # each column in \"codes\") in a single positive integer: if codes . ndim == 1 : # Single key return np . bitwise_or . reduce ( codes ) # Multiple keys return np . bitwise_or . reduce ( codes , axis = 1 )\n\n\n        Summary:", "input": "", "output": "Transform combination ( s ) of uint64 in one uint64 ( each ) in a strictly monotonic way ( i . e . respecting the lexicographic order of integer combinations ) : see BaseMultiIndexCodesEngine documentation ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_arrays ( cls , arrays , sortorder = None , names = None ) : error_msg = \"Input must be a list / sequence of array-likes.\" if not is_list_like ( arrays ) : raise TypeError ( error_msg ) elif is_iterator ( arrays ) : arrays = list ( arrays ) # Check if elements of array are list-like for array in arrays : if not is_list_like ( array ) : raise TypeError ( error_msg ) # Check if lengths of all arrays are equal or not, # raise ValueError, if not for i in range ( 1 , len ( arrays ) ) : if len ( arrays [ i ] ) != len ( arrays [ i - 1 ] ) : raise ValueError ( 'all arrays must be same length' ) from pandas . core . arrays . categorical import _factorize_from_iterables codes , levels = _factorize_from_iterables ( arrays ) if names is None : names = [ getattr ( arr , \"name\" , None ) for arr in arrays ] return MultiIndex ( levels = levels , codes = codes , sortorder = sortorder , names = names , verify_integrity = False )\n\n\n        Summary:", "input": "", "output": "Convert arrays to MultiIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_tuples ( cls , tuples , sortorder = None , names = None ) : if not is_list_like ( tuples ) : raise TypeError ( 'Input must be a list / sequence of tuple-likes.' ) elif is_iterator ( tuples ) : tuples = list ( tuples ) if len ( tuples ) == 0 : if names is None : msg = 'Cannot infer number of levels from empty list' raise TypeError ( msg ) arrays = [ [ ] ] * len ( names ) elif isinstance ( tuples , ( np . ndarray , Index ) ) : if isinstance ( tuples , Index ) : tuples = tuples . _values arrays = list ( lib . tuples_to_object_array ( tuples ) . T ) elif isinstance ( tuples , list ) : arrays = list ( lib . to_object_array_tuples ( tuples ) . T ) else : arrays = lzip ( * tuples ) return MultiIndex . from_arrays ( arrays , sortorder = sortorder , names = names )\n\n\n        Summary:", "input": "", "output": "Convert list of tuples to MultiIndex ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_product ( cls , iterables , sortorder = None , names = None ) : from pandas . core . arrays . categorical import _factorize_from_iterables from pandas . core . reshape . util import cartesian_product if not is_list_like ( iterables ) : raise TypeError ( \"Input must be a list / sequence of iterables.\" ) elif is_iterator ( iterables ) : iterables = list ( iterables ) codes , levels = _factorize_from_iterables ( iterables ) codes = cartesian_product ( codes ) return MultiIndex ( levels , codes , sortorder = sortorder , names = names )\n\n\n        Summary:", "input": "", "output": "Make a MultiIndex from the cartesian product of multiple iterables ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def from_frame ( cls , df , sortorder = None , names = None ) : if not isinstance ( df , ABCDataFrame ) : raise TypeError ( \"Input must be a DataFrame\" ) column_names , columns = lzip ( * df . iteritems ( ) ) names = column_names if names is None else names return cls . from_arrays ( columns , sortorder = sortorder , names = names )\n\n\n        Summary:", "input": "", "output": "Make a MultiIndex from a DataFrame ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_levels ( self , levels , level = None , inplace = False , verify_integrity = True ) : if is_list_like ( levels ) and not isinstance ( levels , Index ) : levels = list ( levels ) if level is not None and not is_list_like ( level ) : if not is_list_like ( levels ) : raise TypeError ( \"Levels must be list-like\" ) if is_list_like ( levels [ 0 ] ) : raise TypeError ( \"Levels must be list-like\" ) level = [ level ] levels = [ levels ] elif level is None or is_list_like ( level ) : if not is_list_like ( levels ) or not is_list_like ( levels [ 0 ] ) : raise TypeError ( \"Levels must be list of lists-like\" ) if inplace : idx = self else : idx = self . _shallow_copy ( ) idx . _reset_identity ( ) idx . _set_levels ( levels , level = level , validate = True , verify_integrity = verify_integrity ) if not inplace : return idx\n\n\n        Summary:", "input": "", "output": "Set new levels on MultiIndex . Defaults to returning new index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def set_codes ( self , codes , level = None , inplace = False , verify_integrity = True ) : if level is not None and not is_list_like ( level ) : if not is_list_like ( codes ) : raise TypeError ( \"Codes must be list-like\" ) if is_list_like ( codes [ 0 ] ) : raise TypeError ( \"Codes must be list-like\" ) level = [ level ] codes = [ codes ] elif level is None or is_list_like ( level ) : if not is_list_like ( codes ) or not is_list_like ( codes [ 0 ] ) : raise TypeError ( \"Codes must be list of lists-like\" ) if inplace : idx = self else : idx = self . _shallow_copy ( ) idx . _reset_identity ( ) idx . _set_codes ( codes , level = level , verify_integrity = verify_integrity ) if not inplace : return idx\n\n\n        Summary:", "input": "", "output": "Set new codes on MultiIndex . Defaults to returning new index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def copy ( self , names = None , dtype = None , levels = None , codes = None , deep = False , _set_identity = False , * * kwargs ) : name = kwargs . get ( 'name' ) names = self . _validate_names ( name = name , names = names , deep = deep ) if deep : from copy import deepcopy if levels is None : levels = deepcopy ( self . levels ) if codes is None : codes = deepcopy ( self . codes ) else : if levels is None : levels = self . levels if codes is None : codes = self . codes return MultiIndex ( levels = levels , codes = codes , names = names , sortorder = self . sortorder , verify_integrity = False , _set_identity = _set_identity )\n\n\n        Summary:", "input": "", "output": "Make a copy of this object . Names dtype levels and codes can be passed and will be set on new copy ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def view ( self , cls = None ) : result = self . copy ( ) result . _id = self . _id return result\n\n\n        Summary:", "input": "", "output": "this is defined as a copy with the same identity"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _is_memory_usage_qualified ( self ) : def f ( l ) : return 'mixed' in l or 'string' in l or 'unicode' in l return any ( f ( l ) for l in self . _inferred_type_levels )\n\n\n        Summary:", "input": "", "output": "return a boolean if we need a qualified . info display"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _nbytes ( self , deep = False ) : # for implementations with no useful getsizeof (PyPy) objsize = 24 level_nbytes = sum ( i . memory_usage ( deep = deep ) for i in self . levels ) label_nbytes = sum ( i . nbytes for i in self . codes ) names_nbytes = sum ( getsizeof ( i , objsize ) for i in self . names ) result = level_nbytes + label_nbytes + names_nbytes # include our engine hashtable result += self . _engine . sizeof ( deep = deep ) return result\n\n\n        Summary:", "input": "", "output": "return the number of bytes in the underlying data deeply introspect the level data if deep = True"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _format_attrs ( self ) : attrs = [ ( 'levels' , ibase . default_pprint ( self . _levels , max_seq_items = False ) ) , ( 'codes' , ibase . default_pprint ( self . _codes , max_seq_items = False ) ) ] if com . _any_not_none ( * self . names ) : attrs . append ( ( 'names' , ibase . default_pprint ( self . names ) ) ) if self . sortorder is not None : attrs . append ( ( 'sortorder' , ibase . default_pprint ( self . sortorder ) ) ) return attrs\n\n\n        Summary:", "input": "", "output": "Return a list of tuples of the ( attr formatted_value )"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _set_names ( self , names , level = None , validate = True ) : # GH 15110 # Don't allow a single string for names in a MultiIndex if names is not None and not is_list_like ( names ) : raise ValueError ( 'Names should be list-like for a MultiIndex' ) names = list ( names ) if validate and level is not None and len ( names ) != len ( level ) : raise ValueError ( 'Length of names must match length of level.' ) if validate and level is None and len ( names ) != self . nlevels : raise ValueError ( 'Length of names must match number of levels in ' 'MultiIndex.' ) if level is None : level = range ( self . nlevels ) else : level = [ self . _get_level_number ( l ) for l in level ] # set the name for l , name in zip ( level , names ) : if name is not None : # GH 20527 # All items in 'names' need to be hashable: if not is_hashable ( name ) : raise TypeError ( '{}.name must be a hashable type' . format ( self . __class__ . __name__ ) ) self . levels [ l ] . rename ( name , inplace = True )\n\n\n        Summary:", "input": "", "output": "Set new names on index . Each name has to be a hashable type ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def is_monotonic_increasing ( self ) : # reversed() because lexsort() wants the most significant key last. values = [ self . _get_level_values ( i ) . values for i in reversed ( range ( len ( self . levels ) ) ) ] try : sort_order = np . lexsort ( values ) return Index ( sort_order ) . is_monotonic except TypeError : # we have mixed types and np.lexsort is not happy return Index ( self . values ) . is_monotonic\n\n\n        Summary:", "input": "", "output": "return if the index is monotonic increasing ( only equal or increasing ) values ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _hashed_indexing_key ( self , key ) : from pandas . core . util . hashing import hash_tuples , hash_tuple if not isinstance ( key , tuple ) : return hash_tuples ( key ) if not len ( key ) == self . nlevels : raise KeyError def f ( k , stringify ) : if stringify and not isinstance ( k , str ) : k = str ( k ) return k key = tuple ( f ( k , stringify ) for k , stringify in zip ( key , self . _have_mixed_levels ) ) return hash_tuple ( key )\n\n\n        Summary:", "input": "", "output": "validate and return the hash for the provided key"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_level_values ( self , level , unique = False ) : values = self . levels [ level ] level_codes = self . codes [ level ] if unique : level_codes = algos . unique ( level_codes ) filled = algos . take_1d ( values . _values , level_codes , fill_value = values . _na_value ) values = values . _shallow_copy ( filled ) return values\n\n\n        Summary:", "input": "", "output": "Return vector of label values for requested level equal to the length of the index"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def get_level_values ( self , level ) : level = self . _get_level_number ( level ) values = self . _get_level_values ( level ) return values\n\n\n        Summary:", "input": "", "output": "Return vector of label values for requested level equal to the length of the index ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_frame ( self , index = True , name = None ) : from pandas import DataFrame if name is not None : if not is_list_like ( name ) : raise TypeError ( \"'name' must be a list / sequence \" \"of column names.\" ) if len ( name ) != len ( self . levels ) : raise ValueError ( \"'name' should have same length as \" \"number of levels on index.\" ) idx_names = name else : idx_names = self . names # Guarantee resulting column order result = DataFrame ( OrderedDict ( [ ( ( level if lvlname is None else lvlname ) , self . _get_level_values ( level ) ) for lvlname , level in zip ( idx_names , range ( len ( self . levels ) ) ) ] ) , copy = False ) if index : result . index = self return result\n\n\n        Summary:", "input": "", "output": "Create a DataFrame with the levels of the MultiIndex as columns ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def to_hierarchical ( self , n_repeat , n_shuffle = 1 ) : levels = self . levels codes = [ np . repeat ( level_codes , n_repeat ) for level_codes in self . codes ] # Assumes that each level_codes is divisible by n_shuffle codes = [ x . reshape ( n_shuffle , - 1 ) . ravel ( order = 'F' ) for x in codes ] names = self . names warnings . warn ( \"Method .to_hierarchical is deprecated and will \" \"be removed in a future version\" , FutureWarning , stacklevel = 2 ) return MultiIndex ( levels = levels , codes = codes , names = names )\n\n\n        Summary:", "input": "", "output": "Return a MultiIndex reshaped to conform to the shapes given by n_repeat and n_shuffle ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _sort_levels_monotonic ( self ) : if self . is_lexsorted ( ) and self . is_monotonic : return self new_levels = [ ] new_codes = [ ] for lev , level_codes in zip ( self . levels , self . codes ) : if not lev . is_monotonic : try : # indexer to reorder the levels indexer = lev . argsort ( ) except TypeError : pass else : lev = lev . take ( indexer ) # indexer to reorder the level codes indexer = ensure_int64 ( indexer ) ri = lib . get_reverse_indexer ( indexer , len ( indexer ) ) level_codes = algos . take_1d ( ri , level_codes ) new_levels . append ( lev ) new_codes . append ( level_codes ) return MultiIndex ( new_levels , new_codes , names = self . names , sortorder = self . sortorder , verify_integrity = False )\n\n\n        Summary:", "input": "", "output": ".. versionadded :: 0 . 20 . 0"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def remove_unused_levels ( self ) : new_levels = [ ] new_codes = [ ] changed = False for lev , level_codes in zip ( self . levels , self . codes ) : # Since few levels are typically unused, bincount() is more # efficient than unique() - however it only accepts positive values # (and drops order): uniques = np . where ( np . bincount ( level_codes + 1 ) > 0 ) [ 0 ] - 1 has_na = int ( len ( uniques ) and ( uniques [ 0 ] == - 1 ) ) if len ( uniques ) != len ( lev ) + has_na : # We have unused levels changed = True # Recalculate uniques, now preserving order. # Can easily be cythonized by exploiting the already existing # \"uniques\" and stop parsing \"level_codes\" when all items # are found: uniques = algos . unique ( level_codes ) if has_na : na_idx = np . where ( uniques == - 1 ) [ 0 ] # Just ensure that -1 is in first position: uniques [ [ 0 , na_idx [ 0 ] ] ] = uniques [ [ na_idx [ 0 ] , 0 ] ] # codes get mapped from uniques to 0:len(uniques) # -1 (if present) is mapped to last position code_mapping = np . zeros ( len ( lev ) + has_na ) # ... and reassigned value -1: code_mapping [ uniques ] = np . arange ( len ( uniques ) ) - has_na level_codes = code_mapping [ level_codes ] # new levels are simple lev = lev . take ( uniques [ has_na : ] ) new_levels . append ( lev ) new_codes . append ( level_codes ) result = self . _shallow_copy ( ) if changed : result . _reset_identity ( ) result . _set_levels ( new_levels , validate = False ) result . _set_codes ( new_codes , validate = False ) return result\n\n\n        Summary:", "input": "", "output": "Create a new MultiIndex from the current that removes unused levels meaning that they are not expressed in the labels ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _assert_take_fillable ( self , values , indices , allow_fill = True , fill_value = None , na_value = None ) : # only fill if we are passing a non-None fill_value if allow_fill and fill_value is not None : if ( indices < - 1 ) . any ( ) : msg = ( 'When allow_fill=True and fill_value is not None, ' 'all indices must be >= -1' ) raise ValueError ( msg ) taken = [ lab . take ( indices ) for lab in self . codes ] mask = indices == - 1 if mask . any ( ) : masked = [ ] for new_label in taken : label_values = new_label . values ( ) label_values [ mask ] = na_value masked . append ( np . asarray ( label_values ) ) taken = masked else : taken = [ lab . take ( indices ) for lab in self . codes ] return taken\n\n\n        Summary:", "input": "", "output": "Internal method to handle NA filling of take"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def append ( self , other ) : if not isinstance ( other , ( list , tuple ) ) : other = [ other ] if all ( ( isinstance ( o , MultiIndex ) and o . nlevels >= self . nlevels ) for o in other ) : arrays = [ ] for i in range ( self . nlevels ) : label = self . _get_level_values ( i ) appended = [ o . _get_level_values ( i ) for o in other ] arrays . append ( label . append ( appended ) ) return MultiIndex . from_arrays ( arrays , names = self . names ) to_concat = ( self . values , ) + tuple ( k . _values for k in other ) new_tuples = np . concatenate ( to_concat ) # if all(isinstance(x, MultiIndex) for x in other): try : return MultiIndex . from_tuples ( new_tuples , names = self . names ) except ( TypeError , IndexError ) : return Index ( new_tuples )\n\n\n        Summary:", "input": "", "output": "Append a collection of Index options together"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def drop ( self , codes , level = None , errors = 'raise' ) : if level is not None : return self . _drop_from_level ( codes , level ) try : if not isinstance ( codes , ( np . ndarray , Index ) ) : codes = com . index_labels_to_array ( codes ) indexer = self . get_indexer ( codes ) mask = indexer == - 1 if mask . any ( ) : if errors != 'ignore' : raise ValueError ( 'codes %s not contained in axis' % codes [ mask ] ) except Exception : pass inds = [ ] for level_codes in codes : try : loc = self . get_loc ( level_codes ) # get_loc returns either an integer, a slice, or a boolean # mask if isinstance ( loc , int ) : inds . append ( loc ) elif isinstance ( loc , slice ) : inds . extend ( lrange ( loc . start , loc . stop ) ) elif com . is_bool_indexer ( loc ) : if self . lexsort_depth == 0 : warnings . warn ( 'dropping on a non-lexsorted multi-index' ' without a level parameter may impact ' 'performance.' , PerformanceWarning , stacklevel = 3 ) loc = loc . nonzero ( ) [ 0 ] inds . extend ( loc ) else : msg = 'unsupported indexer of type {}' . format ( type ( loc ) ) raise AssertionError ( msg ) except KeyError : if errors != 'ignore' : raise return self . delete ( inds )\n\n\n        Summary:", "input": "", "output": "Make new MultiIndex with passed list of codes deleted"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def swaplevel ( self , i = - 2 , j = - 1 ) : new_levels = list ( self . levels ) new_codes = list ( self . codes ) new_names = list ( self . names ) i = self . _get_level_number ( i ) j = self . _get_level_number ( j ) new_levels [ i ] , new_levels [ j ] = new_levels [ j ] , new_levels [ i ] new_codes [ i ] , new_codes [ j ] = new_codes [ j ] , new_codes [ i ] new_names [ i ] , new_names [ j ] = new_names [ j ] , new_names [ i ] return MultiIndex ( levels = new_levels , codes = new_codes , names = new_names , verify_integrity = False )\n\n\n        Summary:", "input": "", "output": "Swap level i with level j ."}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def reorder_levels ( self , order ) : order = [ self . _get_level_number ( i ) for i in order ] if len ( order ) != self . nlevels : raise AssertionError ( 'Length of order must be same as ' 'number of levels (%d), got %d' % ( self . nlevels , len ( order ) ) ) new_levels = [ self . levels [ i ] for i in order ] new_codes = [ self . codes [ i ] for i in order ] new_names = [ self . names [ i ] for i in order ] return MultiIndex ( levels = new_levels , codes = new_codes , names = new_names , verify_integrity = False )\n\n\n        Summary:", "input": "", "output": "Rearrange levels using input order . May not drop or duplicate levels"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def _get_codes_for_sorting ( self ) : from pandas . core . arrays import Categorical def cats ( level_codes ) : return np . arange ( np . array ( level_codes ) . max ( ) + 1 if len ( level_codes ) else 0 , dtype = level_codes . dtype ) return [ Categorical . from_codes ( level_codes , cats ( level_codes ) , ordered = True ) for level_codes in self . codes ]\n\n\n        Summary:", "input": "", "output": "we categorizing our codes by using the available categories ( all not just observed ) excluding any missing ones ( - 1 ) ; this is in preparation for sorting where we need to disambiguate that - 1 is not a valid valid"}, {"instruction": "Here is a Python code, and I hope you can generate a brief code summary based on the code.\n        Note: You only need to generate a concise code summary, without saying any extra words or providing any extra content. \n\n        Here is the task you should finish:\n\n        Code Snippet: \n        def sortlevel ( self , level = 0 , ascending = True , sort_remaining = True ) : from pandas