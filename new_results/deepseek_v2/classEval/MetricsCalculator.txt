```python
"""
This class is designed to calculate various performance metrics such as precision, recall, F1 score, and accuracy for binary classification models. It maintains counters for true positives, false positives, false negatives, and true negatives, which are updated based on predicted and true labels.
"""

class MetricsCalculator:
    def __init__(self):
        """
        Initializes the MetricsCalculator class by setting counters for true positives, false positives, false negatives, and true negatives to zero.
        """
        self.true_positives = 0
        self.false_positives = 0
        self.false_negatives = 0
        self.true_negatives = 0

    def update(self, predicted_labels, true_labels):
        """
        Updates the counters for true positives, false positives, false negatives, and true negatives based on the provided predicted and true labels.

        Parameters:
            predicted_labels (list): A list of predicted binary labels (0 or 1).
            true_labels (list): A list of true binary labels (0 or 1).

        Returns:
            None

        Test cases:
            # Test case 1: All predictions are correct
            calculator = MetricsCalculator()
            calculator.update([1, 0, 1, 0], [1, 0, 1, 0])
            assert calculator.true_positives == 2
            assert calculator.true_negatives == 2
            assert calculator.false_positives == 0
            assert calculator.false_negatives == 0

            # Test case 2: Some predictions are incorrect
            calculator = MetricsCalculator()
            calculator.update([1, 0, 1, 0], [1, 1, 0, 0])
            assert calculator.true_positives == 1
            assert calculator.true_negatives == 1
            assert calculator.false_positives == 1
            assert calculator.false_negatives == 1

            # Test case 3: All predictions are incorrect
            calculator = MetricsCalculator()
            calculator.update([1, 1, 1, 1], [0, 0, 0, 0])
            assert calculator.true_positives == 0
            assert calculator.true_negatives == 0
            assert calculator.false_positives == 4
            assert calculator.false_negatives == 0
        """
        for predicted, true in zip(predicted_labels, true_labels):
            if predicted == 1 and true == 1:
                self.true_positives += 1
            elif predicted == 1 and true == 0:
                self.false_positives += 1
            elif predicted == 0 and true == 1:
                self.false_negatives += 1
            elif predicted == 0 and true == 0:
                self.true_negatives += 1

    def precision(self, predicted_labels, true_labels):
        """
        Calculates the precision metric, which is the ratio of true positives to the sum of true positives and false positives.

        Parameters:
            predicted_labels (list): A list of predicted binary labels (0 or 1).
            true_labels (list): A list of true binary labels (0 or 1).

        Returns:
            float: The precision value, ranging from 0.0 to 1.0.

        Test cases:
            # Test case 1: Perfect precision
            calculator = MetricsCalculator()
            assert calculator.precision([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0

            # Test case 2: No true positives
            calculator = MetricsCalculator()
            assert calculator.precision([1, 1, 1, 1], [0, 0, 0, 0]) == 0.0

            # Test case 3: Mixed results
            calculator = MetricsCalculator()
            assert calculator.precision([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5
        """
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_positives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_positives)

    def recall(self, predicted_labels, true_labels):
        """
        Calculates the recall metric, which is the ratio of true positives to the sum of true positives and false negatives.

        Parameters:
            predicted_labels (list): A list of predicted binary labels (0 or 1).
            true_labels (list): A list of true binary labels (0 or 1).

        Returns:
            float: The recall value, ranging from 0.0 to 1.0.

        Test cases:
            # Test case 1: Perfect recall
            calculator = MetricsCalculator()
            assert calculator.recall([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0

            # Test case 2: No true positives
            calculator = MetricsCalculator()
            assert calculator.recall([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0

            # Test case 3: Mixed results
            calculator = MetricsCalculator()
            assert calculator.recall([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5
        """
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_negatives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_negatives)

    def f1_score(self, predicted_labels, true_labels):
        """
        Calculates the F1 score, which is the harmonic mean of precision and recall.

        Parameters:
            predicted_labels (list): A list of predicted binary labels (0 or 1).
            true_labels (list): A list of true binary labels (0 or 1).

        Returns:
            float: The F1 score, ranging from 0.0 to 1.0.

        Test cases:
            # Test case 1: Perfect F1 score
            calculator = MetricsCalculator()
            assert calculator.f1_score([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0

            # Test case 2: Zero F1 score
            calculator = MetricsCalculator()
            assert calculator.f1_score([1, 1, 1, 1], [0, 0, 0, 0]) == 0.0

            # Test case 3: Mixed results
            calculator = MetricsCalculator()
            assert calculator.f1_score([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5
        """
        self.update(predicted_labels, true_labels)
        precision = self.precision(predicted_labels, true_labels)
        recall = self.recall(predicted_labels, true_labels)
        if precision + recall == 0.0:
            return 0.0
        return (2 * precision * recall) / (precision + recall)

    def accuracy(self, predicted_labels, true_labels):
        """
        Calculates the accuracy metric, which is the ratio of correct predictions to the total number of predictions.

        Parameters:
            predicted_labels (list): A list of predicted binary labels (0 or 1).
            true_labels (list): A list of true binary labels (0 or 1).

        Returns:
            float: The accuracy value, ranging from 0.0 to 1.0.

        Test cases:
            # Test case 1: Perfect accuracy
            calculator = MetricsCalculator()
            assert calculator.accuracy([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0

            # Test case 2: Zero accuracy
            calculator = MetricsCalculator()
            assert calculator.accuracy([1, 1, 1, 1], [0, 0, 0, 0]) == 0.0

            # Test case 3: Mixed results
            calculator = MetricsCalculator()
            assert calculator.accuracy([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5
        """
        self.update(predicted_labels, true_labels)
        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        if total == 0:
            return 0.0
        return (self.true_positives + self.true_negatives) / total
```