```python
"""
This class provides methods to calculate Cohen's Kappa and Fleiss' Kappa, which are statistical measures of inter-rater agreement for categorical items.
"""

import numpy as np


class KappaCalculator:

    @staticmethod
    def kappa(testData, k):
        """
        Calculate Cohen's Kappa coefficient for inter-rater agreement.

        Parameters:
            testData (list of list of int): A confusion matrix where testData[i][j] represents the number of times 
                                           category i was assigned by rater 1 and category j by rater 2.
            k (int): The number of categories or classes.

        Returns:
            float: Cohen's Kappa coefficient, a value between -1 and 1. 
                  A value of 1 indicates perfect agreement, 0 indicates agreement by chance, 
                  and negative values indicate disagreement.

        Test cases:
            # Test case 1: Perfect agreement
            testData = [[10, 0], [0, 10]]
            k = 2
            assert KappaCalculator.kappa(testData, k) == 1.0

            # Test case 2: Agreement by chance
            testData = [[5, 5], [5, 5]]
            k = 2
            assert KappaCalculator.kappa(testData, k) == 0.0

            # Test case 3: Disagreement
            testData = [[0, 10], [10, 0]]
            k = 2
            assert KappaCalculator.kappa(testData, k) == -1.0
        """
        dataMat = np.mat(testData)
        P0 = 0.0
        for i in range(k):
            P0 += dataMat[i, i] * 1.0
        xsum = np.sum(dataMat, axis=1)
        ysum = np.sum(dataMat, axis=0)
        sum = np.sum(dataMat)
        Pe = float(ysum * xsum) / sum / sum
        P0 = float(P0 / sum * 1.0)
        cohens_coefficient = float((P0 - Pe) / (1 - Pe))
        return cohens_coefficient

    @staticmethod
    def fleiss_kappa(testData, N, k, n):
        """
        Calculate Fleiss' Kappa coefficient for inter-rater agreement.

        Parameters:
            testData (list of list of int): A matrix where testData[i][j] represents the number of raters 
                                           who assigned category j to subject i.
            N (int): The number of subjects.
            k (int): The number of categories.
            n (int): The number of raters.

        Returns:
            float: Fleiss' Kappa coefficient, a value between -1 and 1. 
                  A value of 1 indicates perfect agreement, 0 indicates agreement by chance, 
                  and negative values indicate disagreement.

        Test cases:
            # Test case 1: Perfect agreement
            testData = [[10, 0, 0], [0, 10, 0], [0, 0, 10]]
            N = 3
            k = 3
            n = 10
            assert KappaCalculator.fleiss_kappa(testData, N, k, n) == 1.0

            # Test case 2: Agreement by chance
            testData = [[3, 3, 4], [3, 3, 4], [3, 3, 4]]
            N = 3
            k = 3
            n = 10
            assert KappaCalculator.fleiss_kappa(testData, N, k, n) == 0.0

            # Test case 3: Disagreement
            testData = [[0, 10, 0], [0, 0, 10], [10, 0, 0]]
            N = 3
            k = 3
            n = 10
            assert KappaCalculator.fleiss_kappa(testData, N, k, n) == -1.0
        """
        dataMat = np.mat(testData, float)
        oneMat = np.ones((k, 1))
        sum = 0.0
        P0 = 0.0
        for i in range(N):
            temp = 0.0
            for j in range(k):
                sum += dataMat[i, j]
                temp += 1.0 * dataMat[i, j] ** 2
            temp -= n
            temp /= (n - 1) * n
            P0 += temp
        P0 = 1.0 * P0 / N
        ysum = np.sum(dataMat, axis=0)
        for i in range(k):
            ysum[0, i] = (ysum[0, i] / sum) ** 2
        Pe = ysum * oneMat * 1.0
        ans = (P0 - Pe) / (1 - Pe)
        return ans[0, 0]
```