 """
This class is used for calculating various metrics for a given set of predicted and true labels.
"""

import numpy as np

class MetricsCalculator:

    def __init__(self):
        """
        Initializes an instance of MetricsCalculator with all counters reset.
        """
        self.true_positives = 0
        self.false_positives = 0
        self.false_negatives = 0
        self.true_negatives = 0

    def update(self, predicted_labels: np.ndarray, true_labels: np.ndarray):
        """
        Updates the counter variables based on given predicted and true labels.
        
        Parameters:
            predicted_labels (numpy.ndarray): Array of predicted labels
            true_labels (numpy.ndarray): Array of true labels
        
        Returns:
            None
        
        Test cases:
            # Test case 1: Identical labels
            predicted_labels = np.array([1, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            metrics.update(predicted_labels, true_labels)
            assert metrics.true_positives == len(predicted_labels)
            assert metrics.false_positives == 0
            assert metrics.false_negatives == 0
            assert metrics.true_negatives == len(predicted_labels) - metrics.true_positives
            
            # Test case 2: Mismatched labels
            predicted_labels = np.array([0, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            metrics.update(predicted_labels, true_labels)
            assert metrics.true_positives == 2
            assert metrics.false_positives == 1
            assert metrics.false_negatives == 0
            assert metrics.true_negatives == len(predicted_labels) - metrics.true_positives
        """

        for predicted, true in zip(predicted_labels, true_labels):
            if predicted == 1 and true == 1:
                self.true_positives += 1
            elif predicted == 1 and true == 0:
                self.false_positives += 1
            elif predicted == 0 and true == 1:
                self.false_negatives += 1
            elif predicted == 0 and true == 0:
                self.true_negatives += 1

    def precision(self, predicted_labels: np.ndarray, true_labels: np.ndarray):
        """
        Calculates the precision metric for the given set of predicted and true labels.
        
        Parameters:
            predicted_labels (numpy.ndarray): Array of predicted labels
            true_labels (numpy.ndarray): Array of true labels
        
        Returns:
            float: The precision metric value
        
        Test cases:
            # Test case 1: Identical labels
            predicted_labels = np.array([1, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            precision = metrics.precision(predicted_labels, true_labels)
            assert np.isclose(precision, 1.0)
            
            # Test case 2: Mismatched labels
            predicted_labels = np.array([0, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            precision = metrics.precision(predicted_labels, true_labels)
            assert np.isclose(precision, 2.0 / (2.0 + 1.0))
            
            # Test case 3: Imbalanced dataset
            predicted_labels = np.array([0] * 99 + [1])
            true_labels = np.array([0] * 99 + [1])
            metrics = MetricsCalculator()
            precision = metrics.precision(predicted_labels, true_labels)
            assert np.isclose(precision, 1.0)
        
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_positives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_positives)

    def recall(self, predicted_labels: np.ndarray, true_labels: np.ndarray):
        """
        Calculates the recall metric for the given set of predicted and true labels.
        
        Parameters:
            predicted_labels (numpy.ndarray): Array of predicted labels
            true_labels (numpy.ndarray): Array of true labels
        
        Returns:
            float: The recall metric value
        
        Test cases:
            # Test case 1: Identical labels
            predicted_labels = np.array([1, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            recall = metrics.recall(predicted_labels, true_labels)
            assert np.isclose(recall, 1.0)
            
            # Test case 2: Mismatched labels
            predicted_labels = np.array([0, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            recall = metrics.recall(predicted_labels, true_labels)
            assert np.isclose(recall, 2.0 / (2.0 + 1.0))
            
            # Test case 3: Imbalanced dataset
            predicted_labels = np.array([0] * 99 + [1])
            true_labels = np.array([0] * 98 + [1, 1])
            metrics = MetricsCalculator()
            recall = metrics.recall(predicted_labels, true_labels)
            assert np.isclose(recall, 2.0 / (2.0 + 1.0))
        
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_negatives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_negatives)

    def f1_score(self, predicted_labels: np.ndarray, true_labels: np.ndarray):
        """
        Calculates the F1 score metric for the given set of predicted and true labels.
        
        Parameters:
            predicted_labels (numpy.ndarray): Array of predicted labels
            true_labels (numpy.ndarray): Array of true labels
        
        Returns:
            float: The F1 score metric value
        
        Test cases:
            # Test case 1: Identical labels
            predicted_labels = np.array([1, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            f1 = metrics.f1_score(predicted_labels, true_labels)
            assert np.isclose(f1, 1.0)
            
            # Test case 2: Mismatched labels
            predicted_labels = np.array([0, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            f1 = metrics.f1_score(predicted_labels, true_labels)
            assert np.isclose(f1, (2.0 * 2.0) / (2.0 + 1.0))
            
            # Test case 3: Imbalanced dataset
            predicted_labels = np.array([0] * 99 + [1])
            true_labels = np.array([0] * 98 + [1, 1])
            metrics = MetricsCalculator()
            f1 = metrics.f1_score(predicted_labels, true_labels)
            assert np.isclose(f1, 2.0 / (2.0 + 1.0))
        
        self.update(predicted_labels, true_labels)
        precision = self.precision(predicted_labels, true_labels)
        recall = self.recall(predicted_labels, true_labels)
        if precision + recall == 0.0:
            return 0.0
        return (2 * precision * recall) / (precision + recall)

    def accuracy(self, predicted_labels: np.ndarray, true_labels: np.ndarray):
        """
        Calculates the accuracy metric for the given set of predicted and true labels.
        
        Parameters:
            predicted_labels (numpy.ndarray): Array of predicted labels
            true_labels (numpy.ndarray): Array of true labels
        
        Returns:
            float: The accuracy metric value
        
        Test cases:
            # Test case 1: Identical labels
            predicted_labels = np.array([1, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            accuracy = metrics.accuracy(predicted_labels, true_labels)
            assert np.isclose(accuracy, 1.0)
            
            # Test case 2: Mismatched labels
            predicted_labels = np.array([0, 1, 1])
            true_labels = np.array([1, 1, 1])
            metrics = MetricsCalculator()
            accuracy = metrics.accuracy(predicted_labels, true_labels)
            assert np.isclose(accuracy, (1.0 + self.true_positives / len(predicted_labels)))
            
            # Test case 3: Imbalanced dataset
            predicted_labels = np.array([0] * 99 + [1])
            true_labels = np.array([0] * 98 + [1, 1])
            metrics = MetricsCalculator()
            accuracy = metrics.accuracy(predicted_labels, true_labels)
            assert np.isclose(accuracy, (len(predicted_labels) - self.false_negatives) / len(predicted_labels))
        
        self.update(predicted_labels, true_labels)
        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        if total == 0:
            return 0.0
        return (self.true_positives + self.true_negatives) / total</s>