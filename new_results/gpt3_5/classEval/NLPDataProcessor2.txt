"""
This class provides methods for processing natural language data including filtering out non-English characters, converting to lowercase, splitting into words, and calculating word frequency.

It is specifically designed to assist in analyzing text data for natural language processing tasks.
"""

from collections import Counter
import re


class NLPDataProcessor2:

    def process_data(self, string_list):
        """
        Processes a list of strings by removing non-English characters, converting to lowercase, and splitting into words.

        Parameters:
            string_list (list): A list of strings to be processed

        Returns:
            list: A list of lists, where each inner list contains words from a processed string

        Test cases:
            # Test case 1: Normal processing
            assert process_data(["Hello World"]) == [['hello', 'world']]
            
            # Test case 2: Empty string handling
            assert process_data(['']) == [[]]
            
            # Test case 3: Non-English characters removal
            assert process_data(["Ã§a va?"]) == [['a', 'va']]
        """
        words_list = []
        for string in string_list:
            # Remove non-English letters and convert to lowercase
            processed_string = re.sub(r'[^a-zA-Z\s]', '', string.lower())
            # Split the string into words
            words = processed_string.split()
            words_list.append(words)
        return words_list

    def calculate_word_frequency(self, words_list):
        """
        Calculates the frequency of words in the provided list of words.

        Parameters:
            words_list (list): A list of lists containing words to calculate frequency from

        Returns:
            dict: A dictionary containing the top 5 most common words and their frequencies

        Test cases:
            # Test case 1: Normal calculation
            assert calculate_word_frequency([['hello', 'world', 'hello'], ['world', 'world', 'bye', 'world']]) == {'hello': 2, 'world': 4, 'bye': 1}
            
            # Test case 2: Empty input list
            assert calculate_word_frequency([[]]) == {}
            
            # Test case 3: Non-alphabetic words handling
            assert calculate_word_frequency([['123', '567', 'hello', 'hello']]) == {'hello': 2}
        """
        word_frequency = Counter()
        for words in words_list:
            word_frequency.update(words)
        sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda x: x[1], reverse=True))
        top_5_word_frequency = dict(list(sorted_word_frequency.items())[:5])
        return top_5_word_frequency

    def process(self, string_list):
        """
        Combines the data processing and word frequency calculation tasks into a single function.

        Parameters:
            string_list (list): A list of strings to be processed and analyzed

        Returns:
            dict: A dictionary containing the top 5 most common words and their frequencies across all strings

        Test cases:
            # Test case 1: Normal processing
            assert process(["Hello World", "World is great"]) == {'world': 2, 'hello': 1, 'is': 1, 'great': 1}
            
            # Test case 2: Empty input list
            assert process([]) == {}
            
            # Test case 3: Punctuation handling
            assert process(["Hello, World!", "World is great."]) == {'world': 2, 'hello': 1, 'is': 1, 'great': 1}
        """
        words_list = self.process_data(string_list)
        word_frequency_dict = self.calculate_word_frequency(words_list)
        return word_frequency_dict
"""