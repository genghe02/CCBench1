"""
This class provides methods to conduct basic Natural Language Processing tasks such as tokenizing text and calculating word frequency.
"""

from collections import Counter
import re

class NLPDataProcessor2:

    def process_data(self, string_list):
        """
        This method takes a list of strings, removes non-English letters, converts the text to lowercase and tokenizes the text into individual words.

        Parameters:
            string_list (list): A list of strings to be tokenized.
        
        Returns:
            list: A list of lists containing tokenized words.
        
        Test cases:
            # Test case 1: Normal case
            assert process_data(['Hello World!']) == [['hello', 'world']]
            
            # Test case 2: Non-English letters
            assert process_data(['H€ll0 Wørld!']) == [['hl', 'wrld']]
            
            # Test case 3: Multiple strings
            assert process_data(['Hello', 'World!']) == [['hello'], ['world']]
        """
        words_list = []
        for string in string_list:
            # Remove non-English letters and convert to lowercase
            processed_string = re.sub(r'[^a-zA-Z\s]', '', string.lower())
            # Split the string into words
            words = processed_string.split()
            words_list.append(words)
        return words_list

    def calculate_word_frequency(self, words_list):
        """
        This method calculates the frequency of words in a list of words and returns the top 5 most frequent words.

        Parameters:
            words_list (list): A list of lists containing words.
        
        Returns:
            dict: A dictionary containing the top 5 most frequent words and their frequencies.

        Test cases:
            # Test case 1: Normal case
            assert calculate_word_frequency([['hello', 'world']]) == {'hello': 1, 'world': 1}
            
            # Test case 2: Multiple words with different frequencies
            assert calculate_word_frequency([['hello', 'world', 'hello']]) == {'hello': 2, 'world': 1}
            
            # Test case 3: More than 5 unique words
            assert calculate_word_frequency([['a', 'b', 'c', 'd', 'e', 'f']]) == {'a': 1, 'b': 1, 'c': 1, 'd': 1, 'e': 1}
        """
        word_frequency = Counter()
        for words in words_list:
            word_frequency.update(words)
        sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda x: x[1], reverse=True))
        top_5_word_frequency = dict(list(sorted_word_frequency.items())[:5])
        return top_5_word_frequency

    def process(self, string_list):
        """
        This method orchestrates the data processing by tokenizing the input strings and then calculating the word frequency.

        Parameters:
            string_list (list): A list of strings to be processed.
        
        Returns:
            dict: A dictionary containing the top 5 most frequent words and their frequencies.

        Test cases:
            # Test case 1: Normal case
            assert process(['Hello World!']) == {'hello': 1, 'world': 1}
            
            # Test case 2: Non-English letters
            assert process(['H€ll0 Wørld!']) == {'hl': 1, 'wrld': 1}
            
            # Test case 3: Multiple strings
            assert process(['Hello', 'World!']) == {'hello': 1, 'world': 1}
        """
        words_list = self.process_data(string_list)
        word_frequency_dict = self.calculate_word_frequency(words_list)
        return word_frequency_dict
