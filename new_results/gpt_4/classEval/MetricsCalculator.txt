"""
This class calculates different evaluation metrics like precision, recall, F1 score, and accuracy for a binary classification model.
"""

class MetricsCalculator:
    def __init__(self):
        """
        Initialization method to set all metrics to zero.

        No parameters are required.

        No return value.
        """
        self.true_positives = 0
        self.false_positives = 0
        self.false_negatives = 0
        self.true_negatives = 0

    def update(self, predicted_labels, true_labels):
        """
        Updates the counts of true positives, false positives, false negatives, and true negatives based on the 
        predicted and true labels.

        Parameters:
            predicted_labels (list): A list of predicted labels from the model.
            true_labels (list): A list of actual labels.

        No return value.

        Test cases:
            # Test case 1: Normal usage
            update([1, 0, 1, 0], [1, 0, 0, 1])  
            # Test case 2: All predictions are correct
            update([1, 0, 1, 0], [1, 0, 1, 0])
            # Test case 3: All predictions are incorrect
            update([1, 0, 1, 0], [0, 1, 0, 1])
        """
        for predicted, true in zip(predicted_labels, true_labels):
            if predicted == 1 and true == 1:
                self.true_positives += 1
            elif predicted == 1 and true == 0:
                self.false_positives += 1
            elif predicted == 0 and true == 1:
                self.false_negatives += 1
            elif predicted == 0 and true == 0:
                self.true_negatives += 1

    def precision(self, predicted_labels, true_labels):
        """
        Calculates precision of the prediction.

        Parameters:
            predicted_labels (list): A list of predicted labels from the model.
            true_labels (list): A list of actual labels.

        Returns:
            float: Precision of the model predictions.

        Test cases:
            # Test case 1: Normal usage
            precision([1, 0, 1, 0], [1, 0, 0, 1])  
            # Test case 2: All predictions are correct
            precision([1, 0, 1, 0], [1, 0, 1, 0])
            # Test case 3: All predictions are incorrect
            precision([1, 0, 1, 0], [0, 1, 0, 1])
        """
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_positives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_positives)

    def recall(self, predicted_labels, true_labels):
        """
        Calculates recall of the prediction.

        Parameters:
            predicted_labels (list): A list of predicted labels from the model.
            true_labels (list): A list of actual labels.

        Returns:
            float: Recall of the model predictions.

        Test cases:
            # Test case 1: Normal usage
            recall([1, 0, 1, 0], [1, 0, 0, 1])
            # Test case 2: All predictions are correct
            recall([1, 0, 1, 0], [1, 0, 1, 0])
            # Test case 3: All predictions are incorrect
            recall([1, 0, 1, 0], [0, 1, 0, 1])
        """
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_negatives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_negatives)

    def f1_score(self, predicted_labels, true_labels):
        """
        Calculates F1 score of the prediction.

        Parameters:
            predicted_labels (list): A list of predicted labels from the model.
            true_labels (list): A list of actual labels.

        Returns:
            float: F1 score of the model predictions.

        Test cases:
            # Test case 1: Normal usage
            f1_score([1, 0, 1, 0], [1, 0, 0, 1])
            # Test case 2: All predictions are correct
            f1_score([1, 0, 1, 0], [1, 0, 1, 0])
            # Test case 3: All predictions are incorrect
            f1_score([1, 0, 1, 0], [0, 1, 0, 1])
        """
        self.update(predicted_labels, true_labels)
        precision = self.precision(predicted_labels, true_labels)
        recall = self.recall(predicted_labels, true_labels)
        if precision + recall == 0.0:
            return 0.0
        return (2 * precision * recall) / (precision + recall)

    def accuracy(self, predicted_labels, true_labels):
        """
        Calculates accuracy of the prediction.

        Parameters:
            predicted_labels (list): A list of predicted labels from the model.
            true_labels (list): A list of actual labels.

        Returns:
            float: Accuracy of the model predictions.

        Test cases:
            # Test case 1: Normal usage
            accuracy([1, 0, 1, 0], [1, 0, 0, 1])
            # Test case 2: All predictions are correct
            accuracy([1, 0, 1, 0], [1, 0, 1, 0])
            # Test case 3: All predictions are incorrect
            accuracy([1, 0, 1, 0], [0, 1, 0, 1])
        """
        self.update(predicted_labels, true_labels)
        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        if total == 0:
            return 0.0
        return (self.true_positives + self.true_negatives) / total