{"response body": "ChatCompletion(id='chatcmpl-AiODaQm1kMH3cG3kl6gI7R5TkwLiU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"\"\"\\nThis class defines a Kappa Calculator to compute Cohen\\'s Kappa statistic for interrater agreement and Fleiss\\' Kappa statistic for multiple raters.\\n\\nThe class provides two methods:\\n1. kappa: Compute Cohen\\'s Kappa coefficient for binary agreement.\\n2. fleiss_kappa: Compute Fleiss\\' Kappa coefficient for multiple raters.\\n\\n\"\"\"\\n\\nimport numpy as np\\n\\n\\nclass KappaCalculator:\\n\\n    @staticmethod\\n    def kappa(testData, k):\\n        \"\"\"\\n        Computes Cohen\\'s Kappa coefficient for binary agreement.\\n        \\n        Parameters:\\n            testData (list): List of observed ratings (N x N matrix, where N is the number of categories)\\n            k (int): Number of categories\\n            \\n        Returns:\\n            float: Cohen\\'s Kappa coefficient\\n\\n        Test cases:\\n            # Test case 1: Positive case with binary agreement\\n            assert kappa([[22, 18], [10, 50]], 2) == 0.46\\n            \\n            # Test case 2: Edge case with zero agreement\\n            assert kappa([[20, 20], [20, 20]], 2) == 0.0\\n            \\n            # Test case 3: Error case with negative agreement\\n            assert kappa([[10, 20], [30, 40]], 2) == -0.08\\n\\n        dataMat = np.mat(testData)\\n        P0 = 0.0\\n        for i in range(k):\\n            P0 += dataMat[i, i] * 1.0\\n        xsum = np.sum(dataMat, axis=1)\\n        ysum = np.sum(dataMat, axis=0)\\n        sum = np.sum(dataMat)\\n        Pe = float(ysum * xsum) / sum / sum\\n        P0 = float(P0 / sum * 1.0)\\n        cohens_coefficient = float((P0 - Pe) / (1 - Pe))\\n        return cohens_coefficient\\n\\n    @staticmethod\\n    def fleiss_kappa(testData, N, k, n):\\n        \"\"\"\\n        Computes Fleiss\\' Kappa coefficient for multiple raters.\\n        \\n        Parameters:\\n            testData (list): List of observed ratings (N x k matrix, where N is number of subjects and k is number of categories)\\n            N (int): Number of subjects\\n            k (int): Number of categories\\n            n (int): Number of raters per subject\\n            \\n        Returns:\\n            float: Fleiss\\' Kappa coefficient\\n\\n        Test cases:\\n            # Test case 1: Positive case with categorical agreement\\n            assert fleiss_kappa([[0, 0, 0, 14], [1, 1, 4, 8], [3, 4, 8, 4]], 3, 4, 3) == 0.415\\n            \\n            # Test case 2: Edge case with maximum agreement\\n            assert fleiss_kappa([[11, 0, 0], [0, 11, 0], [0, 0, 11]], 3, 3, 3) == 1.0\\n            \\n            # Test case 3: Error case with no agreement\\n            assert fleiss_kappa([[5, 5, 5], [8, 7, 0], [15, 0, 0]], 3, 3, 2) == -0.305\\n        \\n        dataMat = np.mat(testData, float)\\n        oneMat = np.ones((k, 1))\\n        sum = 0.0\\n        P0 = 0.0\\n        for i in range(N):\\n            temp = 0.0\\n            for j in range(k):\\n                sum += dataMat[i, j]\\n                temp += 1.0 * dataMat[i, j] ** 2\\n            temp -= n\\n            temp /= (n - 1) * n\\n            P0 += temp\\n        P0 = 1.0 * P0 / N\\n        ysum = np.sum(dataMat, axis=0)\\n        for i in range(k):\\n            ysum[0, i] = (ysum[0, i] / sum) ** 2\\n        Pe = ysum * oneMat * 1.0\\n        ans = (P0 - Pe) / (1 - Pe)\\n        return ans[0, 0]\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735142890, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=917, prompt_tokens=1407, total_tokens=2324, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "KappaCalculator.txt"}