{"response body": "ChatCompletion(id='chatcmpl-AiLopM4RRhPfkq2U84SgjfwkWMztR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\n\"\"\"\\nThis class serves as a metrics calculator for evaluating binary classification models, providing methods to compute precision, recall, F1 score, and accuracy based on predicted and true labels.\\n\"\"\"\\n\\nclass MetricsCalculator:\\n    def __init__(self):\\n        \"\"\"\\n        Initializes the MetricsCalculator with counters for true positives, false positives,\\n        false negatives, and true negatives set to zero.\\n        \"\"\"\\n        self.true_positives = 0\\n        self.false_positives = 0\\n        self.false_negatives = 0\\n        self.true_negatives = 0\\n\\n    def update(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Updates the counts of true positives, false positives, false negatives, and true negatives\\n        based on the predicted and true labels provided.\\n\\n        Parameters:\\n            predicted_labels (list[int]): A list of predicted labels (0 or 1).\\n            true_labels (list[int]): A list of true labels (0 or 1).\\n\\n        Returns:\\n            None\\n\\n        Test cases:\\n            # Test case 1: All predictions correct\\n            calc = MetricsCalculator()\\n            calc.update([1, 0, 1], [1, 0, 1])\\n            assert calc.true_positives == 2\\n            assert calc.true_negatives == 1\\n            assert calc.false_positives == 0\\n            assert calc.false_negatives == 0\\n\\n            # Test case 2: Some predictions incorrect\\n            calc = MetricsCalculator()\\n            calc.update([1, 1, 0], [1, 0, 1])\\n            assert calc.true_positives == 1\\n            assert calc.true_negatives == 0\\n            assert calc.false_positives == 1\\n            assert calc.false_negatives == 1\\n\\n            # Test case 3: Edge case with empty lists\\n            calc = MetricsCalculator()\\n            calc.update([], [])\\n            assert calc.true_positives == 0\\n            assert calc.true_negatives == 0\\n            assert calc.false_positives == 0\\n            assert calc.false_negatives == 0\\n        \"\"\"\\n        for predicted, true in zip(predicted_labels, true_labels):\\n            if predicted == 1 and true == 1:\\n                self.true_positives += 1\\n            elif predicted == 1 and true == 0:\\n                self.false_positives += 1\\n            elif predicted == 0 and true == 1:\\n                self.false_negatives += 1\\n            elif predicted == 0 and true == 0:\\n                self.true_negatives += 1\\n\\n    def precision(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Computes and returns the precision of the predictions.\\n\\n        Parameters:\\n            predicted_labels (list[int]): A list of predicted labels (0 or 1).\\n            true_labels (list[int]): A list of true labels (0 or 1).\\n\\n        Returns:\\n            float: The precision of the predictions, defined as the ratio of true positives to\\n            the total predicted positives.\\n\\n        Test cases:\\n            # Test case 1: Perfect precision\\n            calc = MetricsCalculator()\\n            calc.update([1, 1, 0], [1, 0, 1])\\n            assert calc.precision([1, 1, 0], [1, 0, 1]) == 0.5\\n\\n            # Test case 2: No positive predictions\\n            calc = MetricsCalculator()\\n            calc.update([0, 0, 0], [1, 1, 1])\\n            assert calc.precision([0, 0, 0], [1, 1, 1]) == 0.0\\n\\n            # Test case 3: No positive ground truth\\n            calc = MetricsCalculator()\\n            calc.update([1, 0, 0], [0, 0, 0])\\n            assert calc.precision([1, 0, 0], [0, 0, 0]) == 0.0\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_positives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_positives)\\n\\n    def recall(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Computes and returns the recall of the predictions.\\n\\n        Parameters:\\n            predicted_labels (list[int]): A list of predicted labels (0 or 1).\\n            true_labels (list[int]): A list of true labels (0 or 1).\\n\\n        Returns:\\n            float: The recall of the predictions, defined as the ratio of true positives to\\n            the total actual positives.\\n\\n        Test cases:\\n            # Test case 1: Perfect recall\\n            calc = MetricsCalculator()\\n            calc.update([1, 1, 0], [1, 1, 0])\\n            assert calc.recall([1, 1, 0], [1, 1, 0]) == 1.0\\n\\n            # Test case 2: No actual positives\\n            calc = MetricsCalculator()\\n            calc.update([0, 0, 0], [0, 0, 0])\\n            assert calc.recall([0, 0, 0], [0, 0, 0]) == 0.0\\n\\n            # Test case 3: Some actual positives missed\\n            calc = MetricsCalculator()\\n            calc.update([0, 1, 1], [1, 1, 0])\\n            assert calc.recall([0, 1, 1], [1, 1, 0]) == 0.67  # 2 out of 3\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_negatives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_negatives)\\n\\n    def f1_score(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Computes and returns the F1 score of the predictions.\\n\\n        Parameters:\\n            predicted_labels (list[int]): A list of predicted labels (0 or 1).\\n            true_labels (list[int]): A list of true labels (0 or 1).\\n\\n        Returns:\\n            float: The F1 score of the predictions, which is the harmonic mean of precision and\\n            recall.\\n\\n        Test cases:\\n            # Test case 1: Balanced precision and recall\\n            calc = MetricsCalculator()\\n            calc.update([1, 0, 1], [1, 0, 1])\\n            assert calc.f1_score([1, 0, 1], [1, 0, 1]) == 1.0\\n\\n            # Test case 2: No true positives\\n            calc = MetricsCalculator()\\n            calc.update([0, 0, 0], [1, 1, 1])\\n            assert calc.f1_score([0, 0, 0], [1, 1, 1]) == 0.0\\n\\n            # Test case 3: One positive example\\n            calc = MetricsCalculator()\\n            calc.update([1, 0, 1], [1, 1, 0])\\n            assert calc.f1_score([1, 0, 1], [1, 1, 0]) == 0.67  # F1 score calculation\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        precision = self.precision(predicted_labels, true_labels)\\n        recall = self.recall(predicted_labels, true_labels)\\n        if precision + recall == 0.0:\\n            return 0.0\\n        return (2 * precision * recall) / (precision + recall)\\n\\n    def accuracy(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Computes and returns the accuracy of the predictions.\\n\\n        Parameters:\\n            predicted_labels (list[int]): A list of predicted labels (0 or 1).\\n            true_labels (list[int]): A list of true labels (0 or 1).\\n\\n        Returns:\\n            float: The accuracy of the predictions, defined as the ratio of correctly predicted\\n            instances (both true positives and true negatives) to the total instances.\\n\\n        Test cases:\\n            # Test case 1: All classifications correct\\n            calc = MetricsCalculator()\\n            calc.update([1, 0, 1], [1, 0, 1])\\n            assert calc.accuracy([1, 0, 1], [1, 0, 1]) == 1.0\\n\\n            # Test case 2: All classifications wrong\\n            calc = MetricsCalculator()\\n            calc.update([0, 0, 0], [1, 1, 1])\\n            assert calc.accuracy([0, 0, 0], [1, 1, 1]) == 0.0\\n\\n            # Test case 3: Mixed classifications\\n            calc = MetricsCalculator()\\n            calc.update([1, 0, 1], [1, 1, 0])\\n            assert calc.accuracy([1, 0, 1], [1, 1, 0]) == 0.67  # 2 correct out of 3\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\\n        if total == 0:\\n            return 0.0\\n        return (self.true_positives + self.true_negatives) / total\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735133667, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=2004, prompt_tokens=1834, total_tokens=3838, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "MetricsCalculator.txt"}