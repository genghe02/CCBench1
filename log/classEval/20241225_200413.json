{"response body": "ChatCompletion(id='chatcmpl-AiKPELxjqjTmwFTIPpiFYvxGtEuMy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\n\"\"\"\\nThe VectorUtil class provides utilities for vector operations commonly used in Natural Language Processing (NLP) and machine learning tasks. \\nIt includes methods for calculating similarities between vectors, specifically focusing on cosine similarity and n-similarity metrics.\\nThis class depends on the NumPy library for numerical computations and gensim for vector normalization.\\n\"\"\"\\n\\nimport numpy as np\\nfrom gensim import matutils\\nfrom numpy import dot, array\\n\\nclass VectorUtil:\\n    @staticmethod\\n    def similarity(vector_1, vector_2):\\n        \"\"\"\\n        Calculate the cosine similarity between two vectors.\\n\\n        Parameters:\\n            vector_1 (array-like): The first vector for comparison.\\n            vector_2 (array-like): The second vector for comparison.\\n\\n        Returns:\\n            float: The cosine similarity value between vector_1 and vector_2, ranging from -1 (opposite) to 1 (same direction).\\n\\n        Test cases:\\n            # Test case 1: Basic similarity\\n            input1 = [1, 0, 0]\\n            input2 = [0, 1, 0]\\n            expected_output1 = 0.0  # Orthogonal vectors\\n            assert np.isclose(VectorUtil.similarity(input1, input2), expected_output1)\\n            \\n            # Test case 2: Identical vectors\\n            input1 = [1, 2, 3]\\n            input2 = [1, 2, 3]\\n            expected_output2 = 1.0  # Same vectors\\n            assert np.isclose(VectorUtil.similarity(input1, input2), expected_output2)\\n\\n            # Test case 3: Opposite vectors\\n            input1 = [1, 1, 1]\\n            input2 = [-1, -1, -1]\\n            expected_output3 = -1.0  # Opposite vectors\\n            assert np.isclose(VectorUtil.similarity(input1, input2), expected_output3)\\n        \"\"\"\\n        return dot(matutils.unitvec(vector_1), matutils.unitvec(vector_2))\\n\\n    @staticmethod\\n    def cosine_similarities(vector_1, vectors_all):\\n        \"\"\"\\n        Calculate the cosine similarities between a vector and multiple other vectors.\\n\\n        Parameters:\\n            vector_1 (array-like): The vector for which similarities are calculated.\\n            vectors_all (array-like): An array of vectors to compare against.\\n\\n        Returns:\\n            array: An array of cosine similarity scores between vector_1 and every vector in vectors_all.\\n\\n        Test cases:\\n            # Test case 1: Basic cosine similarities\\n            vector1 = [1, 0]\\n            vectors = [[0, 1], [1, 0], [0.5, 0.5]]\\n            expected_output = [0.0, 1.0, 0.70710678]  # Cosine similarities for vector1 against vectors\\n            assert np.allclose(VectorUtil.cosine_similarities(vector1, vectors), expected_output)\\n\\n            # Test case 2: Zero vector\\n            vector1 = [0, 0]\\n            vectors = [[1, 2], [0, 0]]\\n            expected_output_zero = [0.0, 0.0]  # Similarities with a zero vector\\n            assert np.allclose(VectorUtil.cosine_similarities(vector1, vectors), expected_output_zero)\\n\\n            # Test case 3: Same vector comparisons\\n            vector1 = [1, 1, 1]\\n            vectors = [[1, 1, 1], [1, 1, 1]]\\n            expected_output_same = [1.0, 1.0]  # Similarities are 1 with the same vector\\n            assert np.allclose(VectorUtil.cosine_similarities(vector1, vectors), expected_output_same)\\n        \"\"\"\\n        norm = np.linalg.norm(vector_1)\\n        all_norms = np.linalg.norm(vectors_all, axis=1)\\n        dot_products = dot(vectors_all, vector_1)\\n        similarities = dot_products / (norm * all_norms)\\n        return similarities\\n\\n    @staticmethod\\n    def n_similarity(vector_list_1, vector_list_2):\\n        \"\"\"\\n        Calculate the similarity between the average vectors of two lists of vectors.\\n\\n        Parameters:\\n            vector_list_1 (array-like): A list of vectors for the first group.\\n            vector_list_2 (array-like): A list of vectors for the second group.\\n\\n        Returns:\\n            float: The cosine similarity between the average vectors of both lists.\\n        \\n        Raises:\\n            ZeroDivisionError: If either vector_list_1 or vector_list_2 is empty.\\n\\n        Test cases:\\n            # Test case 1: Basic n-similarity\\n            list1 = [[1, 2], [2, 3]]\\n            list2 = [[3, 4], [4, 5]]\\n            expected_output = 0.993858\\n            assert np.isclose(VectorUtil.n_similarity(list1, list2), expected_output, rtol=1e-5)\\n\\n            # Test case 2: One list empty\\n            list1 = []\\n            list2 = [[1, 2], [2, 3]]\\n            try:\\n                VectorUtil.n_similarity(list1, list2)\\n                assert False  # Should raise an error\\n            except ZeroDivisionError:\\n                assert True  # Correctly handled empty input case\\n\\n            # Test case 3: Same lists\\n            list1 = [[1, 1], [2, 2]]\\n            list2 = [[1, 1], [2, 2]]\\n            expected_output_same = 1.0\\n            assert np.isclose(VectorUtil.n_similarity(list1, list2), expected_output_same)\\n        \"\"\"\\n        if not (len(vector_list_1) and len(vector_list_2)):\\n            raise ZeroDivisionError(\\'At least one of the passed list is empty.\\')\\n\\n        return dot(matutils.unitvec(array(vector_list_1).mean(axis=0)),\\n                   matutils.unitvec(array(vector_list_2).mean(axis=0)))\\n    \\n    @staticmethod\\n    def compute_idf_weight_dict(total_num, number_dict):\\n        \"\"\"\\n        Compute the Inverse Document Frequency (IDF) weights for terms based on their frequency.\\n\\n        Parameters:\\n            total_num (int): Total number of documents in the corpus.\\n            number_dict (dict): A dictionary of term frequencies where keys are terms and values are their counts.\\n\\n        Returns:\\n            dict: A dictionary where keys are terms and values are their IDF weights.\\n\\n        Test cases:\\n            # Test case 1: Normal case\\n            total_documents = 100\\n            term_counts = {\\'term1\\': 10, \\'term2\\': 5}\\n            expected_output = {\\'term1\\': np.log(101 / 11), \\'term2\\': np.log(101 / 6)}\\n            assert np.allclose(list(VectorUtil.compute_idf_weight_dict(total_documents, term_counts).values()),\\n                               list(expected_output.values()), rtol=1e-5)\\n\\n            # Test case 2: Edge case with zero counts\\n            total_documents = 100\\n            term_counts = {\\'term1\\': 0}\\n            expected_output_zero = {\\'term1\\': np.log(101 / 1)}  # log(101)\\n            assert np.isclose(VectorUtil.compute_idf_weight_dict(total_documents, term_counts)[\\'term1\\'], expected_output_zero[\\'term1\\'], rtol=1e-5)\\n            \\n            # Test case 3: Empty dictionary\\n            total_documents = 10\\n            term_counts = {}\\n            expected_output_empty = {}\\n            assert VectorUtil.compute_idf_weight_dict(total_documents, term_counts) == expected_output_empty\\n        \"\"\"\\n        index_2_key_map = {}\\n\\n        index = 0\\n\\n        count_list = []\\n        for key, count in number_dict.items():\\n            index_2_key_map[index] = key\\n            count_list.append(count)\\n            index += 1\\n\\n        a = np.array(count_list)\\n        # smooth, in case the divide by zero error\\n        a = np.log((total_num + 1) / (a + 1))\\n        result = {}\\n\\n        for index, w in enumerate(a):\\n            key = index_2_key_map[index]\\n            result[key] = w\\n\\n        return result\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735128236, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=1721, prompt_tokens=1705, total_tokens=3426, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "VectorCalculator.txt"}