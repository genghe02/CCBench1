{"response body": "ChatCompletion(id='chatcmpl-AiLpHfsNDvh9zRDyYhuVUG4Jjiu0M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\n\"\"\"\\nThis class provides methods to calculate metrics related to Mean Reciprocal Rank (MRR) \\nand Average Precision (AP) for lists of query results.\\n\"\"\"\\n\\nimport numpy as np\\n\\n\\nclass MetricsCalculator2:\\n    def __init__(self):\\n        \"\"\"\\n        Initialization method for the MetricsCalculator2 class.\\n        \"\"\"\\n        pass\\n\\n    @staticmethod\\n    def mrr(data):\\n        \"\"\"\\n        Calculate the Mean Reciprocal Rank (MRR) from the provided data.\\n\\n        Parameters:\\n            data (list or tuple): A list of tuples or a single tuple containing:\\n                - A list of scores (float), or an array-like structure\\n                - An integer representing the total number of items (total_num).\\n\\n        Returns:\\n            tuple: A tuple where the first element is the MRR (float),\\n                   and the second element is a list containing the individual\\n                   MRR values for each sub-list.\\n\\n        Test cases:\\n            # Test case 1: Normal case with multiple items\\n            input_data = ([1, 0, 1, 0], 4)\\n            expected_output = (1.0, [1.0])\\n            \\n            # Test case 2: Edge case with empty input\\n            input_data = []\\n            expected_output = (0.0, [0.0])\\n\\n            # Test case 3: Error case with invalid data type\\n            try:\\n                MetricsCalculator2.mrr(\\'invalid_type\\')\\n            except Exception as e:\\n                expected_output = \"the input must be a tuple([0,...,1,...],int) or an iteration of list of tuple\"\\n        \"\"\"\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or an iteration of list of tuple\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                mr_np = sub_list * ranking_array\\n\\n                mr = 0.0\\n                for team in mr_np:\\n                    if team > 0:\\n                        mr = team\\n                        break\\n                return mr, [mr]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    mr = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                    mr_np = sub_list * ranking_array\\n\\n                    mr = 0.0\\n                    for team in mr_np:\\n                        if team > 0:\\n                            mr = team\\n                            break\\n\\n                separate_result.append(mr)\\n            return np.mean(separate_result), separate_result\\n\\n    @staticmethod\\n    def map(data):\\n        \"\"\"\\n        Calculate the Average Precision (AP) from the provided data.\\n\\n        Parameters:\\n            data (list or tuple): A list of tuples or a single tuple containing:\\n                - A list of binary indicators (1 for relevant, 0 for non-relevant)\\n                - An integer representing the total number of relevant items.\\n\\n        Returns:\\n            tuple: A tuple where the first element is the Average Precision (AP, float),\\n                   and the second element is a list containing the individual AP values for each sub-list.\\n\\n        Test cases:\\n            # Test case 1: Normal case with multiple items\\n            input_data = ([1, 0, 1, 0], 2)\\n            expected_output = (0.75, [0.75])\\n\\n            # Test case 2: Edge case with empty input\\n            input_data = []\\n            expected_output = (0.0, [0.0])\\n\\n            # Test case 3: Error case with invalid data type\\n            try:\\n                MetricsCalculator2.map(\\'invalid_type\\')\\n            except Exception as e:\\n                expected_output = \"the input must be a tuple([0,...,1,...],int) or an iteration of list of tuple\"\\n        \"\"\"\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or an iteration of list of tuple\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                right_ranking_list = []\\n                count = 1\\n                for t in sub_list:\\n                    if t == 0:\\n                        right_ranking_list.append(0)\\n                    else:\\n                        right_ranking_list.append(count)\\n                        count += 1\\n\\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n                return ap, [ap]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    ap = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                    right_ranking_list = []\\n                    count = 1\\n                    for t in sub_list:\\n                        if t == 0:\\n                            right_ranking_list.append(0)\\n                        else:\\n                            right_ranking_list.append(count)\\n                            count += 1\\n\\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n\\n                separate_result.append(ap)\\n            return np.mean(separate_result), separate_result\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735133695, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=1286, prompt_tokens=2059, total_tokens=3345, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "MetricsCalculator2.txt"}