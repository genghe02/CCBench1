{"response body": "ChatCompletion(id='chatcmpl-AihXy1txqdiHkfCDW5thqWAI1ShzG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"\"\"\\nThis class implements a metrics calculator for classification tasks using confusion matrix values.\\n\\nThe class MetricsCalculator contains methods to calculate precision, recall, F1 score, and accuracy based on true positives, false positives, false negatives, and true negatives.\\n\\nMetricsCalculator:\\n    Methods:\\n        - __init__: Initializes the metrics calculator with initial values of confustion matrix fields.\\n        - update: Updates the confusion matrix fields based on predicted and true labels.\\n        - precision: Calculates precision metric based on the updated confusion matrix.\\n        - recall: Calculates recall metric based on the updated confusion matrix.\\n        - f1_score: Calculates F1 score based on the precision and recall values.\\n        - accuracy: Calculates accuracy based on the updated confusion matrix.\\n\\n\"\"\"\\n\\nclass MetricsCalculator:\\n    def __init__(self):\\n        \"\"\"\\n        Initialize the MetricsCalculator object with the initial values of confusion matrix fields.\\n        \"\"\"        \\n        self.true_positives = 0\\n        self.false_positives = 0\\n        self.false_negatives = 0\\n        self.true_negatives = 0\\n\\n    def update(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Update the confusion matrix fields based on predicted and true labels.\\n\\n        Parameters:\\n            predicted_labels (list): List of predicted labels (0 or 1)\\n            true_labels (list): List of true labels (0 or 1)\\n        \\n        Test cases:\\n            # Test case 1: Increment true_positive on correct positive prediction\\n            update([1, 0, 1], [1, 0, 1])\\n            assert true_positives == 2\\n\\n            # Test case 2: Increment false_positive on incorrect positive prediction\\n            update([0, 1, 1], [1, 0, 0])\\n            assert false_positives == 1\\n\\n            # Test case 3: Increment false_negative on incorrect negative prediction\\n            update([0, 0, 1], [1, 0, 0])\\n            assert false_negatives == 1\\n        \"\"\"\\n        for predicted, true in zip(predicted_labels, true_labels):\\n            if predicted == 1 and true == 1:\\n                self.true_positives += 1\\n            elif predicted == 1 and true == 0:\\n                self.false_positives += 1\\n            elif predicted == 0 and true == 1:\\n                self.false_negatives += 1\\n            elif predicted == 0 and true == 0:\\n                self.true_negatives += 1\\n\\n    def precision(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Calculate the precision metric based on the updated confusion matrix.\\n\\n        Parameters:\\n            predicted_labels (list): List of predicted labels (0 or 1)\\n            true_labels (list): List of true labels (0 or 1)\\n        \\n        Returns:\\n            float: Precision value between 0.0 and 1.0\\n\\n        Test cases:\\n            # Test case 1: Perfect precision\\n            assert precision([1, 1, 1], [1, 1, 1]) == 1.0\\n\\n            # Test case 2: Low precision\\n            assert precision([0, 1, 0], [1, 0, 1]) == 0.0\\n\\n            # Test case 3: Intermediate precision\\n            assert precision([1, 0, 1], [1, 0, 0]) == 0.67\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_positives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_positives)\\n\\n    def recall(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Calculate the recall metric based on the updated confusion matrix.\\n\\n        Parameters:\\n            predicted_labels (list): List of predicted labels (0 or 1)\\n            true_labels (list): List of true labels (0 or 1)\\n        \\n        Returns:\\n            float: Recall value between 0.0 and 1.0\\n\\n        Test cases:\\n            # Test case 1: Perfect recall\\n            assert recall([1, 1, 1], [1, 1, 1]) == 1.0\\n\\n            # Test case 2: Low recall\\n            assert recall([0, 1, 0], [1, 0, 1]) == 0.0\\n\\n            # Test case 3: Intermediate recall\\n            assert recall([1, 0, 1], [1, 0, 0]) == 1.0\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_negatives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_negatives)\\n\\n    def f1_score(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Calculate the F1 score based on the precision and recall values.\\n\\n        Parameters:\\n            predicted_labels (list): List of predicted labels (0 or 1)\\n            true_labels (list): List of true labels (0 or 1)\\n        \\n        Returns:\\n            float: F1 score value between 0.0 and 1.0\\n\\n        Test cases:\\n            # Test case 1: Perfect F1 score\\n            assert f1_score([1, 1, 1], [1, 1, 1]) == 1.0\\n\\n            # Test case 2: Low F1 score\\n            assert f1_score([0, 1, 0], [1, 0, 1]) == 0.0\\n\\n            # Test case 3: Intermediate F1 score\\n            assert f1_score([1, 0, 1], [1, 0, 0]) == 0.80\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        precision = self.precision(predicted_labels, true_labels)\\n        recall = self.recall(predicted_labels, true_labels)\\n        if precision + recall == 0.0:\\n            return 0.0\\n        return (2 * precision * recall) / (precision + recall)\\n\\n    def accuracy(self, predicted_labels, true_labels):\\n        \"\"\"\\n        Calculate the accuracy based on the updated confusion matrix.\\n\\n        Parameters:\\n            predicted_labels (list): List of predicted labels (0 or 1)\\n            true_labels (list): List of true labels (0 or 1)\\n        \\n        Returns:\\n            float: Accuracy value between 0.0 and 1.0\\n\\n        Test cases:\\n            # Test case 1: Perfect accuracy\\n            assert accuracy([1, 1, 1], [1, 1, 1]) == 1.0\\n\\n            # Test case 2: Low accuracy\\n            assert accuracy([0, 1, 0], [1, 0, 1]) == 0.33\\n\\n            # Test case 3: Intermediate accuracy\\n            assert accuracy([1, 0, 1], [1, 0, 0]) == 0.67\\n        \"\"\"\\n        self.update(predicted_labels, true_labels)\\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\\n        if total == 0:\\n            return 0.0\\n        return (self.true_positives + self.true_negatives) / total\\n\"\"\"', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735217190, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1552, prompt_tokens=1556, total_tokens=3108, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "MetricsCalculator.txt"}