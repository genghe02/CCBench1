{"response body": "{\n  \"id\": \"9f1ea295-9fb3-410b-aa09-ec6402fa5ab8\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"```python\\nimport numpy as np\\n\\n# This class provides methods to calculate metrics such as Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) for ranking data.\\nclass MetricsCalculator2:\\n    def __init__(self):\\n        pass\\n\\n    @staticmethod\\n    def mrr(data):\\n        # Calculate the Mean Reciprocal Rank (MRR) for the given data, which can be either a single tuple or a list of tuples.\\n        # param data: tuple or list of tuples, where each tuple contains a sublist of rankings and a total number of relevant items.\\n        # return: tuple, the MRR value and a list of individual MRR values if multiple sets are provided.\\n        # >>> MetricsCalculator2.mrr(([0, 1, 0, 1], 2))\\n        # (0.5, [0.5])\\n        # >>> MetricsCalculator2.mrr([([0, 1, 0, 1], 2), ([1, 0, 0, 0], 1)])\\n        # (0.75, [0.5, 1.0])\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\\\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\\\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                mr_np = sub_list * ranking_array\\n\\n                mr = 0.0\\n                for team in mr_np:\\n                    if team > 0:\\n                        mr = team\\n                        break\\n                return mr, [mr]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    mr = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                    mr_np = sub_list * ranking_array\\n\\n                    mr = 0.0\\n                    for team in mr_np:\\n                        if team > 0:\\n                            mr = team\\n                            break\\n\\n                separate_result.append(mr)\\n            return np.mean(separate_result), separate_result\\n\\n    @staticmethod\\n    def map(data):\\n        # Calculate the Mean Average Precision (MAP) for the given data, which can be either a single tuple or a list of tuples.\\n        # param data: tuple or list of tuples, where each tuple contains a sublist of rankings and a total number of relevant items.\\n        # return: tuple, the MAP value and a list of individual AP values if multiple sets are provided.\\n        # >>> MetricsCalculator2.map(([0, 1, 0, 1], 2))\\n        # (0.5, [0.5])\\n        # >>> MetricsCalculator2.map([([0, 1, 0, 1], 2), ([1, 0, 0, 0], 1)])\\n        # (0.75, [0.5, 1.0])\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\\\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\\\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                right_ranking_list = []\\n                count = 1\\n                for t in sub_list:\\n                    if t == 0:\\n                        right_ranking_list.append(0)\\n                    else:\\n                        right_ranking_list.append(count)\\n                        count += 1\\n\\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n                return ap, [ap]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    ap = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                    right_ranking_list = []\\n                    count = 1\\n                    for t in sub_list:\\n                        if t == 0:\\n                            right_ranking_list.append(0)\\n                        else:\\n                            right_ranking_list.append(count)\\n                            count += 1\\n\\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n\\n                separate_result.append(ap)\\n            return np.mean(separate_result), separate_result\\n```\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1727079647,\n  \"model\": \"deepseek-chat\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"fp_1c141eb703\",\n  \"usage\": {\n    \"completion_tokens\": 1324,\n    \"prompt_tokens\": 1600,\n    \"total_tokens\": 2924,\n    \"prompt_cache_hit_tokens\": 576,\n    \"prompt_cache_miss_tokens\": 1024\n  }\n}", "test_case_name": "MetricsCalculator2.txt"}