{"response body": {"id": "69a2d182-4c24-403d-a1b4-e16113bb91c6", "object": "chat.completion", "model": "gpt-4-32k", "created": "2024-09-23 19:54:13", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "content": "```python\n# This class is for calculating various metrics to assess the performance of a prediction model.\nclass MetricsCalculator:\n    # Initialize the four categories of prediction results.\n    def __init__(self):\n        self.true_positives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.true_negatives = 0\n\n    # This function updates the four categories of prediction results according to the predicted labels and true labels.\n    # param predicted_labels: list, predicted labels from the prediction model\n    # param true_labels: list, true labels for the data\n    # This function does not return anything.\n    # Test cases: \n    # >>> calculator = MetricsCalculator()\n    # >>> calculator.update([1, 0, 1, 0], [1, 0, 0, 1])\n    def update(self, predicted_labels, true_labels):\n        for predicted, true in zip(predicted_labels, true_labels):\n            if predicted == 1 and true == 1:\n                self.true_positives += 1\n            elif predicted == 1 and true == 0:\n                self.false_positives += 1\n            elif predicted == 0 and true == 1:\n                self.false_negatives += 1\n            elif predicted == 0 and true == 0:\n                self.true_negatives += 1\n\n    # This function calculates the precision of the prediction model.\n    # param predicted_labels: list, predicted labels from the prediction model\n    # param true_labels: list, true labels for the data\n    # return: float, precision of the prediction model\n    # Test cases: \n    # >>> calculator = MetricsCalculator()\n    # >>> calculator.precision([1, 0, 1, 0], [1, 0, 0, 1])\n    def precision(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_positives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_positives)\n\n    # This function calculates the recall of the prediction model.\n    # param predicted_labels: list, predicted labels from the prediction model\n    # param true_labels: list, true labels for the data\n    # return: float, recall of the prediction model\n    # Test cases: \n    # >>> calculator = MetricsCalculator()\n    # >>> calculator.recall([1, 0, 1, 0], [1, 0, 0, 1])\n    def recall(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_negatives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_negatives)\n\n    # This function calculates the F1 score of the prediction model.\n    # param predicted_labels: list, predicted labels from the prediction model\n    # param true_labels: list, true labels for the data\n    # return: float, F1 score of the prediction model\n    # Test cases: \n    # >>> calculator = MetricsCalculator()\n    # >>> calculator.f1_score([1, 0, 1, 0], [1, 0, 0, 1])\n    def f1_score(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        precision = self.precision(predicted_labels, true_labels)\n        recall = self.recall(predicted_labels, true_labels)\n        if precision + recall == 0.0:\n            return 0.0\n        return (2 * precision * recall) / (precision + recall)\n\n    # This function calculates the accuracy of the prediction model.\n    # param predicted_labels: list, predicted labels from the prediction model\n    # param true_labels: list, true labels for the data\n    # return: float, accuracy of the prediction model\n    # Test cases: \n    # >>> calculator = MetricsCalculator()\n    # >>> calculator.accuracy([1, 0, 1, 0], [1, 0, 0, 1])\n    def accuracy(self, predicted_labels, true_labels):\n        self.update(predicted_labels, true_labels)\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\n        if total == 0:\n            return 0.0\n        return (self.true_positives + self.true_negatives) / total\n```"}}], "usage": {"completion_tokens": 956, "prompt_tokens": 1151, "total_tokens": 2107}, "feeType": "1", "consume": 1.335468}, "test_case_name": "MetricsCalculator.txt"}