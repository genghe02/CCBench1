{"response body": {"id": "5ca8b96f-f95f-479f-b98f-f9d2e5e38479", "object": "chat.completion", "model": "gpt-4-32k", "created": "2024-12-26 15:53:56", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "content": "\"\"\"\nThis class calculates different evaluation metrics like precision, recall, F1 score, and accuracy for a binary classification model.\n\"\"\"\n\nclass MetricsCalculator:\n    def __init__(self):\n        \"\"\"\n        Initialization method to set all metrics to zero.\n\n        No parameters are required.\n\n        No return value.\n        \"\"\"\n        self.true_positives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.true_negatives = 0\n\n    def update(self, predicted_labels, true_labels):\n        \"\"\"\n        Updates the counts of true positives, false positives, false negatives, and true negatives based on the \n        predicted and true labels.\n\n        Parameters:\n            predicted_labels (list): A list of predicted labels from the model.\n            true_labels (list): A list of actual labels.\n\n        No return value.\n\n        Test cases:\n            # Test case 1: Normal usage\n            update([1, 0, 1, 0], [1, 0, 0, 1])  \n            # Test case 2: All predictions are correct\n            update([1, 0, 1, 0], [1, 0, 1, 0])\n            # Test case 3: All predictions are incorrect\n            update([1, 0, 1, 0], [0, 1, 0, 1])\n        \"\"\"\n        for predicted, true in zip(predicted_labels, true_labels):\n            if predicted == 1 and true == 1:\n                self.true_positives += 1\n            elif predicted == 1 and true == 0:\n                self.false_positives += 1\n            elif predicted == 0 and true == 1:\n                self.false_negatives += 1\n            elif predicted == 0 and true == 0:\n                self.true_negatives += 1\n\n    def precision(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculates precision of the prediction.\n\n        Parameters:\n            predicted_labels (list): A list of predicted labels from the model.\n            true_labels (list): A list of actual labels.\n\n        Returns:\n            float: Precision of the model predictions.\n\n        Test cases:\n            # Test case 1: Normal usage\n            precision([1, 0, 1, 0], [1, 0, 0, 1])  \n            # Test case 2: All predictions are correct\n            precision([1, 0, 1, 0], [1, 0, 1, 0])\n            # Test case 3: All predictions are incorrect\n            precision([1, 0, 1, 0], [0, 1, 0, 1])\n        \"\"\"\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_positives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_positives)\n\n    def recall(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculates recall of the prediction.\n\n        Parameters:\n            predicted_labels (list): A list of predicted labels from the model.\n            true_labels (list): A list of actual labels.\n\n        Returns:\n            float: Recall of the model predictions.\n\n        Test cases:\n            # Test case 1: Normal usage\n            recall([1, 0, 1, 0], [1, 0, 0, 1])\n            # Test case 2: All predictions are correct\n            recall([1, 0, 1, 0], [1, 0, 1, 0])\n            # Test case 3: All predictions are incorrect\n            recall([1, 0, 1, 0], [0, 1, 0, 1])\n        \"\"\"\n        self.update(predicted_labels, true_labels)\n        if self.true_positives + self.false_negatives == 0:\n            return 0.0\n        return self.true_positives / (self.true_positives + self.false_negatives)\n\n    def f1_score(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculates F1 score of the prediction.\n\n        Parameters:\n            predicted_labels (list): A list of predicted labels from the model.\n            true_labels (list): A list of actual labels.\n\n        Returns:\n            float: F1 score of the model predictions.\n\n        Test cases:\n            # Test case 1: Normal usage\n            f1_score([1, 0, 1, 0], [1, 0, 0, 1])\n            # Test case 2: All predictions are correct\n            f1_score([1, 0, 1, 0], [1, 0, 1, 0])\n            # Test case 3: All predictions are incorrect\n            f1_score([1, 0, 1, 0], [0, 1, 0, 1])\n        \"\"\"\n        self.update(predicted_labels, true_labels)\n        precision = self.precision(predicted_labels, true_labels)\n        recall = self.recall(predicted_labels, true_labels)\n        if precision + recall == 0.0:\n            return 0.0\n        return (2 * precision * recall) / (precision + recall)\n\n    def accuracy(self, predicted_labels, true_labels):\n        \"\"\"\n        Calculates accuracy of the prediction.\n\n        Parameters:\n            predicted_labels (list): A list of predicted labels from the model.\n            true_labels (list): A list of actual labels.\n\n        Returns:\n            float: Accuracy of the model predictions.\n\n        Test cases:\n            # Test case 1: Normal usage\n            accuracy([1, 0, 1, 0], [1, 0, 0, 1])\n            # Test case 2: All predictions are correct\n            accuracy([1, 0, 1, 0], [1, 0, 1, 0])\n            # Test case 3: All predictions are incorrect\n            accuracy([1, 0, 1, 0], [0, 1, 0, 1])\n        \"\"\"\n        self.update(predicted_labels, true_labels)\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\n        if total == 0:\n            return 0.0\n        return (self.true_positives + self.true_negatives) / total"}}], "usage": {"completion_tokens": 1352, "prompt_tokens": 1556, "total_tokens": 2908}, "feeType": "1", "consume": 1.85736}, "test_case_name": "MetricsCalculator.txt"}