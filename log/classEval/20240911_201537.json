{"response body": "ChatCompletion(id='chatcmpl-A6GXkuXy5etF98AyiTxh7sFmRzo4O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\nclass MetricsCalculator:\\n    # This class computes various metrics for evaluating binary classification models.\\n\\n    def __init__(self):\\n        # Initializes the MetricsCalculator with counts for true positives, false positives, false negatives, and true negatives.\\n        self.true_positives = 0\\n        self.false_positives = 0\\n        self.false_negatives = 0\\n        self.true_negatives = 0\\n\\n    def update(self, predicted_labels, true_labels):\\n        # Updates the counts of true positives, false positives, false negatives, and true negatives based on the provided predicted and true labels.\\n        # param predicted_labels: list of int, predicted binary labels from the model\\n        # param true_labels: list of int, actual binary labels for the dataset\\n        # return: None, this function modifies the internal state of the object\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.update([1, 0, 1], [1, 1, 0]) \\n        for predicted, true in zip(predicted_labels, true_labels):\\n            if predicted == 1 and true == 1:\\n                self.true_positives += 1\\n            elif predicted == 1 and true == 0:\\n                self.false_positives += 1\\n            elif predicted == 0 and true == 1:\\n                self.false_negatives += 1\\n            elif predicted == 0 and true == 0:\\n                self.true_negatives += 1\\n\\n    def precision(self, predicted_labels, true_labels):\\n        # Calculates the precision of the predictions.\\n        # param predicted_labels: list of int, predicted binary labels from the model\\n        # param true_labels: list of int, actual binary labels for the dataset\\n        # return: float, the precision score (True Positives / (True Positives + False Positives))\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.update([1, 0, 1], [1, 1, 0])\\n        # >>> metrics.precision([1, 0, 1], [1, 1, 0]) \\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_positives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_positives)\\n\\n    def recall(self, predicted_labels, true_labels):\\n        # Calculates the recall of the predictions.\\n        # param predicted_labels: list of int, predicted binary labels from the model\\n        # param true_labels: list of int, actual binary labels for the dataset\\n        # return: float, the recall score (True Positives / (True Positives + False Negatives))\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.update([1, 0, 1], [1, 1, 0])\\n        # >>> metrics.recall([1, 0, 1], [1, 1, 0]) \\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_negatives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_negatives)\\n\\n    def f1_score(self, predicted_labels, true_labels):\\n        # Calculates the F1 score of the predictions.\\n        # param predicted_labels: list of int, predicted binary labels from the model\\n        # param true_labels: list of int, actual binary labels for the dataset\\n        # return: float, the F1 score, which is the harmonic mean of precision and recall\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.update([1, 0, 1], [1, 1, 0])\\n        # >>> metrics.f1_score([1, 0, 1], [1, 1, 0]) \\n        self.update(predicted_labels, true_labels)\\n        precision = self.precision(predicted_labels, true_labels)\\n        recall = self.recall(predicted_labels, true_labels)\\n        if precision + recall == 0.0:\\n            return 0.0\\n        return (2 * precision * recall) / (precision + recall)\\n\\n    def accuracy(self, predicted_labels, true_labels):\\n        # Calculates the accuracy of the predictions.\\n        # param predicted_labels: list of int, predicted binary labels from the model\\n        # param true_labels: list of int, actual binary labels for the dataset\\n        # return: float, the accuracy score ((True Positives + True Negatives) / Total Samples)\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.update([1, 0, 1], [1, 1, 0])\\n        # >>> metrics.accuracy([1, 0, 1], [1, 1, 0]) \\n        self.update(predicted_labels, true_labels)\\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\\n        if total == 0:\\n            return 0.0\\n        return (self.true_positives + self.true_negatives) / total\\n```', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1726056924, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_483d39d857', usage=CompletionUsage(completion_tokens=1086, prompt_tokens=1290, total_tokens=2376))", "test_case_name": "MetricsCalculator.txt"}