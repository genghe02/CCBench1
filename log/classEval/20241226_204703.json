{"response body": "ChatCompletion(id='chatcmpl-AihYLv4e6L8K3Oyl1IF1IzJQAsQeI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"\"\"\\nThis class implements a Metrics Calculator for Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) calculations.\\n\"\"\"\\n\\nimport numpy as np\\n\\n\\nclass MetricsCalculator2:\\n    def __init__(self):\\n        \"\"\"\\n        Initialization method for MetricsCalculator2.\\n        \"\"\"\\n        pass\\n\\n    @staticmethod\\n    def mrr(data):\\n        \"\"\"\\n        Calculate Mean Reciprocal Rank (MRR) from the provided data.\\n\\n        Parameters:\\n            data (list or tuple): Input data consisting of a list of tuples or a single tuple ([0,...,1,...], int)\\n\\n        Returns:\\n            tuple: A tuple containing the MRR value and a list of individual MRR values\\n        \\n        Test cases:\\n            # Test case 1: Single tuple case\\n            assert mrr(([0, 1, 0], 3)) == (1.0, [1.0])\\n            \\n            # Test case 2: Single tuple with zero total_num\\n            assert mrr(([0, 1, 0], 0)) == (0.0, [0.0])\\n            \\n            # Test case 3: Multiple tuple case\\n            assert mrr([([0, 1, 0], 3), ([1, 0, 1], 2)]) == (0.5, [0.3333333333333333, 0.6666666666666666])\\n        \"\"\"\\n\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\"The input must be a tuple([0,...,1,...], int) or an iteration of list of tuple\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        \\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list))) + 1)\\n                mr_np = sub_list * ranking_array\\n\\n                mr = next((team for team in mr_np if team > 0), 0.0)\\n                \\n                return mr, [mr]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    mr = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list))) + 1)\\n                    mr_np = sub_list * ranking_array\\n\\n                    mr = next((team for team in mr_np if team > 0), 0.0)\\n\\n                separate_result.append(mr)\\n            return np.mean(separate_result), separate_result\\n\\n    @staticmethod\\n    def map(data):\\n        \"\"\"\\n        Calculate Mean Average Precision (MAP) from the provided data.\\n\\n        Parameters:\\n            data (list or tuple): Input data consisting of a list of tuples or a single tuple ([0,...,1,...], int)\\n\\n        Returns:\\n            tuple: A tuple containing the MAP value and a list of individual MAP values\\n        \\n        Test cases:\\n            # Test case 1: Single tuple case\\n            assert map(([0, 1, 1], 3)) == (0.6944444444444444, [0.6944444444444444])\\n            \\n            # Test case 2: Single tuple with zero total_num\\n            assert map(([0, 1, 1], 0)) == (0.0, [0.0])\\n            \\n            # Test case 3: Multiple tuple case\\n            assert map([([0, 1, 1], 3), ([1, 0, 1], 2)]) == (0.4444444444444444, [0.25, 0.6388888888888888])\\n        \"\"\"\\n\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\"The input must be a tuple([0,...,1,...], int) or an iteration of list of tuple\")\\n        \\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        \\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list))) + 1)\\n\\n                right_ranking_list = []\\n                count = 1\\n                for t in sub_list:\\n                    if t == 0:\\n                        right_ranking_list.append(0)\\n                    else:\\n                        right_ranking_list.append(count)\\n                        count += 1\\n\\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n                return ap, [ap]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    ap = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list))) + 1)\\n\\n                    right_ranking_list = []\\n                    count = 1\\n                    for t in sub_list:\\n                        if t == 0:\\n                            right_ranking_list.append(0)\\n                        else:\\n                            right_ranking_list.append(count)\\n                            count += 1\\n\\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n\\n                separate_result.append(ap)\\n            return np.mean(separate_result), separate_result\\n\"\"\"', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735217213, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1213, prompt_tokens=1760, total_tokens=2973, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "MetricsCalculator2.txt"}