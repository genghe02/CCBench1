{"response body": "{\n  \"id\": \"8e063dd0-c2f7-4345-96b7-e6c3cb75f91a\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"```python\\n\\\"\\\"\\\"\\nThis class is designed to calculate various performance metrics such as precision, recall, F1 score, and accuracy for binary classification models. It maintains counters for true positives, false positives, false negatives, and true negatives, which are updated based on predicted and true labels.\\n\\\"\\\"\\\"\\n\\nclass MetricsCalculator:\\n    def __init__(self):\\n        \\\"\\\"\\\"\\n        Initializes the MetricsCalculator class by setting counters for true positives, false positives, false negatives, and true negatives to zero.\\n        \\\"\\\"\\\"\\n        self.true_positives = 0\\n        self.false_positives = 0\\n        self.false_negatives = 0\\n        self.true_negatives = 0\\n\\n    def update(self, predicted_labels, true_labels):\\n        \\\"\\\"\\\"\\n        Updates the counters for true positives, false positives, false negatives, and true negatives based on the provided predicted and true labels.\\n\\n        Parameters:\\n            predicted_labels (list): A list of predicted binary labels (0 or 1).\\n            true_labels (list): A list of true binary labels (0 or 1).\\n\\n        Returns:\\n            None\\n\\n        Test cases:\\n            # Test case 1: All predictions are correct\\n            calculator = MetricsCalculator()\\n            calculator.update([1, 0, 1, 0], [1, 0, 1, 0])\\n            assert calculator.true_positives == 2\\n            assert calculator.true_negatives == 2\\n            assert calculator.false_positives == 0\\n            assert calculator.false_negatives == 0\\n\\n            # Test case 2: Some predictions are incorrect\\n            calculator = MetricsCalculator()\\n            calculator.update([1, 0, 1, 0], [1, 1, 0, 0])\\n            assert calculator.true_positives == 1\\n            assert calculator.true_negatives == 1\\n            assert calculator.false_positives == 1\\n            assert calculator.false_negatives == 1\\n\\n            # Test case 3: All predictions are incorrect\\n            calculator = MetricsCalculator()\\n            calculator.update([1, 1, 1, 1], [0, 0, 0, 0])\\n            assert calculator.true_positives == 0\\n            assert calculator.true_negatives == 0\\n            assert calculator.false_positives == 4\\n            assert calculator.false_negatives == 0\\n        \\\"\\\"\\\"\\n        for predicted, true in zip(predicted_labels, true_labels):\\n            if predicted == 1 and true == 1:\\n                self.true_positives += 1\\n            elif predicted == 1 and true == 0:\\n                self.false_positives += 1\\n            elif predicted == 0 and true == 1:\\n                self.false_negatives += 1\\n            elif predicted == 0 and true == 0:\\n                self.true_negatives += 1\\n\\n    def precision(self, predicted_labels, true_labels):\\n        \\\"\\\"\\\"\\n        Calculates the precision metric, which is the ratio of true positives to the sum of true positives and false positives.\\n\\n        Parameters:\\n            predicted_labels (list): A list of predicted binary labels (0 or 1).\\n            true_labels (list): A list of true binary labels (0 or 1).\\n\\n        Returns:\\n            float: The precision value, ranging from 0.0 to 1.0.\\n\\n        Test cases:\\n            # Test case 1: Perfect precision\\n            calculator = MetricsCalculator()\\n            assert calculator.precision([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0\\n\\n            # Test case 2: No true positives\\n            calculator = MetricsCalculator()\\n            assert calculator.precision([1, 1, 1, 1], [0, 0, 0, 0]) == 0.0\\n\\n            # Test case 3: Mixed results\\n            calculator = MetricsCalculator()\\n            assert calculator.precision([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5\\n        \\\"\\\"\\\"\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_positives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_positives)\\n\\n    def recall(self, predicted_labels, true_labels):\\n        \\\"\\\"\\\"\\n        Calculates the recall metric, which is the ratio of true positives to the sum of true positives and false negatives.\\n\\n        Parameters:\\n            predicted_labels (list): A list of predicted binary labels (0 or 1).\\n            true_labels (list): A list of true binary labels (0 or 1).\\n\\n        Returns:\\n            float: The recall value, ranging from 0.0 to 1.0.\\n\\n        Test cases:\\n            # Test case 1: Perfect recall\\n            calculator = MetricsCalculator()\\n            assert calculator.recall([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0\\n\\n            # Test case 2: No true positives\\n            calculator = MetricsCalculator()\\n            assert calculator.recall([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\\n\\n            # Test case 3: Mixed results\\n            calculator = MetricsCalculator()\\n            assert calculator.recall([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5\\n        \\\"\\\"\\\"\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_negatives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_negatives)\\n\\n    def f1_score(self, predicted_labels, true_labels):\\n        \\\"\\\"\\\"\\n        Calculates the F1 score, which is the harmonic mean of precision and recall.\\n\\n        Parameters:\\n            predicted_labels (list): A list of predicted binary labels (0 or 1).\\n            true_labels (list): A list of true binary labels (0 or 1).\\n\\n        Returns:\\n            float: The F1 score, ranging from 0.0 to 1.0.\\n\\n        Test cases:\\n            # Test case 1: Perfect F1 score\\n            calculator = MetricsCalculator()\\n            assert calculator.f1_score([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0\\n\\n            # Test case 2: Zero F1 score\\n            calculator = MetricsCalculator()\\n            assert calculator.f1_score([1, 1, 1, 1], [0, 0, 0, 0]) == 0.0\\n\\n            # Test case 3: Mixed results\\n            calculator = MetricsCalculator()\\n            assert calculator.f1_score([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5\\n        \\\"\\\"\\\"\\n        self.update(predicted_labels, true_labels)\\n        precision = self.precision(predicted_labels, true_labels)\\n        recall = self.recall(predicted_labels, true_labels)\\n        if precision + recall == 0.0:\\n            return 0.0\\n        return (2 * precision * recall) / (precision + recall)\\n\\n    def accuracy(self, predicted_labels, true_labels):\\n        \\\"\\\"\\\"\\n        Calculates the accuracy metric, which is the ratio of correct predictions to the total number of predictions.\\n\\n        Parameters:\\n            predicted_labels (list): A list of predicted binary labels (0 or 1).\\n            true_labels (list): A list of true binary labels (0 or 1).\\n\\n        Returns:\\n            float: The accuracy value, ranging from 0.0 to 1.0.\\n\\n        Test cases:\\n            # Test case 1: Perfect accuracy\\n            calculator = MetricsCalculator()\\n            assert calculator.accuracy([1, 1, 0, 0], [1, 1, 0, 0]) == 1.0\\n\\n            # Test case 2: Zero accuracy\\n            calculator = MetricsCalculator()\\n            assert calculator.accuracy([1, 1, 1, 1], [0, 0, 0, 0]) == 0.0\\n\\n            # Test case 3: Mixed results\\n            calculator = MetricsCalculator()\\n            assert calculator.accuracy([1, 0, 1, 0], [1, 1, 0, 0]) == 0.5\\n        \\\"\\\"\\\"\\n        self.update(predicted_labels, true_labels)\\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\\n        if total == 0:\\n            return 0.0\\n        return (self.true_positives + self.true_negatives) / total\\n```\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1735194477,\n  \"model\": \"deepseek-chat\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"fp_1a5c54ff19\",\n  \"usage\": {\n    \"completion_tokens\": 1904,\n    \"prompt_tokens\": 1606,\n    \"total_tokens\": 3510,\n    \"prompt_cache_hit_tokens\": 960,\n    \"prompt_cache_miss_tokens\": 646\n  }\n}", "test_case_name": "MetricsCalculator.txt"}