{"response body": "ChatCompletion(id='chatcmpl-AiKA8nCQG7AWY1aGuEAstVZ4d0ZsJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\n\"\"\"\\nMetricsCalculator2 is a utility class designed to compute evaluation metrics \\nfor ranking systems, specifically Mean Reciprocal Rank (MRR) and Mean Average \\nPrecision (MAP). It provides static methods to calculate these metrics based \\non various input formats, including single tuples and lists of tuples. \\n\\nThis class relies on the numpy library for efficient numerical operations and \\nrequires proper input formatting to ensure accurate calculations.\\n\\nMain functionality:\\n- Calculate MRR from ranked lists to evaluate the effectiveness of search \\n  algorithms.\\n- Compute MAP to assess the precision of ranked results over multiple queries.\\n\\nUsage examples:\\n- MRR can be calculated by passing a ranked list of items and their positions.\\n- MAP can be computed similarly across multiple ranking scenarios.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass MetricsCalculator2:\\n    def __init__(self):\\n        \"\"\"\\n        Initializes a new instance of the MetricsCalculator2 class.\\n        This class does not maintain any internal state.\\n        \"\"\"\\n        pass\\n\\n    @staticmethod\\n    def mrr(data):\\n        \"\"\"\\n        Computes the Mean Reciprocal Rank (MRR) for the given data.\\n\\n        Parameters:\\n            data (tuple or list): \\n                If a tuple, expects a tuple of (sub_list, total_num) where \\n                sub_list is a list/array of relevance scores and total_num \\n                is an integer count of items.\\n                If a list, expects a list of such tuples.\\n\\n        Returns:\\n            tuple: A tuple containing:\\n                - float: The MRR value for the input data.\\n                - list: A list of separated MRR results for each sub_list.\\n\\n        Test cases:\\n            # Test case 1: Normal usage with a tuple\\n            input_data = ([1, 0, 0], 3)\\n            expected_output = (1.0, [1.0])  # Expected MRR = 1/1\\n\\n            # Test case 2: Edge case with empty list\\n            input_data = ([], 0)\\n            expected_output = (0.0, [0.0])  # No items to rank\\n\\n            # Test case 3: Error handling with incorrect input type\\n            try:\\n                MetricsCalculator2.mrr(\"invalid_input\")\\n            except Exception as e:\\n                expected_output = \"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\"\\n        \"\"\"\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                mr_np = sub_list * ranking_array\\n\\n                mr = 0.0\\n                for team in mr_np:\\n                    if team > 0:\\n                        mr = team\\n                        break\\n                return mr, [mr]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    mr = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                    mr_np = sub_list * ranking_array\\n\\n                    mr = 0.0\\n                    for team in mr_np:\\n                        if team > 0:\\n                            mr = team\\n                            break\\n\\n                separate_result.append(mr)\\n            return np.mean(separate_result), separate_result\\n\\n    @staticmethod\\n    def map(data):\\n        \"\"\"\\n        Computes the Mean Average Precision (MAP) for the given data.\\n\\n        Parameters:\\n            data (tuple or list): \\n                If a tuple, expects a tuple of (sub_list, total_num) similar to MRR.\\n                If a list, expects a list of such tuples.\\n\\n        Returns:\\n            tuple: A tuple containing:\\n                - float: The MAP value for the input data.\\n                - list: A list of separated MAP results for each sub_list.\\n\\n        Test cases:\\n            # Test case 1: Normal usage with a tuple\\n            input_data = ([1, 0, 1], 2)\\n            expected_output = (1.0, [1.0])  # Expected MAP = (1+1)/2\\n\\n            # Test case 2: Edge case with empty list\\n            input_data = ([], 0)\\n            expected_output = (0.0, [0.0])  # No items to compute MAP\\n\\n            # Test case 3: Error handling with incorrect input type\\n            try:\\n                MetricsCalculator2.map(\"invalid_input\")\\n            except Exception as e:\\n                expected_output = \"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\"\\n        \"\"\"\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                right_ranking_list = []\\n                count = 1\\n                for t in sub_list:\\n                    if t == 0:\\n                        right_ranking_list.append(0)\\n                    else:\\n                        right_ranking_list.append(count)\\n                        count += 1\\n\\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n                return ap, [ap]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    ap = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                    right_ranking_list = []\\n                    count = 1\\n                    for t in sub_list:\\n                        if t == 0:\\n                            right_ranking_list.append(0)\\n                        else:\\n                            right_ranking_list.append(count)\\n                            count += 1\\n\\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n\\n                separate_result.append(ap)\\n            return np.mean(separate_result), separate_result\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735127300, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=1459, prompt_tokens=2051, total_tokens=3510, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "MetricsCalculator2.txt"}