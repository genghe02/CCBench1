{"response body": "{\n  \"id\": \"8fdce23e-989d-4e78-8702-3df11cf87f2e\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"```python\\n\\\"\\\"\\\"\\nThis class provides methods to calculate Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) \\nfor evaluating the performance of ranking systems. These metrics are commonly used in information \\nretrieval and recommendation systems.\\n\\\"\\\"\\\"\\n\\nimport numpy as np\\n\\n\\nclass MetricsCalculator2:\\n    def __init__(self):\\n        \\\"\\\"\\\"\\n        Initializes the MetricsCalculator2 class.\\n        \\\"\\\"\\\"\\n        pass\\n\\n    @staticmethod\\n    def mrr(data):\\n        \\\"\\\"\\\"\\n        Calculates the Mean Reciprocal Rank (MRR) for a given set of ranked lists.\\n\\n        Parameters:\\n            data (list or tuple): A single tuple or a list of tuples. Each tuple contains:\\n                - sub_list (list): A list of binary values (0 or 1) indicating the presence of a relevant item.\\n                - total_num (int): The total number of relevant items.\\n\\n        Returns:\\n            tuple: A tuple containing:\\n                - float: The mean reciprocal rank across all provided lists.\\n                - list: A list of reciprocal ranks for each individual list.\\n\\n        Test cases:\\n            # Test case 1: Single tuple with relevant items\\n            input_data = ([1, 0, 1], 2)\\n            expected_output = (0.5, [0.5])\\n            assert MetricsCalculator2.mrr(input_data) == expected_output\\n\\n            # Test case 2: List of tuples with no relevant items\\n            input_data = [([0, 0, 0], 1), ([0, 0], 1)]\\n            expected_output = (0.0, [0.0, 0.0])\\n            assert MetricsCalculator2.mrr(input_data) == expected_output\\n\\n            # Test case 3: Invalid input type\\n            input_data = \\\"invalid\\\"\\n            try:\\n                MetricsCalculator2.mrr(input_data)\\n            except Exception as e:\\n                assert str(e) == \\\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\\\"\\n        \\\"\\\"\\\"\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\\\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\\\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                mr_np = sub_list * ranking_array\\n\\n                mr = 0.0\\n                for team in mr_np:\\n                    if team > 0:\\n                        mr = team\\n                        break\\n                return mr, [mr]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    mr = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n                    mr_np = sub_list * ranking_array\\n\\n                    mr = 0.0\\n                    for team in mr_np:\\n                        if team > 0:\\n                            mr = team\\n                            break\\n\\n                separate_result.append(mr)\\n            return np.mean(separate_result), separate_result\\n\\n    @staticmethod\\n    def map(data):\\n        \\\"\\\"\\\"\\n        Calculates the Mean Average Precision (MAP) for a given set of ranked lists.\\n\\n        Parameters:\\n            data (list or tuple): A single tuple or a list of tuples. Each tuple contains:\\n                - sub_list (list): A list of binary values (0 or 1) indicating the presence of a relevant item.\\n                - total_num (int): The total number of relevant items.\\n\\n        Returns:\\n            tuple: A tuple containing:\\n                - float: The mean average precision across all provided lists.\\n                - list: A list of average precisions for each individual list.\\n\\n        Test cases:\\n            # Test case 1: Single tuple with relevant items\\n            input_data = ([1, 0, 1], 2)\\n            expected_output = (0.5, [0.5])\\n            assert MetricsCalculator2.map(input_data) == expected_output\\n\\n            # Test case 2: List of tuples with no relevant items\\n            input_data = [([0, 0, 0], 1), ([0, 0], 1)]\\n            expected_output = (0.0, [0.0, 0.0])\\n            assert MetricsCalculator2.map(input_data) == expected_output\\n\\n            # Test case 3: Invalid input type\\n            input_data = \\\"invalid\\\"\\n            try:\\n                MetricsCalculator2.map(input_data)\\n            except Exception as e:\\n                assert str(e) == \\\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\\\"\\n        \\\"\\\"\\\"\\n        if type(data) != list and type(data) != tuple:\\n            raise Exception(\\\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\\\")\\n\\n        if len(data) == 0:\\n            return 0.0, [0.0]\\n        if type(data) == tuple:\\n            (sub_list, total_num) = data\\n            sub_list = np.array(sub_list)\\n            if total_num == 0:\\n                return 0.0, [0.0]\\n            else:\\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                right_ranking_list = []\\n                count = 1\\n                for t in sub_list:\\n                    if t == 0:\\n                        right_ranking_list.append(0)\\n                    else:\\n                        right_ranking_list.append(count)\\n                        count += 1\\n\\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n                return ap, [ap]\\n\\n        if type(data) == list:\\n            separate_result = []\\n            for (sub_list, total_num) in data:\\n                sub_list = np.array(sub_list)\\n\\n                if total_num == 0:\\n                    ap = 0.0\\n                else:\\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\\n\\n                    right_ranking_list = []\\n                    count = 1\\n                    for t in sub_list:\\n                        if t == 0:\\n                            right_ranking_list.append(0)\\n                        else:\\n                            right_ranking_list.append(count)\\n                            count += 1\\n\\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\\n\\n                separate_result.append(ap)\\n            return np.mean(separate_result), separate_result\\n```\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1735194507,\n  \"model\": \"deepseek-chat\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"fp_1a5c54ff19\",\n  \"usage\": {\n    \"completion_tokens\": 1445,\n    \"prompt_tokens\": 1773,\n    \"total_tokens\": 3218,\n    \"prompt_cache_hit_tokens\": 960,\n    \"prompt_cache_miss_tokens\": 813\n  }\n}", "test_case_name": "MetricsCalculator2.txt"}