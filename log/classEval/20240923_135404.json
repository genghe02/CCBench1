{"response body": {"id": "34c47015-72be-40cf-bb62-2584eff746f9", "object": "chat.completion", "model": "gpt-35-turbo", "created": "2024-09-23 13:54:04", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "content": "```python\nimport numpy as np\n\nclass MetricsCalculator2:\n    # Class to calculate metrics such as MRR (Mean Reciprocal Rank) and MAP (Mean Average Precision) for ranking evaluations.\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def mrr(data):\n        # Calculate Mean Reciprocal Rank (MRR) metric for ranking evaluations.\n        # param data: list or tuple, input data containing ranking scores and total number of items\n        # return: float, MRR value for the input data\n        # some test cases for the function\n        if type(data) != list and type(data) != tuple:\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or an iteration of a list of tuples\")\n\n        if len(data) == 0:\n            return 0.0, [0.0]\n        if type(data) == tuple:\n            (sub_list, total_num) = data\n            sub_list = np.array(sub_list)\n            if total_num == 0:\n                return 0.0, [0.0]\n            else:\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n                mr_np = sub_list * ranking_array\n\n                mr = 0.0\n                for team in mr_np:\n                    if team > 0:\n                        mr = team\n                        break\n                return mr, [mr]\n\n        if type(data) == list:\n            separate_result = []\n            for (sub_list, total_num) in data:\n                sub_list = np.array(sub_list)\n\n                if total_num == 0:\n                    mr = 0.0\n                else:\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n                    mr_np = sub_list * ranking_array\n\n                    mr = 0.0\n                    for team in mr_np:\n                        if team > 0:\n                            mr = team\n                            break\n\n                separate_result.append(mr)\n            return np.mean(separate_result), separate_result\n\n    @staticmethod\n    def map(data):\n        # Calculate Mean Average Precision (MAP) metric for ranking evaluations.\n        # param data: list or tuple, input data containing ranking scores and total number of items\n        # return: float, MAP value for the input data\n        # some test cases for the function\n        if type(data) != list and type(data) != tuple:\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or an iteration of a list of tuples\")\n\n        if len(data) == 0:\n            return 0.0, [0.0]\n        if type(data) == tuple:\n            (sub_list, total_num) = data\n            sub_list = np.array(sub_list)\n            if total_num == 0:\n                return 0.0, [0.0]\n            else:\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n\n                right_ranking_list = []\n                count = 1\n                for t in sub_list:\n                    if t == 0:\n                        right_ranking_list.append(0)\n                    else:\n                        right_ranking_list.append(count)\n                        count += 1\n\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\n                return ap, [ap]\n\n        if type(data) == list:\n            separate_result = []\n            for (sub_list, total_num) in data:\n                sub_list = np.array(sub_list)\n\n                if total_num == 0:\n                    ap = 0.0\n                else:\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n\n                    right_ranking_list = []\n                    count = 1\n                    for t in sub_list:\n                        if t == 0:\n                            right_ranking_list.append(0)\n                        else:\n                            right_ranking_list.append(count)\n                            count += 1\n\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\n\n                separate_result.append(ap)\n            return np.mean(separate_result), separate_result\n```"}}], "usage": {"completion_tokens": 884, "prompt_tokens": 1355, "total_tokens": 2239}, "feeType": "1", "consume": 0.014550811436}, "test_case_name": "MetricsCalculator2.txt"}