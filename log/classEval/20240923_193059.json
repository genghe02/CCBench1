{"response body": {"id": "d1311739-6d74-4274-9960-bf40e5a95295", "object": "chat.completion", "model": "gpt-4-32k", "created": "2024-09-23 19:30:59", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "content": "```python\nimport numpy as np\n\n# This class is used to calculate Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) given a dataset\nclass MetricsCalculator2:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    # This function calculates Mean Reciprocal Rank (MRR) for the given data\n    # The data can either be a tuple or a list of tuples\n    # A tuple consists of a list and an integer. The list has binary values indicating whether an item is relevant or not\n    # The integer is the total number of items\n    # The function returns the mean reciprocal rank and a list of reciprocal ranks for each tuple in the data\n    # If the input data is a tuple, it returns the reciprocal rank for that tuple and a list containing that value\n    def mrr(data):\n        # Checking the input type is either list or tuple\n        if type(data) != list and type(data) != tuple:\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\")\n\n        if len(data) == 0:\n            return 0.0, [0.0]\n        if type(data) == tuple:\n            (sub_list, total_num) = data\n            sub_list = np.array(sub_list)\n            if total_num == 0:\n                return 0.0, [0.0]\n            else:\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n                mr_np = sub_list * ranking_array\n\n                mr = 0.0\n                for team in mr_np:\n                    if team > 0:\n                        mr = team\n                        break\n                return mr, [mr]\n\n        if type(data) == list:\n            separate_result = []\n            for (sub_list, total_num) in data:\n                sub_list = np.array(sub_list)\n\n                if total_num == 0:\n                    mr = 0.0\n                else:\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n                    mr_np = sub_list * ranking_array\n\n                    mr = 0.0\n                    for team in mr_np:\n                        if team > 0:\n                            mr = team\n                            break\n\n                separate_result.append(mr)\n            return np.mean(separate_result), separate_result\n    # Test cases for the mrr function\n    # >>> mrr = MetricsCalculator2.mrr\n    # >>> mrr(([0, 0, 1, 0, 1], 5))\n    # >>> mrr(([0, 0, 0, 0, 0], 5))\n    # >>> mrr(([], 0))\n    # >>> mrr([([0, 0, 1, 0, 1], 5), ([0, 0, 0, 0, 0], 5)])\n\n    @staticmethod\n    # This function calculates Mean Average Precision (MAP) for the given data\n    # The data can either be a tuple or a list of tuples\n    # A tuple consists of a list and an integer. The list has binary values indicating whether an item is relevant or not\n    # The integer is the total number of items\n    # The function returns the mean average precision and a list of average precisions for each tuple in the data\n    # If the input data is a tuple, it returns the average precision for that tuple and a list containing that value\n    def map(data):\n        # Checking the input type is either list or tuple\n        if type(data) != list and type(data) != tuple:\n            raise Exception(\"the input must be a tuple([0,...,1,...],int) or a iteration of list of tuple\")\n\n        if len(data) == 0:\n            return 0.0, [0.0]\n        if type(data) == tuple:\n            (sub_list, total_num) = data\n            sub_list = np.array(sub_list)\n            if total_num == 0:\n                return 0.0, [0.0]\n            else:\n                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n\n                right_ranking_list = []\n                count = 1\n                for t in sub_list:\n                    if t == 0:\n                        right_ranking_list.append(0)\n                    else:\n                        right_ranking_list.append(count)\n                        count += 1\n\n                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\n                return ap, [ap]\n\n        if type(data) == list:\n            separate_result = []\n            for (sub_list, total_num) in data:\n                sub_list = np.array(sub_list)\n\n                if total_num == 0:\n                    ap = 0.0\n                else:\n                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)\n\n                    right_ranking_list = []\n                    count = 1\n                    for t in sub_list:\n                        if t == 0:\n                            right_ranking_list.append(0)\n                        else:\n                            right_ranking_list.append(count)\n                            count += 1\n\n                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num\n\n                separate_result.append(ap)\n            return np.mean(separate_result), separate_result\n    # Test cases for the map function\n    # >>> map = MetricsCalculator2.map\n    # >>> map(([0, 0, 1, 0, 1], 5))\n    # >>> map(([0, 0, 0, 0, 0], 5))\n    # >>> map(([], 0))\n    # >>> map([([0, 0, 1, 0, 1], 5), ([0, 0, 0, 0, 0], 5)])\n```\n"}}], "usage": {"completion_tokens": 1257, "prompt_tokens": 1355, "total_tokens": 2612}, "feeType": "1", "consume": 1.686884}, "test_case_name": "MetricsCalculator2.txt"}