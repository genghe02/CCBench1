{"response body": "{\n  \"id\": \"9112cb73-2c3f-4db5-b06c-af4208ba7051\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"```python\\n# This class is designed to calculate various metrics for evaluating the performance of a classification model.\\nclass MetricsCalculator:\\n    def __init__(self):\\n        # Initialize the counters for true positives, false positives, false negatives, and true negatives.\\n        self.true_positives = 0\\n        self.false_positives = 0\\n        self.false_negatives = 0\\n        self.true_negatives = 0\\n\\n    def update(self, predicted_labels, true_labels):\\n        # Update the counters based on the predicted and true labels.\\n        # param predicted_labels: list, the predicted labels from the model\\n        # param true_labels: list, the ground truth labels\\n        # return: None\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.update([1, 0, 1], [1, 1, 0])\\n        for predicted, true in zip(predicted_labels, true_labels):\\n            if predicted == 1 and true == 1:\\n                self.true_positives += 1\\n            elif predicted == 1 and true == 0:\\n                self.false_positives += 1\\n            elif predicted == 0 and true == 1:\\n                self.false_negatives += 1\\n            elif predicted == 0 and true == 0:\\n                self.true_negatives += 1\\n\\n    def precision(self, predicted_labels, true_labels):\\n        # Calculate the precision metric, which is the ratio of true positives to the sum of true positives and false positives.\\n        # param predicted_labels: list, the predicted labels from the model\\n        # param true_labels: list, the ground truth labels\\n        # return: float, the precision value\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.precision([1, 0, 1], [1, 1, 0])\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_positives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_positives)\\n\\n    def recall(self, predicted_labels, true_labels):\\n        # Calculate the recall metric, which is the ratio of true positives to the sum of true positives and false negatives.\\n        # param predicted_labels: list, the predicted labels from the model\\n        # param true_labels: list, the ground truth labels\\n        # return: float, the recall value\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.recall([1, 0, 1], [1, 1, 0])\\n        self.update(predicted_labels, true_labels)\\n        if self.true_positives + self.false_negatives == 0:\\n            return 0.0\\n        return self.true_positives / (self.true_positives + self.false_negatives)\\n\\n    def f1_score(self, predicted_labels, true_labels):\\n        # Calculate the F1 score, which is the harmonic mean of precision and recall.\\n        # param predicted_labels: list, the predicted labels from the model\\n        # param true_labels: list, the ground truth labels\\n        # return: float, the F1 score value\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.f1_score([1, 0, 1], [1, 1, 0])\\n        self.update(predicted_labels, true_labels)\\n        precision = self.precision(predicted_labels, true_labels)\\n        recall = self.recall(predicted_labels, true_labels)\\n        if precision + recall == 0.0:\\n            return 0.0\\n        return (2 * precision * recall) / (precision + recall)\\n\\n    def accuracy(self, predicted_labels, true_labels):\\n        # Calculate the accuracy metric, which is the ratio of correct predictions to the total number of predictions.\\n        # param predicted_labels: list, the predicted labels from the model\\n        # param true_labels: list, the ground truth labels\\n        # return: float, the accuracy value\\n        # >>> metrics = MetricsCalculator()\\n        # >>> metrics.accuracy([1, 0, 1], [1, 1, 0])\\n        self.update(predicted_labels, true_labels)\\n        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives\\n        if total == 0:\\n            return 0.0\\n        return (self.true_positives + self.true_negatives) / total\\n```\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1727081559,\n  \"model\": \"deepseek-chat\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"fp_1c141eb703\",\n  \"usage\": {\n    \"completion_tokens\": 1072,\n    \"prompt_tokens\": 1335,\n    \"total_tokens\": 2407,\n    \"prompt_cache_hit_tokens\": 576,\n    \"prompt_cache_miss_tokens\": 759\n  }\n}", "test_case_name": "MetricsCalculator.txt"}