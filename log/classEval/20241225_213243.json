{"response body": "ChatCompletion(id='chatcmpl-AiLmuZF2dnn96uFJFhSfFkkxvrkwE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```python\\n\"\"\"\\nThis class provides methods for computing Kappa statistics, which are used to measure inter-rater agreement.\\n\"\"\"\\nimport numpy as np\\n\\n\\nclass KappaCalculator:\\n\\n    @staticmethod\\n    def kappa(testData, k):\\n        \"\"\"\\n        Calculates Cohen\\'s Kappa coefficient for a given confusion matrix.\\n\\n        Parameters:\\n            testData (list of list of int): A confusion matrix where testData[i][j] represents the number of observations \\n                                              that are classified by both raters as belonging to category j.\\n            k (int): The number of categories in the confusion matrix.\\n\\n        Returns:\\n            float: Cohen\\'s Kappa coefficient indicating inter-rater agreement.\\n            \\n        Test cases:\\n            # Test case 1: Basic example with perfect agreement\\n            input_data = [[10, 2], [1, 7]]\\n            expected_output = 0.7593  # This may vary based on actual calculations\\n            \\n            # Test case 2: Example with no agreement\\n            input_data = [[0, 12], [0, 8]]\\n            expected_output = -0.4000  # This may vary based on actual calculations\\n            \\n            # Test case 3: Example with random classification\\n            input_data = [[5, 5], [5, 5]]\\n            expected_output = 0.0000  # This may vary based on actual calculations\\n        \"\"\"\\n        dataMat = np.mat(testData)\\n        P0 = 0.0\\n        for i in range(k):\\n            P0 += dataMat[i, i] * 1.0\\n        xsum = np.sum(dataMat, axis=1)\\n        ysum = np.sum(dataMat, axis=0)\\n        total_sum = np.sum(dataMat)\\n        Pe = float(ysum * xsum) / total_sum / total_sum\\n        P0 = float(P0 / total_sum * 1.0)\\n        cohens_coefficient = float((P0 - Pe) / (1 - Pe))\\n        return cohens_coefficient\\n\\n    @staticmethod\\n    def fleiss_kappa(testData, N, k, n):\\n        \"\"\"\\n        Calculates Fleiss\\' Kappa, a statistic for assessing the reliability of agreement between a fixed number of raters \\n        assigning categorical ratings to a number of items.\\n\\n        Parameters:\\n            testData (list of list of int): A matrix where testData[i][j] is the number of raters that assigned the \\n                                              j-th category to the i-th item.\\n            N (int): The number of items being rated.\\n            k (int): The number of categories being used.\\n            n (int): The number of raters.\\n\\n        Returns:\\n            float: Fleiss\\' Kappa indicating the level of agreement among raters.\\n            \\n        Test cases:\\n            # Test case 1: Basic example with perfect agreement\\n            input_data = [[10, 0, 0], [0, 10, 0], [0, 0, 10]]\\n            expected_output = 1.0  # Perfect agreement\\n            \\n            # Test case 2: Example with low agreement\\n            input_data = [[2, 2, 6], [3, 3, 4], [5, 1, 4]]\\n            expected_output = 0.148  # Low agreement\\n            \\n            # Test case 3: Example with complete disagreement\\n            input_data = [[10, 0], [0, 10]]\\n            expected_output = -1.0  # Complete disagreement\\n        \"\"\"\\n        dataMat = np.mat(testData, float)\\n        oneMat = np.ones((k, 1))\\n        total_sum = 0.0\\n        P0 = 0.0\\n        for i in range(N):\\n            temp = 0.0\\n            for j in range(k):\\n                total_sum += dataMat[i, j]\\n                temp += 1.0 * dataMat[i, j] ** 2\\n            temp -= n\\n            temp /= (n - 1) * n\\n            P0 += temp\\n        P0 = 1.0 * P0 / N\\n        ysum = np.sum(dataMat, axis=0)\\n        for i in range(k):\\n            ysum[0, i] = (ysum[0, i] / total_sum) ** 2\\n        Pe = ysum * oneMat * 1.0\\n        ans = (P0 - Pe) / (1 - Pe)\\n        return ans[0, 0]\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735133548, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_d02d531b47', usage=CompletionUsage(completion_tokens=948, prompt_tokens=1652, total_tokens=2600, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))", "test_case_name": "KappaCalculator.txt"}