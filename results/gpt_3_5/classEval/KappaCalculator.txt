import numpy as np


class KappaCalculator:

    @staticmethod
    def kappa(testData, k):
        # Calculate Cohen's coefficient for inter-rater agreement using given testData and k.
        # param testData: numpy array, the data matrix with ratings
        # param k: int, the number of raters
        # return: float, Cohen's coefficient representing agreement between raters
        # Test cases can include scenarios with varying k and diverse testData samples.
        dataMat = np.mat(testData)
        P0 = 0.0
        for i in range(k):
            P0 += dataMat[i, i] * 1.0
        xsum = np.sum(dataMat, axis=1)
        ysum = np.sum(dataMat, axis=0)
        sum = np.sum(dataMat)
        Pe = float(ysum * xsum) / sum / sum
        P0 = float(P0 / sum * 1.0)
        cohens_coefficient = float((P0 - Pe) / (1 - Pe))
        return cohens_coefficient

    @staticmethod
    def fleiss_kappa(testData, N, k, n):
        # Calculate Fleiss' Kappa for agreement among raters using testData, N, k, and n.
        # param testData: numpy array, the data matrix with ratings
        # param N: int, the number of subjects
        # param k: int, the number of categories
        # param n: int, the number of raters
        # return: float, Fleiss' Kappa coefficient indicating agreement among raters
        # Test cases can involve different values for N, k, and n along with various testData configurations.
        dataMat = np.mat(testData, float)
        oneMat = np.ones((k, 1))
        sum = 0.0
        P0 = 0.0
        for i in range(N):
            temp = 0.0
            for j in range(k):
                sum += dataMat[i, j]
                temp += 1.0 * dataMat[i, j] ** 2
            temp -= n
            temp /= (n - 1) * n
            P0 += temp
        P0 = 1.0 * P0 / N
        ysum = np.sum(dataMat, axis=0)
        for i in range(k):
            ysum[0, i] = (ysum[0, i] / sum) ** 2
        Pe = ysum * oneMat * 1.0
        ans = (P0 - Pe) / (1 - Pe)
        return ans[0, 0]