 <description for whole class>
# MetricsCalculator2 is a static class that calculates two common ranking metrics: Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) for given data.

class MetricsCalculator2:
    def __init__(self):
        pass

    @staticmethod
    def mrr(data):
        <description for whole function>
        # Calculate Mean Reciprocal Rank (MRR) for the given data.

        if type(data)!= list and type(data)!= tuple:
            raise Exception("The input must be a tuple of the format (list, int) or an iteration of list of such tuples.")

        if len(data) == 0:
            return 0.0, [0.0]

        if type(data) == tuple:
            (sub_list, total_num) = data
            <description for all parameters>
            # sub_list: list, the list of relevance scores for each query
            # total_num: int, the total number of relevant documents for the given query

            sub_list = np.array(sub_list)
            if total_num == 0:
                return 0.0, [0.0]
            else:
                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)
                mr_np = sub_list * ranking_array

                mr = 0.0
                for team in mr_np:
                    if team > 0:
                        mr = team
                        break
                <description for return statement>
                # Return the MRR value and a list containing the MRR value
                return mr, [mr]

        if type(data) == list:
            separate_result = []
            for (sub_list, total_num) in data:
                <description for all parameters>
                # sub_list: list, the list of relevance scores for each query in the list
                # total_num: int, the total number of relevant documents for the given query

                sub_list = np.array(sub_list)

                if total_num == 0:
                    mr = 0.0
                else:
                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)
                    mr_np = sub_list * ranking_array

                    mr = 0.0
                    for team in mr_np:
                        if team > 0:
                            mr = team
                            break

                separate_result.append(mr)
            <description for return statement>
            # Return the average MRR value and a list containing the MRR values for each query
            return np.mean(separate_result), separate_result

    @staticmethod
    def map(data):
        <description for whole function>
        # Calculate Mean Average Precision (MAP) for the given data.

        if type(data)!= list and type(data)!= tuple:
            raise Exception("The input must be a tuple of the format (list, int) or an iteration of list of such tuples.")

        if len(data) == 0:
            return 0.0, [0.0]

        if type(data) == tuple:
            (sub_list, total_num) = data
            <description for all parameters>
            # sub_list: list, the list of relevance scores and ranks for each document in the given query
            # total_num: int, the total number of documents for the given query

            sub_list = np.array(sub_list)
            if total_num == 0:
                return 0.0, [0.0]
            else:
                ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)

                right_ranking_list = []
                count = 1
                for t in sub_list:
                    if t[0] == 0:
                        right_ranking_list.append(0)
                    else:
                        right_ranking_list.append(count)
                        count += 1

                ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num
                <description for return statement>
                # Return the average precision value and a list containing the precision value for each relevant document
                return ap, [ap]

        if type(data) == list:
            separate_result = []
            for (sub_list, total_num) in data:
                <description for all parameters>
                # sub_list: list, the list of relevance scores and ranks for each document in the given query
                # total_num: int, the total number of documents for the given query

                sub_list = np.array(sub_list)

                if total_num == 0:
                    ap = 0.0
                else:
                    ranking_array = 1.0 / (np.array(list(range(len(sub_list)))) + 1)

                    right_ranking_list = []
                    count = 1
                    for t in sub_list:
                        if t[0] == 0:
                            right_ranking_list.append(0)
                        else:
                            right_ranking_list.append(count)
                            count += 1

                    ap = np.sum(np.array(right_ranking_list) * ranking_array) / total_num

                separate_result.append(ap)
            <description for return statement>
            # Return the average precision value and a list containing the precision values for each query
            return np.mean(separate_result), separate_result