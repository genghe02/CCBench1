```python
class MetricsCalculator:
    # This class computes various metrics for evaluating binary classification models.

    def __init__(self):
        # Initializes the MetricsCalculator with counts for true positives, false positives, false negatives, and true negatives.
        self.true_positives = 0
        self.false_positives = 0
        self.false_negatives = 0
        self.true_negatives = 0

    def update(self, predicted_labels, true_labels):
        # Updates the counts of true positives, false positives, false negatives, and true negatives based on the provided predicted and true labels.
        # param predicted_labels: list of int, predicted binary labels from the model
        # param true_labels: list of int, actual binary labels for the dataset
        # return: None, this function modifies the internal state of the object
        # >>> metrics = MetricsCalculator()
        # >>> metrics.update([1, 0, 1], [1, 1, 0]) 
        for predicted, true in zip(predicted_labels, true_labels):
            if predicted == 1 and true == 1:
                self.true_positives += 1
            elif predicted == 1 and true == 0:
                self.false_positives += 1
            elif predicted == 0 and true == 1:
                self.false_negatives += 1
            elif predicted == 0 and true == 0:
                self.true_negatives += 1

    def precision(self, predicted_labels, true_labels):
        # Calculates the precision of the predictions.
        # param predicted_labels: list of int, predicted binary labels from the model
        # param true_labels: list of int, actual binary labels for the dataset
        # return: float, the precision score (True Positives / (True Positives + False Positives))
        # >>> metrics = MetricsCalculator()
        # >>> metrics.update([1, 0, 1], [1, 1, 0])
        # >>> metrics.precision([1, 0, 1], [1, 1, 0]) 
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_positives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_positives)

    def recall(self, predicted_labels, true_labels):
        # Calculates the recall of the predictions.
        # param predicted_labels: list of int, predicted binary labels from the model
        # param true_labels: list of int, actual binary labels for the dataset
        # return: float, the recall score (True Positives / (True Positives + False Negatives))
        # >>> metrics = MetricsCalculator()
        # >>> metrics.update([1, 0, 1], [1, 1, 0])
        # >>> metrics.recall([1, 0, 1], [1, 1, 0]) 
        self.update(predicted_labels, true_labels)
        if self.true_positives + self.false_negatives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_negatives)

    def f1_score(self, predicted_labels, true_labels):
        # Calculates the F1 score of the predictions.
        # param predicted_labels: list of int, predicted binary labels from the model
        # param true_labels: list of int, actual binary labels for the dataset
        # return: float, the F1 score, which is the harmonic mean of precision and recall
        # >>> metrics = MetricsCalculator()
        # >>> metrics.update([1, 0, 1], [1, 1, 0])
        # >>> metrics.f1_score([1, 0, 1], [1, 1, 0]) 
        self.update(predicted_labels, true_labels)
        precision = self.precision(predicted_labels, true_labels)
        recall = self.recall(predicted_labels, true_labels)
        if precision + recall == 0.0:
            return 0.0
        return (2 * precision * recall) / (precision + recall)

    def accuracy(self, predicted_labels, true_labels):
        # Calculates the accuracy of the predictions.
        # param predicted_labels: list of int, predicted binary labels from the model
        # param true_labels: list of int, actual binary labels for the dataset
        # return: float, the accuracy score ((True Positives + True Negatives) / Total Samples)
        # >>> metrics = MetricsCalculator()
        # >>> metrics.update([1, 0, 1], [1, 1, 0])
        # >>> metrics.accuracy([1, 0, 1], [1, 1, 0]) 
        self.update(predicted_labels, true_labels)
        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        if total == 0:
            return 0.0
        return (self.true_positives + self.true_negatives) / total
```